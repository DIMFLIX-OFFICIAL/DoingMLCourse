Company Name,Description,Link,Article Title,Publication Date,Full Content
СберМаркет,,,Купер,,Кодим будущее доставки товаров
СберМаркет,,,Как в Купере масштабировали машинное обучение и что из этого получилось,2024-10-04T12:59:50.000Z,"Не секрет, что ML‑модели требуют огромного количества данных. Информации не просто много, она организовывается в многообразные структуры, версионируется, употребляется разными моделями. Скорость обращения данных тоже критична, особенно для систем, взаимодействующих с пользователями в режиме реального времени.При возросшей сложности не обойтись без специализированных инструментов, например Feature Store. Однако случается, что все решения на рынке не годятся по тем или иным причинам. Тогда приходится рассчитывать исключительно на свои силы.Рассказываем, как в Купере внедрили Feast, хранилище признаков (Feature Store) с открытым исходным кодом. После прочтения вы познакомитесь с инструментом и сможете решить, подходит ли Feast для коммерческого использования. Подробности под катом!Статья написана по мотивамвыступления Юрия Классена, тимлида MLOps‑команды в Купере — одного из крупнейших онлайн-сервисов доставки из магазинов и ресторанов.Используйте навигацию, если не хотите читать текст целиком:→Предыстория→Поиск подходящего решения→Схема движения данных→Угроза прекращения поддержки Feast→ВыводыПредысторияДля любой современной компании c большой базой клиентов предсказание спроса — уже не конкурентное преимущество, а необходимость. Рекомендательные системы в розничной торговле стали обычным явлением. В Купере машинное обучение используют повсеместно.«Использование ML‑практик во всей логистической цепочке, вплоть до сборщиков и курьеров, позволяет оптимизировать затраты и сделать наше предложение на рынке еще более привлекательным по цене»—Юрий Классен,тимлид MLOps-команды в КупереОсновные верхнеуровневые сущности, на которых строится обучение ML-моделей — это признаки (features). Чем их больше, тем существеннее требования к информационной системе. В какой-то момент становится очевидно, что без Feature Store дальнейшее развитие сложного ML-проекта оказывается под сомнением.Feature Store — объединение лучших ML‑практик по работе с признаками (features), интерфейс между данными и использующими их моделями.Без специального хранилища признаков на MLOps-инженеров наваливаются дополнительные рутинные задачи, а интерфейс, который видят перед собой пользователи, начинает серьезно подтормаживать. Команда Купера ощутила это на своем опыте.«Перед нашими подразделениями MLOps стоит много задач. Самое важное — работа над рекомендательной и поисковой системами. Проекты высоконагруженные, производят сотни и даже тысячи запросов в секунду и каждый день пополняют базу данных на десятки гигабайт»—Юрий Классен,тимлид MLOps-команды в КупереРекомендательная система обрабатывает около 300 запросов в секунду, а поисковая — в 10 раз больше.Пока объем обучающих данных невелик, достаточно простых инструментов, а признаки можно хранить в памяти. По мере разрастания данных команда ожидаемо упирается в предел, а простые инструменты, как известно, не масштабируются.Задача Купера состояла в том, чтобы добиться скорости обслуживания запросов, не превышающей 50 мс. Смягчить требования нельзя — пользователи увидят задержку. Архитектура Feature Store позволяет упорядочить и ускорить работу с признаками: данные отделены от ML-процессов, а роли участников четко определены и регламентированы.Самый разумный шаг в подобном случае — поиск готового решения. Тогда вся работа сводится к сравнению предложений и более обстоятельному ознакомлению с «победителем» соревнования. Если бы все было так просто!Поиск подходящего решенияРазумеется, MLOps‑команда Купера не первые, кто столкнулся с необходимостью выбора.«Готовых решений много, они прекрасны. Мы сравнили все, что присутствует на рынке — по разным критериям, обстоятельно. Пришли к неутешительному заключению: ни одно из исследованных нами решений не подходит — они или работают только за рубежом, или чреваты внезапным отказом.Наша команда не верит в безвыходные ситуации и после долгих рассуждений решились: будем пробовать Feast»—Юрий Классен,тимлид MLOps-команды в КупереВыбор Feast вполне оправдан. Основные причины в том, что выбранное хранилище признаков:имеет открытый исходный код;не является полноценной Data-платформой, а значит, не придется переделывать весь пайплайн;относительно легко интегрируется в устоявшиеся процессы;при необходимости позволит дорабатывать его самостоятельно.Чтобы убедиться в правильности своего выбора и соответствии Feast требованиям, инженеры MLOps Купера начали с испытаний гипотезы в синтетических тестах. На локальной машине эмулировали нагрузку. Для хранения данных взяли Redis — резидентную СУБД, ориентированную на максимальную производительность при работе с «плоскими» данными. Хранилища признаков использовали только там, где без них было не обойтись. Redis, потребитель данных и серверы Feature Store находились в отдельных контейнерах с 2 CPU и 4 ГБ оперативной памяти. Все контейнеры запускались с помощью Docker Compose.Вариантов работы с данными было немного. В самом простом случае потребитель обращается к Redis напрямую с помощью Feast SDK. В более сложном — работает с признаками через Feature Store Server, в качестве которого рассматривался Python Feature Server и Java Feature Sever.Тестирование вариантов показало следующую взаимосвязь между количеством строк в запросе (ось X) и задержкой (ось Y). Линии на графиках отражают количество признаков в запросе: один (голубая), десять (зеленая) или сто (серая).Графики производительности Feast на разных платформах обмена данными.Отзывчивость Feast SDK не понравилась: 300 мс очень далеки от требуемых значений. Еще хуже, как ни странно, показал себя Python-сервер — он работает медленнее Feast SDK.Инженеры предположили, что в коде нет серьезных ошибок — причина в скорости самого Python. Долго разбираться не стали и перешли на Java‑сервер, который также поставляется с Feast. С ним удалось получить искомые 50 мс для 100 признаков при обработке данных 1 000 пользователей. С такой производительностью хранилище можно внедрять.Если вам интересна тема статьи, присоединяйтесь к нашему сообществу «MLечный путь» в Telegram. Там мы вместе обсуждаем проблемы и лучшие практики организации production ML-сервисов, а также делимся собственным опытом. А еще там раз в неделю выходят дайджесты по DataOps и MLOps.Схема движения данныхСхема для работы с Feast была построена следующим образом.Описания признаков хранятся в GitLab как код — просто, понятно и соответствует рекомендациям того же Feast.CI/CD передает описание признаков в Registry, хранящемся на S3. На практике можно также использовать PostgreSQL.AirflowDAG по расписанию материализует признаки из S3 в Redis с помощью Bytewax и Spark — одних из немногих масштабируемых решений, которые предлагаются вне западных облаков.Конечные ML‑сервисы забирают признаки из Redis через Java Feature Store Server. Сервисы, для которых производительность не критична, работают напрямую с менее шустрым Feast SDK.Схема движения данных для работы с Feast.Получившуюся схему согласовали со специалистами по работе с большими данными и перешли к внедрению. Но, как это часто бывает, все пошло не так.Угроза прекращения поддержки FeastЕдва начали трудиться над Feast, как его основной мейнтейнер Tecton отказался от дальнейшего сопровождения проекта. На какое-то время будущее Feast стало туманным.Такой востребованный замысел, как Feast, обречен на успех. Ситуация оставалась неопределенной недолго. Через три месяца сформировалось сообщество энтузиастов, которые активно занялись дальнейшим развитием проекта под покровительством Linux Foundation.Надо сказать, что возобновившаяся разработка Feast не выглядела такой уж беспроблемной. Новые мейнтейнеры представили первую версию плана дальнейшего развития, который внушал определенную тревогу. Сейчас можно с облегчением сказать, что опасения в значительной степени не подтвердились: с момента ухода Tecton появились уже две версии Feast.Главный недостаток нового плана развития был в том, что он скорее походил на список желаний. Не обозначались сроки реализации той или иной функциональности. Не обсуждались пути достижения заявленных целей. Такая расплывчатость не позволяла сделать однозначные выводы о качестве, с которым будут воплощены предполагаемые решения.Несмотря на все сомнения, команда «Купера» приняла риск. У нее достаточно знаний и необходимых ресурсов, чтобы дорабатывать Feast самостоятельно. Но они не предполагали, какого рода препятствия окажутся на этом пути.Практически полное отсутствие документацииМатериализация признаков, а именно загрузка их из S3 в Redis, не работает так, как заявлено в документации: возникают множественные ошибки, такие как отсутствие запрашиваемого ресурса и прочие. Попытки разобраться наталкиваются на то, что документация содержит совершенно неверную информацию, а указанные в документации примеры попросту не существуют.Выяснение причин неработоспособности Feast привело к открытию трудностей более фундаментального характера. Все предлагаемые решения масштабируемые, но взаимодействуют только с западными облаками, для работы с которыми и создавались.Привязка к проприетарным облачным сервисамТе, у кого есть подписка на Google Snowflake или Amazon Red Shift вообще не заметят никаких сложностей — все заработает «из коробки». Всем остальным неизбежно придется дорабатывать Feast для совместимости с альтернативными облачными сервисами. За основу, скорее всего, придется брать Spark или другую подобную технологию. Главное, чтобы разработчикам она была хорошо известна.В «Купере» так и сделали: адаптировали Spark так, чтобы использовать его с S3, а не Amazon. Но это было только начало.Критические недостатки Java‑сервераJava‑сервер позиционируется как «масштабируемое, высоконагруженное, готовое к промышленной эксплуатации» решение. Выяснилось, что это не совсем так. На слайде отражено примерное количество доработок, которые потребовалось привнести в код Java-сервера просто чтобы тот запустился.Доработки Java-сервера.Сверх того пришлось добавить поддержку сервера метрик, потому что Java Feature Server предоставлял очень мало отладочной информации. Во многих ситуациях было совершенно неясно, что происходит. Какая-либо система мониторинга также отсутствовала. Продуктивная работа при таких вводных невозможна, команде «Купера» пришлось совершенствовать и Java‑сервер.Могло показаться, что привязанность к проприетарным облакам преодолена. Выяснилось, что не совсем.Java‑сервер ориентирован на работу в Amazon Cloud. Если есть необходимость в использовании своего S3 — который живет в «Яндексе», Selectel или где‑то еще — нужно снова разбираться в исходном коде Java‑сервера и вносить изменения. В это трудно поверить, но Java‑сервер совершенно игнорирует переменные среды, которые отвечают за настройку доступа к S3.Наконец, объемные работы по перепрограммированию Java‑сервера завершены, можно запускать тесты. Но снова неприятная неожиданность.Трудности с производительностьюИтоговая производительность оказалась далека от той, что получилась в синтетических тестах. Расчет был на 50 мс для задержки при P99 (когда только для 1% запросов допускается выход за пределы ожидаемого значения). В действительности же величина задержки оказалась превышенной в 20 раз! Заметьте, замеры проводились при нагрузке всего 120 запросов в секунду.Получившаяся производительность. На верхнем графике — интенсивность запросов, RPS (requests per second). На нижнем — задержка.Впереди предстояла борьба за миллисекунды. Чтобы определить причину такого неудовлетворительного результата, потребовалось небольшое дополнительное исследование, которое и вывело на правильный путь. В качестве бэкенда использовался шардированный Redis.Шардирование или шардинг — разбиение базы данных на части, каждая из которых размещается на отдельном узле внутри кластера, состоящего из одной или нескольких реплик. Такой архитектурный принцип позволяет легко производить горизонтальное масштабирование БД.Выяснилось, что из‑за особенностей взаимодействия Feast с Redis много времени уходило на поиск хоста.Оптимизировать в Java‑сервере обращения к Redis достаточно сложно — потребуется отдельный разработчик, который все свое время отдаст этой работе. Посвятить себя улучшению Java-сервера команда Купера не планировала, у нее другие задачи.Поиск узких мест продолжился в потоках данных. Обнаружилось, что очень много времени уходит на сериализацию и десериализацию запросов. А вот здесь уже есть смысл заниматься поиском оптимизационного решения.В испытательной схеме хранения данных на один признак приходился один ключ. Напрашивается вывод, что в условиях коммерческой эксплуатации такой подход себя не оправдает. Как правило, признаки используются совместно и объединяются в вектор, а потому правильнее опробовать схему, где сотня признаков передается совокупно, в едином массиве.Быстрый синтетический тест показал десятикратный прирост производительности по сравнению с извлечением признаков поодиночке: 50 мс против 5−7 мс.Итоговая зависимость задержки от количества строк в запросе.Улучшение существенное, но захотелось посмотреть, насколько результаты синтетических тестов будут соответствовать показателям в нагрузочном тестировании, приближенном к условиям эксплуатации.Испытание не обрадовали: производительность возросла, но недостаточно. Задержка для P99 составила уже обнадеживающие 150 мс вместо первоначальных 1 000. Однако при 300 запросах в секунду по-прежнему не попадала в целевые значения.MLOps Купера продолжили исследования и сосредоточились на поиске причин существенного расхождения между синтетическими тестами, на показания которых они опирались изначально, и реальным кейсом.Загвоздка оказалась в том, что в синтетическом тесте обращение идет только к одной таблице. В действительности одной таблицей ограничиться невозможно. Если в нее собирать все признаки для разных сущностей, она займет слишком много места из‑за дублирования данных и лишних пустых значений.Стала понятна причина недостаточной производительности: в одном запросе обращение осуществляется не ко всем таблицам с признаками одновременно, а по очереди. После проведения соответствующего теста, в котором обращение ко всем 14‑ти таблицам осуществлялась параллельно в разных запросах, удалось достичь приемлемой производительности.Итоговая производительность.Допустимая нагрузка на Feature Store возросла с 300 до 4 000 запросов в секунду. Задержка составила не более 58 мс при достоверности P99. Для рекомендательной системы задержка стала не превышать 100 мс для P95 (в 95% случаев) и значительно возрастала только при нагрузке свыше 200 обращений в секунду.Достигнутые показатели можно считать удовлетворительными, а значит, система готова к внедрению для коммерческого использования.ВыводыПриходится признать — Feast на данный момент не является решением, готовым для production. Работать с инструментом можно, но тогда необходимо закладывать время и ресурсы на изучение его исходного кода, а также существенную доработку имеющейся функциональности. Кроме того, важно быть готовым к отсутствию документации приемлемого качества.«Резюмируя наш опыт и взвешивая все «за» и «против», допустимо сказать так: использовать Feast можно, но серебряной пулей для реализации Feature Store он, к сожалению, не является»—Юрий Классен,тимлид MLOps-команды в КупереНесмотря на все найденные недостатки, его можно использовать как основу для создания собственной платформы. Опыт Купера показал, что Feast содержит очень много хорошо реализованных функциональных возможностей и нуждается лишь в относительно небольшом вмешательстве для адаптации к нестандартным техническим требованиям.Tech-команда Купера (ex «СберМаркет») ведет соцсети с новостями и анонсами. Если хотите узнать, что скрывается под капотом высоконагруженного e‑commerce, следите за выпускамив Telegramина YouTube, а также слушайте подкаст «Для tech и этих» от IT‑менеджеров Купера."
СберМаркет,,,Мой опыт использования Plumber: UI-инструмент для тестирования Kafka,2024-10-01T12:30:40.000Z,"Привет, Хабр! Меня зовут Марина, я QA-инженер в Купере. Я работаю в команде, где около 80% всех взаимодействий между микросервисами осуществляется асинхронно через Kafka. Это создает дополнительные сложности в функциональном тестировании, поскольку наш сервис интегрирован с множеством других. Проверка правильности передачи сообщений требует анализа не только прохождения happy cases, но и edge и corner cases, что добавляет сложности в тестирование. Уверена, многие QA-инженеры, да и разработчики знакомы с подобными вызовами.На одном из проектов, где я работаю, у меня возникла проблема: используемые инструменты для тестирования Kafka были недостаточно удобными:Консольная утилита Protokafне имеет UI-интерфейса и полученные данные для лучшей читаемости нужно отформатировать в json структуру (а это еще одно доп. приложение).UI-приложение Kowlудобно только для мониторинга состояния топиков, и только недавно в нём стала доступна возможность чтения сообщений без сложного флоу для расшифровки, но всё так же нет возможность producer.В поисках более удобного решения коллега посоветовал Plumber — графическое приложение, с возможностью консьюмера и продюсера сообщения.В этой статье я не буду объяснять, что такое Kafka и как работают брокеры — на эти темы уже есть множество отличных материалов,например, вот. Хочу поделиться своим опытом использования этого инструмента. Я не ставлю цель сравнивать его с другими существующими решениями, а просто расскажу, как Plumber помог мне упростить процесс ручного тестирования Kafka на стейджах.Знакомство с PlumberPlumber— это графическое приложение, разработанное для взаимодействия с Apache Kafka. Оно предоставляет удобный и минималистичный интерфейс для работы и включает следующие возможности:Подключение к Kafka-брокеру.Consumer и producer в топики.Поддержка различных форматов данных (JSON, protobuf, Avro и других).Сообщение отображается в формате JSON, что облегчает читаемость и дальнейшую работу.Установка и настройка PlumberНачнем с того что установка и настройка не столь сложны.Установка PlumberПерейдите настраницу релизовпроекта.Скачайте последнюю версию Plumber для вашей операционной системы (доступны версии для Windows, macOS и Linux).Распакуйте архив и запустите приложение. Бинарные сборки не подписаны, поэтому Windows и OS будут ругаться. Обходим блокировку системы безопасности macOS вручную:- или через контекстное меню(Control+клик), с последующим выбором ""Открыть""- или удаляем атрибуты безопасности с помощью команды xattr:$ xattr -cr plumber.appПосле запуска Plumber приступаем к настройке подключения к Kafka.Настройка KafkaСоздайте кластер выбрав New Kafka ClusterЧтобы настроить подключение к вашему Kafka-брокеру, необходимо указать следующие параметры:2.1.Название кластера: введите понятное имя кластера, чтобы быстро определить, к какому вы хотите подключиться, если у вас будет одновременно несколько настроенных. Запускать одновременно два не получится, но удобно переключаться.2.2.Адрес Kafka-брокера(bootstrap.servers) — укажите DNS-имя сервера или IP-адрес. Kafka использует адрес брокера для установления связи и передачи сообщений.2.3. Параметры аутентификации и безопасности:sasl.mechanisms— механизм аутентификации, который определяет способ авторизации клиента. В большинстве случаев используется механизм PLAIN или SCRAM-SHA-256/512. Выбор механизма зависит от настроек вашего кластера Kafka.sasl.usernameиsasl.password— учётные данные для аутентификации в кластере Kafka. Эти параметры используются вместе с механизмом SASL должны соответствовать учетным данным настроенным на стороне брокера Kafka.security.protocol— протокол безопасности, который указывает, каким образом данные будут передаваться между клиентом и брокером. Наиболее распространённые значения — SASL_SSL или SASL_PLAINTEXT. Использование SASL_SSL предполагает защищенное соединение, в то время как SASL_PLAINTEXT подходит для тестовых сред без шифрования.3. Для проверки, всё ли верно настроено, жмем Test Kafka Connectivity. Получаем одобрительный ОК. Далее «Сохранить» и переходим в только что созданный кластер.4. Автоматически отобразится количество всех топиков, к которым есть доступ через указанный Kafka-брокер, в разделе Overview.Начнем с прослушки сообщений.Настройка Consumer (чтения сообщений)1. Перейдите в раздел Topics.2. Конечно, быстрее всего найти нужный топик можно через поиск.3. Выберите топик, который необходимо прослушивать, нажатием на лупу.4. Настройте параметры потребления сообщений, например, тип ключа(если есть), формат десериализации (protobuf, JSON и другие).Для прослушивания топиков в Apache Kafka, когда сообщения сериализованы в формате protobuf, требуется наличие .proto файла. Этот файл описывает структуру данных, позволяя приложению корректно десериализовать сообщения и отобразить их в удобочитаемом формате. Поэтому предварительно скачала из репозитория проекта и указываю путь к .proto файлу.5. Нажмите Start. Если лимит не задан, то в режиме реального времени будут поступать новые сообщения.Одной из полезных функций Plumber является возможность настройки фильтров для поиска нужных сообщений в потоке. Но хочу отметить, что я столкнулась с проблемой: иногда фильтрация не срабатывала корректно, и ранее отправленные сообщения не отображались. Тем не менее, с новыми сообщениями после настройки фильтров проблем не возникало — они отображались стабильно.Не могу не отметить что еще одной полезной функцией для меня является история сообщений. Приложение позволяет просматривать не только текущие, но и историю сообщений, позволяя анализировать предыдущие события или ошибки, которые могли произойти.6. Для чтения сообщения открываем  двойным кликом или по кнопке details. Можно использовать их в дальнейших тестах продюсера или для фиксирования в артефактах.В consumer есть очистка истории Clear, если поток сообщений становится слишком большим, или когда хотите удалить предыдущие данные для удобства анализа. Это очень удобно когда надо проверить с чистым потоком для тестирования нового сценария.Настройка Producer (отправка сообщений)Мы дошли до самой полезной функции Plumber — producer. Инструмент позволяет имитировать отправку сообщений от смежного сервиса, что открывает массу возможностей для тестирования. Например, я модифицирую данные: заменяю значение в полях, таких как id, даты, статусы и др, чтобы проверить устойчивость обработки. Или изменяю структуру сообщения: переставляя порядок полей или удаляя/добавляя новые для симуляции изменений протоколов взаимодействия. Помогает и ускоряет работу при тестировании corner case: передача некорректных данных или пустых значений. При тестирование редких или сложных кейсов, которые сложно воспроизвести естественным образом, например, когда сообщение генерируется только в редких  условиях или долго ждать отправки. В таких случаях использование producer значительно ускорит процесс тестирования без ожидания реального возникновения таких ситуаций.Использование продюсера позволяет эффективно тестировать логику на первичных проверках, например, когда ваш сервис с доработками готов раньше, чем сервис-producer. Это позволяет отловить потенциальные баги до интеграционного тестирования и сэкономит время. Конечно, полная интеграция всё равно нужна, чтобы убедиться, что все сервисы корректно взаимодействуют и всё работает, как задумано.Приступим.1. Нажмите на кнопку Producer.2. В открывшемся окне выберите топик из списка. Поиска, к сожалению, нет, надо искать глазами и тут может быть больно, если топиков много.3. Выберите формат данных, который будет отправлен. Я все также вибираю protobuf и загружаю прото-файл.4. Копируем сообщение из консьюмера, немного изменим данные. Для примера я установлю для поляautomaticRoutingзначениеfalse, а в полеdeleted_atукажу дату. НажимаемProduce to topicдля отправки сообщения. В правой колонке можно увидеть, в какое время было отправлено сообщение.5. Можно проверить в базе, что изменилось/записалось при получение такого сообщения.Преимущества использования PlumberПодводя итоги  хотела бы еще раз выделить ключевые преимущества, на основе моего опыта:Минималистичный и интуитивный графический интерфейс:Plumber делает работу с Kafka проще и быстрее. Благодаря простому  и функциональному интерфейсу, взаимодействие с сообщениями становится наглядным и не требует дополнительных усилий(приложений).Гибкость в работе с форматами данных:Plumber поддерживает различные форматы, такие как protobuf, JSON, что позволяет адаптироваться под конкретные требования проекта.Простота настройки:Настройки выполняются легко и быстро, что особенно важно в условиях динамичного рабочего процесса.Открытый исходный код и бесплатное использование.Долгосрочная перспектива. В современных условиях важно иметь инструменты, которые не зависят от политических (или иных) синтуации(й). Но тут скрывается и один минус: на данный момент поддержание приложения приостановлено со стороны разработчика.Однако, как я уже упоминала, у приложения есть и небольшие недостатки. Еще одной проблемой, с которой я столкнулась, было неожиданное завершение работы программы. К счастью, настройки кластера сохраняются, так что их не нужно вводить повторно. Но это становится неприятным, если вы подготовили сообщение для producer прямо в Plumber, а не в текстовом редакторе. После первого такого случая я стала готовить тело сообщения в блокноте, а затем переносить его в Plumber. Это даёт возможность вернуться к подготовленным данным в случае сбоя.ЗаключениеPlumber стал для меня отличным инструментом для ручного тестировании микросервисов, использующих Kafka. Не призываю и не утверждаю, что это супер приложение без изъянов, но он значительно упростил мне процесс тестирования по сравнению с предыдущими инструментами.Если вы работаете с Kafka и ищете удобный UI-инструмент для тестирования, Plumber — это именно, то решение, которое может существенно облегчить вашу работу. Надеюсь, мой опыт будет полезен для тех, кто сталкивается с подобными задачами.Tech-команда Купера (ex СберМаркет) ведет соцсети с новостями и анонсами. Если хочешь узнать, что под капотом высоконагруженного e-commerce, следи за нами вTelegramи наYouTube. А также слушайподкаст «Для tech и этих»от наших it-менеджеров."
СберМаркет,,,Контрактные тесты с Pact: гарантия стабильности микросервисов,2024-09-27T06:50:19.000Z,"Привет! Меня зовут Юрий, я старший разработчик в Купере в команде Ruby Platform — занимаюсь разработкой внутренних библиотек, инструментов мониторинга и поддержки микросервисов.У нас в Купереболее 200 микросервисовна Go, Ruby, JS, Python, etc, а также несколько монолитов. С точки зрения инфраструктуры интеграционное тестирование такого количества компонентов — довольно затратная задача, но при этом хочется обеспечить стабильность системы, не проводя ручные интеграционные регресс-тесты. В таких условиях оптимальным решением являются контрактные тесты.Из этой статьи вы узнаете:общий принцип работы контрактных тестов;о проблемах, с которыми мы столкнулись при внедрении контрактного тестирования, и как их решали;как мы разработали свое решение для контрактного тестирования Ruby-приложений;о настройке CI/CD для автоматизации контрактных тестов.Материал будет полезен тем, кто задумывается о повышении надежности интеграций между сервисами и внедрении контрактных тестов в свои проекты.О контрактных тестахОсновые термины:Контракт (contract)— соглашение или спецификация API, описывающая структуру и форматы данных при взаимодействии между сервисом-поставщиком и сервисами-потребителями.Провайдер (provider)— поставщик контракта, предоставляет API.Консюмер (consumer)— потребитель контракта, является клиентом API.Контрактное тестирование— это способ проверки сервиса-поставщика и сервиса-потребителя данных на соответствие контракту API в точке интеграции.Впирамиде тестированияMike Kohn контрактные тесты находятся в блоке Service Tests вместе с интеграционными тестами.Контракты между сервисами можно тестировать в рамках UI-тестов (end-to-end), однако:это медленно (как по времени работы, так и по циклу обратной связи);это хрупко (высокая сложность интеграций приводит к разного рода edge-кейсам).Следовательно, ошибки в контрактах (например, обратно-несовместимые изменения, человеческий фактор, etc) гораздо удобнее (и дешевле) выявлять на раннем на этапе разработки. Для этих целей и существуют контрактные тесты.PactВ качестве решения для организации контрактного тестирования был выбран фреймворкPact. Наши основные аргументы:consumer-driven подход;поддержка разных стеков: у нас как минимум используются Ruby, Golang, JS/TS, Python;удобство организации CI/CD за счет существующего инструментария: pact-broker, pact-cli;хорошая документация и поддержка.Основные термины:Pact-манифест— специальный json-файл (пример ниже), в котором описаны форматы запросов/ответов, их матчеры, а также используемые транспорты (http, grpc, etc).Pact-спецификация— описание формата pact-манифеста и поддерживаемой им функциональности. На момент написания статьи существует четыре версииспецификации:V1/V2 - поддерживает только http-взаимодействияV3 - поддерживает асинхронные (message) взаимодействияV4 - поддерживает плагины, позволяющие реализовать поддержку любых транспортов/форматов (например, grpc/protobuf) и их матчингаВзаимодействие (interaction)— описанные в pact-манифесте форматы запроса-ответа в рамках контракта и их соответствие ожиданиям.Матчеры (matchers)— специальные правила соответствия форматов запросов/ответов, поддерживаемые спецификацией.Формат pact-манифестаРассмотрим пример pact-манифеста, который генерируется консюмер-тестами из данной статьиservice-consumer-service-provider.json{
  ""consumer"": {
    ""name"": ""service-consumer""
  },
  ""interactions"": [
    {
      ""contents"": {
        ""content"": ""CAEQAQ=="",
        ""contentType"": ""application/protobuf;message=.protobuf.order_data.Order"",
        ""contentTypeHint"": ""BINARY"",
        ""encoded"": ""base64""
      },
      ""description"": ""async: order via kafka"",
      ""interactionMarkup"": {
        ""markup"": ""```protobuf\nmessage Order {\n    int32 id = 1;\n    enum .protobuf.order_data.Order.OrderStatus status = 2;\n}\n```\n"",
        ""markupType"": ""COMMON_MARK""
      },
      ""matchingRules"": {
        ""body"": {
          ""$.id"": {
            ""combine"": ""AND"",
            ""matchers"": [
              {
                ""match"": ""integer""
              }
            ]
          },
          ""$.status"": {
            ""combine"": ""AND"",
            ""matchers"": [
              {
                ""match"": ""regex"",
                ""regex"": ""(?-mix:(PENDING|COMPLETED|CANCELED))""
              }
            ]
          }
        },
        ""metadata"": {
          ""key"": {
            ""combine"": ""AND"",
            ""matchers"": [
              {
                ""match"": ""regex"",
                ""regex"": ""(?-mix:.*)""
              }
            ]
          },
          ""topic"": {
            ""combine"": ""AND"",
            ""matchers"": [
              {
                ""match"": ""regex"",
                ""regex"": ""(?-mix:.*)""
              }
            ]
          }
        }
      },
      ""metadata"": {
        ""contentType"": ""application/protobuf;message=.protobuf.order_data.Order"",
        ""key"": ""key"",
        ""topic"": ""orders-topic""
      },
      ""pending"": false,
      ""pluginConfiguration"": {
        ""protobuf"": {
          ""descriptorKey"": ""2a5b88336a6f5a708460709e23f3c701"",
          ""message"": "".protobuf.order_data.Order""
        }
      },
      ""providerStates"": [
        {
          ""name"": ""order exists"",
          ""params"": {
            ""contentType"": ""application/protobuf;message=.protobuf.order_data.Order"",
            ""order_id"": 1
          }
        }
      ],
      ""type"": ""Asynchronous/Messages""
    },
    {
      ""description"": ""grpc: fetch order via grpc"",
      ""interactionMarkup"": {
        ""markup"": ""```protobuf\nmessage OrderStatusResponse {\n    message .orders.Order order = 1;\n}\n```\n"",
        ""markupType"": ""COMMON_MARK""
      },
      ""pending"": false,
      ""pluginConfiguration"": {
        ""protobuf"": {
          ""descriptorKey"": ""5a39c2b98badf0e1d0ed2e038cba0d62"",
          ""service"": "".orders.Orders/StatusById""
        }
      },
      ""providerStates"": [
        {
          ""name"": ""order exists"",
          ""params"": {
            ""order_id"": 1
          }
        }
      ],
      ""request"": {
        ""contents"": {
          ""content"": ""CAE="",
          ""contentType"": ""application/protobuf;message=.orders.OrderStatusRequest"",
          ""contentTypeHint"": ""BINARY"",
          ""encoded"": ""base64""
        },
        ""matchingRules"": {
          ""body"": {
            ""$.id"": {
              ""combine"": ""AND"",
              ""matchers"": [
                {
                  ""match"": ""integer""
                }
              ]
            }
          }
        },
        ""metadata"": {
          ""contentType"": ""application/protobuf;message=.orders.OrderStatusRequest""
        }
      },
      ""response"": [
        {
          ""contents"": {
            ""content"": ""CgQIChAD"",
            ""contentType"": ""application/protobuf;message=.orders.OrderStatusResponse"",
            ""contentTypeHint"": ""BINARY"",
            ""encoded"": ""base64""
          },
          ""matchingRules"": {
            ""body"": {
              ""$.order.id"": {
                ""combine"": ""AND"",
                ""matchers"": [
                  {
                    ""match"": ""integer""
                  }
                ]
              },
              ""$.order.status"": {
                ""combine"": ""AND"",
                ""matchers"": [
                  {
                    ""match"": ""equality""
                  }
                ]
              }
            }
          },
          ""metadata"": {
            ""contentType"": ""application/protobuf;message=.orders.OrderStatusResponse""
          }
        }
      ],
      ""transport"": ""grpc"",
      ""type"": ""Synchronous/Messages""
    },
    {
      ""description"": ""http: fetch order via http"",
      ""pending"": false,
      ""providerStates"": [
        {
          ""name"": ""order exists"",
          ""params"": {
            ""order_id"": 1
          }
        }
      ],
      ""request"": {
        ""method"": ""GET"",
        ""path"": ""/api/v1/orders/1""
      },
      ""response"": {
        ""body"": {
          ""content"": {
            ""id"": 1,
            ""status"": ""COMPLETED""
          },
          ""contentType"": ""application/json"",
          ""encoded"": false
        },
        ""headers"": {
          ""Content-Type"": [
            ""application/json""
          ]
        },
        ""matchingRules"": {
          ""body"": {
            ""$.id"": {
              ""combine"": ""AND"",
              ""matchers"": [
                {
                  ""match"": ""integer""
                }
              ]
            },
            ""$.status"": {
              ""combine"": ""AND"",
              ""matchers"": [
                {
                  ""match"": ""regex"",
                  ""regex"": ""(?-mix:(PENDING|COMPLETED|CANCELED))""
                }
              ]
            }
          }
        },
        ""status"": 200
      },
      ""transport"": ""http"",
      ""type"": ""Synchronous/HTTP""
    }
  ],
  ""metadata"": {
    ""pactRust"": {
      ""ffi"": ""0.4.22"",
      ""mockserver"": ""1.2.9"",
      ""models"": ""1.2.3""
    },
    ""pactSpecification"": {
      ""version"": ""4.0""
    },
    ""plugins"": [
      {
        ""configuration"": {
          ""2a5b88336a6f5a708460709e23f3c701"": {
            ""protoDescriptors"": ""Cr0BCgtvcmRlci5wcm90bxITcHJvdG9idWYub3JkZXJfZGF0YSKQAQoFT3JkZXISDgoCaWQYASABKAVSAmlkEj4KBnN0YXR1cxgCIAEoDjImLnByb3RvYnVmLm9yZGVyX2RhdGEuT3JkZXIuT3JkZXJTdGF0dXNSBnN0YXR1cyI3CgtPcmRlclN0YXR1cxILCgdQRU5ESU5HEAASDQoJQ09NUExFVEVEEAESDAoIQ0FOQ0VMRUQQAmIGcHJvdG8z"",
            ""protoFile"": ""syntax = \""proto3\"";\n\npackage protobuf.order_data;\n\nmessage Order {\n  enum OrderStatus {\n    PENDING = 0;\n    COMPLETED = 1;\n    CANCELED = 2;\n  }\n\n  int32 id = 1;\n  OrderStatus status = 2;\n}\n""
          },
          ""5a39c2b98badf0e1d0ed2e038cba0d62"": {
            ""protoDescriptors"": ""Cu0CCgxvcmRlcnMucHJvdG8SBm9yZGVycyKIAQoFT3JkZXISDgoCaWQYASABKAVSAmlkEiwKBnN0YXR1cxgCIAEoDjIULm9yZGVycy5PcmRlci5TdGF0dXNSBnN0YXR1cyJBCgZTdGF0dXMSCwoHUEVORElORxAAEg0KCUNPTVBMRVRFRBABEgwKCENBTkNFTEVEEAISDQoJUFJPQ0VTU0VEEAMiJAoST3JkZXJTdGF0dXNSZXF1ZXN0Eg4KAmlkGAEgASgFUgJpZCI6ChNPcmRlclN0YXR1c1Jlc3BvbnNlEiMKBW9yZGVyGAEgASgLMg0ub3JkZXJzLk9yZGVyUgVvcmRlcjJPCgZPcmRlcnMSRQoKU3RhdHVzQnlJZBIaLm9yZGVycy5PcmRlclN0YXR1c1JlcXVlc3QaGy5vcmRlcnMuT3JkZXJTdGF0dXNSZXNwb25zZUIP6gIMR3JwYzo6T3JkZXJzYgZwcm90bzM="",
            ""protoFile"": ""syntax = \""proto3\"";\n\npackage orders;\noption ruby_package = \""Grpc::Orders\"";\n\nservice Orders {\n  rpc StatusById(OrderStatusRequest) returns (OrderStatusResponse);\n}\n\nmessage Order {\n int32 id = 1;\n enum Status {\n    PENDING = 0;\n    COMPLETED = 1;\n    CANCELED = 2;\n    PROCESSED = 3;\n }\n Status status = 2;\n}\n\nmessage OrderStatusRequest {\n int32 id = 1;\n}\n\nmessage OrderStatusResponse {\n Order order = 1;\n}\n""
          }
        },
        ""name"": ""protobuf"",
        ""version"": ""0.5.1""
      }
    ],
    ""sbmt-pact"": {
      ""pact-ffi"": ""0.4.22""
    }
  },
  ""provider"": {
    ""name"": ""service-provider""
  }
}Общее представление pact-манифестаБлок interaction и использование матчеров разберем далее в конкретных примерах.Таким образом pact-манифест — это служебный файл, содержащий в себе набор данных, необходимый для проверки контракта между двумя сервисами.Процесс тестирования консюмера и провайдераГде pact-core — ядро pact, общее для клиентских библиотек разных стеков.В консюмер-тестах:описываются форматы запросов и ответов во взаимодействии с провайдером;pact-core поднимает мок-сервер (мок-провайдер) на основе описанного взаимодействия;консюмер делает реальные запросы в мок-сервер в соответствии со своими ожиданиями;pact-core по результатам взаимодействия формирует и записывает всю необходимую информацию в pact-манифест;pact-манифест публикуется в pact-брокере, задача которого централизованно хранить версионированные манифесты, статус их верификации и вести реестр взаимодействующих компонентов (консюмеров и провайдеров).В провайдер-тестах:поднимается сервер провайдера;pact-core, обращаясь к pact-брокеру, определяет все консюмеры, имеющие контракты с данным провайдером и получает их pact-манифесты;pact-core для каждого консюмера с помощью своего мок-клиента делает тестовые запросы в провайдер и таким образом верифицирует контракт на основе описанных там форматов запросов/ответов;pact-core публикует результат верификации (да/нет) каждого консюмера с текущим провайдером в pact-брокере.Provider StatesСтоит подробнее остановиться насостояниях провайдера.При тестировании провайдера pact-core выступает клиентом, воспроизводящим тестовые запросы из pact-манифеста. В этот момент запущен реальный сервер провайдера, который эти запросы получает и обрабатывает.В случаях, когда логика провайдера предполагает получение информации из БД — необходимо заранее подготовить ее состояние, используя метаданные из pact-манифеста.Поддержка Ruby и V3/V4-спецификацийЕсли с Golang/JS проблем на старте не было, то для Ruby возникли некоторые нюансы:официальный руби-гемподдерживал только V1/V2-спецификации, которые предполагают возможность тестирования только http-взаимодействий;необходимые нам grpc/kafka-взаимодействия поддерживаются в V3/V4-спецификациях;немногим ранее в процессе эволюции и поддержки V3/V4-спецификаций в pact-foundation решили переработать архитектуру и перешли наshared rust-core, предполагающий тест-библиотекам для разных стеков использовать FFI (foreign-function interface) как единый интерфейс для взаимодействия с ядром на rust;официальный руби-гем на длительное время остался в подвешенном состоянии и не развивался, параллельно был созданpact-ruby-ffi, предоставляющий низкоуровневый интерфейс к pact-core;и лишь недавно появились планы по развитию официального руби-гема и поддержке V3/V4 — см.Pact V3 Tracking IssueиPact V4 Tracking Issue.Таким образом на старте использования единственным вариантом для нас была реализация своего решения на базе pact-ruby-ffi. Так появился гемsbmt-pact, предоставляющий высокоуровневый интерфейс для написания пакт-тестов и поддерживающий спецификации V3/V4 для Ruby.Архитектура гема sbmt-pactОсновные возможности:поддержка актуальных pact-спецификаций благодаря использованию официального pact-ffi, возможность расширения и поддержки новых протоколов взаимодействия;высокоуровневый rspec-DSL для написания pact-тестов, поддержкаprovider-statesиconsumer version selectors;встроенная поддержка серверов провайдера: HTTP (Rails), gRPC (Gruf), Kafka;возможность разделения тестов одного провайдера на несколько модулей по используемым транспортам, а также под каждого консюмера за счетconsumer version selectors;конфигурирование pact-broker — несколькими ENV-переменными. В том числе, присутствует возможность указания версии провайдера в консюмер-тестах, что позволяет легко запускать и отлаживать pact-тесты локально.Далее рассмотрим несколько примеров использования для тестирования контрактов HTTP, gRPC и Kafka.Пример использования: HTTPРассмотрим два микросервиса, взаимодействующих по HTTP:Пример консюмер-тестаRSpec.describe ""Http::Orders"", :pact do  
  # декларируем тип взаимодействия
  has_http_pact_between ""service-consumer"", ""service-provider""
  
  let(:order_id) { 1 }  
  
  let(:client) { Http::Orders::V1::Client.new }  
  let(:make_request) { client.order_status(id: order_id) }  

  # определяем заимодействие между сервисами и матчеры запроса/ответа
  let(:interaction) do  
    new_interaction
    # определяем provider state с необходимыми метаданными
    # которые можно будет использовать позже, в момент запуска провайдер-теста
    .given(""order exists"", order_id: order_id)  
    .upon_receiving(""fetch order via http"")
    # описываем формат запроса
    .with_request(:get, ""/api/v1/orders/#{order_id}"")  
    # и формат ответа
    .with_response(200, headers: {}, body: {
        id: match_any_integer,
        status: match_regex(/(PENDING|COMPLETED|CANCELED|PROCESSED)/, ""COMPLETED"")
    })
  end  
  
  it ""executes the pact test without errors"" do
    # запускаем тест, в этот момент pact-core поднимает mock-сервер,
    # а наш http-клиент делает реальный запрос в данный мок
    interaction.execute do
        # в примере нам важен только критерий успешности запроса,
        # форматы проверяются под капотом pact-core
        expect(make_request).to be_success  
    end
    # по результатам будет сгенерирован (локально) pact-манифест
  end
endКонсюмер-тест состоит из 3х основных блоков:декларация типа взаимодействия;определение форматов запроса-ответа с матчерами;запуск теста.Особо стоит отметить использованиеprovider states. Грубо говоря, это способ описания требуемого состояния провайдера в момент его тестирования (метаданные, которые мы укажем в консюмер-тесте, будут записаны в pacе-манифест и доступны в рантайме провайдер-теста).Сгенерированный в тесте pact-манифест:service-consumer-service-provider.json{
  ""consumer"": {
    ""name"": ""service-consumer""
  },
  ""interactions"": [
    {
      ""description"": ""http: fetch order via http"",
      ""pending"": false,
      ""providerStates"": [
        {
          ""name"": ""order exists"",
          ""params"": {
            ""order_id"": 1
          }
        }
      ],
      ""request"": {
        ""method"": ""GET"",
        ""path"": ""/api/v1/orders/1""
      },
      ""response"": {
        ""body"": {
          ""content"": {
            ""id"": 1,
            ""status"": ""COMPLETED""
          },
          ""contentType"": ""application/json"",
          ""encoded"": false
        },
        ""headers"": {
          ""Content-Type"": [
            ""application/json""
          ]
        },
        ""matchingRules"": {
          ""body"": {
            ""$.id"": {
              ""combine"": ""AND"",
              ""matchers"": [
                {
                  ""match"": ""integer""
                }
              ]
            },
            ""$.status"": {
              ""combine"": ""AND"",
              ""matchers"": [
                {
                  ""match"": ""regex"",
                  ""regex"": ""(?-mix:(PENDING|COMPLETED|CANCELED))""
                }
              ]
            }
          }
        },
        ""status"": 200
      },
      ""transport"": ""http"",
      ""type"": ""Synchronous/HTTP""
    }
  ],
  ""metadata"": {
    ""pactRust"": {
      ""ffi"": ""0.4.22"",
      ""mockserver"": ""1.2.9"",
      ""models"": ""1.2.3""
    },
    ""pactSpecification"": {
      ""version"": ""4.0""
    },
    ""sbmt-pact"": {
      ""pact-ffi"": ""0.4.22""
    }
  },
  ""provider"": {
    ""name"": ""service-provider""
  }
}Рассмотрим его чуть подробнее.pact-манифест: http-interactionПример провайдер-тестаRSpec.describe ""Orders::Http"", :pact do
  # аналогично - декларируем тип взаимодействия и название провайдера
  # тут указывать консюмер уже не нужно: все консюмеры определяются в рантайме,
  # по запросу в pact-брокер
  http_pact_provider ""service-provider""

  # наш provider-state
  provider_state 'order exists' do  
	set_up do |params|
	  # для которого необходимо в БД предсоздать сущность
	  # заказа с ID, который берем из метаданных
	  FactoryBot.create(:order, id: params['order_id'])  
	end  
  end

  # под капотом будет поднят http-сервер провайдера
  # в который pact-core mock-client сделает запрос, 
  # описанный в pact-манифесте
endС провайдер-тестами все немного проще. Тут уже не требуется описывать какие-то форматы запросов/ответов, т.к. они уже описаны в pact-манифесте на этапе консюмер-теста. Нам лишь требуется при необходимости учесть все provider states.В данном случае наш тест требует, чтобы заказ с указанным ID существовал в БД - что мы и сделали, создав его.Провайдер-тест состоит из 1-2х основных блоков:декларация типа взаимодействияж;(опционально) описание 1 или более provider states.Мы рассмотрели простое взаимодействие на базе REST API. В микросервисной архитектуре часто используется gRPC, рассмотрим соответствующий пример.Пример использования: gRPCНемного усложним предыдущий кейс и рассмотрим те же микросервисы, но взаимодействующие по gRPC.Proto-контрактsyntax = ""proto3"";  
  
package orders;  
  
service Orders {  
 rpc StatusById(OrderStatusRequest) returns (OrderStatusResponse);  
}  
  
message Order {  
 int32 id = 1;  
 enum Status {  
    PENDING = 0;  
    COMPLETED = 1;  
    CANCELED = 2;  
    PROCESSED = 3;  
 }  
 Status status = 3;  
}  
  
message OrderStatusRequest {  
 int32 id = 1;  
}  
  
message OrderStatusResponse {  
 Order order = 1;  
}Пример консюмер-тестаRSpec.describe ""Grpc::Orders"", :pact do  
  # декларируем тип взаимодействия
  has_grpc_pact_between ""service-consumer"", ""service-provider""
  
  let(:order_id) { 1 }  
  
  let(:client) { Grpc::Orders::V1::Client.new }  
  let(:make_request) { client.order_status_by_id(id: order_id) }  

  # определяем заимодействие между сервисами и матчеры запроса/ответа
  let(:interaction) do
    new_interaction
      # указываем proto-файл и название тестируемого rpc-сервиса
      .with_service(""deps/services/orders.proto"", ""Orders/StatusById"")
      .upon_receiving(""fetch order via grpc"")
      # определяем provider state с необходимыми метаданными
      # которые можно будет использовать позже, в момент запуска провайдер-теста
      .given(""order exists"", order_id: order_id)  
  	  # описываем формат данных с матчерами
      .with_request(id: match_any_integer(order_id))
      .with_response(
        order: {
          id: match_any_integer,
          status: match_exactly(""PROCESSED"")
        }
      )
  end
  
  it ""executes the pact test without errors"" do
    # запускаем тест, в этот момент pact-core поднимает mock-сервер,
    # а наш grpc-клиент делает реальный запрос в данный мок
    interaction.execute do
        # в примере нам важен только критерий успешности запроса,
        # форматы проверяются под капотом pact-core
        expect(make_request).to be_success  
    end
    # по результатам будет сгенерирован (локально) pact-манифест
  end
endТут все аналогично http-консюмер-тесту, за исключением специфики gRPC: необходимо указать proto-файл и название rpc-сервиса, эта информация будет использована внутри pact-core для того, чтобы корректно замокать и провалидировать запросы/ответы и их типы данных.Пример провайдер-тестаRSpec.describe ""Orders::Grpc"", :pact do
  # аналогично - декларируем тип взаимодействия и название провайдера
  # тут указывать консюмер уже не нужно: все консюмеры определяются в рантайме,
  # по запросу в pact-брокер
  grpc_pact_provider ""service-provider""

  # определяем provider-state
  provider_state 'order exists' do  
	set_up do |params|
	  # для которого нам нужно в БД предсоздать сущность
	  # заказа с ID, который берем из метаданных
	  FactoryBot.create(:order, id: params['order_id'])  
	end  
  end

  # под капотом будет поднят grpc-сервер провайдера
  # в который pact-core mock-client сделает запрос, 
  # описанный в pact-манифесте
endservice-consumer-service-provider.json{
  ""consumer"": {
    ""name"": ""service-consumer""
  },
  ""interactions"": [
    {
      ""description"": ""grpc: fetch order via grpc"",
      ""interactionMarkup"": {
        ""markup"": ""```protobuf\nmessage OrderStatusResponse {\n    message .orders.Order order = 1;\n}\n```\n"",
        ""markupType"": ""COMMON_MARK""
      },
      ""pending"": false,
      ""pluginConfiguration"": {
        ""protobuf"": {
          ""descriptorKey"": ""5a39c2b98badf0e1d0ed2e038cba0d62"",
          ""service"": "".orders.Orders/StatusById""
        }
      },
      ""providerStates"": [
        {
          ""name"": ""order exists"",
          ""params"": {
            ""order_id"": 1
          }
        }
      ],
      ""request"": {
        ""contents"": {
          ""content"": ""CAE="",
          ""contentType"": ""application/protobuf;message=.orders.OrderStatusRequest"",
          ""contentTypeHint"": ""BINARY"",
          ""encoded"": ""base64""
        },
        ""matchingRules"": {
          ""body"": {
            ""$.id"": {
              ""combine"": ""AND"",
              ""matchers"": [
                {
                  ""match"": ""integer""
                }
              ]
            }
          }
        },
        ""metadata"": {
          ""contentType"": ""application/protobuf;message=.orders.OrderStatusRequest""
        }
      },
      ""response"": [
        {
          ""contents"": {
            ""content"": ""CgQIChAD"",
            ""contentType"": ""application/protobuf;message=.orders.OrderStatusResponse"",
            ""contentTypeHint"": ""BINARY"",
            ""encoded"": ""base64""
          },
          ""matchingRules"": {
            ""body"": {
              ""$.order.id"": {
                ""combine"": ""AND"",
                ""matchers"": [
                  {
                    ""match"": ""integer""
                  }
                ]
              },
              ""$.order.status"": {
                ""combine"": ""AND"",
                ""matchers"": [
                  {
                    ""match"": ""equality""
                  }
                ]
              }
            }
          },
          ""metadata"": {
            ""contentType"": ""application/protobuf;message=.orders.OrderStatusResponse""
          }
        }
      ],
      ""transport"": ""grpc"",
      ""type"": ""Synchronous/Messages""
    }
  ],
  ""metadata"": {
    ""pactRust"": {
      ""ffi"": ""0.4.22"",
      ""mockserver"": ""1.2.9"",
      ""models"": ""1.2.3""
    },
    ""pactSpecification"": {
      ""version"": ""4.0""
    },
    ""plugins"": [
      {
        ""configuration"": {
          ""5a39c2b98badf0e1d0ed2e038cba0d62"": {
            ""protoDescriptors"": ""Cu0CCgxvcmRlcnMucHJvdG8SBm9yZGVycyKIAQoFT3JkZXISDgoCaWQYASABKAVSAmlkEiwKBnN0YXR1cxgCIAEoDjIULm9yZGVycy5PcmRlci5TdGF0dXNSBnN0YXR1cyJBCgZTdGF0dXMSCwoHUEVORElORxAAEg0KCUNPTVBMRVRFRBABEgwKCENBTkNFTEVEEAISDQoJUFJPQ0VTU0VEEAMiJAoST3JkZXJTdGF0dXNSZXF1ZXN0Eg4KAmlkGAEgASgFUgJpZCI6ChNPcmRlclN0YXR1c1Jlc3BvbnNlEiMKBW9yZGVyGAEgASgLMg0ub3JkZXJzLk9yZGVyUgVvcmRlcjJPCgZPcmRlcnMSRQoKU3RhdHVzQnlJZBIaLm9yZGVycy5PcmRlclN0YXR1c1JlcXVlc3QaGy5vcmRlcnMuT3JkZXJTdGF0dXNSZXNwb25zZUIP6gIMR3JwYzo6T3JkZXJzYgZwcm90bzM="",
            ""protoFile"": ""syntax = \""proto3\"";\n\npackage orders;\noption ruby_package = \""Grpc::Orders\"";\n\nservice Orders {\n  rpc StatusById(OrderStatusRequest) returns (OrderStatusResponse);\n}\n\nmessage Order {\n int32 id = 1;\n enum Status {\n    PENDING = 0;\n    COMPLETED = 1;\n    CANCELED = 2;\n    PROCESSED = 3;\n }\n Status status = 2;\n}\n\nmessage OrderStatusRequest {\n int32 id = 1;\n}\n\nmessage OrderStatusResponse {\n Order order = 1;\n}\n""
          }
        },
        ""name"": ""protobuf"",
        ""version"": ""0.5.1""
      }
    ],
    ""sbmt-pact"": {
      ""pact-ffi"": ""0.4.22""
    }
  },
  ""provider"": {
    ""name"": ""service-provider""
  }
}Провайдер-тест почти полностью аналогичен http-провайдер-тесту, отличается только тип взаимодействия.Теперь очередь за асинхронным взаимодействием. Мы для этих целей используем kafka, рассмотрим следующий пример.Пример использования: KafkaЕсли тестирование синхронных http/gRPC взаимодействий практически не отличается друг от друга, то с асинхронным взаимодействием все чуть сложнее (на примере Кафки):мы не очень хотим поднимать реальный кафка-брокер: это долго и ресурсоемко (хотя при желании — возможно);в pact уже придумали механизм тестирования асинхронных взаимодействий: можно указать специальный http-сервер, который будет использоваться в качестве транспорта для тестируемого взаимодействия.Рассмотрим все те же микросервисы, но взаимодействующие по gRPC через Кафка-топики (producer сообщений в Кафку в данном случае является провайдером).Рассмотрим простейшие продюсер/консюмер, реализованные на базе гемаsbmt-kafka_consumer(karafka 2).Класс продюсераПродюсер получает сущность Order, кодирует ее в protobuf и публикует в кафкуclass OrderProducer < Sbmt::KafkaProducer::BaseProducer  
  option :topic, default: -> { ""orders-topic"" }  
  
  def publish(order)  
    payload = encode_payload(order)  
    sync_publish(payload)  
  end  

  private  
  
  def encode_payload(order)  
    {  
      id: order.id,  
      status: order.status.upcase  
    }  
  end  
endКласс консюмераКонсюмер вычитывает из кафки закодированный proto-пейлоад (который раскодируется под капотом гемаsbmt-kafka_consumer) и логирует его параметры.class OrdersConsumer < Sbmt::KafkaConsumer::BaseConsumer  
  def process_message(message)  
    logger.info ""Processing message #{message.payload.id}: status:#{message.payload.status}""  
  end  
endПример консюмер-тестаRSpec.describe ""Consumer::Orders"", :pact do  
  # декларируем тип взаимодействия
  has_message_pact_between ""service-consumer"", ""service-provider""

  let(:deserializer) do  
	Sbmt::KafkaConsumer::Serialization::ProtobufDeserializer.new(  
	  # где Protobuf::Order - сгенерированный руби-класс
	  # на основе orders.proto
	  message_decoder_klass: ""Protobuf::Order""
	)  
  end

  let(:consumer) { build_consumer(OrdersConsumer.consumer_klass.new) }
  let(:order_id) { 123 }
  
  # определяем заимодействие между сервисами и матчеры запроса/ответа
  let(:interaction) do
    new_interaction
      # указываем proto-файл и название data-класса (message)
      .with_proto_class(""deps/services/orders.proto"", ""Order"")
      # определяем provider state с необходимыми метаданными
      # которые можно будет использовать позже, в момент запуска провайдер-теста
      .given(""order exists"", order_id: order_id)  
      .upon_receiving(""order via kafka"")
  	  # описываем формат данных с матчерами
      .with_proto_contents(
        id: match_any_integer(order_id),
        status: match_regex(/(PENDING|COMPLETED|CANCELED)/, ""COMPLETED"")
      )
      # описываем метаданные: топик и ключ партиционирования
      .with_metadata(
        topic: match_exactly(""orders-topic""),
        key: match_any_string
      )
  end
  
  it ""executes the pact test without errors"" do
    # запускаем тест, в этот момент под капотом конфигурируется pact-core,
    # который в параметрах блока вернет уже провалидированные payload
    # и метаданные, которые мы задали в interaction
	interaction.execute do |proto_payload, meta|  
	    # ""публикуем"" сообщение в кафку с помощью testing-инструментов
	    # sbmt-kafka_consumer (взаимодействия с реальным кафка-брокером нет)
		publish_to_sbmt_karafka(  
		    proto_payload, deserializer: deserializer,  
		    topic: meta[""topic""], key: meta[""key""]
		)

		# простой expectation для демонстрации консюминга
		expect(Rails.logger).to receive(:info).with(/Processing message/)

		# консюмим опубликованное сообщение с помощью testing-инструментов
		# гема sbmt-kafka_consumer - вызывается класс консюмера,
		# определенный выше
		consume_with_sbmt_karafka
	end
	# по результатам будет сгенерирован (локально) pact-манифест
  end
endВ целом, кафка-консюмер-тест практически не отличается от синхронных http/gRPC:декларация типа взаимодействия;определение форматов запроса-ответа с матчерами;запуск теста с обвязкойsbmt-kafka_consumer(конфигурация консюмера, десериализатора).Пример провайдер-тестаRSpec.describe ""Consumers::Kafka"", :pact do
  # аналогично - декларируем тип взаимодействия и название провайдера
  # тут указывать консюмер уже не нужно: все консюмеры определяются в рантайме,
  # по запросу в pact-брокер
  message_pact_provider ""service-provider""  

  handle_message ""order via kafka"" do |provider_state|  
    # получаем метаданные из provider state
    # и создаем в БД заказ с нужным ID
	order_id = provider_state.dig(""params"", ""order_id"")  
	order = FactoryBot.create(:order, id: order_id)

	# это специальный хелпер, позволяющий запродюсить событие
	# в mock-message-server, тем самым взаимодействуя с pact-core,
	# где будет производиться матчинг формата пейлоада
	with_pact_producer do |client|
		# client - мок-клиент sbmt-kafka_producer
		OrderProducer.new(client: client).publish(order)  
	end  
  end
endПровайдер-тест для асинхронного взаимодействия отличается от http/gRPC тем, что:вместо описания provider states (которые опциональны) тут описывается как обрабатывать каждое сообщение (в консюмер-тесте его название указывается в upon_receiving);provider state уже находится внутри и передается в параметрах блока;специальный хелпер with_pact_producer позволяет сильно упростить написание тестов, абстрагируя логику взаимодействия с pact-core иsbmt-kafka_producer.service-consumer-service-provider.json{
  ""consumer"": {
    ""name"": ""service-consumer""
  },
  ""interactions"": [
    {
      ""contents"": {
        ""content"": ""CAEQAQ=="",
        ""contentType"": ""application/protobuf;message=.protobuf.order_data.Order"",
        ""contentTypeHint"": ""BINARY"",
        ""encoded"": ""base64""
      },
      ""description"": ""async: order via kafka"",
      ""interactionMarkup"": {
        ""markup"": ""```protobuf\nmessage Order {\n    int32 id = 1;\n    enum .protobuf.order_data.Order.OrderStatus status = 2;\n}\n```\n"",
        ""markupType"": ""COMMON_MARK""
      },
      ""matchingRules"": {
        ""body"": {
          ""$.id"": {
            ""combine"": ""AND"",
            ""matchers"": [
              {
                ""match"": ""integer""
              }
            ]
          },
          ""$.status"": {
            ""combine"": ""AND"",
            ""matchers"": [
              {
                ""match"": ""regex"",
                ""regex"": ""(?-mix:(PENDING|COMPLETED|CANCELED))""
              }
            ]
          }
        },
        ""metadata"": {
          ""key"": {
            ""combine"": ""AND"",
            ""matchers"": [
              {
                ""match"": ""regex"",
                ""regex"": ""(?-mix:.*)""
              }
            ]
          },
          ""topic"": {
            ""combine"": ""AND"",
            ""matchers"": [
              {
                ""match"": ""equality""
              }
            ]
          }
        }
      },
      ""metadata"": {
        ""contentType"": ""application/protobuf;message=.protobuf.order_data.Order"",
        ""key"": ""any"",
        ""topic"": ""orders-topic""
      },
      ""pending"": false,
      ""pluginConfiguration"": {
        ""protobuf"": {
          ""descriptorKey"": ""2a5b88336a6f5a708460709e23f3c701"",
          ""message"": "".protobuf.order_data.Order""
        }
      },
      ""providerStates"": [
        {
          ""name"": ""order exists"",
          ""params"": {
            ""contentType"": ""application/protobuf;message=.protobuf.order_data.Order"",
            ""order_id"": 1
          }
        }
      ],
      ""type"": ""Asynchronous/Messages""
    }
  ],
  ""metadata"": {
    ""pactRust"": {
      ""ffi"": ""0.4.22"",
      ""models"": ""1.2.3""
    },
    ""pactSpecification"": {
      ""version"": ""4.0""
    },
    ""plugins"": [
      {
        ""configuration"": {
          ""2a5b88336a6f5a708460709e23f3c701"": {
            ""protoDescriptors"": ""Cr0BCgtvcmRlci5wcm90bxITcHJvdG9idWYub3JkZXJfZGF0YSKQAQoFT3JkZXISDgoCaWQYASABKAVSAmlkEj4KBnN0YXR1cxgCIAEoDjImLnByb3RvYnVmLm9yZGVyX2RhdGEuT3JkZXIuT3JkZXJTdGF0dXNSBnN0YXR1cyI3CgtPcmRlclN0YXR1cxILCgdQRU5ESU5HEAASDQoJQ09NUExFVEVEEAESDAoIQ0FOQ0VMRUQQAmIGcHJvdG8z"",
            ""protoFile"": ""syntax = \""proto3\"";\n\npackage protobuf.order_data;\n\nmessage Order {\n  enum OrderStatus {\n    PENDING = 0;\n    COMPLETED = 1;\n    CANCELED = 2;\n  }\n\n  int32 id = 1;\n  OrderStatus status = 2;\n}\n""
          }
        },
        ""name"": ""protobuf"",
        ""version"": ""0.5.1""
      }
    ],
    ""sbmt-pact"": {
      ""pact-ffi"": ""0.4.22""
    }
  },
  ""provider"": {
    ""name"": ""service-provider""
  }
}CI/CDНеотъемлемой частью организации контрактного тестирования является автоматизация выполнения тестов и поддержка со стороны инфраструктуры.Требования к CI/CD, которые можно выделить в нашем случае:унифицированное решение, т.к. микросервисов / тестов множество;поддержка разных стеков;минимум лишних действий для конечных пользователей (разработчиков);оптимизация времени выполнения: один большой сервис может быть консюмером для 20+ провайдер-сервисов — крайне желательно запускать каждую такую группу тестов (per provider) отдельно/параллельно и минимизировать задержки деплоя очередного релиза.Данные требования привели нас к интересному и нетривиальному решению.Был реализован специальный тулинг для CI, использующийавтогенерацию pact-пайплайнов, который:генерирует тест-джобы под каждый микросервис в рамках общего pact-пайплайна;учитывает зависимости между микросервисами;учитывает наличие консюмер/провайдер-тестов в репо сервиса;абстрагирует запуск тестов под разные стеки (например,bundle exec rspecилиgo test);предоставляет возможность при необходимости поднимать сопутствующие докер-контейнеры (например, postgres, необходимый в рамках провайдер-тестов);поддерживает работу с feature-ветками провайдеров (например, когда разработка контракта ведется параллельно в консюмере и провайдере и нужно периодически валидировать их консистентность);интегрирован с утилитойcan-i-deploy, с помощью которой на основе данных верификации в pact-брокере мы можем определить, возможен ли деплой данной версии консюмера и провайдера;позволяет использоватьconsumer version selectors и environmentsв провайдер-тестах.CI: консюмер-пайплайнСтоит добавить, что в сложных случаях, когда ведется параллельная разработка провайдера и консюмера, pipeline-builder позволяет указывать git-ветку провайдера и таким образом поддерживать целостность контрактов на всех этапах разработки зависимостей.CI: провайдер-пайплайнПровайдер-пайплайн значительно проще предыдущего — все потому, что провайдер, как владелец контракта, не зависит от потребителей.Опыт эксплуатацииМожно выделить несколько интересных моментов, с которыми мы столкнулись:Зачастую в контрактных тестах возникает желание начать тестировать бизнес-логику провайдеров и консюмеров. Это возможно, однако в контексте контрактных тестов - не совсем корректно. Основное преимущество контрактных тестов - простота и скорость их работы, фокус на форматах данных и их обратной совместимости, а также быстрая обратная связь. Тестирование бизнес-логики - отдельная задача.В сложных случаях, когда часть микросервисов имеет транзитивные зависимости, которые тоже нужно тестировать в процессе эволюции контрактов, CI-пайплайны становятся более сложными и чуть более хрупкими (например, несколько MR с зависимостями друг от друга в разных проектах). Какого-либо универсального решения этот кейс не имеет, все зависит от конкретной ситуации. Мы придерживаемся подхода: мержим сначала MR провайдеров, затем консюмеры.По умолчанию в провайдер-тестах pact-core определяет перечень консюмеров, зависимых от данного провайдера, как: “последняя версия из main-ветки, опубликованная в pact-брокере”. Это не всегда удобно, т.к. открывает широкие возможности для появления race conditions. Для решения этой проблемы мы используемconsumer version selectors- отличное решение от меинтейнеров Pact.ИтогМы рассмотрели опыт использования CDC-решения на базе Pact, реализации поддержки V3/V4-спецификаций в Ruby, а также специфику тестирования большого количества связанных микросервисов в CI.Pact оказался не просто фреймворком, а целой экосистемой, готовой к любым вызовам современной микросервисной архитектуры. С поддержкой различных языков и протоколов, он становится незаменимым союзником в борьбе за качество кода.Да, настройка CI/CD для контрактных тестов может показаться сложной, но результат определенно стоит усилий. Автоматизация и параллелизация становятся ключами к эффективному процессу, превращая потенциальный хаос в стройную систему.Ruby-разработчикам особенно приятно: даже когда официальная поддержка отстает, community всегда найдет выход. Наш опыт с sbmt-pact - яркое тому подтверждение. Это еще раз доказывает, что в мире open-source нет нерешаемых задач.А если вы уже используете контрактные тесты, поделитесь своим опытом! Ваша история может стать вдохновением для других разработчиков, делающих первые шаги в этом увлекательном мире.Ссылки:pact.iosbmt-pactsbmt-kafka_consumersbmt-kafka_producerTech-команда Купера (ex СберМаркет) ведет соцсети с новостями и анонсами. Если хочешь узнать, что под капотом высоконагруженного e-commerce, следи за нами вTelegramи наYouTube. А также слушайподкаст «Для tech и этих»от наших it-менеджеров."
СберМаркет,,,Готовим по рецепту: CI/CD в MLOps,2024-09-26T10:21:06.000Z,"Всем привет! Меня зовут Роза и я MLOps-инженер. В этой статье расскажу, как построить CI/CD-пайплайн для ML-приложений с нуля, поэтапно и без боли. Ну почти :)Я работаю в Купере — сервисе доставки из магазинов и ресторанов, где занимаюсь разработкой ML-платформы. Наши ML-сервисы проникают во все бизнес-процессы работы приложения, начиная от рекомендации бананов в корзине и заканчивая прогнозом того, через сколько приедет ваш курьер.Немного цифр: у нас 9 ML-команд, 78 сервисов, 41 человек и почти четыре сотни DAG’ов в Airflow, которые ежедневно и даже ежеминутно переобучают ML-модели.Раньше очень часто работа DS-инженера заканчивалась на подготовке кода модели в Jupyter-ноутбуке, а дальше его подхватывали команды разработки и доводили до продакшена. У такого подхода есть минусы. Например, если произойдёт инцидент, непонятно кто ответственен за сервис  — команда разработки или авторы ML-модели?К счастью, культура разработки меняется: теперь ML-инженер — это специалист, который разрабатывает свой ML-сервис на всем пути от общения с бизнесом до продакшена. Этот подход хорошо описывает принцип «you build it, you run it»: кто построил модель, тот её и запускает. Как раз в этом здорово помогает CI/CD.С чего начинаемВсе пайплайны и оптимизации ниже описаны для GitLab CI/CD, но их  достаточно легко перенести на другие фреймворки типа Jenkins. Для экспериментов были доступны 48 GitLab раннеров (4 CPU, 8Gb RAM каждый). В качестве сборщика зависимостей был выбранpoetryиз-за его гибкости и функциональности. Также все образы собираются с официального образа Python для репрезентативности, но на практике обычно в зеркалах компании собирают обогащённый образ, где не только Python, но и poetry, и всякие удобные тулкиты для работы.├── my_project
│   ├── cli
│   │   ├── ...
│   ├── my_module
│   │   ├── ...
│   ├── my_second_module
│   │   ├── __init__.py
│   │   └── print_hello.py
│   └── __init__.py
├── notebooks
│   └── my_report.ipynb
├── tests
│   └── __init__.py
├── .dockerignore
├── .gitignore
├── .gitlab-ci.yml
├── Dockerfile
├── Makefile
├── README.md
├── lint.toml
├── poetry.lock
└── pyproject.tomlДля примера возьмём типичный ML-проект. В нем есть отдельные директории для исходного кода, тестов и Jupyter-ноутбуков. Из важных «системных» файлов можно отметить.gitlab-ci.yaml— в нем как раз будет описан наш CI/CD, а такжеDockerfileдля сборки образа иMakefile(к нему вернёмся ниже). В файлахpoetry.lockиpyproject.tomlописаны зависимости проекта.Что касается зависимостей, соберём небольшой набор из популярных ML-библиотек.[tool.poetry.dependencies]
 python = ""^3.11""
 catboost = ""^1.2.5""
 click = ""8.1.7""
 clickhouse-driver = ""^0.2.7""
 clickhouse-sqlalchemy = ""^0.3.1""
 lightgbm = ""^4.3.0""
 loguru = ""0.7.2""
 numpy = ""^1.26.4""
 pandas = ""^2.2.2""
 polars = ""^0.20.25""
 prophet = ""^1.1.5""
 protobuf = ""^5.26.1""
 pyarrow = ""^16.0.0""
 pymysql = ""^1.1.0""
 requests = ""^2.31.0""
 scikit-learn = ""^1.4.2""
 sqlalchemy = ""^2.0.30»

 # Tests and Linters
 jupyter = ""1.0.0""
 mypy = ""1.7.1""
 pytest = ""7.4.3""
 pytest-cov = ""4.1.0""Базовый пайплайн, с которого мы начнём, выглядит просто и минималистично. Сначала собирается образ, затем в нем запускаются линтеры и тесты и дальше происходит деплой:В образе копируем код из раннера, указываем poetry создать окружение в текущей директории. Далее устанавливаем сам poetry и затем уже зависимости проекта. Команда вCMDвызывает напрямую скрипт из проекта.FROM your/company/hub/python:3.11

ENV POETRY_VIRTUALENVS_CREATE=true \
    POETRY_VIRTUALENVS_IN_PROJECT=true
WORKDIR /app
COPY . .

RUN pip install --no-cache-dir poetry==1.8.1 \
    && poetry install --no-root

CMD [ ""poetry"", ""run"", ""python"", ""my_project/my_second_module/print_hello.py"" ]Проблема #1: ML-инженер ждёт вечность, пока соберётся пайплайнПервая же проблема, с которой сталкивается ML-инженер в описанном выше простеньком пайплайне — это время его работы. Оно составляет4 минуты 25 секунд.Выглядит не так уж страшно, но это время легко может вырасти до десятков минут, если усложнить проект сборкой CUDA, например.Как будем измерять успех?Чтобы понимать, насколько успешно идет оптимизация, нужны метрики. Для начала это скорость (наши 4,5 минуты) и вес образов (около 3 ГБ), для которого стоит сразу обозначить нижнюю границу.Если базовый образ Python (не с тегомslim) весит около 1 ГБ и зависимости в распакованном виде весят около 1,2 ГБ, то предел, до которого можно обезжирить образ — это где-то2,2 ГБ. На эту нижнюю границу и будем ориентироваться.Но вернёмся к основной цели — бусту скорости, и попробуем ускорить сборку зависимостей.РешениеPython-окружение всегда состоит из двух частей: это само приложение и его зависимости:Код при этом меняется очень часто (его исправляет и коммитит разработчик), а зависимости пересобираются очень редко. Что можно сделать с тем, что переиспользуется очень часто, но меняется очень редко? Конечно, кэшировать!В GitLab CI/CD есть удобная фича из коробки — операторcache:, который позволяет заархивировать папку с кэшем в zip в каком-нибудь хранилище типа S3 бакета, после чего просто передавать этот архив другим джобам в пайплайне (подробнее можно прочитатьтут). При этом можно задать набор файлов, изменение которых инвалидирует кэш. Давайте попробуем воспользоваться этим оператором и добавим новую джобу кэширования зависимостей перед сборкой образа:build:deps:
   image: your/company/hub/python:3.11
   stage: build
   script:
 	- pip install --no-cache-dir poetry==1.8.1
 	- poetry config virtualenvs.in-project true
 	- poetry install --no-root
   cache:
 	key:
   	files:
      - ""poetry.lock""
      - ""pyproject.toml""	
	paths:
   	  - .venv/
   needs: [ ]Фактически в джобе делается всё то же самое, что раньше мы делали в докер-образе. Мы все так же говорим рoetry сделать окружение внутри папки и установить зависимости. Здесь важно отметить, что инвалидация кэша происходит тогда, когда мы меняемpyproject.tomlилиpoetry.lock, то есть те места, в которых эти зависимости описываются.FROM your/company/hub/python:3.11

WORKDIR /app

COPY . .

CMD [ "".venv/bin/python"", 
      ""my_project/my_second_module/print_hello.py"" ]Как изменится сборка образа? Теперь она станет совсем минималистичной: -  в ней останется только базовый образ Python и копирование кода. Однако мы помним, что в папке.venvв самой директории с кодом находится Python-окружение с зависимостями, поэтомуCOPY . .нам ещё и окружение заодно скопирует в образ. И вместо того, чтобы запускать скрипт черезpoetry runдляCMD, мы будем делать это напрямую через ванильный Python, так как poetry в образе уже не будет.Что это даст? На логе этой джобы видно, что при попытке установки зависимостей ничего не устанавливается, потому что кэш уже подключён к окружению, то есть все зависимости уже находятся в окружении. Профит!Вот так будет выглядеть пайплайн с новой джобой кэширования зависимостей:Здесь необходимо отметить, что от сборки образа теперь зависит только деплой, потому что линтеру и тестам для работы нужно только окружение и исходный код проекта.За счёт того, что теперь зависимости кэшируются, а линтеры и тесты запускаются параллельно сборке, время сократилось до 3 минут 40 секунд. Причём, несмотря на то, что целью оптимизации была скорость, в процессе получилось также уменьшить вес образов до той самой нижней границы в 2 ГБ. Это произошло, потому что в образе теперь остались только наше окружение и исходный код, когда раньше там хранился кэш poetry, сам poetry и еще кэш pip.Проблема #2: хотим, чтобы контейнер с кодом был ридонлиЗачем вообще нужны иммутабельные контейнеры? Чтобы код в нем всегда совпадал с кодом в репозитории и чтобы никто не смог зайти и поменять этот код в рантайме через простойexec.РешениеДля этого мы будем ставить код черезwhlили, другими словами, ставить проект не копипастой исходного кода, а установкой проекта через пакет в наше окружение наравне с его зависимостями.Для этого нам нужно перейти кмультистайдийной сборке, где в первой стадии собирается сам whl черезpoetry build, а во второй — он устанавливается в образ.FROM your/company/hub/python:3.11 AS builder

WORKDIR /app
COPY pyproject.toml .
COPY my_project my_project

RUN pip install --no-cache-dir poetry==1.8.1 \
 	&& poetry build

FROM your/company/hub/python:3.11 AS app-image

WORKDIR /app
ENV PATH=""/app/.venv/bin:$PATH""

COPY .venv .venv
COPY --from=builder /app/dist /app/dist

RUN python -m pip install --no-deps -v dist/*.whl \
  	&& rm -rf dist

CMD [ ""my_project"", ""--help"" ]poetry buildсохранит whl в директории./dist(по дефолту), который потом копируется черезCOPY --fromв финальный образ. А дальше необходимо просто установить его через старый-добрый pip в уже существующее окружение из прошлого шага. Не забываем удалить./dist, чтобы в образе не было лишнего.Важно отметить, что здесь явно копируется.venv(кэш с зависимостями) с раннера — это необходимо, так как мы теперь не копируем всю директорию с проектом (черезCOPY . .).И неочевидная плюшка, которая появилась в этой сборке — теперь можно вызывать CLI проекта напрямую.По метрикам у нас появился новый критерий — это безопасность. Сделали контейнер ридонли. Что интересно — сократилось время пайплайна, до2,5 минут! Это значительно, если сравнивать его с предыдущей версией и тем более с пятью минутами на старте. Что же так заметно ускорило сборку?Все дело в том самом явном копированииCOPY .venv .venv. Теперь мы копируем только конкретную директорию, а как мы помним, меняется она очень редко. За счёт этого при последующих коммитах Docker закэширует этот слой в образе, и наш гигабайт зависимостей будет реально копироваться только тогда, когда они поменяются. Отсюда такая значительная экономия времени.Проблема #3: ИБ просит удалить Jupyter в продакшен-образеТеперь давайте посмотрим на наш CI/CD с другой стороны. В какой-то момент к вам приходит ИБ (информационная безопасность) и требует убрать Jupyter из зависимостей.Казалось бы, что плохого в Jupyter? Все мы любим и знаем его, но если чуть внимательнее посмотреть, то окажется, что этотметапакеточень давно не обновлялся (аж с 2015 года на момент статьи, хотя уже вышла более свежая версия) и тянет за собой уязвимости. Поэтому Jupyter для сканеров безопасности — как красная тряпка для быка.Однако «голый» Jupyter (тот самый метапакетjupyter) скорее всего почти никто не использует: есть просто Jupyter Notebook (notebook), Jupyter Lab (jupyterlab) и кластерный Jupyter Hub (jupyterhub). Мы долго пытались понять, откуда в зависимостях протекает именно этот пакет. И оказалось, что у VS Code (IDE, которой у нас в компании пользуются почти все ML-щики) естьплагин, который упрощает работу с Jupyter, и именно он тянет этот метапакет.Убрать его полностью из зависимостей мы не можем, поскольку это означает, что придётся заставлять разработчиков ставить пакет вручную.Кто-то может спросить: а зачем вообще нужен Jupyter? Он позволяет запускать интерактивную среду для экспериментов, при этом часто с ним устанавливают какие-то дополнительные плагины, например, библиотекуtqdm. Ещё ML-инженеры очень любят рисовать графики, и, конечно же, логировать всё это в MLflow. А объединяет все эти действия то, что они относятся не к продакшену, а к этапу разработки.Поэтому на самом деле проблема шире! Она звучит так — в продакшн образе не должно быть ничего лишнего. Не должно быть тех фреймворков и тулкитов, которые нужны именно для работы на стадии экспериментов.РешениеЧто мы можем с этим сделать? Поделить наши зависимости на основные и для разработки, а ещё отдельно собирать тестовый образ.На самом деле группировать зависимости достаточно просто: в poetry эта фича идётиз коробки.…                                      …       
[tool.poetry.dependencies]             [tool.poetry.dependencies]                             
python = ""^3.11""                       python = ""^3.11""                   
catboost = ""^1.2.5""                    catboost = ""^1.2.5""                      
click = ""8.1.7""                        click = ""8.1.7""                  
…                             ==>      …    
                                          
# Tests and Linters                    [tool.poetry.group.dev.dependencies]                      
jupyter = ""1.0.0""                      jupyter = ""1.0.0""                    
mypy = ""1.7.1""                         mypy = ""1.7.1""                 
pytest = ""7.4.3""                       pytest = ""7.4.3""                   
pytest-cov = ""4.1.0”                   pytest-cov = ""4.1.0”                       
…                                      …Есть основные зависимости (main), и мы сделаем ещё одну группу и обозначим еёdevдля зависимостей на этапе разработки. Теперь можем ставить их обе по отдельности. В соответствии с группами также удваивается и количество кэшей: первый только для основных зависимостей, второй — для всех зависимостей (и основных, и dev).build:deps:main:
  image: your/company/hub/python:3.11
  stage: build
  script:
    - …
    - poetry install --no-root --only main
  cache:
    key:
      prefix: main
      files:
        - ""poetry.lock""
        - ""pyproject.toml""
    paths:
      - .venv/


build:deps:dev:
  image: your/company/hub/python:3.11
  stage: build
  script:
    - …
    - poetry install --no-root
  cache:
    key:
      prefix: dev
      files:
        - ""poetry.lock""
        - ""pyproject.toml""
    paths:
      - .venv/Заодно добавим префиксы в название архива (строки 9 и 25), чтобы можно было видеть и различать, какой из них мы подкладываем в следующие джобы.Снова смотрим на пайплайн: в процесс добавилась одна параллельная джоба со сборкой dev-зависимостей.В итоге мы избавились от уязвимых пакетов, потому что теперь кэш с dev-зависимостями подкладывается только в линтеры и тесты, где он нужен.Также появилась гибкость, потому что теперь есть такая сущность, как группа зависимостей. poetry позволяет делать столько групп, сколько нужно.Например, у нашей команды был кейс, когда мы эту фичу активно использовали при построении платформенного шаблона ML-сервиса. Были созданы четыре группы. Первые две — это как раз те зависимости, которые идут от платформы и которые пользователь не должен трогать. Они всегда должны быть, чтобы сервис работал. И аналогично есть две группы зависимостей для самих пользователей — в них он может прописывать нужные ему библиотеки, которые не покрываются платформой. Очень удобно!Все ещё проблема #3: что делать, если нужен тестовый образ?До этого мы сделали только кэш, который существовал исключительно в рамках пайплайнов в CI/CD. Теперь давайте разберёмся, что делать, если нам всё-таки нужен дополнительный образ с dev-зависимостями.Это может быть необходимо, например, когда пользователь хочет сверху дополнительные слои в образе. У нас был кейс, когда платформой поставляется базовый образ с минимально необходимым сервису тулкитом, и при этом есть возможность пользователям добавить свои слои, например, с какими-то фреймворками для дебага.РешениеЧто мы можем с этим всем сделать? Добавляем просто сборку ещё одного образа, ровно такого же, который у нас уже есть, с тем же Dockerfile, но с одним отличием: мы передаём ему не main-зависимости, а dev и main. Таким образом, мы делаем «тестовый» образ, в котором есть основное окружение и ещё dev-зависимости.По метрикам у нас ничего не поменяется, так как мы добавляем только параллельную джобу в пайплайн (скорее здесь все упирается в производительность вашего Container Registry):В итоге, у нас все ещё один Dockerfile, но с него билдится два образа. Теперь попробуем оптимизировать общую часть, а именно сборку whl, чтобы не выполнять её два раза. Вынесем её в отдельную джобу и облегчим немного наш докер: эта сборка будет запускаться только один раз.Чуть-чуть схитрим и вынесем сборку whl в уже существующую джобу сборки кэша зависимостей. Более правильно сделать отдельную джобу, но таким образом мы сэкономим время на накладные расходы при выполнении.build:deps:main:
   image: your/company/hub/python:3.11
   stage: build
   script:
 	- …
 	- poetry build  # собирает whl в dist/
   cache:
 	key:
   	prefix: main
   	files:
     	- ""poetry.lock""
     	- ""pyproject.toml""
 	paths:
   	- .venv/
  artifacts:
 	paths:
   	- dist/*.whlДля этого воспользуемся операторомartifact:в Gitlab, который позволит нам выгрузить whl на сервер Gitlab и аналогично кэшу передать его в следующие джобы (подробнее можно почитатьтут).Таким образом наш Dockerfile стал короче:ARG PYTHON_VERSION=""3.11""
FROM your/company/hub/python:3.11

WORKDIR /app
ENV PATH=""/app/.venv/bin:$PATH""
ENV VIRTUAL_ENV=""/app/.venv""

COPY .venv .venv
COPY dist dist

RUN python -m pip install --no-deps -v dist/*.whl \
    && rm -rf dist

CMD [ ""my_project"", ""--help"" ]В нем изчезла первая стадия, и вместо нее мы копируем с раннера директорию./dist, в которой лежит наш артефакт в виде whl пакета.Внесла ли эта оптимизация изменения?На самом деле небольшие — всего на 15 секунд уменьшили наше время. Однако мы получили кастомный образ для тестирования, плюс возможность гибко собирать и настраивать больше дополнительных образов. Также мы добились, что наш whl собирается для всех образов всего один раз.Проблема #4: несколько ML-инженеров обновляют версию в своих MR (или не обновляют)Очень часто случается так, что несколько ML-инженеров, работая в одном проекте, в своих мердж-реквестах меняют версию вpyproject.toml. И также часто случается так, что они эту версию не меняют. Почему и то, и то – проблема?Без обновления версии приложение никак не версионируется, в нем всегда одна и та же версия, несмотря на то, что оно изменяется. А если версию команда всё-таки обновляет – это делается руками. Такой подход приводит к тому, что в реквестах происходят конфликты, которые требуют ручного вмешательства. Другими словами: если один инженер сумел свой реквест задеплоить, то второму придётся этот локальный конфликт как-то решать.А усугубляет проблему то, что у нас есть на самом деле несколько версий:версия вpyproject.tomlверсия в самом репозитории (git tag)версия приложения, с которой оно публикуется в репозиторий (PyPi или внутреннее зеркало типа Nexus)Нам нужно все эти версии сделать сквозными, к тому же добавить фичу автоматического обновления.РешениеВ этом нам поможет опен-сорс проектgitlab-semantic-versioning. Это достаточно простой Python-скрипт, который позволяет автоматически обновлять git тег проекта согласноsemverнотации. Разработчик проставляет в реквесте соответствующий лейбл версии, которую он хочет обновить (major,minorилиpatch), а скрипт берёт текущий тег, обновляет его и пушит в репозиторий.Таким образом мы решили две проблемы из трёх: теперь версии обновляются автоматически. Осталось git тег пролить вpyproject.toml. Решение этой проблемы автоматически прольет нам эту же версию и в Nexus, потому что poetry при публикации как раз использует версию изpyproject.toml.Как это сделать?Сначала добавим основную джобу с бампом версии: возьмём готовый образ с указанными выше скриптом и сохраним полученный git тег в виде артефакта (операторdotenv:, подробнеетут). Теперь добавим вторую джобу, которая проливает этот тег в poetry черезpoetry versionи заодно коммитит это изменение в репозиторий.version:bump:
  stage: version
  image: your/company/hub/gitlab-semantic-versioning:1.1.0
  script:
    - printf ""GIT_TAG="" > bump.env
    - python3 /version-update/version-update.py >> bump.env
   …
   artifacts:
    reports:
      dotenv: bump.env

version:publish:
  stage: version
  image: your/company/hub/git/image:latest
  script:
    - ...
    - git checkout -b $CI_DEFAULT_BRANCH
    - poetry version -vv ""$GIT_TAG""
    - git add -A
    - git commit -m ""Version $GIT_TAG""
    - git push origin
  needs:
    - ""version:bump""  # takes $GIT_TAG from artifactВ целом, такой набор джоб уже обеспечит автоматическую сквозную версию. Но часто в командах есть какой-то уже устоявшийся релизный цикл выпуска версий. Вы можете легко адаптировать эти джобы конкретно под ваши нужды. Выше приведён минимальный сетап.Проблема #5: ML-инженер не хочет ждать CI/CD, чтобы прогнать линтеры и тестыМы настроили супер-пупер CI/CD пайплайн, в котором оптимизировали самые узкие бутылочные горлышки и добавили много фичей. Давайте теперь вернёмся к нашему ML-инженеру.Стандартный флоу его работы состоит в том, что он пишет какой-то код, пушит его в репозитории и с тревогой смотрит на пайплайн этого CI/CD, не окрасилось ли там что-то красным. Если всё зелёное, значит, всё супер и можно спокойно выкатывать это на ревью и деплоить свой реквест.Но если что-то окрасилось красным, значит что-то упало и нужно править код. А это опять пушить, опять ждать 2–3 минуты, опять смотреть, снова править, пушить, ждать, и так по кругу, много раз, пока все не будет зелёным.Две-три минуты — это терпимо, если проблема решается сразу. Но когда ошибка сложная и фидбек на исправления в коде приходится ждать несколько раз, процесс ожидания может здорово утомлять.РешениеЧто можно сделать? Можно и дальше пробовать оптимизировать пайплайны, но хочется, чтобы инженер мог ещё до коммита понимать, проходит ли его код линтеры и тесты. Этого можно добиться, добавив в репозиторий старый-добрыйMakefile..EXPORT_ALL_VARIABLES:

show-version:
     poetry version
tests:
     echo ""Run tests.""
     poetry run pytest tests
mypy:
     echo ""Run mypy checks.""
     poetry run mypy --config-file ../lint.toml

lint: mypy  ## Start all lintersОбычно Makefile — это что-то из мира C/C++. Но и здесь его очень удобно применять, потому что он решает несколько проблем.Первое — в нём мы можем имитировать весь CI/CD. Если прописать в нем линтеры, тесты, сборку нужного нам окружения, чтобы проверить какую-то фичу, то все эти команды будут абсолютно идентичны нашему CI/CD, то есть они будут запускаться локально.Вторая проблема, которую решает Makefile — унификация локальной работы с ML-приложением. Если раньше каждая команда могла придумать свой велосипед, а потом в ступоре сказать: «Ой, у меня локально всё работает, а что-то все пайплайны красные, помогите!», то с Makefile такой проблемы нет. Все используют один и тот же Makefile, в нём запускаются какие-то стандартные линтеры (для всех одинаковые), всё становится более унифицированным и приятным для поддержки.Ну и неочевидная плюшка: если к вам в команду приходит новичок, ему не нужно шерстить Confluence и README проектов, искать команды в попытках запустить приложение. Все это в удобной форме хранится в Makefile.Ну и финальные результаты наших метрик:А что насчёт деплоя?Давайте посмотрим, чем ещё можно помочь ML-инженеру, ведь помимо того, что он пушит код в репозитории, ему приходится делать ещё много разной сложной работы до и после коммита.Чтобы обучать модель на регулярной основе, ML-инженер должен прогнать свои в даги в Airflow или аналогичном оркестраторе. Также ему часто нужно распределённо посчитать какой-нибудь большой датасет или сделать сэмпл данных, например, через Spark или Trino. Или даже запустить свои dbt-модели для обработки данных. Ну и конечно для воспроизводимости нужно логировать свои эксперименты с моделями в MLflow. А если это онлайн-сервис, то добавляется получение фичей из online фичастора.РешениеНа самом деле здесь сложно дать универсальный рецепт. Ниже будет приведён пример, как именно наша команда справилась с такими требованиями, но в реальности все очень сильно зависит от конкретных ML-команд, которые приходят с такими запросами.Мы пошли по пути стейджинг-окружения on demand, где под on demand я подразумеваю какую-нибудь зелёную кнопку в GitLab, которую ML-инженер нажимает, и у него в Kubernetes разворачивается неймспейс, в котором есть все эти инструменты.Таким образом весь флоу его работы от и до поднимается в рамках одного неймспейса: поднимается маленький Airflow, к нему рядом Jupyter, чтобы он мог легко и быстро данные считать, тут же Spark или spark-operator, MLflow для экспериментов. И в этом же неймспейсе и само приложение.Получается такой изолированный небольшой контур, который работает только для одного разработчика, поднимается из конкретного окружения, например, из мердж-реквеста, и в нем разработчик может протестировать всё, что ему нужно: свои фичи, свои модели.Такой подход ведет к тому, что когда мы выкатываем сервис на продакшн, ML-инженер уверен, что его код в идентичной среде будет работать ожидаемо. Конечно, не всегда можно построить окружение, один в один повторяющее условия продакшена. Однако по возможности нужно повторить большую часть инфраструктурных элементов, чтобы в будущем было меньше головной боли с тем, как же проверить, почему локально все летает, а в продакшене падает.ИтогоКоротко подведём итог и зафиксируем решения, которые мы исследовали в статье.собирать зависимости только тогда, когда они меняются;ставить python-приложение через whl;делать продакшен-образ чистым и легковесным;делать сквозное автоматическое версионирование;дать возможность ML-инженерам работать локально as is в CI/CD;дать возможность ML-инженерам на всех этапах разработки быстро тестировать свою работу.Все эти «рецепты» наша команда нашла и сформулировала «в бою», и будем надеяться, что они помогут кому-то еще облегчить жизнь ML-инженерам :)Псс, подписывайся на tg-канал ML-команды КупераML Доставляет."
СберМаркет,,,Алгоритм управления доставкой по расписанию и динамичесий прайсинг. Как мы сделали это в Купере,2024-09-06T11:51:52.000Z,"Привет! Меня зовут Юрий Беляков, я старший ML-инженер в Купере. Сегодня предлагаю вместе разобраться, что такое плановая доставка и как устроен алгоритм управления слотами в нашем сервисе. Обсудим, как проходило тестирование и масштабирование от одного магазина до всех гипермаркетов, на какие грабли мы наступили и как реализовали динамическое ценообразование на этой базе.Этот проект и эту статью мы делали вместе с моим коллегой старшим ML-инженером Даней Богдановым.Основы основ. Что такое плановая доставкаКупер — это онлайн-сервис доставки из магазинов и ресторанов. По сути, мы объединяем:пользователей, оформляющих заказы;сборщиков заказов и курьеров;магазины-ритейлеры и рестораны;бренды, которые продвигают свою продукцию на нашей платформе.Мы предоставляем два типа доставки:быстрая— когда клиент хочет, чтобы заказ приехал к нему как можно быстрее;плановая— заказ несрочный, клиент выбирает удобное время, чтобы принять его.Плановая доставка — это в первую очередь доставка из гипермаркетов, таких как METRO и Ашан. У каждого магазина своя территория доставки, поделенная на зоны. Курьеры прикреплены к конкретному магазину и развозят заказы только оттуда. В приложении у сборщиков и курьеров видны смены, которые они могут занимать, продлевать и отменять в течение дня.Важный термин, когда мы говорим о плановой доставке — «слот». Это временной интервал, в который мы обещаем привезти заказ. Как правило, у каждой зоны доставки свой набор слотов. Слоты имеют разную длину, пересекаются друг с другом, а кроме того — проставлены не только на сегодня, но и на семь дней вперед. Помимо слотов доставки есть слоты самовывоза — когда клиент может забрать уже собранный заказ из магазина самостоятельно.Ещё одна важная особенность плановой доставки — все заказы разводятся большими рейсами, в среднем по восемь-десять заказов, на автомобиле. Рейсы обычно строит маршрутизатор.Путь заказа: от оформления до передачи клиентуКлиент выбирает слот доставки и оформляет заказ.Примерно за пять часов до начала слота заказ попадает вдиспатч— это алгоритм, который в соответствии с приоритетом назначает заказы на сборщиков. Сначала собираются заказы более ранних слотов.Пока заказ в сборке, маршрутизатор подбирает рейс для него.По окончании сборки заказ ожидает доставки в соответствии с очередью — первыми доставляются заказы на более раннее время.Когда очередь подходит, заказ вместе с другими заказами своего рейса грузится в автомобиль и отправляется в доставку. Здесь очередь зависит уже от адресов получателей и оптимального маршрута.У слота плановой доставки есть несколько атрибутов.Время попадания в диспатч— это нижняя граница того, когда заказ может начать собираться.Дедлайн сборки— время, когда заказ должен быть готов к доставке, чтобы не было риска опоздания.Сборка происходит всегда до начала слота, и доставка обычно тоже, потому что еще необходимо заложить время на погрузку всех заказов в автомобиль и путь курьера от магазина до клиента.В чем же трудности?Как минимум в том, что слотами плановой доставки нужно управлять!Если мы понимаем, что больше не готовы принимать заказы в конкретный слот, его нужно закрыть. Если закроем его раньше, чем нужно, то потеряем часть заказов. Если позже — это приведет к большому количеству опозданий или даже отмен.Для того, чтобы слоты закрывались «вовремя», и нужен алгоритм, который сможет контролировать нагрузку слотов в режиме реального времени, в том числе ночью.Уже на старте проработки такого алгоритма мы столкнулись с несколькими проблемами.На исторических данных нельзя было понять, какие действия «правильные»— размеченной выборки не было. Мы не могли сказать, что было бы, если бы в слот упал еще один заказ, или если бы слот был закрыт чуть раньше. Чтобы получить ответы, нам пришлось бы переместиться в параллельную вселенную, где события развернулись по-другому.Мы постоянно работаем с контекстом постоянно меняющейся информации. Поступают новые заказы, у сборщиков и курьеров меняются смены. Алгоритм должен принимать решения именно на основе той информации, которая есть в настоящий момент времени. Это серьезно усложняет интерпретацию и вообще процесс аналитики.Когда мы начали искать информацию, мы поняли, чтоподобных алгоритмов на рынке просто нет. Нам неоткуда было брать идеи, и мы все придумывали с нуля. Поэтому я и делюсь кейсом в этой статье :)Плановая доставка — очень хрупкая сущность. Когда заказы начинают опаздывать в первом слоте, это чаще всего приводит к опозданиям в последующих. Как на приеме у врача: один пациент задерживается, и вся очередь сдвигается.Суть алгоритмаОсновная идея нашей разработки на самом деле несложная. В каждый момент времени, когда работает алгоритм, мы берем самую актуальную информацию о еще не исполненных заказах на точке и прогнозируем время сборки и доставки для каждого из них. Также мы берем самую актуальную информацию о сменах сборщиков и курьеров и о слотах доставки. На основании всего этого мы выстраиваем очередь исполнения заказов.Скажем, в одном слоте лежит два заказа. У каждого заказа есть свои атрибуты, и с помощью какой-нибудь базовой модели типа градиентного бустинга мы получаем прогноз по времени сборки для каждого из них — например, 32 и 23 минуты. Далее мы суммируем прогнозируемое время и получаем суммарное время сборки всех заказов в конкретный слот. С процессом доставки то же самое.Здесь возникают новые нюансы:Мы не знаем заранее, кто будет собирать и отвозить заказ. Опытные сборщики и курьеры работают ловчее, чем новички. Мы пытаемся учитывать общую производительность смены на точке.Сборщик может одновременно работать с двумя заказами. Какие заказы будут собираться параллельно тоже доподлинно неизвестно. Чтобы это учитывать, мы пользуемся различными эвристиками.Нам важно в режиме реального времени учитывать статус заказа, когда он находится в сборке. На основании того, какое количество позиций уже собрано, можно корректировать прогнозное время.Время доставки сильно зависит от маршрута,в рамках которого поедет заказ, но маршруты заранее не известны. Поэтому мы используем для расчета среднеисторический маршрут для данной зоны доставки. Когда маршрут уже построен — берем данные из маршрутизатора.Помимо самой доставкинеобходимо иметь в виду время на погрузку заказови на возвращение курьера из рейса на точку.Все это — кирпичики, из которых складывается наш алгоритм.Разберем, как строится очередь, на простом примере.В магазине один курьер и один сборщик. У нас есть два двухчасовых слота: 11:00-13:00 и 12:00-14:00. Сборщик выходит на смену в 9:00. Мы знаем, что в первом слоте лежит три заказа с суммарным временем сборки полтора часа, а значит — заказы первого слота будут собираться до 10:30. Второй слот начнет собираться после первого, и если там два заказа с суммарным временем сборки один час, то сборка заказов второго слота продлится 11:30. С доставкой все так же, с одной ремаркой: сборка не зависит от доставки, а вот доставка завязана на сборке и не может начаться до того, как будут собраны заказы рейса.В реальности может быть не два слота, а сотняВ нашем примере в первом слоте больше заказов, то есть суммарное время их доставки тоже больше. Несмотря на это, ближе к дедлайну будут развезены заказы второго слота. Вероятность опоздания во втором слоте выше. Поэтому важно учитывать нагрузку всей очереди заказов.Нагрузка на слотДля приведения нагрузки к числовому виду мы придумали метрику, которая показывает вероятность опозданий, и назвали ее surge-level.У каждого слота есть левая граница — по сути, минимальное время начала погрузки. Также мы можем вытащить ожидаемое время окончания доставки для слота. Берем разность между этими двумя моментами времени и получаем числитель (на картинке ниже — два часа и пять минут).В знаменателе мы вычитаем время начала погрузки не из ожидаемого времени окончания доставки, а из правой границы слота (на картинке — два с половиной часа).Дальше делим числитель на знаменатель — и получаем метрику нагрузки.Если прогнозируется опоздание на конкретный слот и время окончания доставки выходит за правую границу слота, surge-level будет больше единицы. А если он больше единицы, нужно закрыть слот.Разумеется, есть нюансы. И нам нужно учитывать контекст работы с очередями. Здесь для визуализации я привлеку разноцветных человечков.Сначала должны стоять зеленые, потом желтые, потом красные человечки. Наша задача — чтобы все человечки находились до линии своего цвета: зеленые до зеленой линии, желтые до желтой, красные до красной. Мы видим, что красный цвет на пределе: любой красный человечек, который придет в очередь, будет находиться за своей линией, и тогда мы проиграем. Поможет ли то, что мы запретим красным приходить в очередь? Нет, блокировать нужно все цвета.Думаю, аналогия ясна: человечки — это заказы, а цветные линии — слоты. Если у нас есть проблемный слот с опозданием, то блокировать нужно не только его, но и все слоты до него.Подытоживая все вышесказанное, флоу алгоритма выглядит так:Берем самые актуальные данные о заказе и точке.Прогнозируем время сборки и доставки каждого заказа.Выстраиваем, исходя из прогноза, очередь сборки и доставки.Понимая очередь, рассчитываем surge-level.Зная surge-level и учитывая контекст очереди, принимаем решение, что делать со слотом: закрывать или открывать.Далее действия становятся видны пользователю: некоторые слоты закрываются, а некоторые могут открыться по новой.Поехали тестироватьИзначально алгоритм представлял собой DAG в Airflow. Он прогонял весь флоу от начала до конца раз в пять минут. Эффекта памяти предусмотрено не было. Каждый раз алгоритм выстраивал очередь с нуля, поскольку одно маленькое изменение влияет на всю последующую очередь.Мы не использовали А/В-тесты или свитч-тесты, просто сразу проверяли алгоритм в полевых условиях. Проводить тестирование решили на регионах, потому что в столице работа точек более напряженная и потому что масштабировать алгоритм тоже планировали по регионам. Первой была Рязань.Мы создали Telegram-чаты, в которые добавили логистов, супервайзеров и сити-менеджеров наших ритейлеров. Там участники тестирования задавали вопросы, писали о недочетах, просили экстренно вмешаться и что-то починить, если, например, по их мнению алгоритм беспричинно закрыл все слоты.Все основные изменения в алгоритм вносились на основании фидбека от супервайзеров и сити-менеджеров. Получился Reinforcement Learning разработчиков with Human Feedback от супервайзеров.По каким метрикам мы оценивали эффективность?Количество заказов, процент опозданий и доступность. Под доступностью имеется в виду время до ближайшего слота, на который пользователь может оформить заказ.Изначально мы управляли слотами только на четыре часа вперед, чтобы избежать неприятных инцидентов (например, с закрытием точки на всю неделю вперед). По мере роста нашей уверенности количество управляемых нами слотов тоже увеличивалось.Первая проблема открылась очень быстро. Сурдж управлял слотами опираясь только на себя и свои расчеты. Соответственно, если сотрудники вручную вносили изменения, то при новом запуске (через пять минут) эти изменения слетали и алгоритм делал со слотами то, что он сам считал нужным. При этом возможность вмешиваться в процесс вручную жизненно необходима из-за различных возможных обстоятельств. Поэтому мы создали обвязку, чтобы алгоритм просматривал изменения, которые мануально вносились по слоту, и отдавал им высший приоритет.Со второй проблемой мы столкнулись,когда расширили горизонт работы алгоритма до суток, то есть когда он уже умел управлять слотами в том числе на завтра. Оказалось, что сборщики и курьеры не слишком заранее проставляют смены на последующий день: иногда это происходит утром, за час до выхода на работу. Если на завтра уже есть много заказов, но смен нет, алгоритм думает, что заказы просто некому будет собирать и доставлять, а значит — надо закрыть все слоты. Чтобы это решить, мы настроили модель, которая прогнозирует количество сборщиков и курьеров. Утром происходит переход с прогноза на фактические данные о проставленных сменах.Третья интересная проблема— алгоритм плохо работал при дисбалансе партнеров (например, когда курьеров, было сильно больше, чем сборщиков). Для решения этой проблемы мы разделили показатель нагрузки. Раньше это был один surge-level, теперь показателя два:  по одному на сборку и доставку. Все действия алгоритма основаны на наибольшем из двух показателей.Кроме того, каждая из тысячи точек обладает целым рядом уникальных особенностей. Чтобы алгоритм подстраивался под них, мы настроили простой механизм адаптации параметров его работы, по духу похожий на Reinforcement Learning.Динамическое ценообразованиеРазобравшись с управлением слотов плановой доставки, мы перешли ко внедрению динамического ценообразования на базе того же алгоритма.Но прежде всего давайте разберемся, зачем нужны наценки. Они позволяют более плавно распределять спрос из более нагруженных слотов в менее нагруженные, и при этом не просаживать доступность. Без наценок мы можем управлять спросом, только закрывая слоты, а это доступности не способствует. При равномерном распределении заказов точка способна качественно отработать больше заказов.Первая версия алгоритма динамического ценообразования выглядела как лесенка. Surge-level на 0,7 — включается первый уровень наценки, на 0,8 — второй уровень, на 0,9 — третий уровень. Дальше слот закрывается.Быстро сделали лесенку и начали тестирование. Сразу хорошо не получилось, потому что нам вновь нужно было учитывать контекст очереди.Вернемся к разноцветным человечкам.Зеленые и красные человечки находятся близко к своему дедлайну, то есть у них высокий показатель нагрузки и высокая наценка. Желтые человечки далеко от дедлайна, и у них низкий показатель загрузки и нулевая наценка. Но суммарная нагрузка на все три цвета высокая! Если спрос будет направлен на желтый слот, туда неистово начнут падать заказы, желтые человечки сдвинут красных, и мы будем вынуждены закрыть все три слота и пожертвовать доступностью.Теперь мы используем скорректированный показатель нагрузки на слот, который представляет собой максимум из нагрузок текущего и будущих слотов. При таком подходе желтый и красный слот имеют одинаковые показатели нагрузки, а соответственно, одинаковую наценку. Спрос идет в будущие, менее нагруженные слоты.РефлексируемВ мае 2023 года мы развернулись в одном магазине в Рязани и быстро масштабировались на весь город. В июне присоединились Волгоград и Челябинск. Уже в июле алгоритм работал во всех городах-миллионников! И хотя изначально планировалось, что алгоритм будет лишь помогать людям управлять слотами, в августе было автоматизировано управление 99% слотов. За четыре месяца, к сентябрю, мы выросли до динамического ценообразования, а к концу сентября внесли правки и успешно масштабировались до всех гипермаркетов страны.На достигнутом мы не останавливаемся, есть несколько идей для развития — в том числе почерпнутые из чатов тестирования. Например, surge-level сейчас предпринимает действия, когда все заказы уже сделаны, а мы хотим, чтобы он предвидел повышенный спрос. Также в планах поиск оптимальных наценок через поюзерные A/B-тесты. Самое сложное и масштабное — это полный переход на рельсы обучения с подкреплением. Будем надеяться, что получится!В комментариях вы можете пожелать удачи и задать вопросы о деталях нашего алгоритма :)Псс, подписывайся на tg-канал ML-команды СберМаркетаML Доставляет."
СберМаркет,,,Выжить в IT: Уровень сложности — СДВГ,2024-08-30T12:25:23.000Z,"Наверняка вы за собой замечали, что иногда какую-то задачу настолько не хочется делать, что курсор мыши сам выпрыгивает из IDE, а пальцы отбивают на клавиатуре «YouTube — шортсы с котами». Возможно, вы пытались разобраться, почему же так?А вы точно пытались — на Хабре написано аж 39 (!) страниц статей по запросу «прокрастинация».Меня зовут Арина, я работаю продуктовым аналитиком в Купере. Поделюсь тем, как сама боролась с прокрастинацией, а в результате узнала о СДВГ, получила диагноз, полюбила его и научилась с ним жить. Расскажу об СДВГ в целом и о своем опыте жизни в IT с таким мозгом.С помощью поддержки и просвещения можно прийти к пониманию, что у СДВГ помимо минусов так же много и плюсов, и в мире много успешных людей с этим диагнозом. Например, чемпион мира по плаванию Майкл Фелпс, основатель IKEA Ингвар Кампрад, основатель корпорации Virgin Group Ричард Брэнсон, основатель JetBlue Девид Нилеман, режиссерка «Барби» Грета Гервиг, актеры Том Круз, Орландо Блум, Зоуи Дешанель, Эмма Уотсон и еще многие другие люди с этим диагнозом живут и строят успешные карьеры.Многие из них, если речь заходила про СДВГ, говорили, что в конечном итоге это стало их сильной стороной. Ведь на самом деле это не про отсутствие самодисциплины или лень, а просто про другой механизм работы мозга. И если знать, что с этим делать, то все сложности можно нивелировать или даже обратить себе во благо.Эту статью я пишу для всех, кто испытывает сложности с прокрастинацией и отвлекаемостью. Методики, о которых я расскажу точно могут быть полезны и для вас, даже если ваш случай не такой клинический.Как работает СДВГ: сложности на работе и не толькоКак диагностируют и лечат СДВГЗаметки из терапии: методы, которые помогли мне бороться с прокрастинациейОбратная сторона: какие сильне стороны у СДВГ и как они помогают в ITПолезные ссылкиКак работает СДВГСДВГ— это неврологическое расстройство. Согласно Международной классификации болезней (МКБ), его классические симптомы — сильная рассеянность, импульсивность и, иногда, гиперактивность.У людей с СДВГ нарушен механизм регуляции дофамина лобной долей мозга: если представить способность регулировать внимание в виде шкалы от «витать в облаках», до «суперконцентрация, нет ничего кроме текущей задачи», то у обычного человека есть способность контролировать свое перемещение по ней. А для человека с СДВГ это крайне сложно. Наша лобная доля очень «ленива» и дает нам дофамин лишь в двух случаях: высокий интерес к занятию (пряник) или высокий стресс (кнут).На иллюстрации для этой статьи меня вдохновил блогhttps://adhdoers.com/Покажу на примере — следите за руками:Алиса — без СДВГАлиса увидела на полу перекати-поле из пыли.Алиса сделала уборку.Мозг Алисы выделил немного дофамина, чтобы закрепить положительный результат.Через две недели Алиса снова сделала уборку: она знает, что после уборки ее мозг даст ей немного дофамина и она испытает приятное ощущение завершенности.В квартире Алисы всегда прибрано — никакого стресса в жизни Алисы нет.Боб — у него СДВГБоб увидел на полу перекати поле из пыли.Боб сделал уборку — мозг Боба не дал ему дофамин.Через две недели Боб снова увидел пыль, но он не может заставить себя сделать уборку, потому что понимает, что ничего, кроме физической усталости, за это не получит.Через еще две недели перекати поле на столе превратилось в таракана под раковиной. Мозг Боба выработал адреналин и Боб, наконец-то, прибрался.В следующий раз Боб снова не захочет убираться, но заставит себя (мозг Боба снова выделит адреналин), потому что испытывает стресс от потенциального возвращения тараканов.Квартира Боба чистая, но каждые две недели он сильно нервничает.Конечно, все мы иногда Боб. Но разница между Алисой и Бобом в том, что у него вся жизнь такая. Ему сложно справляться с регулярной уборкой, раз в неделю он теряет очки и каждый дейли забывает подвинуть тикеты.C какими проблемами сталкивается Боб на работе?Без внешних стимулов Бобу очень сложно взять себя в руки. Если у задачи нет дедлайна или к Бобу не пришел менеджер с напоминанием, задача скорее всего не будет выполнена.Егоотвлекает буквально всё: держать фокус во время созвонов Бобу ужасно сложно. Если во время мита он не разговаривал, а слушал, то после частенько понимает, что не запомнил буквально ничего. Это может усугубляться, если мит был с камерой и включенным микрофоном: большая часть оперативки его мозга съедается тем, что нужно следить не только за разговором, но и стараться не ерзать и не крутить ничего в руках (люди считают, что в этом случае Боб их не слушает, но на самом деле как раз наоборот) .Также могут страдать и задачи. Если что-то обсудили только на словах, но не завели тикет, то Боб тожечасто про них забывает. Из-за страха забыть даже собственные идеи во время разговоров Боб часто перебивает своих собеседников. Из-за этого страдает коммуникация в команде, отношение коллег к Бобу становится хуже.Если Боб смог «поймать»гиперфокус, то он будет очень продуктивно работать весь день и скорее всего за день закроет тикет, рассчитанный на неделю. Но ближе к вечеру он даже не заметит, как ему стало очень плохо –он так заработался, что забыл пообедать или сходить в туалет.И каждый раз, когда Боб пытается поделиться своими проблемами, он слышит: «У меня точно так же, надо просто стараться лучше!»Если подобные проблемы возникли с вами недавно, например, при переходе в новую команду или компанию, стоит искать ответ в чем-то на поверхности: вы устали, задачи оказались неинтересными или личные проблемы вытягивают из вас всю энергию.Но если так было в школе, универе и на каждом месте работы, где вы успели попробовать себя, закономерно задуматься, что проблема находится более глубоко — в вашей голове.Как диагностируют СДВГСДВГ — это диагноз, которыйможет быть поставлен только врачом-психиатром. Если вы подозреваете у себя его наличие, лучше обратиться к специалисту, чтобы подтвердить или опровергнуть эту гипотезу.СДВГ —  сложная штука. Сейчас ученые сходятся на том, что постановка диагноза требует предварительного анализаинтенсивности и регулярностисимптомов. Иначе говоря, если вы раз в год теряете очки, то все нормально, но если это повторяется так часто, что значительно влияет на вашу жизнь, то это повод пойти к специалисту.Мой путь был долгим: многие врачи в России считают «взросление» единственным лечением от СДВГ — долгое время этот диагноз ставили только детям и считалось, что со временем они перерастают свои симптомы. Поэтому врачей, которые специализируются на диагностике СДВГ и работают со взрослыми, довольно мало.Приема мне пришлось ждать четыре недели. Я очень боялась что-то забыть и заранее выписала все «семейные мемы», которые могли быть симптомами еще в детстве. Их список с краткими пояснениями занял две страницы.Разговор с психиатром начался с вопроса: «Когда, по вашему, у вас это началось?». После рассказа о проблемах с домашним заданием в школе, институте, постоянной потере и порче вещей из-за невнимательности и многом другом врач уточнил лишь несколько нюансов из моей медкарты — вердикт был однозначный.Как лечат СДВГОбщемировая практика терапии СДВГ — это стимуляторы различных групп, а также разговорная терапия с психологом или психотерапевтом. В РФ и других странах СНГ сейчас доступен только один препарат от СДВГ – Страттера (атомоксетин). Но он дорогой даже с айтишными зарплатами :) и подходит далеко не всем. Лично мне данный препарат не подошел и я сосредоточилась на терапии с психологом.Чаще всего терапевты, которые работают с СДВГ, используют техникикогнитивно поведенческой терапии(КПТ).Согласно последним исследованиямКПТ значительно улучшает качество жизни людей с СДВГ — по сути этот метод помогает создавать и закреплять новые, более полезные и эффективные привычки с учетом особенностей этого диагноза. В рамках КТП пациенты пытаются установить взаимодействие между своими мыслями, чувствами и действиями, а также осваивают навыки, которые позволяют им справляться с трудностями.Полтора года назад я начала посещать КПТ-терапевта и, по моему опыту, это и правда помогает!Как КПТ помогло Бобу (и мне)Итак, Боб устал постоянно стрессовать из-за вещей, которые кажутся обыденными для его кружения, обратился к врачу и получил диагноз. Боб решил начать заниматься с психотерапевтом, и посмотреть как это повлияет на его жизнь.На первой сессии Боб рассказал промаленькие однотипные задачи, которые раз в пару дней скидывает ему один из менеджеров. Поток этих задач кажется бесконечными, Боб часто забывает про них. Когда таких задач накапливается много, ему пишет недовольный менеджер, Боб стрессует и садится делать их поздно вечером.Терапевт выслушал Боба и предложил попробовать сделать из скучных адхоков игру. Теперь сразу после того, как менеджр пишет с очередной задачкой, Боб создает напоминание в календаре, вносит туда все вводные, а сев за задачу в назначенное время, включает таймер и старается уложиться в отведенный слот. Если у него получилось не отвлечься и сделать все за обозначенное время, Боб вознаградит себя чем-то, что дает ему дофамин — 15 минут полистает шортсы с котами, съест десерт или закажет кофе в ближайшей кофейне по дороге домой.Эта идея воодушевила Боба, он поставил себе на телефон красивое приложение стаймером-помидоркой, написал список потенциальных вознаграждений. Когда к нему пришел менеджер, он сразу ответил ему, завел слот в календаре, вечером быстро все сделал и пошел пить свой заветный сладкий кофе.Через месяц интерес к новому процессу стал спадать, но тревожность по поводу этого типа задач уменьшилась — появилась отработанная система. Бобу стало проще работать, он перестал засиживаться по вечерам, а менеджер на перфревью оценил работу Боба выше, чем раньше. Все счастливы.Терапевт помог Бобу пересмотреть его рутину и привычки, понимая особенности его мозга:добавить элементы поддержки(слот в календаре и заметки о задаче в моменте) и игры (закончить до истечения таймера) там, где Бобу было слишком скучно и тревожно, чтобы начать;добавить внешнюю стимуляцию(любимая музыка и награда) для вещей, от которых легко отвлечься и где нужна мотивация для продолжения.В этой истории Боб — это я полтора года назад. Всю жизнь, я сталкивалась с подобными проблемами, но моих усилий не хватало, чтобы их решить, и они появлялись снова и снова. После постановки диагноза я стала смотреть на эти ситуации по-другому. Например, признала, что мне сложно делать рутинные задачи в срок. Если не признать это как проблему, не получится подобрать механизмы решения — вместо этого вы просто замучаете себя чувствам вины и стыда.При этом диагноз не является оправданием, он просто делает эти проблемы видимыми и помогает найти варианты решения. Он объясняет, почему бывает сложно делать те или иные вещи. Это не значит, что ко мне предъявляются другие требования, а лишь то, что у меня другие механизмы их достижения.Мои заметки из терапии: первая помощьПрекратите пытаться стать человеком, которым якобы должны (например, образцовым студентом или супер организованным руководителем) иразрешите себе быть таким, какой вы есть на самом деле. Лучше предвидеть неудачи, а не удивляться им и страдать по этому поводу. Например, после постановки диагноза я смирилась с тем, что любой формат домашней работы это то, что я не смогу регулярно выполнять в силу своих особенностей, и школу английского себе искала учитывая это.С повышенной отвлекаемостью могут помочь списки, бумажные заметки, скрины или любые другие артефакты — они помогут фиксировать мысль в моменте. Даже если вы отвлеклись,у вас будет лог, по которому можно восстановить ход мысли, поэтому в идеале всегда держать под рукой бумагу, ручку и запомнить сочетание клавиш для быстрого скрина.Со срочными заданиями пользуйтесь принципом «взялся — доведи до конца». Получив письмо на почту или срочное сообщение от коллеги, попытайтесь сохранить мысль о том, чем вы занимались до этого, чтобы потом вернуться исделайте срочную задачку: ответьте на сообщение, заведите задачу, либо перекиньте «мяч» дальше. Если это живой диалог, то буквально попросите пару минут у собеседника и сделайте, что нужно. Если вы не сохраните артефакт от срочного пинга, вы скорее всего про него забудете. Лучше подстраховывать свою вечно занятую чем-то оперативку.Чтобы не срывать сроки и не доделывать всё в последний момент, делите задачу на «этапы». Например, написать«черновой черновик», а не «первый абзац итогового текста». Так браться за работу эмоционально легче, и меньше шансов, с наполовину выполненным делом, понять, что на завершение нет ни сил, ни интереса. Хорошим дополнением «чернового черновика» будетвнешний контроль. Договоритесь созвониться с коллегой за несколько дней до дедлайна, чтобы обсудить промежуточный результат. Адреналин от приближающейся встречи и необходимости озвучить какие-то результаты появится раньше, и в случае чего, у вас будет время на шлифовку результата.Если нет сил, чтобы начать, то помогаеттаймер: выделить себе понятное и фиксированное время — это добавит азарта, чтобы закончить на скорость и сделает «непонятную» работу более предсказуемой и менее стрессовой. Но не ограничивайте себя: если к концу времени вы втянулись, возможно у вас получится завершить все сразу.Так как наш мозг очень любит дофамин, но плохо регулирует его,нужно научится дополнительно его подкармливать, чтобы меньше хотелось вместо работы смотреть видосики. Напитки с ярким вкусом, порционные вкусности, ритмичная музыка могут очень облегчить рабочие процессы, которые не дают достаточно дофамина.Помните, что СДВГ — это еще и склонность к чрезмерной фокусировке, а иногдагиперфокусу. Эти качества можно использовать и конструктивно, и деструктивно. Со временем вы поймете, как входить в это состояние, но помните об отрицательных сторонах: залипнуть не на той задаче или только вечером вспомнить о том, что надо было поесть.Нудный, но важный совет: регулярно делайтефизические упражнения. Буквально забейте для них слот в календаре и придерживайтесь плана. Это помогает избавляться от избытка энергии, уменьшает шум в голове, успокаивает и дает заветный дофамин.В чем же плюсы СДВГ для Бобов и их работодателейЛюди с СДВГ хороши со всем, что касается нестандартных решений. Нам проще придумать концепт или план действий, т.к. наш мозг «гиперактивен» и перебирает сразу множество возможных вариантов.Как бы это не было парадоксально, люди с СДВГ показывают лучшие результаты в условиях большого давления: задачи с горящим дедлайном, «все сломалось и надо починить прямо сейчас» — все это активизирует очень обкатанный механизм работы в условиях стресса, знакомый еще с детства.Мы все равно часто прокрастинируем и отвлекаемся, и работа из дома с плавающим графиком идеально подходит для нас, чтобы особенности минимально влияли на результаты.Работа с компьютером — это одновременно благословение и проклятие. Имея постоянную необходимость в доступе в интернет, довольно сложно не отвлечься на что-то, что дает больше дофамина, чем текущая задача. Но если ты знаешь, что твой мозг работает именно так, можно заранее поставить себе внешние ограничители: черный список сайтов, сознательно не ставить не связанные с работой программы на рабочий ноутбук итд. Бонусом мы получаем логирование наших действий: если ты отвлекся и потерял контекст, история браузера и логи помогут восстановить процесс.Благодаря гиперконцентрации, если нас что-то очень зацепило, мы очень быстро и хорошо учимся, а значить освоить новый инструмент мы сможем быстрее и лучше, особенно, если там есть какие-то очень удобные или красивые фичи.Что в итоге?За последние 1,5 года, что я нахожусь в терапии и выстраиваю новые привычки, проблемы прокрастинации, забывчивости и невнимательности стали влиять на мою жизнь куда меньше.Мне очень хочется, чтобы все, кто страдает от этого как и я, перестали воспринимать это как мистическое проклятье: можно научиться эффективным стратегиям, а ещё у ваших особеннностей точно есть обратная сторона — суперсила.Я надеюсь, что мой опыт откликнется у вас и вы сможете забрать к себе какие-то из методов, которые помогли мне, и тоже почувствовать себя лучше. Буду рада пообщаться в комментариях!Полезные ссылки«Перебиваю, потому что боюсь забыть, что хотел сказать»: выпуск про СДВГ с психиатром Елисеем Осиным— Подкаст с разбором психиатра о том, как диагностируют и лечат СДВГ в России, что это такое и как может проявляться.119 СДВГ, фокус и полезная рутина— Подкаст про то, как принять свой диагноз и построить для себя полезную рутину“Почему я отвлекаюсь”— Книга Эдварда Хэлловэлла и Джона Рэйти. История происхождения диагноза и несколько кейсов практикующего психотерапевта, разбор которых может быть полезен.https://adhdoers.com/— Блог, мемы из которого вдохновили меня на иллюстрации в этой статье"
СберМаркет,,,Оценка задач в сторипоинтах: мой путь от абстрактного к конкретному,2024-08-22T13:44:52.000Z,"Привет! Меня зовут Артём Коньков, я тимлид команды продуктовой разработки в Купере. У меня в команде шесть разработчиков, по два на каждый стек: мобилка, фронтенд, бекенд и два QA. В статье расскажу о том, как, став тимлидом в уже почти сложившейся команде, менял систему оценки задач и переводил абстрактные сторипоинты в конкретные.Дисклеймер:понимаю, что тема холиварная, и что с точки зрения строгой методологии нужно выбрать между оценкой по времени или по сторипоинтам, не пытаясь поженить одно с другим. У нас в компании допустимы разные варианты и я, как тимлид, очень рад, что такие решения даются на откуп юнитов и отдельных команд — можно настроить процессы под себя и свои задачи более персонально.Здесь хочу поделиться своей логикой и тем, как искал аргументы, чтобы реализовать свой эксперимент. Любые мнения по теме приветствую в комментариях.Сторипоинты— метрика оценки сложности задач в методологии разработки Scrum. Они используются для определения объема работы, которую нужно выполнить для завершения задачи.Сторипоинты представляют собой числовую оценку сложности задачи. Чем выше число, тем сложнее задача. Например, задача с оценкой в пять сторипоинтов будет считаться более сложной, чем задача с оценкой в три сторипоинта.Оценка сложности задачи производится командой на основе таких факторов, как объём работы, требуемые навыки, зависимость от других задач и т.д. Важно отметить, что сторипоинты не являются абсолютными значениями и могут различаться в зависимости от команды и проекта. В этом как раз и крылся хаос в моём примере.ПредысторияЯ переходил на должность руководителя из отдела с настроенными и отлаженными процессами, в отдел, где процессы вроде были теми же, но работали непривычно, и не так, как мне, кажется, правильным. Оценка задач работала так: фронтендеры, бэкендеры и мобильные разработчики оценивали свои задачи каждый отдельно, каждые по своей градации сторипоинтов, то есть три сторипоинта у мобильного разработчика не равны трём у бэкендера.Из-за этой разницы элементарная задача на мобилке, которая занимает меньше одного дня, оценивалась, условно, в пять сторипоинтов, а задача бэкенда, которую делали четыре дня, могла быть оценена всего в два сторипоинта. Это влияло на метрики — показатель выполнения работы становился неактуальным, и от данных нельзя было отталкиваться, чтобы проанализировать производительность команды или сотрудника.Казалось бы, не так важно, что у тебя кривой burndown график, если работа сделана. Но из-за разницы в оценках между стеками получалось, что на задачи коллег очень сильно подвергались «инфляции», и их выполненная работа вообще ничего не меняла на графике, а метрики нам важны, потому что только так мы можем оценивать реальный объём работы сотрудника, ставить точные сроки, а потом их соблюдать.При таком разбросе в оценках очень сложно спрогнозировать, когда будет готова та или иная задача. Мне не подходил такой сценарий, и я предложил перейти от оценок в абстрактных сторипоинтах к конкретным.Что такое конкретная и абстрактная оценка в сторипоинтахКонкретная оценка подразумевает использование сторипоинтов, чтобы определить точные временные затраты на выполнение задачи с учётом опыта и навыков команды. Например, если команда знает, что задача требует 3 дня работы, она может оценить её в 3 сторипоинтов, приравняв 1 сторипоинт к 1 дню.Абстрактная оценка фокусируется на относительной сложности задачи без привязки к конкретным временным рамкам. Например, команда может оценить задачу в 3 сторипоинта, основываясь на том, что она сложнее, чем задача, оцененная в 2 сторипоинта, но проще, чем задача, оцененная в 5. В этом случае оценка не подразумевает конкретного времени выполнения, а лишь относительную сложность.Преимущества абстрактной оценки в гибкости: позволяет избежать давления сроков, и помогает нивелировать различия в восприятии времени между членами команды и заказчиками.Но есть 4 причины, по которым абстрактные оценки в сторипоинтах могут быть неэффективными в определенных ситуациях — во всяком  случае, я так вижу:Необходимость в точных сроках: Когда перед командой стоят четкие временные рамки, руководству и заказчикам требуется более точная информация о затратах времени, чем может предоставить оценка в сторипоинтах.Различия в производительности:Разные члены команды могут иметь разную производительность, что делает оценки в сторипоинтах менее точными. Опытный разработчик может выполнить задачу, оцененную в 5 сторипоинтов, за 3 дня, в то время как новичок может потратить на неё 5 дней.Сложность масштабирования:Когда команда растет или меняется состав, сложно сохранять единообразие в оценках в сторипоинтах. Это может привести к несогласованности и неточности в планировании.Отсутствие связи с реальностью:Абстрактные оценки в сторипоинтах могут быть оторваны от реальных временных затрат, особенно когда команда сталкивается с непредвиденными сложностями или внешними факторами, влияющими на ход работ.Стоит заметить, что оценка в абстрактных сторипоинтах — не всегда плохо, иногда это помогает сэкономить время и трудозатраты. Например, команда только начинает работать над новым проектом, и нужно расставить приоритеты, не вдаваясь в сложные расчёты. Тогда подойдут абстрактные сторипоинты: например, задачу по внедрению новой функции можно оценить 8 сторипоинтов, понимая, что она примерно в четыре раза сложнее, чем задача по исправлению ошибки, оцененная в 2 сторипоинта.Сначала мнения разделилисьМнения в юните разделились: я топил за конкретные сторипоинты, чтобы у фронтендера, бэкендера и мобильного разработчика были одни и те же метрики, а другой лид считал, что это не добавит никакой конкретики, а только потратит время при попытке перестроиться.Аргументы лида, который был против конкретизации сторипоинтов, и считал их пустой тратой времениМои аргументы за конкретные сторипоинтыКак я победил абстрактные оценки и какие результаты это далоМы выбрали систему, где 1 сторипоинт = 1 день.Точнее «примерно 1 день», потому что:мы никого не ругаем, когда не попадаем в диапазон оценки;учитываем, что не можем на 100% оценить влияние других команд на сроки.Кажется, что с тем же успехом можно было пользоваться таймтрекингом, но практика показывает, что считать задачи в днях, а не в часах вызывает меньше сопротивления. Одно слово «таймтрекинг» часто ассоциируется с тем, что за сотрудником будут постоянно следить и скриншотить экран.При оценке задач мы учитываем не только дни, но и коэффициент производительности (он же фокус-фактор) каждого сотрудника. Например, у меня в отделе есть middle-разработчик, который за 10 рабочих дней в спринте делает 13 сторипоинтов. То есть за десять дней он выполняет работу, рассчитанную на две недели.А есть middle, который за 10 дней выполняет 9 сторипоинтов.Коэффициент производительности одного миддла будет 1,3, а для второго — 0,9. Умножая 10 рабочих дней на их коэффициенты производительности, мы понимаем, что нам нужно запланироваться для сеньора на 13 сторипоинтов в спринт, а для миддл-разработчика — на 9.То, что мы можем так точно прогнозировать загрузку каждого специалиста, и наши прогнозы подчиняются единой логике, помогает команде быть более сплочённой, легче масштабироваться и переходить с проекта на проект. Например, сейчас мы занимаемся ресторанами, через неделю будем работать над аптеками или цветами, и, если бы у нас не было чётких внутренних процессов, мы могли бы не пережить такой переход.И вот лид, который был сначала против конкретизации оценок задач, поменял своё мнениеBurndown-график до перехода на конкретные сторипоинты и после:Очень интересно послушать, были ли у вас похожие сложности в принятии системы сторипоинтов и как вы их разрешили. Тема горячая, готов к диалогу :)Tech-команда Купера (ex СберМаркет) ведет соцсети с новостями и анонсами. Если хочешь узнать, что под капотом высоконагруженного e-commerce, следи за нами вTelegramи наYouTube. А также слушайподкаст «Для tech и этих»от наших it-менеджеров."
СберМаркет,,,Вы таки внедрили сканеры безопасности в пайплайны — на этом все?,2024-08-21T07:59:04.000Z,"Привет! Я Максим Коровенков, DevSecOps Lead в Купере (ex СберМаркет). Хочу поделиться мыслями по поводу минимально необходимого набора процессов, сопутствующих внедрению сканеров безопасности в пайплайны разработки.В результате отвечу на вопрос: «А что, собственно, стоит иметь в виду под фразой “Мы внедрили сканеры безопасности в пайплайны разработки”?»Да, в тысячный раз про пайплайны, но, как вы, думаю, догадываетесь, желание поделиться казалось бы очевидными мыслями появилось не случайно! Не так давно я завершил найм и укомплектовал свою DevSecOps-команду. В рамках поиска пришлось провести достаточное количество интервью, на которых я любопытствовал, как обстоят дела у соискателей с пайплайнами безопасности на текущем/предыдущем месте работы. Это удивительно, но для 90% респондентов фраза «внедрение сканеров безопасности в пайплайны» означает только факт внедрения. Лишь некоторые кандидаты упоминали отправку результатов в VMs/ASOC систему и выстраивание сопутствующих с этим процессов.Ведь действительно кажется, что все просто: запаслись вокабуляром из нескольких аббревиатур, внедрили инструменты, SAST, SCA, может что-то еще, настроили отправку результатов для AppSec-ов и, как-будто, можно идти бить баклуши. Но я на своем опыте убедился, что DevSecOps — это про глубокую проработку процессов. Поэтому в данной статье сконцентрируюсь больше на процессах, нежели на технике.Предлагаю наконец переходить к сути!Все началось с идеи описать полностью наш опыт построения DevSecOps процессов с нуля. Я искренне надеюсь, что это действительно кому-то будет полезно.Таким образом данная статья являетсялогическим продолжением предыдущей, вероятно не будет последней, и, что важно, в ней будут повторяться некоторые тезисы из предыдущей статьи, поскольку невозможно не повторив базу, в полной мере раскрыть текущую тему.Сначала контекст. У нас все как у всех:Архитектура — микросервисная;Основной языковой стек — Golang, Ruby, JS/TS, Python;Кодохранилка — GitLab;CI/CD — GitLab CI;Существует свой PaaS, с помощью которого мы автоматически шерим пайплайны безопасности на все новые микросервисы;Размещаем все наши сканеры в своем downstream pipeline и триггерим его из основного, чтобы иметь полный контроль над набором проверок/джобов;Типы проверок безопасности, выполняемые в пайплайнах — SAST, анализ на наличие секретов, SCA;Насколько «слева» проводим сканирование — сканируем каждый коммит.Важная особенность которая отличает наш подход к внедрению сканеров безопасности от большинства других — приверженность идее построения девелопер-центричной системы и выстраивание DevSecOps-процессов As a service.Тезисно про выбранный подход:Используем в построении собственных процессов инструменты разработчиков там, где это возможно и уместно.Задействуем разработчиков, где они могут быть полезны, например, в процессе триажа файндингов.Живем в парадигме, где безопасность является неотъемлемой частью разработки (SSDLC без первой «эс»).Наша реализация сканирования кода проектовМы довольно давно сделали первые шаги для построения процесса анализа исходного кода. Коротко о нашей реализации сканирования:Отказались от системы управления уязвимостями. Сами реализовали нормализацию/дедупликацию результатов.Показываем результаты сканирования в отдельных джобах, в нашем даунстрим-пайплайне. Результаты доступны как разработчикам, так и AppSec-инженерам.Статусы джобов с результатами демонстрируют наличие/отсутствие необработанных файндингов.Реализовали фреймворк для работы с исключениямиреализовали QGКак выглядит наш даунстрим:На скриншоте мы видим три джобы с результатами отдельно для каждого типа сканирования — их название заканчивается на:results. Также видим джобуquality gate, в который выполняется анализ полученных результатов и сравнение с приемлемыми значениями риска (threshold). Все джобы, в которых выполняется сканирование, сгруппированы вscanner.Весь процесс анализа и обработки результатов отражен на следующем скрине:С точки зрения процесса:Разработчик пушит код в ветку.Отрабатывает CI, в рамках которого триггерится даунстрим с проверками безопасности.В даунстриме выполняются проверки безопасности всех трех типов: SAST, SCA, Secrets.Если для сервиса уже были внедрены исключения, то эти исключения применяются для полученных результатов. Все исключения хранятся в S3.Отправляются метрики в Prometheus.В джоб-логе отражаются результаты сканирования (в тех трех джобах, о которых речь велась выше).Если были найдены новые файндинги, разработчик имеет возможность перейти в каждую джобу с результатами и сделать анализ.Если конкретный файндинг подтверждается, он переходит в статус уязвимости, разработчик ее исправляет.Если файндинг не подтверждается, т.е. разработчик посчитал его «фолсом», разработчик добавляет исключение для этого файндинга в отдельном репозитории в виде MR'а.AppSec ревьюит изменения в репозитории и мержит их.После аппрува новые исключения отправляются в S3 и применяются уже при следующем сканировании.И вот, наконец, дав контекст и базовую информацию об уже сложившемся процессе, можем приступить к рассмотрению различных сопутствующих процессов и нюансов без которых «внедрение сканеров безопасности в пайплайны» не может быть полноценным.Дисклеймер: автор ни в коем случае не претендует на истину в последней инстанции, лишь делится своим видением.Процессы до внедрения security-проверок. Выбор сканеровНачнем с самых простых вещей, а именно: «Чем будем сканировать?»На что стоит смотреть при выборе, SAST-сканеров:Используемые ЯП и фреймворки.Типы уязвимостей.Следует получить список CWE для сравниваемых сканеров из документации/конфигурации в исходном коде/вывода в cli и выбрать тот, в котором реализовано бОльшее количество проверок. Если используются несколько сканеров, следует выбрать их так, чтобы покрытие CWE было максимальным.Типы проверок.Разные сканеры по-разному находят уязвимости. Основных типов проверок три: pattern-based, семантический анализ и taint-анализ. Следует использовать несколько сканеров, выполняющих разный тип проверок. Этот подход даст существенный плюс в виде минимального количества ложноотрицательных срабатываний, но также и минус в виде большого количества ложноположительных. При правильно выстроенной работе с файндингами минус можно нивелировать.Какие сканеры используем мы:Semgrep для pattern-based анализа;Golangci-lint/brakeman/bandit для семантического анализа;{Не названный} коммерческий сканер для taint-анализа.Итого 2-3 сканера анализируют каждый коммит на наличие уязвимостей и делают это по-разному.На что важно обратить внимание, говоря о композиционном анализе (SCA):поддерживаемые ЯП и сборочные файлы;качество построения SBOM;показ дерева происхождения зависимостей с акцентом на связь между прямыми и транзитивными зависимостями.{пункт со звездочкой} определение использования уязвимых функций библиотеки в исходном коде разрабатываемого сервиса.Сразу про пункт со звездочкой: если в разработке используется Golang, следует использовать govulncheck, который умеет определять используются ли уязвимые функции библиотеки в коде. Это снизит количество ложных срабатываний.Что касаемо остальных пунктов, с ними отлично справляется нами используемый сканер trivy. Он репрезентативно показывает транзитивные зависимости. Выглядит это так:Горячо любим и рекомендуем к использованию.Конфигурирование сканеровНе будем останавливаться на банальных вещах, что нужно правильно конфигурировать сканеры, исключать тестовые директории и прочее. Есть темы поинтереснее.Регулярный пересмотр конфигурации сканеровДля оптимизации времени сканирования и минимизации ложноположительных срабатываний необходимо периодически пересматривать конфигурацию сканеров.Случай номер 1Пролема:Вы настроили сканеры, исключили тестовые директории, но AppSec-специалисты, работая с результатами сканирования, постоянно исключают файндинги из одних и те же директорий для всех сервисов.Решение:Следует пересмотреть конфигурации сканеров и добавить в исключения директории, файндинги в которых постоянно/автоматически исключаются на стадии обработки результатов.Случай номер 2Проблема:Вы используете несколько сканеров, но для каких-то типов проверок два использующиеся вами сканера находят одни и те же уязвимости.Решение:Проблемы нет, если вы корректно дедуплицируете результаты. Однако есть возможность оптимизировать время сканирования путем отключения проверок для сканера, полностью дублирующего результаты другого.Получение достоверных результатов сканированияПомимо пересмотра конфигураций сканеров, также следует побеспокоиться о достоверности получаемых результатов сканирования. У разработчика есть как минимум несколько вариантов как, возможно не нарочно, изменить результаты сканирования, проводимого в пайплайне:Добавить инлайн комментарии в исходном коде проекта. Многие анализаторы поддерживают инлайн комментарии — если оставить комментарий, например,#nosecс идентификатором типа уязвимости, то ранее находившаяся уязвимость сканером bandit больше обнаружена не будет.Как бороться:включить игнорирование инлайн комментариев с помощью параметров командной строки;перед стартом сканирования «вырезать» с помощью find и sed все инлайн комментарии из проекта, благо все они должны быть известны для вами используемых сканеров.Разместить файлы конфигурации для конкретного сканера в корне проекта. Сканеры могут считывать конфигурационный файл из различных стандартных директорий —текущая директория/корень сканируемого проекта/специально создаваемая директория самим сканером. Если конфигурационные файлы будут расположены сразу в нескольких директориях стандартных для сканера, они будут применены с разным приоритетом. Соответственно конфигурационный файл, лежащий в корне проекта, конкретно для используемого вами сканера, может иметь первый приоритет, что создаст потенциальные проблемы. Очевидно, с помощью кастомного конфигурационного файла можно исключить и директории проекта, и типы проверок, тем самым изменив результаты сканирования.Бороться с этим просто: названия файлов конфигурации заранее известны, поэтому можно как с инлайн комментариями удалить их перед выполнением сканирования.Самый вопиющий случай, но вполне реализуемый — разработчик может в локальном .gitlab-ci.yml файле разместить джобу с тем же названием как у джобы в даунстриме, переопределив, например, секцию script. Результатом подобных действий, очевидно, может быть то, что сканирование реально выполнено не будет.Бороться с этим превентивно возможно не имеет смысла, но можно раз в определенный период искать в GitLab все джобы с таких же названием, как в основном даунстриме с проверками безопасности.Добавление кастомных правил сканированияЕще из достаточно просто и полезного — внедрение кастомных правил сканирования. Распространенный запрос от AppSec-инженеров — иметь возможность проверить наличие найденной вручную уязвимости, во всех сервисах, где внедрены пайплайны безопасности. Если проверку можно формализовать в виде pattern-based правила, то с этой задачей точно справится Semgrep. Этот сканер обладает своим плейграундом для тестирования правил и очень подробной документацией о том, как писать кастомные правила. Все, что нужно сделать:организовать репозиторий где AppSec-и будут хранить правиласделать CI с линтером, тестами, публикацией правил и триггером пересборки образа со сканероморганизовать общее место хранения правил в S3Визуализация описанного выше:Данная схема — лишь пример, показывающий, что решение задачи с внедрением кастомных правил лежит на поверхности. Тем более, что линтер и тест кастомных правил реализован в самом Semgrep и выполняется командамиsemgrep --validateиsemgrep --test.Работа с результатамиВажно то, как будет организована работа с результатами сканирования: кто в какой системе будет лицезреть файндинги и какие действия предпринимать в зависимости от результата их анализа.Для решения задачи нормализации, дедупликации, а также остальных необходимых функций для работы с результатами, был разработан фреймворк, который мы называем DSOP — о нём подробнее было рассказано впервой статье. Тут рассмотрим только его ключевые функции.Как известно мы показываем результаты сканирования в выводе специальных джоб {sast|secrets|dependencies}:results. Вот как это выглядит:Можно признать вывод «портянкой», особенно если представить в результатах 100+ файндингов. Но над выводом можно поработать так, чтобы даже будучи портянкой, он имел удобную для анализа структуру. Что нужно сделать:сгруппировать файндинги по файлам;отсортировать файлы по наивысшей степени критичности файндингов (сверху показывать те, в которых критические уязвимости и т.д.);отсортировать файндинги для одного файла по степени критичности.Итого мы получили удобный список, который можно анализировать листая сверху вниз.Данный вывод доступен как AppSec инженерам, так и разработчикам (так как он представлен в самом GitLab).После того, как мы решили задачу с демонстрацией результатов сканирования, необходимо предоставить возможность обработать результаты.Какие две основные опции тут есть:Если файндинг подтверждается как уязвимость, то ставится задача на ее исправление. С нашей стороны дополнительно ничего не требуется.Если файндинг признается ложноположительным, его следует исключить из результатов сканирования. Для обработки этого типового случая был разработан функционал добавления и применения исключений. Он был описан в общей схеме обработки и анализа результатов сканирования; более подробно — впредыдущей статье.На текущий момент могу заверить, что этот метод обработки файндингов работает на ура. Разработчики знают о нашем фреймворке и научились самостоятельно с ним работать. Более 60 авторов в репозитории с исключениями помимо AppSec-ов подтвердят =)Далее, как подсветить проблемы, найденные в репозитории сканерами безопасности? Разработчики могут не заметить или просто проигнорировать варнинги в триггер джобе безопасности. На помощь нам пришел сервис Scoring. О нем опять же подробно было рассказано в предыдущей статье. Тут скажу, что данный сервис аккумулирует количественные показатели различных метрик, в том числе метрики по безопасности. В зависимости от критичности найденных проблем выставляется оценка от C, в случае наличия критичных уязвимостей, до A+, если проблем безопасности нет. Плохие оценки влияют на квартальные оценки разработчиков. Вот как это выглядит:Оповещать разработчиков о проблемах безопасности мы научились. Но как в отсутствии ASOC/VMs системы AppSec-ам реагировать на потенциальные проблемы безопасности и помогать справляться разработчикам с новыми файндингами? Для  этого мы написали бота, который в ежедневном формате рапортует об ухудшении оценок в Scoring:Бот умеет тегать нужного AppSec BP, в сервисах которого, произошли ухудшения оценок.Помимо ухудшившихся оценок, бот также еженедельно присылает информацию обо всех сервисах с оценками ниже A.Набор из тех практик/сервисов, что были описаны выше, помог нам просто и эффективно наладить работу с файндингами сканеров и вовремя реагировать на проблемы безопасности. В качестве промежуточного вывода, перечислим, что было реализовано:показ файндингов в логах джобы;внедрение возможности работать с исключениями;публикация результатов в сервис оценок Scoring для разработчиков;публикация ухудшения оценок/сервисов с плохими оценками с помощью бота для AppSec инженеров.Оптимизация времени сканированияЗадумывались ли вы, как долго сканируются коммиты с помощью внедренных вами проверок безопасности? Мы задумывались и в оптимизации времени сканирования, кажется, уже взяли черный пояс.Как обстояли дела сначала. Медиана по времени работы всего даунстрима:95 перцентиль:Как вы понимаете, начинали мы с неприемлемых значений времени сканирования, с которыми пришлось комплексно поработать.Ответим на главный и насущный вопрос: «Какое время сканирования (время работы даунстрима со всеми проверками безопасности) является оптимальным?» Практика показала, что оптимальным является среднее время выполнения всех джобов на том же стейдже. В нашем случае речь про 3-4 минуты.Способы оптимизации времени работы инструментов в пайплайне:Кэши.Если инструмент до выполнения сканирования собирает проект или скачивает зависимости проекта, вероятно, это можно сделать заранее. Сюда же можно отнести все, что касается обновлений (как самих инструментов, так и их сигнатур). Если можно сделать заранее, лучше сделать, чтобы не иметь штраф по времени во время каждого сканирования.Инкременты.Если сканер умеет в полное/инкрементальное сканирование, следует этим воспользоваться. Тут дополнительно подчеркну, что git diff подходит только для инструментов, которые выполняют pattern-based проверки. Но правда жизни заключается в том, что они, как правило, и так достаточно быстро сканируют.Смена инструмента. Если на рынке есть инструмент, который обеспечивает то же качество сканирования, что и используемый инструмент, но делает это быстрее, следует перейти на него.Точечное увеличение количества ресурсов. Если ничего из вышеперечисленного не сработало, можно попробовать увеличить количество ресурсов раннера для сканирования «больших» проектов или для конкретного сканера.Далее рассмотрим каждый метод по-отдельности, потому что мы использовали их все для оптимизации времени сканирования.Использование кешей. Первая попытка ускорить GosecGosec был одним из тех сканеров, которые использовались в нашем даунстриме с самого начала, поскольку Golang — это основаной ЯП для разработки. Проблема заключалась в том, что он в выполнял сканирование достаточно долго: медиана в районе 6 минут, тогда как нужно было дойти до 3-4.Когда мы стали изучать, что происходит во время тестирования, обнаружили непустые директории c кешами результатов сборки и с используемыми зависимостями проекта.Еще одно наблюдение: если запустить сканирование одного проекта два раза подряд, то мы всегда получали меньшее время сканирования для второго прогона, при этом разница была существенной — 20-40%. Далее мы стали прорабатывать идею с сохранением кэшей, их обновлением раз в n-релизов и использованием в пайплайнах разработки.В итоге пришли к следующей схеме выполнения сканирования:Коротко о том, что на схеме: был реализован отдельный даунстрим для полного сканирования проектов с помощью Gosec без кэшей (да, даунстрим от даунстрима) и сохранением результирующих кешей в S3. Работа этого даунстрима не аффектила по времени основной даунстрим безопасности — там перед сканированием с помощью Gosec проверялось наличие кешей и, в случае их наличия, производилось их скачивание и применение при сканировании. Обновлялись кеши раз в пять пайплайнов на master-ветке.Помогло ли это? Да, сканирование стало проводиться быстрее на, в среднем, минуту-полторы, при этом мы не потеряли в качестве (если кэши в той или иной степени были неактуальными, Gosec умный, он докачивал/дособирал то, что ему не хватало). Но это все равно было недостаточно быстро.Смена инструментаКоллеги, разрабатывающие наш PaaS, довольно давно использовали Golangci-lint для своих целей тестирования. Как известно Golangci-lint имеет поддержку линтера Gosec и в какой-то момент было решено сравнить Golangci-lint с линтером gosec и Gosec по времени и качеству сканирования.По качеству сканирования результаты двух сканеров были идентичными. По скорости сканирования Golangci-lint был быстрее в 2-3 раза. Почему, осталось загадкой, но трейс сравнения этих инструментов остался, можно поудивляться:Итого мы переехали на Golanci-lint, сократив среднее время сканирования с 6 минут до 3-4, как нам и было нужно.Инкременты. Если сканер умеет, надо делать{Не названный} коммерческий сканер, который по-взрослому выполняет taint-анализ, логично выполняет сканирование дольше всех. К счастью этот сканер умеет в инкрементальное сканирование. Поделюсь схемой сканирования:Схема очень похожа на схему сканирования Gosec с кешами, но есть небольшие различия:в даннном случае реализован не даунстрим от даунстрима, а триггер отдельного пайплайна, в котором выполняется полное сканирование;в S3 сохраняются кеши, собранные в результате полного сканирования, для использования в основном даунстриме безопасности и выполнения инкрементальных сканирований;кеши обновляются каждые 3 дня, а не через n-релизов.Почему схемы для сбора кешей Gosec'а и применения инкрементов для {Не названного} коммерческого сканера различаются? Потому, что было интересно понять плюсы и минусы двух разных подходов. Обсудить их, если будет интересно, можем в комментариях, ибо действительно свои плюсы и минусы у каждого подхода имеются.Но какая бы ни была схема выполнения полных/инкрементальных сканирований, это принесло свои плоды, время сканирования значительно уменьшилось. Но, как и с Gosec-ом этого было недостаточно, поэтому был применен еще один способ оптимизации времени сканирования, о котором поговорим далее.Увеличиваем количество ресурсов для «больших» проектовЕсли ни инкременты, ни применение кешей, ни смена инструмента не помогли, можно попробовать увеличить количество ресурсов на раннере, выполняющем сканирование. Но сделать это целесообразно точечно.Следует использовать раннеры с увеличенным количеством ресурсов только для сканирования тех проектов, которые сканируются дольше остальных или дольше оптимального для нас времени сканирования. Как правило, речь ведется о монолитах или сервисах с большим количеством строк кода.Как это сделали мы: собрали список «больших» проектов и сохранили в переменную:В зависимости от наличия СI_PROJECT_PATH_SLUG текущего проекта в переменной GLOBAL_HEAVY_PROJECT_LIST выставляем тег с раннером, обладающим нужным количеством ресурсов:По дефолту переменная TAG_VAR содержит имя раннера со стандартным минимальным количеством ресурсов и применяется для всех джоб безопасности.Соответственно для нужного сканера вместо захардкоженного значения тега используется переменная, меняющее значение в зависимости от имени проекта и его наличия в списке «больших» проектов.Подытожим про процессы оптимизации времени сканирования: была проделана большая работа, которая в конечном счете привела к нужным показателям.Внедрение Quality GateНаконец, мы переходим к самой интересной теме — к оценке степени риска информационной безопасности и автоматизированном принятии решения о допуске сканируемой версии сервиса в релиз.Я называю внедрение QG (особенно в блокирующем режиме) «венцом успеха» и вот почему: к моменту внедрения QG у вас должны быть:настроены пайплайны для сканирования сервисов;оптимизировано время сканирования так, что разработчики разучились отменять джобы безопасности;налажен процесс обработки найденных файндингов в пайплайнах.Все вышеперечисленное говорит о достаточной степени зрелости процессов  AppSec/DevSecOps для начала проработки процессов включения QG.Какие сопутствующие процессы/нюансы нужно проработать для включения QG:Критерии прохождения QG. Вы должны выработать условия, при которых не допустите сканируемую версию сервиса в релиз. Как пример: наличие высокого уровня критичности уязвимости или наличие хотя бы одного секрета.Нормативная база. Разработчики должны быть ознакомлены с тем, что потенциальная блокировка релиза при несоблюдении политик ИБ — это то, с чем они добровольно-принудительно согласились и подписали.Подход включения QG в блокирующем режиме. Вручную или автоматизированно.Нотификации о включении QG в блокирующем режиме. Очевидно, команда разработки сервиса, для которого включается QG в блокирующем режиме, должна знать об этом. Также о блокировке должны быть уведомлены AppSec BP, если блокировка включается автоматизированно.Процесс изменения конфигурации или смены/добавления сканеров. Должны быть выявлены все случаи потенциально резкого и неконтролируемого изменения результатов сканирования, что может сказаться на прохождении QG.Критерии для внедрения QG в блокирующем режиме.Напомню, как это работает у нас. В нашем даунстриме есть отдельная джоба QG. Результат ее работы выглядит следующим образом:Для принятия решения о прохождении QG веса найденных файндингов складываются отдельно по каждому типу сканирования и сравниваются со значением приемлемого уровня риска (threshold). Если получившееся значение больше, QG считается не пройденным.Теперь про QG в блокирующем режиме. С начала процесса внедрения блокировок на QG мы вручную добавляли имена сервисов, СI_PROJECT_PATH_SLUG, для которых QG не просто должен был завершаться не успехом, но и сама джоба с QG должна падать, останавливая пайплайн и запрещая релиз. Как работает логика выбора статуса завершения джобы QG:выполняется анализ количественных показателей, полученных в ходе сканирования безопасности - если общий вес по одному из типов сканирования превышает приемлемый уровень риска, выставляется флаг “QG не пройден”.при выставлении флага  “QG не пройден” выбираем с каким конкретно системным кодом ошибки завершить джобу - если сканируемый сервис входит в список на блокировку, завершаем джобу с кодом ошибки 1, если не входит - с кодом ошибки 2.джобе с QG разрешаем завершаться успехом (pass with warning) при завершении с определенным кодом ошибки, в нашем случае - при завершении с кодом ошибки 2. Сделать это можно с использованием ключевых слов allow_failure/exit_codes в GitLab CI.Таким образом, если сервис входит в список на блокировку и QG не был пройден, джоба завершится со статусомFailи остановит весь пайплайн. Если сервис не входит в список на блокировку — завершится со статусомPass with warning, не прерывая пайплайн.На текущий момент мы реализовали сервис для автоматического включения QG в блокирующем режиме. Какими критериями мы оперируем при выборе сервисов готовых к блокировке на QG:Наличие хороших оценок в сервисе Scoring, которые мы отправляем по результатам сканирования. Хорошие оценки (A и A+) означают, что в исходном коде сервиса не найдены секреты, нет уязвимостей высокого уровня критичности и нет использующихся с уязвимостями высокого уровня критичности библиотек.Наличие файла конфигурации в репозитории с исключениями devsecops-configs. Наличие этого файла гарантирует, что была проведена работа по анализу результатов сканирования — были добавлены исключения или был добавлен пустой файл конфигурации, как подтверждение отсутствия проблем.Еще раз вспомним про девелопер-центричность. Да, круто, что при появлении уязвимостей, результаты публикуются и для разработчиков, и для AppSec инженеров, а также влияют на метрики в Scoring и на успешность прохождения QG. Обратная сторона этого преимущества — проблема резкого ухудшения результатов сканирования. Факт того, что ухудшение результатов сканирования сразу сказывается на прохождении или непрохождении QG, становится большой проблемой.Я бы выделил 2 причины «резкого» или «неожиданного» ухудшения результатов сканирования:Когда мы сами меняем инструмент или конфигурацию сканеров. Наши примеры: смена Gosec на Golangci-lint (да, тут разницы в результатах не было, но стало понятно, что нужен процесс), а также добавление сканера секретов deepsecrets.Обновление сигнатур в trivy: код не изменился, а уязвимость в библиотеке появилась — вечером QG проходили, утром, ничего не меняя, уже не прошли. Как вы понимаете, так работать не должно.Что было сделано для реализации плавного перехода на новый сканер или изменения конфигурации:внедрили grace-период равный двум неделям на обработку файндингов нового сканера;внедрили атрибут для нового сканера в DSOP, который позволяет не учитывать файндинги нового сканера в Scoring и при оценке QG на протяжении grace-периода;добавили нотификацию в лог джобы, если был найден файндинг нового сканера;добавили визуализацию в Grafana для AppSec-инженеров, демонстрирующую количество файндингов нового сканера после дедупликации с другими (т.е. учитываются только уникальные файндинги).Вот так выглядит нотификация в логе джобы:Атрибут нового сканера добавляется вручную. Он представлен в виде переменной с названием сканера, файндинги которого исключаются из метрик.Для обработки случая с обновлением сигнатур trivy мы также реализовали grace-период, но равный трем дням. Три дня — это среднее время обнаружение уязвимости AppSec инженерами плюс 1 день с момента ее публикации. В среднем времени обнаружения заложены временные штрафы на:обновление сигнатур;проведение первого сканирования;появление информации об ухудшении оценки.Этих трех дней достаточно, чтобы обработать информацию о новой уязвимости и, в случае, если например уязвимость принята не применимой, добавить ее в исключения для одного сервиса или для всех сервисов сразу.В grace-период данные о новых уязвимостях учитываются в Scoring, но не учитываются при прохождении QG.Реализовано просто — данные о времени публикации уязвимости есть в отчете trivy и для реализации grace-периода достаточно сравнить текущую дату и дату публикации, чтобы учитывать или нет уязвимость при подсчете количественных показателей при прохождении QG.МетрикиВажность метрик очевидна — ни один из вышеперечисленных процессов не мог бы быть реализован в их отсутствии. Также нельзя было бы утверждать об эффективности применяемых изменений для улучшения каких-либо практик и показателей.Допустим, в анализируемом сервисе не были найдены файндинги, это потому, что действительно все хорошо или из-за того, что сканер не отработал?Джоба с проверкой QG зеленая из-за отсутствия уязвимостей или из-за того, что разработчик отменил джобы со сканерами?Если ваш даунстрим долго по времени выполняется, то из-за какой конкретно джобы? А в каком количестве сервисов она долго выполняется? Что общего у этих сервисов?У вас все пайплайны зеленые в N количестве сервисов, а N — это какой процент от всех сервисов в продакшене?На эти и на многие другие вопросы помогут дать ответы собранные и грамотно интерпретированные метрики. Какие метрики точно стоило бы собирать:по покрытию;количественные: количество файндингов до/после применения исключений;временные: время выполнения отдельных джобов/пайплайна (медиана/95 перцентиль) без учета/с учетом подготовки окружения/старта раннеров;статусы джобов: необходимо собирать все статусы джобов, pass/fail/warning/cancel, особенно, если статусы имеют дополнительную смысловую нагрузку, как например, в джобе c QG. СтатусWarningозначает не прохождение QG, но не заблокированный релиз, аFailне прохождение QG и блокировку пайплайна. Статус джобыCancelпри включенном QG в блокирующем режиме — инцидент ИБ.ИтогоДля того, чтобы с полной уверенностью утверждать об успешном внедрении сканеров безопасности в пайплайны разработки, необходимо учесть большое количество сопутствующих процессов.Я бы обобщил все вышеописанное следующим образом:Внедрение сканеров безопасности в пайплайны разработки — это не решение проблем, а их приобретение =)Просуммируем все, что следовало бы сделать для достижения минимально необходимого уровня зрелости процесса сканирования кода сервисов в пайплайне разработки:выбрать сканеры;выполнить конфигурирование сканеров и учесть не самое очевидное, например, возможность разработчиков влиять на результаты сканирования;выстроить процесс обработки файндингов — в нашем случае это DSOP и девелопер-центричность;настроить сбор метрик для подсчета эффективности всех сопутствующих процессов;оптимизировать время до приемлемого — все зависит от вашей модели внедрения сканеров, но если как у нас, сканируете каждый коммит, следует сократить время насколько это возможно сохраняя эффективность;{пункт со звездочкой} внедрить QG и по возможности внедрить возможность блокировки релизов в случае несоблюдения политик ИБ.Возможно, у вас будут свои идеи как то, что освещалось в статье, можно улучшить или оптимизировать. Может каких-то процессов не хватает? Делитесь своим опытом в комментариях. Давайте развивать DevSecOps-практики вместе.Tech-команда Купера (ex СберМаркет) ведет соцсети с новостями и анонсами. Если хочешь узнать, что под капотом высоконагруженного e-commerce, следи за нами вTelegramи наYouTube. А также слушайподкаст «Для tech и этих»от наших it-менеджеров."
СберМаркет,,,Зверь по имени Диско. Как упорядочить процессы дизайн-Discovery и облегчить жизнь команде,2024-08-13T10:44:36.000Z,"Привет всем! Меня зовут Таня Конюшенко, я продуктовый дизайнер в Купере. В этой статье рассказываю о том, как открыла для себя дизайн-Discovery и внедрила его в своей продуктовой команде. Думаю, мой опыт будет полезен дизайнерам, которые много слышали о Disco, но не понимают, в чём его смысл.Ещё год назад я ничего не знала о Discovery, потому что работала в заказной разработке. О том, в чем разница между дизайнерами в агентстве и продукте, рассказывала всвоей прошлой статье.Когда я пришла в Купер (тогда он был ещё СберМаркетом), меня сразу познакомили с понятием Discovery, но смысла я в нем не увидела. Сейчас мое отношение кардинально другое. Discovery хорош, но нужно правильно его выстроить. Мой путь к идеальным процессам был тернистым…Но давайте по порядку. Что входит в дизайн-Discovery и чем это отличается от общего Discovery команды?Азы дизайн-DiscoveryВсе, кто работает в IT, знают, что в разработке новых фич продукта и поддержке старых выделяются два последовательных процесса:Discovery— это все действия до начала разработки.Delivery— все, что касается создания кода.В Disco входит:генерация идеи;аналитика по существующим событиям, то есть по статистике, которую уже собрал продукт;изучение проведенных исследований;создание дизайнерских макетов;количественные тесты макетов;UX-интервью с респондентами;согласования с командами;заключительная подготовка макетов.Если смотреть на этот процесс с точки зрения дизайнера, то обнаружатся два частично параллельных процесса:дизайн-Disco(этапы 1–3, 5 и 6) идизайн-планирование(этапы 4, 7 и 8). В дизайн-Disco, помимо дизайнера, участвует продакт, аналитик и исследователь.Design-диско заканчивается в тот момент, когда мы ответили на вопросы «что?» и «почему?» — дальше остаётся отшлифовать решение, составить роадмеп обновления и реализовать то, что задумано.Покажу, что такое дизайн-Disco, на конкретном примере.История из жизниНа заключительном этапе оформления заказа в Купере покупатель должен выбрать временной диапазон, в который курьер привезет ему заказ.Старая реализацияИногда люди пропускают этот блок, что мешает завершить заказ и ломает конверсию.Мы провели A/B-тестирование, в котором реализовали предвыбор первого из возможных слотов. Количество ошибок упало, но при этом сильно выросло количество отмен.На Discovery-встрече:С аналитиком вывели гипотезу, что блок плохо заметен, поэтому пользователи его пропускают, но само время доставки имеет для них значение, а поэтому автоматический предвыбор ведет к росту отмен.C продактом провели большой бенчмаркинг (анализ конкурентов), и я отрисовала множество самых разных решений.Затем с продактом и исследователем выбрали несколько вариантов, которые должны были отправиться в исследования.Главные критерии выбора — понятность, упрощение флоу и масштабируемость. Масштабируемость означает, что новый вариант отображения временных слотов вписывается в планы на развитие экрана (а мы можем планировать в рамках Discovery и на квартал, и на несколько кварталов вперед).Для этой задачи мы сделали выбор в пользу количественных исследований, чтобы убедиться, что на практике у людей не возникнет проблем с изменением времени доставки. Было несколько итераций: отдельно проверяли на понимание, отдельно — на First Click, отдельно — на текстовые формулировки (какая читается однозначно).На исследованиях два варианта показали почти одинаково хорошие результаты, поэтому вместо A/B-теста мы решили сделать A/B/C-тест и сравнить старый дизайн сразу с двумя новыми версиями.Варианты для тестаВыглядит логично, правда? Но есть нюанс: идеальный процесс не рождается сам по себе, его должен кто-то наладить…И чаще всего это тот человек, который больше других страдает от неопределенности. Потому что построить процесс и управлять им может кто угодно, не только продакт. Нужно только иметь желание систематизировать информацию, ставить задачи, уточнять у всех статусы и вести встречи.Стадия принятия: отрицаниеВо время адаптации в Купере я по совету своего лида поставила встречи с тремя старшими дизайнерами, которые уже наладили такие процессы в своих командах. Они рассказали, что и как делают, а я с умным видом выслушала их и даже задала несколько вопросов.А потом я пошла внедрять Disco в своих четырех небольших командах. Поставила встречу по Discovery на первый понедельник раз в две недели, а встречу по планированию — на второй понедельник. Получились спринт на Discovery и спринт на планирование, которые идут параллельно, со сдвигом в неделю.Вверенные мне команды ранее состояли только из разработчиков и не работали с дизайнерами, поэтому дизайн-процессы были полностью в моей зоне ответственности.В итоге каждую неделю я получала новые задачи на две недели вперед, они путались в голове и в спринтах. Ситуацию осложняло то, что продакты из моих команд периодически пропускали встречи, а потом ставили мне отдельные созвоны. Аналитики честно приходили, но я не понимала, что и когда у них спрашивать. А еще у нас на тот момент не было выделенного исследователя, и приходилось стучаться за помощью к другим командам.Пожив в таком режиме около пяти месяцев, я приняла волевое решение удалить встречу по Disco из календарей. Все равно на ней мы делали то же самое, что и на планировании, — обсуждали задачи и прогресс по ним.Процесс Discovery был признан нерабочим и непонятным. Я заподозрила, что это бесполезная встреча ради встречи.Без отладки Disco далеко не убежишьЧерез пару месяцев после того, как я отвергла процесс дизайн-Discovery, у меня изменился состав команд: их осталось всего две на регулярной основе (если не считать периодическую помощь коллегам).Казалось бы, так проще сфокусироваться. Но нет, переключение между задачами продолжилось: мы планировали одно, потом резко меняли курс на другое, а потом возвращались к первому.И тут случилась демо-встреча, на которой старший дизайнер из нашей компании рассказала про свой опыт отладки Discovery-процессов внутри команды и предложила помочь всем заинтересованным.У Дашиесть статьяо лайфхаках, как продуктовому дизайнеру улучшить Discovery-процессы в команде, это именно тот опыт, который стал отправной точкой для меня.Мы созвонились. Мне было интересно не столько Discovery (я по-прежнему не видела в нем смысла), сколько то, как повысить продуктивность и сделать работу более понятной. А еще я хотела узнать, как она дважды получила оценку А+ по итогам Perf Review — это у нас аналог 5+.Даша помогла мне составить список проблем:бардак с приоритезацией задач;гипотезы и требования теряются в личных сообщениях;непонятно, чем занимается разработка;нет отдельного места для хранения аналитики и результатов исследований;аналитики и исследователи получают задачи без возможности поучаствовать в составлении гипотез;никому не понятен результат работы (что происходит с задачей потом);нет обсуждения итогов A/B-тестов — и непонятно, когда они начинаются и заканчиваются.И тогда Даша предложила вернуть встречи по Discovery. Но для начала — только для одной команды.Второй подходБыло бы нечестно сказать, что одна встреча прояснила все. Это не так. Скорее, я увидела дорогу, по которой могла пойти, чтобы что-то наладить.Я собрала всех участников Disco-команды. Помимо меня, это продакт, исследователь, три аналитика и (урывками) редактор. И подготовила рабочую среду:Завела отдельный чат, чтобы мы могли обсуждать задачи и новости, а также фиксировать резюме по встречам и договоренности на спринт.Завела страничку во внутренней вики, где стала публиковать темы, а также ссылки на все результаты ресерчей и прошедших A/B-тестов.Вернула регулярную встречу по Disco, чтобы обсуждать задачи, которые пойдут в работу, приоритеты, текучку, результаты A/B-тестов и исследований. Так как бэклог большой, собираться решили раз в неделю.На титульной страничке в разделе Discovery в вики:План задач на кварталПланирование дизайна по спринтамЗадачи в JiraПроджи в JiraВсе про инклюзивностьUX-исследования (разворачивающийся список ссылок с датами)Аналитика (разворачивающийся список ссылок с датами)А/б тесты (разворачивающийся список ссылок с датами)Договоренности с другими командамиБеклогПервые встречи прошли не идеально, но удачно. Аналитики принесли результаты последних ресерчей. Исследователь получил понимание, когда ему принесут макеты на исследование, и сориентировал нас по своей загрузке (так как он работает также с другой командой).Вся команда стала не просто получать задачи, а обсуждать их уместность. Мы начали спорить!Продакт как будто поверил в мои силы и предложил отныне самой фиксировать в функциональных требованиях для разработчиков изменения в дизайне и логику. И это оказалось полезным:разработчики внимательно смотрят ФТ и не очень внимательно — макеты.Уже к четвертой встрече мы разгребли бэклог и поймали ритм, чтобы успевать проанализировать все необходимые данные до начала непосредственной работы над макетами.Пример тем, которые обсуждаются на встрече по итогам неделиВ этот момент я наконец-то поняла смысл дизайн-Discovery!Дизайн-Discovery — это процесс, который помогает объединить команду, сделать работу над задачами прозрачной, вовремя информировать друг друга о результатах, систематизировать уже полученные данные и нормально планировать на несколько спринтов вперед.Спустя полтора месяца в новом режиме я собрала фидбэк от коллег. Явных проблем не было, однако точки роста мы выделили. Не хватало структурированности встреч: четкой адженды, то есть списка тем, и тайминга (мы на протяжении всей встречи могли обсуждать только одну тему).С хаосом в A/B-тестах поконченоУ нас постоянно идут тесты. Поэтому в макетах одновременно живет десяток итераций, которые внезапно становятся (или нет) актуальными.Более того, тесты идут не только у нас. Другие команды могут запускать что-то, относящееся к нашей зоне ответственности. Например, команда лояльности может добавить что-то в дизайн корзины, чтобы повысить свои метрики. У лояльности новый компонент в дизайне есть, а у меня — нет.Я собрала прямо в Фигме таблицу для всех известных A/B-тестов, прямо или косвенно относящихся к нашей команде, и раз и навсегда перестала путаться, где актуальные, а где устаревшие макеты.Все просто: заголовок, ссылка на макеты в Фигме, период A/B-теста, результат теста и имя продакта.Такая табличка не только помогает в синхронизации, но и визуализирует рабочий процесс и дает возможность потом обсудить с командой, сколько всего сделано и какие мы молодцы :) Вдобавок мне есть что принести на Performance Review.Что еще я добавилаЯ стала показывать команде промежуточные макеты, чтобы повысить чувство причастности. Это действительно работает, ребята смотрят с удовольствием!Кроме того, я чаще стала ходить на дейли-встречи команды Delivery, чтобы держать руку на пульсе и как можно более прозрачно транслировать цели для команды Discovery. Это тоже эффективно: всем нравится видеть, как наработки превращаются в конечный продукт.High level — выносить на встречи новости других команд, которые так или иначе нас касаются, и тоже повышать таким образом прозрачность.Так в чем профит Discovery-процессов?Нашу команду покинула неопределенность. У всех появилось четкое понимание, кто над чем работает, что случится после, что уже сделано к настоящему моменту, когда ждем следующие результаты и какие это результаты.Мы ускорили дизайн-планирование, поскольку все задачи обсуждены и происследованы заранее. Продакт может менять приоритеты, но дополнительных встреч по обсуждению функционала в этом случае не требуется.Также мы обдуманно отказываемся от части возможных задач, если аналитика показывает, что их бессмысленно брать в работу.Некоторые фичи уходят в бэклог — опять же потому, что мы так решили, а не потому, что провалили планирование. Так бывает, если фича зависит от многих команд; например, если ошибка тянется с главной страницы, странно начинать ее чинить с корзиныПример аналитического ресерча, где видно, что конверсия ломается задолго до корзины с чекаутом (нашей зоны ответственности). Фича требует кросскомандной работы, а значит — сдвигается в планахНовые члены команды быстрее вливаются в процесс, потому что он уже продуман и структурирован. И все члены команды знают, на что они могут влиять и в чем смысл их работы. Когда Discovery-процессы у нас уже устаканились, я получила обратную связь, что в нашей команде ребята более проактивны и чаще что-то от себя предлагают.Больше ясности — больше удовлетворенности, согласны? А ещё после того, как я настроила этот процесс, смогла получить A+ за свою работу и наконец-то стала Senior-ом :)Расскажите в комментариях о своих лайфхаках в выстраивании дизайнерских процессов!Product&data команда СберМаркета ведет соцсети с новостями и анонсами. Если хочешь узнать, что под капотом высоконагруженного e-commerce, следи за нами вTelegramи наYouTube. А также слушайподкаст «Для tech и этих»от наших it-менеджеров."
СберМаркет,,,"Как увидеть три важнейших софт-скилла, чтобы нанять лучшего инженера",2024-08-06T10:01:54.000Z,"Чтобы нанять хорошего инженера, недостаточно проверить только его харды. В статье я расскажу о трех софт-скиллах, которые я обязательно проверяю у каждого кандидата. Если вы начнете проверять эти три навыка, вы начнете нанимать лучших специалистов. А еще я расскажу обратную, темную сторону каждого из качеств.Меня зовут Олег Федоткин, я программист и менеджер в ИТ. Я провел более сотни собеседований (мне HR даже толстовку «Hiring Hero» по такому случаю подарили) и нанял десятки человек: программистов, тим лидов, юнит лидов, архитекторов — да всех. После всех интервью я выделил три качества, которые неизменно определяют классного специалиста.Кстати, если захотите почитать больше про менеджмент и карьеру в ИТ — заглядывайте в мойТГ канал.Качество #1: ему не пофигу на работуСамое ценное качество в специалисте.Это те люди, которые не закроют ноутбук, пока не пофиксят тот плавающий баг. Они будут повышать coverage тестов в чужом коде и следуют правилу «оставляй код чище, чем он был до тебя». Они сами предлагают менеджеру идеи для улучшения продукта и приносят в бэклог найденный технический долг. Успехи и провалы проекта они воспринимают как свои собственные. Скорее всего, вам придется уговаривать их прекратить овертаймить, и я советую вам быть настойчивым. Если нашли такого — удерживайте всеми силами и следите, чтобы не выгорел.Даже если сотрудник не тащит по хардам, ничего страшного — хардам вы его научить сможете. А вот научить человека болеть душой за свое дело — нет.Как увидеть на интервьюСпросите, какие проблемы у него были на прошлом месте и как они решились. Именно решились — если вы спросите «как ты их решал», это зафреймит вопрос не в ту сторону. Хорошим ответом будет рассказ, как кандидат самостоятельно решил проблему или эскалировал ее, если самостоятельно не получилось.Задайте вопрос, как его фичи влияли на продукт. Если человеку не пофигу на работу, ему и на продукт не пофигу и он понимает, как именно его фичи сделали продукт лучше.Узнайте, влиял ли кандидат на бэклог команды. Проактивные люди сами наполняют бэклог найденным техдолгом или приносят идеи продакт менеджеру. Я сам видел и руководил командами, где программисты напрямую влияют на фичи.Обратная сторона«Не пофигу» — не тумблер, который вы сможете выключить. Такие специалисты не смогут долго работать в компании, где их инициативы не видят, не ценят и не дают ход. Погрузите его в пучину легаси без возможности исправления — и вы его потеряете.Если «не пофиг» у человека развито чрезмерно, он будет рьяно указывать людям на точки роста — их проекта и их личные. Некоторых будет триггерить такая назойливость. Если у вашей команды или отдела проблема, готовьтесь услышать на 1-1 критику в ваш адрес.«Не пофигу» — самое ценное качество. Нанимайте таких людей, цените их и держитесь за них. Из них вырастут ваши будущие тимлиды, юнитлиды и — кто знает — ваша будущая замена, когда вы решите сменить работу.Качество #2: самонаводимостьВторое качество, которое нужно смотреть. Впервые мне про него рассказал Миша Петров — СТО Рокетбанка. Когда я только начинал карьеру тимлида, я спросил у Миши, кто для него идеальный разработчик? Михаил ответил: «Идеальный разработчик — это самонаводящийся дятел, который получает задачу и летит долбить нужное дерево, пока не выдолбит». За точность цитаты не ручаюсь, но посыл такой.По пунктам, что для меня значит самонаводимость:Человек способен сам доуточнить задачу. Высокая самонаводимость позволяет человеку решать задачи с высокой неопределенностью. Например, если задача связана с бухгалтерией, он доуточнит у конкретного бухгалтера критерий готовности задачи. Конечно, в идеальном мире задача поставлена так, что доуточнять не придется — но где вы видели идеальный мир?Человек сам зайдет в другие команды, если нужна их помощь. Например, дойдет до девопс или фронтов и положит задачу к ним в бэклог без помощи своего тимлида.Когда человек «навелся», он не забьет на задачу, пока не получит результат. Например, как-то раз программист поставил задачу соседней команде и забыл про нее. В итоге, про задачу забыли вообще все, никто ее не сделал, а спустя время задача стала критическим блокером для проекта. Когда программиста спросили, почему так, он ответил, что с его стороны пули вылетели. Классика! Самонаводящийся боец продолжал бы раз в день-два спрашивать, как поживает его задача и пушил ее дальше.Как увидеть на интервьюСпросите про задачи с самыми туманными требованиями и послушайте ответ. Не пугайтесь, если туманные требования соискателю не понравились — это не красный флаг, это нормально. Важно то, как он обработал эти туманные требования.Узнайте, как часто ему приходилось общаться к людьми не из своего отдела. В идеале, если это были люди даже не из ИТ, как можно ближе к бизнесу. Самонаводимые люди неизбежно контактируют с бизнесом.Задайте вопрос «расскажи мне про самую долгую задачу». Здесь важно узнать, насколько человек контролировал расплывшуюся по времени задачу и не забил ли он на нее.Обратная сторонаЧем выше самонаводимость, тем выше соблазн забить на формулировку задачи и просто говорить человеку «сделай хорошо, чо ты». Во-первых, это может демотивировать человека. Во-вторых, результат может в корне отличаться от того, что вы изначально ожидали.Самонаводимость — отличное качество человека, которое в разы снижает время на его менеджмент и сильно выделяет человека из рядов обычных специалистов.Качество #3: внутренний локус контроляВ психологии есть такая абстракция как локус контроля. У человек превалирует либо внешний, либо внутренний локус. В чем разница:Внешний локус контролядиктует человеку, что в его неудачах виноваты внешние факторы. Опоздал на работу — это проклятые пробки, ничего не могу поделать. Завалил спринт — это коллеги не вовремя завершили тестирование или развернули стенд, ничего не попишешь.Внутренний локус контролядиктует, что во всех неудачах виноваты мы сами. Если опоздали, то надо было раньше выходить из дома. Если спринт завалили, надо было активнее пинговать тестеров или девопсов.К сожалению, мой опыт говорит, что локус контроля очень сложно сместить во внутрь, если у человека он находится вовне. Очень сложно — не значит невозможно.Главный плюс человека с внутренним локусом — он не просто принимает внешние факторы, он старается их менять. Такая любимая всеми «проактивность» это следствие внутреннего локуса.А еще с такими людьми просто приятнее общаться: сами подумайте, кому нравится, когда ваш собеседник винит в своих проблемах всех, кроме себя? Я заметил: чем более явно выражен у человека внешний локус, тем токсичнее его поведение.Как увидеть на интервьюЗадать вопросы вида:«Расскажи про свой самый большой провал. Что привело тебя к нему?» — максимально лобовой вопрос. Если виноваты вообще все, кроме самого человека это повод копнуть глубже, но делайте выводы лишь на основании одного ответа!«Расскажи про случаи, когда не удалось выстроить отношения с другой командой или человеком». Если соискатель обвиняет лишь другую сторону, это не здорово. Часто в проваленной коммуникации виноваты обе стороны, здорово, если соискатель подсветит эти моменты.Обратная сторонаЛюди с внутренним локусом любят загоняться там, где не надо. Иногда ситуация действительно диктует условия, с которыми можно только смириться. Но внутренний локус — это не отключаемая фича и за провалы такие люди съедают себя очень сурово.ИтогТри качества, которые я обязательно выясняю на интервью:Насколько «не пофигу» на работуУровень самонаводимостиГде находится локус контроляПопробуйте задать мои вопросы или переформулируйте их под себя — и качество вашего найма возрастет.Больше контента про менеджмент, мотивацию и карьеру в ИТ вы найдете у меня вТГ канале— подпишитесь, если еще не.Tech-команда Купера (ex СберМаркет) ведетподкаст «Для tech и этих»от наших it-менеджеров. Я там тоже есть :)"
СберМаркет,,,Инженерия устойчивости — основной инструмент выживания вашей организации,2024-08-06T09:52:31.000Z,"Привет! Меня зовут Сергей Реусин и последние пять лет я занимаюсь эксплуатацией production-систем с непрерывной практикой инцидент-менеджмента. Каждый день, сталкиваясь с аномалиями и проблемами, невольно спрашиваешь себя: «Почему это происходит? А главное: как с этим дальше жить?». Три нелегких года работы в Купере (ex СберМаркет), где мне доверили строить культуру SRE и инцидент-менеджмента, помогли утвердиться во мнении и подходах, которые действительно помогают справляться с подобными вызовами. О них и поговорим!Чтобы сложить цельную картину о создании устойчивых систем, мы пройдём по следующим шагам:Определим, ради чего вся эта «доступность» и «стабильность» нужнаПопробуем устаканить терминологию, чтобы говорить на одном языкеПосмотрим на понятие системы с позиции устойчивостиОбратимся к историческому опытуИзучим возможные паттерны отказов систем и способы их митигацииВизуализируем модель восприятия аномалийПознакомимся с ключевыми личностями в подходах Resilience EngineeringВажные допущения в этой статьеЭта статья основана на моих докладах с HighLoad++ 2023 и DevOpsConf 2024, но не является их прямой расшифровкой и обогащена дополнительной информацией.Некоторые материалы намеренно представленыв оригинале,чтобы не играть в испорченный телефон. Для простоты чтения буду использовать вольный перевод цитат, но с обязательным указанием источников со ссылками.Часть примеров упрощена и помечена специальным знаком ✨ с целью сохранения стройного повествования. Винтерактивных уточненияхобязательно представлены оригинальные идеи, из которых складывается утверждение.Этот материал составлен спубличного одобрения авторов. Бóльшая часть материалов не является моим собственным трудом. В своей работе хочу провести параллели с другими доменными областями и ни в коем случае не хочу исказить оригинальные идеи авторов.Так ли важна доступность?Много копий уже было сломано о темынадёжности,стабильности, качества и доступности работы сервисов. Когда ты обеспечиваешь работу крупной платформы с миллиардным оборотом, игнорироватьпоказатели доступностипросто не получится: вам об этом напомнят либо сами люди, либо деградирующие метрики продуктов и систем.Для сервиса, аналогичного Куперу, можно согласиться спредставлением Уве Фридрихсена:«Бизнес-ценность достигается системой при наличии её в нужном окружении и доступности клиенту в нужное время».В своем докладе Уве не говорит «нужное время», однако из контекста понятно:если функции системы недоступны, то ценность не достигается в принципе.В своих процессах определения SLO мы прямо так и пишем. На практике для некоторых функций критерий недоступности ещё жестче и начинается уже с ~30%Подобное согласие позволяет лишь из определения решить, что на уровне организации нам обязательно следует отслеживать доступность,качество и потребностьработы системы.Доступность можно представить как выражение, на которое влияют 2 процесcа:наличие и обработка ошибок (MTTF),а так же процессвосстановления (MTTR).Согласно этой формуле наша цель — максимальное увеличение времени между ошибками. То есть чем реже происходят сбои, тем доступнее наша система.Но это очень рисковый путь. Большинство современных систем являются распределёнными✨, а значит на практике в них можно встретить проблемы, вызванные одним изшироко известных заблуждений. Более того, многие из нас на себе ощущают, что сбои неизбежны и зачастую непредсказуемы!Доктор Ричард Кук, известный по своей работе «How Complex Systems Fail», постулирует:«Failure is the normal function of your systems, not the abnormal one».Но если появление сбоев неуправляемо, то какие альтернативы? Чтобы воздействовать на переменнуюMTTR, мы можемуменьшить время восстановления, наделив системы устойчивостью —способностью  справляться с непредвиденными ситуациями и восстанавливаться после сбоев.Определение устойчивости не ново и уверен, большинство из вас с ним знакомы. Когда мы доходим до этого этапа, то как инженеры сразу ищем конкретные способы и инструменты для достижения цели — в нашем случае, для достижения устойчивости. А их много! Некоторые энтузиасты (как Уве) даже пробуют создать карты методов с описаниями и примерами.Вот примеры таких картИнтерактивной карты так и не появилось, но естьссылка на статью.Я же хочу поделиться с вами мнением: если мы копнем глубже, то кроме практик получим куда более значимые идеи для построения и развития систем устойчивости у себя.Говорим на одном языкеНельзя просто так заявить что-то об инженерии устойчивости и сделать это без ссылок. Одна из мотиваций создания этого материала —  трудность поиска информации в РФ сегменте интернета. Пожалуй, первым доступным докладом на эту тему былматериалАлексея Кирпичникова.Если мы работаем в компании с достаточно зрелыми процессами, чтобы переживать за доступность, то зачастую сами вводим эти понятия.Пойдя по самому простому пути и взяв оригинальное«Availability», мы без труда найдем по перекрёстным ссылкам термины «Safety engineering», «Reliability engineering», «Resilience engineering» и ещё много всего. В таком утрированном примере видно, что без понимания предмета поиска не так-то просто разобраться в теме. А разбираться есть в чём: большинство упомянутых определений буквально говорятоб одном и том же—умении системы не только падать, но и вставать.Забегу вперед и скажу, что человеческое восприятие очень важно в создании устойчивой системы:«Не сами эти вещи, а только наши представления о них делают нас счастливыми или несчастными»— Эпиктет.✨Истина — продукт интеллектуального анализа человеком его эмпирического опыта. Идеи о важности адаптации нашли отражение в самых разных сферах зания, далеко не только в IT:«Резильентность» в физике— способность упругих тел восстанавливать свою форму после механического давления.«Резильентность»в психологии и биологииподразумеваетпластичность—  умение адаптироваться к травмирующим событиям, считается врожденным свойством, которое можноразвивать.«Резильентность»в экономикепонимается ближе к дословному переводу этого слова — «упругость», «эластичность», «гибкость».Встроительной инженериивводится свойствоrapidity— способность делать эффективные выводы для предотвращения сбоев в будущем.ЕщёНейман в 1956 годуупоминал неотвратимость ошибки и важность реагирования на них. Во многих доменах и науках мы оперируем едиными понятиями, зачастую выполняя похожую работу, добиваемся похожих выводов и характеристик эксплуатируемых систем!Итого:все практические определения устойчивости исходят из «гибкости» субъектов и их способности переживать стресс.Как с позиции устойчивости выглядит системаВ подавляющем большинстве систем гибкость реализуется за счёт прямого участия человека и принимаемых им решений. Все, кто запускают релизы, проводят плановые работы, проектируют и тестирует сервисы с учётом рисков, сами являются частью ими создаваемойсоциотехнической системы.У этого видения есть конкретная модель, явно описаннаяв работе Stella.reportи более ранних материалах:Иллюстрация, показывающая разность ментальных моделей у людей, которые эксплуатируют системы через явный уровень представления — зелёную линию на рисунке.Для построения действительно устойчивой системы не следует игнорировать факт, что люди одновременно являются источниками и изменений, и реагирования на них. Именно поэтому снижение когнитивной нагрузки на инженера — помощь ему в решении аналитических задач или упрощение рутиных операций — может привести к куда более позитивному влиянию на повторяемость инцидентов, чем создание очередного тест-кейса или автоматического rollback.Вот пара докладов на эту темуИнженерия устойчивостиТекущий взгляд на вопрос — результат десятилетий исследовательской работы. Централизованно она началась  в1987 годугруппой энтузиастов из числа докторов наук на базеЛаборатория когнитивных систем университета Огайо: учёные в области медицины, автоматизации, разведки, авиации и атомной энергетики проводили исследования в области контроля инцидентов в своих доменах. Несмотря на регулярную частную работу друг с другом, первая совместная конференция из 14 участников состоялась в 2004 году.Ключевые современники в области инженерии устойчивости. Все основные работы собраны наresiliencepapers.club (github: lorin/resilience-engineering)Существуют несколько взглядов на природу устойчивости, которые описаны и обсуждаются сообществом, но все они дополняют друг друга.Профессор Hollnagel предлагает4 ключевых способности: если система наделена ими, то может считаться устойчивой.1. The potential to respond — реагирование.2. The potential to monitor — отслеживание.3. The potential to learn — обучение.4. The potential to anticipate — прогнозирование.На первый взгляд они звучат банально и даже базово, но не всегда мы можем их правильно применить. Подобный пример приведу чуть ниже.Доктор David Woods выделяетдва критерия устойчивости:Graceful extensibility (плавная расширяемость)— способность системы расширять свои возможности к адаптации, когда неожиданные события нарушают ее границы. Она содержит в себе способности управлять рисками, иметь адаптивные связанные компоненты и  обходить различные неожиданные ограниченияSustained adaptability (устойчивая адаптивность)— способность системы продолжать адаптироваться к неожиданностям в течение длительных периодов времени.В системе, которая способна оперативно адаптироваться и делать это постоянно, влияние инцидентов выступает в роли драйвера изменений, а не только как источник потерь и нежелательных препятствий в работе. Если попытаться представить границы системы (на уровне технических ограничений или процессов), то расширяемость будет направлена на раздвигание границ, а адаптивность на удержание дистанции от их пересечения. Это выражается в характеристикахreboundиrobustness.Расширяемость ведёт к адаптивности всей системы и, как следствие, возможности отработать аномалииПаттерны отказов в современных системах«Допустим!» — скажете вы, — «Но что конкретно мне с этим делать?». На практике применить это подходы довольно просто. Более того, скорее всего используете их по наитию. Сейчас вы это отрефлексируете и будете пользоваться этим ещё эффективнее :)Переводя теорию в практические примеры, Вудс предлагает несколько базовых паттернов отказов:Decompensation — «не вывезли»«Потеря системой способности эффективно функционировать при появлении стрессовых ситуаций, выходящих за пределы её предполагаемых возможностей или дизайна».Приведу пример из практики. В 2020-2021 годах в системах СберМаркет работа с ожиданиями существовала только в виде статичных трешхолд алертов. Мы ловили десятки инцидентов в разных местах системы, которые показывали наше незнание границ нагрузки или их некорректное проставление.Определение ожиданий, требований и условий работы — это такая база, без которой сложно представить современную сложную систему. На практике для этого существуют десятки инструментов и специальные подходы: SRE предоставляет намконцепцию SLOи допустимый бюджет ошибок, регулярные тесты на достижимость пределов. Как раз это и подчёркивает Resilience engineering!Практики существуют, они не новы, но концентрироваться на них полезно именно с позицииадаптации систем, а не мнимой уверенности предотвращения проблем или достижения высокой доступности.Примерный вид детализации индикаторов и зависимость не только от единичных технических метрик, но и некоторых сценариев.Концепт SLO можно применить как к сервисам, так и к функциям, которые обеспечивает продукт. В конечном счёте важна работоспособность именно сценариев, приносящих пользу, а не сферических сервисов из 100 API-эндпоинтов.Working at cross purposes — конфликт целей«Различные компоненты системы стремятся достичь своих целей, но при этом несовместимы или даже противоречат друг другу» .Мы как большая система в процессе изменений обросли собственной структурой, иерархией, набором правил. Работая со 100+ командами, обязательно столкнёшься с пересечением задач и даже интересов, что иногда приводит к конфликтам. В неустойчивой системе такое движение регулярно, а конкуренция целей приводит к пагубному влиянию на пересекающие функции.Широкоизвестная методология DevOps одним из решений предлагаетShift Left. Получив возможность внедрять практики дизайна, тестирования и валидации раньше, а не последовательно друг за другом, мы способны предостеречь немалое количество потенциальных проблем. В этом вопросе Resilience engineering тоже не противопоставляет себя уже существующей информации, а дополняет и качественно описывает влияние таких проблем на наши системы.Getting stuck in outdated behaviors — устаревшие правила«Cистема или её компоненты продолжают применять стратегии, которые стали неактуальными или неэффективными из-за изменений в окружающей среде или условиях использования».На примере нашего же Купера можно утверждать, что нагрузка неоднородна и в основном растет — на это сказывается сезонность нагрузки в сфере e-com.‎Для tech и этих: Как в IT справляются с пиковыми нагрузками. Бонус on Apple Podcastspodcasts.apple.comИспользуя подходы устойчивости можно заранее подготовить систему к тому, что способы работать с определенной нагрузкой постоянно будут терять актуальность.Если в 2021 году мы могли масштабировать систему физически или вообще позволить себе деградацию 50% без влияния на продуктовые метрики, то с увеличением числа клиентов такая возможность рискует исчезнуть. Поэтому каждый раз на планировании следует задавать себе вопрос про альтернативные способы и готовиться:отрезать кусок нагрузки, которые не сможешь вывезти (graceful degradation);использовать заранее подготовленные данные для балансировки потока (кеширование, шардирование);а может и вовсе отказаться от ряда функций, заменив их более легкой версией (пересмотр архитектуры).Мы прошли через похожие примеры.Первый касается процессов с технической стороны:→ Представим, мы внедряемканареечные развертываниядля снижения влияния инцидентов и появления автоотката. Какое-то время это положительно сказывается на инцидентах и мы считаем, что успешно справились с проблемой «плохих релизов».→ Но не применяя эту практику повсеместно, столкнёмся с тем, что подходfeature flagв разработке без аналогичногоиспользования акторовточно так же откатит нас на прошлое хаотичное состояние — включение флага возможно сразу на всех клиентов и, если он безопасно доставляется канарейкой, то включение вернёт проблему «сломано всё и сразу».→ ОтслеживаниеChange Failure Rateпоможет не упустить этот момент и вовремя адаптировать подходы.Второй — с организацинной стороы:→ В сезон выскоких нагрузок стандартный цикл итерации в 2 недели может вам сильно навредить. Более 50% задач в  высокий сезон — ad-hoc.→ Желание сохранить процесс ради процесса помешает сохранить предсказуемость и гибкость в выборе приоритетов, которая изначально в него закладывалась.→ Переход на недельные итерации или выделение пула специалистов под эти задачи будет более адаптивным решение в новых условиях.Как применять теорию на практике можно подсмотреть и у другихНапример, разбирая упомянутое и в работах доктора Кука «известное состояние ошибки», инженеры из Яндекс.Go отрабатываютMetastable Failure State.Давайте визуализируем этоЗная теорию, мы можем попытаться собрать всё воедино.В любой момент времени наша социотехническая система балансирует среди множества ограничений вида ресурсной вместимости, бюджетов, ожиданий и границ нагрузок.Если представить её за точку в пространстве, то она может двигаться под давлением этих границ или же в попытках избежать с их пересечения. Ярким известным примером будет невозможность расширения ресурсов с одновременным увеличением прибыльного потока, что нередко приводит к неспособности организации справиться с входящими техническими нагрузками.Визуально это можно выглядеть так:Тогда способность адаптироваться можно изобразить как создание отдельной границы собственных ожиданий и появление бюджета ошибок — та самая практика в виде SRE-подходов, которые позволяют заранее отойти от критической линии появления инцидента.Реакция системы на прохождение неожиданных границ.Представлена в докладе от 2013и визуализирует свойствоreboundВ реальности проблема заключается в том, что в ежеминутно меняющемся мире мы не всегда можем узнать положение границ, а тем более конкретного способа их нахождения. Для этого на практике мы используем какие-то подходы, например, нагрузочное тестирование, чтобы попытатьсяпоэтапно определить границыи выстроить собственные ожидания. Всё наше представление — не более чем желания и предположения, которые устаревают с течением времени и при отсутствии регулярных перепроверок.Поэтапное движение системы в попытках «прощупать» границу допустимых значенийЕсли довериться такому представлению, то устойчивость системы строится на понимании:Где мы находимся в действительности — реагирование.Как изменяется происходящее вокруг  — отслеживание.Где именно проходит грань инцидента — прогнозирование.Что мы делаем под давлением факторов влияния — обучение.Как можно заметить, это прямое пересечение с предложением Холлнагела.Ключевые личности, материалы и ссылкиМодель, описанную выше, изначально представилJens Rasmussen. В коллаборации с другими исследователями она была адаптирована под современную теорию. Не менее ключевым примером может являтьсяархетип устойчивости, предложенный доктором Ричардом Куком в работеA Few Observations on the Marvelous Resilience of Bone & Resilience Engineering:Он описалкостькак архетип устойчивости, ведь  на самом деле (вопреки распространенному мнению о том, что кости статичны) они динамичны. Кости непрерывно заменяются в течение примерно десяти лет, балансируя между разрушением старой кости и созданием новой. Этот процесс, являющийся динамическим балансом, иллюстрирует ключевые принципы устойчивости, такие как адаптивность и постоянная трансформация в ответ на механическое напряжение. Реализуя свойство «network of adaptive units», кость является многоуровневой сетью без центрального элемента. Большое количество перекрестных связей позволяют ей быть хорошим примером отказоустойчивой системы, поскольку она демонстрирует как расширяемость, так и устойчивую адаптивность.Эта метафора помогает лучше понять идею проектирования устойчивости: как системы могут быть одновременно надежными и гибкими, способными адаптироваться и развиваться перед лицом стресса. На примере взаимодействия с системой, которая сама по себе является устойчивой, Ричард приводит и примеринженерии, которая совершается над ней – за счет понимания только её принципов, без знания истинной природы объекта.Ссылка на сам докладВ содержании я слукавил. С активными участниками мы уже познакомились и можем узнать подробнее по приложенным ссылкам, а общая агрегация поддерживается наGithub. Этот раздел оставил потому, что считаю важным подробное изучение вклада личностей — это помогает самому лучше понять, куда движется направление. А ещё предостеречь от судьбы других полезных практик, которые сейчас скрываются за десятками инструментов и абстрактным текстом, потеряв в массах изначальные идеи.Теперь, обратившись к таким источникам, какmap.r9y.dev, попробуйте посмотреть через призму адаптивности на вашу команду или компанию.Вместо выводаЯ не претендую на собственный научный взгляд, хоть в процессе и старался подтвердить информацию с помощью авторитетных источников. Но уверенно называю инженерию устойчивостиважнейшим инструментом для выживания, потому что аналогично доктору Вудсу убедился: «Reliable systems still fail».Надежная и высокодоступная система не защищена от инцидентов и, более того, она может быть хрупкой:Хрупкость появляется тогда, когда мы достигаем наших границ понимания и производительности.Мы не можем прятаться за линейными упрощениями. Нам нужно воспринимать систему комплексно, явно учитывать её связи и всех «агентов», включая людей.Данный доклад и статья — отправная точка для самостоятельной работы каждого из тех, кого эта тема заинтересует.Tech-команда Купера (ex СберМаркет) ведет соцсети с новостями и анонсами. Если хочешь узнать, что под капотом высоконагруженного e-commerce, следи за нами вTelegramи наYouTube. А также слушайподкаст «Для tech и этих»от наших it-менеджеров."
СберМаркет,,,Как мы апгрейднули поисковик в приложении Купера с помощью fastText и XGBRanker,2024-08-02T10:25:43.000Z,"Привет, Хабр! Меня зовут Аня Южанина, я работаю ML-инженером в Купере (ex СберМаркет). Сегодня я расскажу о межретейлерном поиске. Это когда вы ищете какой-то товар и Купер показывает его в ассортименте разных ретейлеров. Зачем вообще нужен такой поиск и как внедрить умное ранжирование магазинов?Виды поиска в КупереУ нас есть два вида поиска.Первый — внутри магазина. Пользователь выбирает одного ретейлера (например, METRO) и собирает корзину там. Этот поиск полезен если, пользователь предпочитает заказывать из конкретного ретейлера.Второй вид — межретейлерный поиск, который позволяет искать товары среди всех магазинов, которые представлены в Купере. Этот поиск полезен, если нужные пользователям приложения позиции представлены в разных ретейлерах и решение о выборе магазина для заказа они принимают, основываясь на другие параметры — скажем, по времени доставки.Два вида поиска в интерфейса нашего приложенияПоиск среди всех магазинов работает так:Пользователь вводит запрос.Мы определяем все доступные магазины по геопозиции пользователя.В каждом из доступных магазинов осуществляем поиск товаров по запросу пользователя.Ранжируем магазины, в которых что-то нашли, и собираем поисковую выдачу товаров для каждого магазина.Допустим, я готовлю завтрак и вдруг понимаю, что забыла купить авокадо. Ввожу «Авокадо» в поиск Купера, и на экране появляется вот такая страница:Сначала идут магазины, где есть авокадо (METRO, «Лента», «Глобус»), потом — выдача из выбранного магазина.Но как приложение определяет, что именно для этого запроса сначала нужно показать именно METRO, а не «Ленту»? Каковы принципы и технология ранжирования? Все ответы даю дальше!Давным-давно в далекой-далекой галактике……у Купера не было умного ранжирования.Вне зависимости от соответствия предпочтениям пользователя на экран выводились товары ретейлера с наибольшей выручкой (при наличии совпадений). Чаще всего это был METRO.Но в Купере представлены самые разные товары: это и продукты, и электроника, и украшения, и лекарства, и что еще только не!В приложении больше 20 категорийЕсли пользователь хотел приобрести, скажем, новый смартфон и вводил в поиск «Apple», он мог в первую очередь наткнуться на бурбон из METRO:Чем это было плохо?Во-первых, пользователю нужно было листать список магазинов до тех пор, пока на экране не появлялся релевантный товар. Это могло раздражать и снижать уровень удовлетворенности от работы с приложением.А во-вторых, пользователь мог просто не сориентироваться и подумать, что ему показывают нерелевантные товары просто потому, что нужных нет в наличии.Обе ситуации ведут к тому, что пользователь реже заходит в приложение или даже перестает быть нашим клиентом.Первый подход к ранжированному поискуКогда мы впервые сели за задачу, то решили разделить процесс ранжирования на два этапа.На первом этапе мы определяли, связан ли поисковый запрос с ресторанами. В Купере рестораны представляют собой около четверти всех ретейлеров. В Москве пользователи могут иметь одновременный доступ примерно к 500 наименованиям!Классификатор разработали на основе моделиfastText. Она содержит обученные векторные представления слов, то есть смыслы, которые зашиты в форму, которую может понять компьютер.Мы обучили модель бинарной классификации, где целевой переменной было добавление товара или из ресторана, или из магазина. Например, по запросу «хинкали» товар может быть добавлен из обоих типов ретейлеров, однако чаще мы заказываем хинкали все-таки из ресторанов, а потому модель склонна классифицировать этот запрос как относящийся к ресторанам. И так со многими блюдами.На втором этапе мы ранжировали рестораны относительно друг друга и магазины относительно друг друга на основе статических данных: сколько раз товар по данному поисковому запросу добавлялся из данного ресторана или магазина?Когда мы провели A/B-тестирование, то поняли, что поиск с ранжированием воспринимается пользователями гораздо лучше. Это мотивировало нас продолжить работу и учесть при ранжировании дополнительные факторы.Доработанная модельВо втором подходе мы сохранили классификатор на основе fastText, но заменили ранжирование на основе статистических данных ранжированием на основе предсказаний моделиXGBRanker.Мы стали учитывать:статистику по добавлениям в различных срезах: в целом по запросу, по запросу и региону, по запросу и конкретному пользователю;стоимость и скорость доставки;популярность ритейлера;цены товаров в ритейлере.Чтобы модель XGBRanker понимала, насколько релевантен для пользователя тот или иной ритейлер, ее обучали по следующим целевым значениям:Пользователь выбрал магазин, добавил из него товар и совершил покупку.Пользователь выбрал магазин, добавил товар, но потом удалил.Пользователь выбрал магазин, посмотрел выдачу или какой-то товар, но не совершил дальнейших действий;Магазин был в поисковой выдаче, но пользователь проигнорировал его.Шансы, что пользователь быстро доберется до нужного товара, стали гораздо выше. Релевантность выдачи в межритейлерном поиске (NDCG) выросла на 5 п.п.Улучшилась даже выдача для категорий, не связанных с продуктами питания. Например, по запросу «наушники» теперь первым делом показываются специализированные магазины, такие как «Технопарк», вместо продуктовых магазинов, где тоже могут продаваться недорогие наушники.Влияние на бизнесПосле второго подхода мы снова провели A/B-тестирование, и оно снова показало, что новое решение лучше :) Выросли все метрики конверсии и среднее количество товаров, добавленных из межритейлерного поиска. Статистически значимо увеличилась выручка — даже жаль, что я не могу поделиться конкретными цифрами! А еще снизилась доля пустых выдач и частота использования межритейлерного поиска. Последнее свидетельствует о том, что пользователи стали быстрее находить нужные товары и, следовательно, реже возвращаться к поиску.Большой прирост по метрикам ранжирования магазинов и ресторанов дала двухэтапная модель, где на первом этапе используется классификатор, а на втором — модель XGBRanker.Что еще мы попробовали + наши планыОтдельно хочется упомянуть поиск по названиям магазинов и ресторанов.Как я писала выше, в нашем приложении представлены сотни ресторанов. Раньше поиск выдавал нужный ресторан только в том случае, если запрос полностью совпадал с официальным наименованием: по запросу «Кулинарная лавка Братьев Караваевых» выдача была, а по неполному запросу «Братья Караваевы» — уже нет.Мы внедрили полнотекстовый поиск по названиям и их синонимам: приложение делает запрос в Elasticsearch и бустит наверх те магазины и рестораны, с названиями и синонимами которых найдены пересечения.Однако эта функция пока остается в разработке, потому что результаты A/B-тестирования оказались неоднозначными.Также мы собираемся расширить классификатор, добавив разделение не только на рестораны и магазины, но и на магазины разных видов (продукты, электроника, аптека и т. д.).Буду рада, если моя статья окажется для коллег интересной и полезной. Как вы думаете, как еще можно улучшить пользовательский опыт в Купере? Поделитесь идеями в комментариях :)Tech-команда Купера (ex СберМаркет) ведет соцсети с новостями и анонсами. Если хочешь узнать, что под капотом высоконагруженного e-commerce, следи за нами вTelegramи наYouTube. А также слушайподкаст «Для tech и этих»от наших it-менеджеров."
СберМаркет,,,Как начинающему тимлиду не сойти с ума от обилия задач: практический гайд,2024-07-25T10:14:21.000Z,"Привет, Хабр! Меня зовут Капитолина Кузнецова, я Ruby-разработчик и на своём текущем месте работы в Купере (ex СберМаркет) доросла до роли тимлида. За 2,5 года я, так сказать, освоилась в этом статусе и, если верить моей команде, руководитель из меня получился неплохой, но вьетнамские флэшбэки о своих первых месяцах работы в новой роли ещё свежи.Быть начинающим тимлидом — это...когда встречи занимают по семь, восемь, а то и девять часов подряд;когда ты целый день что-то делаешь, дико устаешь, а в конце не видишь никакого результата;когда о Jira знаешь больше, чем о собственной семье, а встречи с друзьями планируешь на новогодние праздники 2025 года.Эта статья — попытка структурировать всё, что может помочь вам в новой роли. А также моя рефлексия о том, какие способы помочь себе, реально облегчат вам жизнь и помогут быстрее справиться с новым масштабом. Знаю, что такие проблемы были не только у меня, но и у многих других ребят, которые раньше просто писали код, а не вот это вот всё. В общем, надеюсь, мои заметки будут вам полезны :)Чтобы не сойти с ума нужно всего лишь...Начнём с базы. Итак, вам предложили перейти в должность тимлида. Рекомендую сразу же после этого...1. Пройти базовое обучениеОбязанности тимлида сильно разнятся от компании к компании. Но есть основы, которые стоит изучить: как давать фидбэк команде, как нанимать, как ставить цели и презентовать результаты.Возможно, вам повезло и у вас была классная ролевая модель в виде лучшего на свете руководителя, который научил вас эмпатии, лидерству и адекватности (а не чайка-менеджменту) личным примером. Но если есть возможность, рекомендую всё же попросить корпоративное обучение по менеджменту. Лишним это точно не будет, может и с синдромом самозванца поможет справиться.У нас я попала на специальный курс для «новеньких» тимлидов, но можно поискать и что-то еще. Вот хорошийсписок курсовпо тимлидерству. Ещё я очень советую книгу«Практики регулярного менеджмента»— она помогает найти простые алгоритмы почти на любую ситуацию, с которой вы столкнетесь в новой должности.2. Узнать критерии успехаВыяснить, что хотят от вас в новой должности — половина успеха. Для этого стоит обратиться к своему руководителю и спросить его про ожидания. Банально, ожидает ли твой руководитель, что ты будешь погружать его во все активности или что ты будешь решать все самостоятельно. Нужно ли «совершить прорыв» или цель на ближайшие месяцы — чтобы всё просто работал и не ломалось. Лучше синхронизироваться на этот счёт как можно раньше, так тебе будет понятнее, как себя оценивать и ты не будешь дополнительно себя накручивать. Стресса и так будет много, загоны в стиле «А что обо мне на самом деле думает руководитедь» — точно лишние :)3. Расставить приоритетыЭффективно делать всё, везде и сразу невозможно, особенно на старте. Поэтому важно понять, какие из поставленных задач наиболее важны, а что можно отложить или уделить этому меньше внимания. В первые месяцы лучше сделать это вместе с руководителем — умение правильно расставлять приоритеты приходит со временем.4. Синхронизироваться с командойПроговорить ожидания насчет результатов стоит не только с руководителем, но и с командой. Я вообще топлю за открытую и дружелюбную коммуникацию, мы же в одной лодке, в конце концов. Обсудите цели, важные для вас моменты по процессам и коммуникациям, обозначьте, какие ритуалы вам кажутся действительно важными, а чем можно пренебречь. Так команда тоже будет меньше стрессовать.Впрочем, любой начинающий тимлид может столкнуться с тем, что задач много, а в сутках по-прежнему 24 часа.Где взять время?Здесь могу дать пять практических советов, которые помогли мне разгрузить свой график:Избавиться от лишнего— пересмотреть свой календарь, почистить и убрать из него ненужное. Подумайте: «А мне реально нужно быть на этой встрече? Может просто короткой переписки будет достаточно?» Полезный лайфхак — отмечать разными цветами слоты под совместную работу, личное время и все остальные встречи. Тимлиду в начале пути реально нужно высвободить себе время, чтобы был ресурс решать рабочие проблемы. А ещё есть такая штука как «день или время без встреч» — она эффективна, чтобы спокойно поработать не отвлекаясь на разговоры.Вести списки— эту идею я подсмотрела в«Джедайских техниках»Максима Дорофеева и она здорово упростила мне жизнь.Есть множество приложений для этого, например, я сама пользуюсьMicrosoft ToDo. Списки помогают не забыть ничего важного и освободить голову для важных мыслей. Их можно тоже разбить на категории: сделать сегодня, в ближайшие дни или на когда-нибудь потом.Установить границы —начинающему тимлиду может показаться, что всех нужно спасать, но помогать команде в режиме 24/7 невозможно. Это прямой путь к выгоранию. Определите границы своего рабочего дня и соблюдайте их. Например, я завожу робот-пылесос на 8 вечера каждый день — это сигнал для меня, что пора закрывать ноутбук.Не стараться делать все идеально, ведь десять хорошо сделанных дел лучше, чем одно идеальное, а времени они отнимут практически одинаковое количество.Делегировать— этому можно и нужно учиться, даже если вам кажется, что вы все об этом знаете. Например, вот крутаястатья на Хабрена эту тему.А еще можно брать несрочные задачи «для души». Если вы изначально были разработчиком, вам точно будет не хватать написания кода. Такие задачки возвращают жажду к жизни, во всяком случае, мне.Как же не ошибиться и не расстроить всех?Разрешить себе ошибаться. Накосячили, сделали выводы и идем дальше, стараясь этого больше не допускать.Знаю, некоторым помогает найтиментора— опять же можно попробовать сделать это за счет обучения в компании хе-хе. Но самое главное, как мне кажется, обязательно регулярно просить обратную связь. Чем быстрее наломаете дров и узнаете об этом, тем быстрее научитесь их не ломать. Проводите встречи со своими непосредственным руководителем раз в неделю, чтобы обсудить все, что вас волнует. И не бойтесь идти за помощью.Как оценить результаты?Итог работы руководителя — это результат работы команды. Чтобы его оценить, можно выбрать важные именно для вас (или для компании) метрики. Например, текучка. Если люди бегут из вашей команды, сюрприз-сюрприз, что-то не так с процессами или условиями работы.Я доверяю такому инструменту какHappy Index (Health check)— это опросник о вовлеченности или удовлетворенности работы в команде. Обычно он делается на компанию, но с разбивкой по подразделениям. Там можно найти свою команду, либо сделать подобное исследование именно для нее. Мне нравится проводить его самой — тогда я могу выбрать важные для меня вопросы. Повторять нужно регулярно и смотреть, как меняются показатели. Многим проще поставить анонимную оценку, чем сказать что-то при личной встрече, но, опять же, чем больше вы культивируете доверительные отношения в команде, тем более правдивую информацию вы получите с помощью такого среза. Другое дело, важно на это реагировать и действительно заботиться о состоянии своих ребят. Поэтому обратная сторона такого доверия — вам придется действительно решать возникающие вопросы :)В числе других используемых метрик можно выделить перфревью команды, попадание в сроки по проектам, статистику инцидентов в компании, но по вине команды, отчеты в Jire (загрузка команды, время на задачи) и др.Как организовать команду?В этом помогут регулярные встречи 1:1 и всей командой. Они занимают немного времени, но помогают избежать кризисов. Главное, проводить их не формально, а качественно: готовьтесь к встрече, ведите адженду. Так ты не забудешь, о чем хочет поговорить коллега и сможешь подготовиться.Развивать команду тоже важно, с этим может помочь индивидуальный план развития (ИПР). У нас он каждого сотрудника tech-блока. Еще я веду«Профиль разработчика»— список того, в чем хорош или не очень тот или иной инженер. Лайфхак — попросите коллег самих описать свои сильные и слабые стороны.  Это даст основу, от которой можно отталкиваться. Еще для развития задачи важно усложнять, но делать это постепенно.Важно не забывать показывать команде ее ценность и доносить результат выполненных задач до остальных коллег. Рассказывайте о своей команде на уровне компании, делитесь их успехами, это повышает мотивацию ребят и помогает им становиться ещё эффективнее, вдохновившись своими «подвигами».Куда стремиться?Я советую каждому тимлиду найти свой идеал, которому хочется подражать. Это может быть человек в компании, герой из интернета или классный специалист, встреченный на конференции. Черпать вдохновение можно из докладов или статей, читая их, например, в дороге, слушая подкасты во время пробежки или за рулем. Мне нравится хак с чтением профессиональной литературы в рамках занятий английским: так можно и подтянуть язык, и узнать новое, и увидеть другую точку зрения от коллег из разных стран. А наиболее интересные для реализации идеи заносить в долговременный список дел.Подспорьем может оказаться общение в рабочем комьюнити. Если его нет, можно даже создать свое сообщество. Так, у нас уже больше двух лет существует свойклуб тимлидов— мы организовали его, когда почувствовали необходимость в том, чтобы поддерживать друг друга и делиться опытом тимлидства. Это классная площадка, чтобы получить совет и поддержку и просто провести время в компании интересных людей.Тимлидом быть трудно, особенно в начале пути. Но оно того стоит. Надеюсь, что мои советы помогут вам решить возникающие проблемы, прийти к идеальному work-life balance и получать кайф от работы, успевая все.Tech-команда Купера (ex СберМаркет) ведет соцсети с новостями и анонсами. Если хочешь узнать, что под капотом высоконагруженного e-commerce, следи за нами вTelegramи наYouTube. А также слушайподкаст «Для tech и этих»от наших it-менеджеров."
СберМаркет,,,"«Коллеги, добрый день»: как проводить онлайн-встречи эффективно и укладываться в полчаса",2024-07-18T09:54:09.000Z,"Привет, меня зовут Марина Гончарова, я старший менеджер проектов в Купере (ex СберМаркет). Я работаю в IT с 2008 года, а проджектом — с 2015, и за эти годы у меня накопилось много знаний, приёмов и лайфхаков, которыми хочется делиться. Одна из таких животрепещущих тем — как сделать даже короткие встречи эффективными и результативными для всех участников.При работе в разных компаниях я сталкивалась с тем, что все встречи по умолчанию ставятся минимум на час, и даже за час мы не всегда успеваем всё обсудить. 8 рабочих часов, 7-8 встреч — и вот весь день занят, а работать некогда, ну или приходится перерабатывать вечером. Лёд тронулся во время пандемии, когда компании перешли на удалёнку, и стало ясно, что с таким обилием часовых встреч нужно что-то делать.Началась практика встреч на полчаса, но чтобы уложиться, обсудить только важное, и не уйти с открытыми вопросами, такие встречи нужно проводить по определённым правилам. И я подумала, что может быть полезно эти правила сформулировать и рассказать. Вам может показаться, что эти правила — это базовая база, но мне ни разу не попадались ни статьи на эту тему, ни в книжках я не видела лаконичного описания этой базы.Какие основные правила важно соблюдать, чтобы встречи проходили эффективноЯ считаю, что гигиенический минимум каждой встречи состоит из четырёх правил:Ставить встречи по календарю. Важно не просто назначать дату и время, а открывать календари всех участников и выбирать свободный для всех слот. Советую учитывать все календари, даже если встреча на большую группу.  Конечно, если во встрече участвуют топы, можно попросить остальных участников по возможности подстроиться, но в идеале нужно искать свободный слот для всех.Писать адженду. В приглашении  стоит написать, что вы хотите обсудить на встрече, чтобы участники могли подготовиться и настроиться на тему, и чтобы для всех повестка была одинаковой. Потому что одно и то же название встречи, например, «Обсуждаем итоги квартала» может быть воспринято по-разному: вы ждёте, что все подготовят мини-доклады про результаты, часть сотрудников думает, что нужно прийти и послушать речь руководителя, а кому-то может показаться, что итоги неудовлетворительные, и на встрече будут озвучивать санкции. А ещё понимание адженды позволяет участникам дополнительно позвать на встречу людей, которые, как им кажется, нужны для обсуждения темы.Проговаривать на встрече, каких результатов от неё вы ждёте. Это можно написать в адженду, но стоит дополнительно озвучить в начале встречи. Просто собраться и обсудить что-то — это одно, но если вы хотите принять какое-то решение, достичь цели — важно для начала это сформулировать для себя, а потом озвучить всем участникам. Чтобы понять для себя цели встречи, может помочь задать себе вопрос — а что будет успехом встречи?Писать протокол встречи(MoM — moments of meeting) и отправлять его всем участникам. Во-первых, это поможет восстановить хронологию событий, а во-вторых, из-за огромного количества коммуникаций и потока входящей информации людям свойственно довольно быстро забывать подробности встречи. Протокол помогает по пунктам восстановить всё, что вы обсудили, и заодно пропинговать вовремя тех, кто взял на себя задачи. Его удобно писать прямо во время встречи, заранее подготовив себе инструмент для записей, это может быть даже бумажный блокнот.И, конечно, важно следить за ходом встречи, не давать разговору уйти от темы и не отступать от финальной цели.Как регулировать ход встречи, чтобы участники не отвлекались от повесткиПравило удержания внимания участников во время встречи одно —постоянно напоминать, для чего мы собрались. Бывает, что люди подключают эмоции, и уходят от повестки, и здесь важно вернуться к теме, к цели этой встречи. Исключением здесь могут бытьретро, на которых сама цель встречи — обсудить прошедшую неделю, и нет задачи, которую нужно решить.Если обсуждение начинает уходить от заданного русла, я обычно записываю  возникшие новые вопросы и предлагаю участникам зафиксировать их и назначить отдельную встречу, или, например, обменяться мнениями в режиме чата.Бывает и такое, что в процессе обсуждения меняется цель встречи, и это тоже нормальная практика. Например, когда поднимается вопрос, который блокирует основную цель, и, пока он не будет решён, вы не сможете идти дальше по изначальному плану.Почему важно резюмировать встречу и подробно проговаривать все итогиВыше мы уже обсудили, что важно писать протокол встречи, но хочется заострить внимание на том, как важно проговорить итоги встречи в конце подробно со всеми участниками.Мы все разные, с разным бэкграундом и восприятием слов и контекстов. Одни и те же поинты в обсуждении могут быть восприняты совершенно по-разному. Поэтомув конце встречи я проговариваю достигнутые договоренностив первую очередь для себя, чтобы убедиться, что я на 146% поняла всё правильно, потому что с итогами встречи работать дальше в первую очередь предстоит мне. Обычно я зачитываю то, что было зафиксировано в процессе встречи, ну или рассказываю то, что планирую записать после встречи в качестве МоМ.Это поможет сделать работу над задачей прозрачной для всех участников, к этим записям можно будет возвращаться при возникновении спорных моментов, и для всех будут ясны следующие шаги по проекту.Как правила работают на практике: пример установочной встречиСразу оговорюсь, что сейчас для меня выполнение «гигиенического минимума» встречи — такая же простая и очевидная рутина, как почистить зубы. Я не прилагаю к этому специальных усилий, просто иду и делаю. Но так было не всегда — как и любой навык, ведение встреч выстраивалось постепенно.Приведу пример на установочной встрече по проекту. Не важно, будет ли эта встреча получасовой — механизм проведения всегда один.Сначала ячётко формулирую, что я хочу получить от этой встречи— отвечаю себе на вопрос: что я буду считать успехом этой встречи? Это может быть как формирование плана проекта (тут имею в виду глубокую проработку до плана прямо на встрече), так и назначение ответственных по проекту. Всё зависит от глубины предварительной проработки, состава участников и прочих факторов.На установочной встрече важно не уйти в бесконечно долгую дискуссию(читай как холивар)о проекте и о том, сколько он нам принесёт прибыли или другого профита.Потому что главная цель встречи — не похоливарить, а сформировать план проекта/определить ответственных по проекту/подставьте свою цель.В зависимости от цели ясоставляю список участников, выбираю по календарю удобный для всех слот, ставлю встречу, и пишу адженду о проекте, и о том, зачем мы его делаем, а главное — какой результат от этой встречи нам нужно получить. Потом дополнительно проговариваю это в начале встречи, немного углубляясь в детали, чтобы убедиться, что все участники в курсе.Слежу за ходом встречи: если не направлять дискуссию в нужное русло, совещаться можно бесконечно. В конце встречи у меня готово резюме, в котором подробно описано, о каких шагах мы договорились, на ком стоят задачи, какие у них дедлайны, и когда мы промежуточно сверяем результаты. Этот протокол встречи я отправляю в минимум два канала связи, это может быть письмо в почту и сообщение в общую группу в мессенджере. Это позволяет минимизировать недопонимания и ситуации, когда кто-то из участников просто не увидел резюме встречи и не знал или забыл, что от него чего-то ждут.Согласованные на встрече ответственные и дедлайны позволяют прийти к участникам встречи с kindly reminder о задачах. Лучше делать это не впритык к дедлайну, а заранее, так будет больше шансов вовремя синхронизироваться. Из-за огромного потока информации люди забывают о некоторых задачах и это нормально, моя задача — помочь и напомнить вовремя.ВыводыЧтобы встречи проходили эффективно, их результат был прозрачен для всех участников и задачи выполнялись в срок, важно соблюдать четыре основных правила:Искать время для встреч по календарю, а не придумывать его рандомно, и по возможности искать свободный слот для всех участников.Писать адженду, чтобы все участники встречи понимали, что именно вы будете обсуждать, и могли подготовиться.Проговаривать на каждой встрече, каких результатов вы от неё ждёте.Писать протокол встреч (МоМ) и отправлять его на всех участников во всех доступных каналах связи.Вероятно, все эти правила вы уже знаете. Но применяете ли? Каждый ли раз? Если не получается включить правила в рутину, но хотелось бы — можно выписать их на стикер и приклеить его на видное место, и через некоторое время, скорее всего, вы заметите за собой, что их соблюдение перестало быть «каторгой» и «тратой времени», а стало необходимым элементом в действии «назначить встречу».Конечно, я понимаю, что это не все лайфхаки, которые могут быть, и поэтому буду рада, если вы поделитесь своими правилами встреч в комментариях. Обсудим! :)Tech-команда Купера (ex СберМаркет) ведет соцсети с новостями и анонсами. Если хочешь узнать, что под капотом высоконагруженного e-commerce, следи за нами вTelegramи наYouTube. А также слушайподкаст «Для tech и этих»от наших it-менеджеров."
СберМаркет,,,Cтатистические критерии для начинающих,2024-07-16T12:08:56.000Z,"Привет, Хабр! Меня зовут Евгений Узянов, я продуктовый аналитик в команде геймификации Купера (ex СберМаркет). Каждый месяц мы проводим десятки экспериментов на миллионах клиентов, чтобы улучшить опыт пользователей и экономику сервиса. Мы выдвигаем много гипотез, одновременно проводим большое количество тестов с разнообразной спецификой —  поэтому нам важно глубоко понимать процесс проверки гипотез и уметь разобрать эксперимент до атомов.В этой статье поговорим про статистические критерии:узнаем, что это такоеразберем, в чем их «физический смысл»посмотрим, для чего они используются, и что с ними делатьизучим часто встречающиеся на практике кейсыЭта статья будет полезна для тех, кто начинает изучать AB-тестирование или хочет структурировать имеющиеся знания.Когда я только начинал изучать методы количественного тестирования, я искал информацию в большом количестве источников: университетские лекции, онлайн-курсы, литература разной степени глубины и, конечно же, ютуб. В значительном количестве случаев при знакомстве с очередной статистикой информация преподносилась в следующем формате:Держи страшную формулуВот какие-то графики с хвостамиНу а дальше все понятноИди работайВместо такого подхода мы разберем по винтикам несколько статистических критериев и попытаемся понять, что лежит за математическими формулами. В процессе вы увидите, что за громоздкими и страшными математическими конструкциями лежат простые и понятные идеи.ТерминологияСначала зафиксируем несколько важных понятий и разберем их на простых примерах.Функция плотности распределения случайной величины в точке Х: для простоты будем считать, что этовероятность принятияслучайной величинойзначения Х.Если же не привязываться к конкретной точке, то плотность распределения — это математическое правило, описывающее для каждого допустимого значения случайной величины вероятность того, что именно это значение будет принято.Пример c кубикомДопустим, мы рассматриваем идеальный игральный кубик: вероятность выпадения каждого из значений фиксирована и равна ⅙. Следовательно, если примем за Х значение кубика при произвольном броске, то наша функция плотности распределения величины Х будет выглядеть так:То есть вероятность принятия значений от 1 до 6 равна 1/6, а вероятность выпадения чисел меньше 1 или больше 6 равна нулю.Пример со школьникамиПредположим, мы изучаем выборку из миллиона случайных школьников с точки зрения распределения роста: практика покажет, что есть некоторый “средний рост”, равный 165 см, который наиболее распространен в рассматриваемой выборке. Также выяснится, что вероятность того, что у произвольного ученика окажется любой другой рост, падает по мере отдаления выбранного роста от среднего. То есть школьников ростом 160 и 170 см будет меньше, чем 165-сантиметровых. А дети ростом 130 см или 2 метра будут встречаться еще реже.Нормальное распределение миллиона школьников по ростуФункция плотности такого распределения называется функцией Гаусса, или просто гауссианом. Формула, описывающая рассматриваемый выше пример, как правило, не оставляет желания изучить ее поглубже:Но все не так плохо, ведь множитель— это всего лишь постоянная величина, отображающая максимальную высоту графика (когда степень экспоненты равна нулю). А степень экспонентытолько измеряет отклонение рассматриваемого значения х (в данном случае роста случайного школьника) от среднего роста по генеральной совокупности (165 см для миллиона школьников), выраженное в единицах стандартного отклонения множества вариантов роста школьников. Про последний коэффициент можем сказать следующее:он минимален и равен нулю, когда рассматриваемый рост равен среднемучем сильнее рост отклоняется от среднего, тем больше коэффициент b, и тем меньше значение плотности для такой точкиот μ зависит смещение графика относительно нуляот σ зависит «ширина» распределенияТаким образом, гауссиан представляет собой колоколообразную функцию, которая управляется средним значением и стандартным отклонением совокупности элементов.Функция распределения случайной величины:математическое правило, описывающее вероятностьпопадания случайной величины винтервал значений. Это определение связано с функцией плотности следующим образом: функция распределения — это интеграл от функции плотности ( то есть суммой плотности вероятности в каждой точке рассматриваемого интервала).Пример с кубикомВернемся к примеру с игральным кубиком. Для такого случая функция распределения может выглядеть, например, как вероятность выпадения числа не больше определенного значения p (допустим, не больше четверки). Тогда формула распределения будет следующей:Например, вероятность того, что выпадет значение меньше или равное 3:Это соответствует тому, что из 6 возможных исходов только 3 удовлетворяют условию x <= 3.Пример со школьникамиДля описанного случая со школьниками функция распределения может иметь смысл, например, вероятности получения случайного роста, не большего х. С точки зрения формулы это выглядит так:Если мы хотим узнать вероятность, с которой произвольный выбранный школьник имеет рост не больше, чем 145 см, необходимо подставить в формулу выше следующие значения:В результате получим вероятность около 2.3%.Статистическая гипотеза: если мы возьмем какую-нибудь случайную величину и выдвинем некоторое утверждение насчет его распределения, то оно и будет статистической гипотезой.Пример с кубикомВозьмем случайную величину «значение, выпавшее на игральном кубике» и выдвинем следующую гипотезу:кубик неисправен, и вероятность выпадения каждого из чисел не равна 1/6Пример со школьникамиВозьмем две произвольные школы из города N, ученики которых распределяются по росту, как в предыдущих примерах. Выберем из них по одному десятому классу соответственно. И выдвинем статистическую гипотезу:средний рост учащихся из двух выбранных классов одинаковСтатистический критерий (он же статистика и статистический тест):математическое правило, в соответствии с которым принимается или отвергается та или иная статистическая гипотеза с заданным уровнем значимости.Для этого определения мы рассмотрим более подробные примеры ниже:Критерий СтьюдентаДопустим, вы аналитик, к которому пришел менеджер продукта с новой фичей. Согласно гипотезе продакта, такое изменение гипотетически должно увеличить суммарную выручку с каждого клиента (ARPU).Теперь представим, что после построения дизайна и проведения эксперимента, проверяющего гипотезу, у вас есть набор данных о выручке с каждого клиента из тестовой и контрольной групп. Но как сравнить эти данные?Самое очевидное решение: сравнить средние значения суммарной выручки в каждой из групп. Но как в таком случае понять, насколько мала или велика полученная разница? Допустим, отличие составляет 17,5 рублей, как в таблице ниже — как интерпретировать такое значение?МетрикаКонтрольная группаТестовая группаАбсолютная разницаARPU3048.1 рублей3065.6 рублей17.5 рублейНасколько полученная разность выборочных средних значительна, будем оценивать исходя изстандартного отклонения разности выборочных средних— оно представляет собой меру разброса возможных значений разности средних выручек с клиента между тестовой и контрольной группой. Полученную оценку «значительности» назовем переменной t.Еще раз и помедленнееВ проведенном выше эксперименте мы случайным образом выбрали из множества пользователей тестовую и контрольную группы, для которых посчитали разность выборочных средних значений. Если проведем такую операцию достаточно большое количество раз, получим распределение разности выборочных средних — то есть распределение случайной величины «разность ARPU между тестовой и контрольной группой».У этого распределения есть стандарное отклонение — его и будем использовать как меру, описывающую уровень разброса возможных значений разности выборочных средних.В реальности бывает затруднительно определить истинное стандартное отклонение разности выборочных средних. Вместо него в знаменателе будем использоватьоценку данного параметра— ее называютстандартной ошибкой разности выборочных средних. После математических преобразований получим формулу:Поздравляем, мы только что изобрели критерий Стьюдента! Он используется в ситуациях, когда:рассматриваются случайные нормально распределенные независимые величины,стандартное отклонение генеральных совокупностей неизвестны,сравниваемые выборки независимы, имеют одинаковое стандартное отклонение, а в каждой из них не менее 30 наблюдений.В моем опыте это наиболее часто используемый критерий, который посленекоторых преобразованийданных применим в огромном количестве случаев.Критерий Манна-УитниДопустим, мы столкнулись со следующей ситуацией: служба поддержки использовала новый скрипт для общения с клиентами. После этого собирали обратную связь от пользователей о качестве сервиса — от 1 до 10 баллов. Результаты получились следующими:Старый скриптНовый скриптid пользователяОценкаid пользователяОценкаcc61bb003e2192d0f1cffcc1c0-7452aac80b2d5e2722a5fdfdebaa2973bf0105aec0dd983488ebe02675133ce6315f659e076d0b7521435ad51008e1726c4690f215aef89d3916d299ef620b7858c7244991661987e87af79d139201a9ce0937adf0a810Как в такой ситуации выявить лучший скрипт?Заметно, что данные не распределены нормально, а количество наблюдений слишком мало для применения критерия Стьюдента. В таком случае применимы непараметрические критерии. В качестве примера рассмотрим критерий Манна — Уитни. Он используется, когда:есть две независимые выборки (большее количество рассматриваетсякритерием Краскера — Уоллиса),выборки ранжированы, а элементы упорядочены относительно друг друга,в каждой выборке не меньше трех наблюдений.Расчет состоит из нескольких шагов.Пронумеруем все значения в порядке возрастания.Старый скриптНовый скриптidОценкаРангidОценкаРангcc61bb0035e2192d0f11cffcc1c0-747.552aac80b22.5d5e2722a59.5fdfdebaa22.5973bf01059.5aec0dd9835488ebe0261175133ce63515f659e07126d0b752147.535ad5100814.5e1726c469190f215aef814.59d3916d29199ef620b7814.558c724499191661987e814.57af79d13919201a9ce091937adf0a81022Когда встречаются несколько одинаковых значений, они нумеруются произвольно. Затем с каждым сопоставляется среднее значение среди таких рангов. Например, оценки, равные 3, имеют ранги 4 (клиент cc61bb00), 5 (клиент 52aac80b) и 6 (клиент fdfdebaa). Взяв среднее арифметическое по рангам одинаковых оценок, всем «тройкам» сопоставляется ранг «5».Для каждой группы произведем расчет суммы рангов.Основываясь на предположении, что гипотеза о равенстве медиан верна и элементы обеих выборок равномерно распределены между собой, для обеих групп посчитаем ожидаемую сумму рангов. Формулы ниже по сути представляют собой сумму арифметической прогрессии.Добавим к ожидаемым рангам дополнительный коэффициент, равный общему числу пар элементов, которые можно образовать из элементов первой и второй выборок. При неравных размерах выборок (как в нашем случае) он помогает учитывает вклад большей выборки в общую сумму рангов и позволяет нам корректно оценить ожидаемую сумму рангов элементов меньшей выборки.И, наконец, посчитаем критерий Манна-Уитни в переменной U.Как видим, суть параметра U и рассматриваемого статтеста заключается в поиске минимального размера отклонения фактической суммы рангов от ожидаемой (т. е. Той, которая существовала бы при равномерном распределении рангов в группах).Критерий ПирсонаТеперь рассмотрим пример с анализом категориальных данных — таких, у которых все возможные значения составляют фиксированный набор категорий, а не чисел, измеряющих величину на непрерывной шкале. Например, человек может описывать свой пол как мужской или женский, а деталь может быть синей или зеленой.Допустим, коллеги из маркетинга принесли вам данные ниже и попросили оперативно выяснить, есть ли связь между выдачей промокода на первый заказ в сервисе и оформлением второго заказа — то есть тем, возвращается ли клиент.Сделали первый заказ с промокодомСделали первый заказ без промокодаНе вернулись в повторный заказ60300Вернулись в повторный заказ10390Для этого упрощенного случая хорошо применим критерий Пирсона (еще его называют критерием независимости хи-квадрат). Говоря простыми словами, эта статистика основывается на сравнении разницы между ожидаемыми и фактическими значениями в каждой ячейке таблицы. Наблюдаемые значения — те, которые мы просто получили на практике. Ожидаемые — те, которые мы ожидали бы увидеть, если бы переменные «факт выдачи промокода на первый заказ» и «факт возвращения в повторный заказ» были действительно независимыми.Формула для критерия Пирсона выглядит так:Как видим, мы всего лишь считаем и суммируем отклонение наблюдаемого значения в ячейке относительно ожидаемого значения в единицах ожидаемого значения, суммируя результаты такой операции по всей таблице.Подробнее про расчетыДля расчета ожидаемого значения в ячейке в строке i, столбце j используем формулу ниже:Например, для ячейки с индексами i=1 и j=1 (клиенты, которые сделали первый заказ с промокодом, но не вернулись в повторный заказ) ожидаемое значение будет выглядеть так:Таким образом, исходная таблица с ожидаемыми значениями будет выглядеть следующим образом:Сделали первый заказ с промокодомСделали первый заказ без промокодаНе вернулись в повторный заказ33.16362.84Вернулись в повторный заказ36.84363.16Используя наблюдаемые и вычисленные ожидаемые значения, а также приведенную выше формулу критерия Пирсона, получим:Критерий Пирсона применяется в ситуациях, когда:есть две выборки с независимыми наблюдениями,в каждой из выборок не менее 30 наблюдений,данные дискретны.Что делать с полученным критерием?Допустим, мы начали понимать, что лежит в механизмах некоторых критериев, а также научились считать их для ряда случаев. Конечно же, возникает вопрос: что дает посчитанное значение критерия? И как с его помощью оценивать уровень значимости принятия / отвержения гипотезы?Для поиска ответа вернемся к примеру с критерием Стьюдента: предположим, t-значение равно нулю. Это означает, что средние выборочные значения метрик в тестовой и контрольной группах совпадают. В таком случае почти наверняка можно сказать, что отличий между группами нет. Увеличение значения критерия Стьюдента будет означать, что разность выборочных средних значений исследуемой метрики между тестом и контролем становится все больше по сравнению со стандартным отклонением разности выборочных средних. И поэтому чем больше значение t-критерия, тем меньше вероятность, что разность между группами случайна.Таким образом, на качественном уровне значение критерия — это прокси-метрика значимости отличий между группами!Чтобы проводить количественное соответствие между полученным значением статистики и значимостью отличий, используются специальные таблицы для соответствующего статистического критерия. В случае автоматизированного подхода используются функции специализированных библиотек, которые подбирают p-value под ваши выборки, степени свободы и посчитанное значение критерия.Теперь я все знаю?Для аналитика понимание принципов работы статистических критериев — один из самых базовых навыков, необходимых для выстраивания и поддержки культуры АБ-тестирования. Полезно понимать область применимости и «физический смысл», который лежит за математическими формулами — чтобы в любой ситуации знать, что предложить для грамотного количественного тестирования решений для бизнеса.Если вам интересно влиять на бизнес с помощью данных, приходите к нам в команду —всегда найдутся направления по вкусу.Product&data команда СберМаркета ведет соцсети с новостями и анонсами. Если хочешь узнать, что под капотом высоконагруженного e-commerce, следи за нами вTelegramи наYouTube. А также слушайподкаст «Для tech и этих»от наших it-менеджеров."
СберМаркет,,,Пайплайны Gitlab CI: моя коллекция граблей,2024-07-12T11:46:49.000Z,"Привет, Хабр! Я Евгений Малышев, SRE-инженер в Купере (так теперь называется СберМаркет). Моя основная задача — это надежная работа сервисов фронтенда, и немалую роль в этом играют правильно построенные пайплайны CI/CD. В этом нам помогает Gitlab CI. В компании мы широко используем этот инструмент для создания общих шаблонов для сервисов на различных языках. На уровне отдельного репозитория легко расширить или настроить шаблонные джобы и добавить свои.До этого у меня был опыт с Jenkins и Azure Devops, так что Gitlab CI мне показался довольно простым: есть стадии, есть правила запуска джоб с shell-подобным синтаксисом, да и скрипты джоб тоже используют bash-интерпретатор. Но в процессе близкого знакомства не раз возникали ситуации, когда поднимается то одна бровь, то обе, а то и руки в праведном гневе. Заходите посмотреть, какую коллекцию граблей собрал я.Весь код с примерами граблей можно посмотреть врепозитории.allow_failure это тоже не failureНачнем с простого. Нам нужно выстроить зависимость двух джоб:first_job:
  script: ./run_stuff.sh

dependent_job:
  script: ./run_more_stuff.sh
  needs: first_jobЭлементарно же? Теперь допустим, чтоfirst_jobможет завершаться неуспешно — пускай это будут нестрогие тесты, которые завершаются какwarningи не фейлят весь пайплайн. Для этого мы включаемallow_failure. Есть соблазн использовать это и в более сложных кейсах.Допустим, нам требуется выстроить какую-то логику запуска зависимой задачи, которая выходит за возможностиrules. Казалось бы, вот он механизм, можно просто увести вwarningпервую джобу и тогда зависимая на запустится!first_job:
  script: ./check_if_we_need_dependent_job.sh || echo ""Nothing to do, skipping dependent job""
  allow_failure: true

dependent_job:
  script: ./run_more_stuff.sh
  needs: first_jobНо нет, это неправильные пчелы и неправильныйnёёd. В этом случае зависимая задача все равно запустится. Что делать? Собирать нужные переменные артефактом из первой задачи черезdotenv reportи проверять их в зависимой задаче. Не очень красиво, но переживём.Пляски по старинным граблям с set -eТот факт, что джобы в Gitlab выполняются вbash, должно помогать разработчикам, освоившим этот универсальный инструмент, который, как суперклей, помогает соединить вместе самые разные инструменты в *nix-системе. Однако, тут можно наткнуться на куда более старые грабли, заложенные уже разработчикамиbash:check_condition:
  script: |
    set -e
    ./check_condition.sh && echo 'Условие проверено, продолжаем!'Вот казалось бы, у нас есть скриптcheck_condition.sh, мы его выполняем и в зависимости от кода возврата либо продолжаем, либо джоба завершается неуспешно. Почему неуспешно? А для этого мы устанавливаем флаг Errexit командойset -e, чтобы любая неуспешная команда в скрипте приводила к ошибке всего скрипта.Но вот в чем загвоздка, у этого флага есть множество исключений и наличие&&или||это одно из них. Поэтому, если логика джобы строится на кодах ошибки, лучше обрабатывать это явным образом, а не расчитывать на поведение шелла.Здесьвсе тонкостиset -eразобраны очень подробно. Также эти грабли описаны вдокументации по дебагу пайплайнов.Исправить такую конструкцию можно, поместив конструкцию с && в дочернюю оболочку с помощью скобок:check_condition:
  script: |
    set -e
    (./check_condition.sh && echo 'Условие проверено, продолжаем!')В этом случае Gitlab корректно обработает код возврата из дочерней оболочки и джоба завершится неуспешно, как и задумывалось.Кстати, указывать явным образомset -eбыло необязательно: в шелл-раннерах Gitlab по-умолчаниюустановлены флагиset -eo pipefail, хотя в документации об этом не упоминается.Благодаря этому можно подобрать еще одни грабли наподобие таких:cat application.log | grep 'этого_там_точно_нет' | tee -a found.loggrepне просто ничего не найдет, но еще и уронит джобу. Ой.Грабли известныеМногие грабли описаны в документации, но все же читают инструкции, когда уже все сломалось, да?Как пример, шишек можно набить и на оформлении многострочных скриптов. Здесь Gitlab даёт и широкие возможности, чтобы выстрелить себе в ногу:multiline_script_job:
  script:
    - for i in {1..10}
    - do echo $i   # здесь мы ждем 10 строк с числами 1..10
    - done         # но получаем ошибкуДа, такие конструкции нельзя разносить по элементам YAML-списка (кстати, почему?). Ну да ладно, сделаем другим способом:multiline_script_job:
  script: >
    for i in {1..10}; do 
      echo $i
    doneНо здесь сработала магия баша при обработке переносов строки внутри цикла. В других случаях можно напороться на такое:If you use the- >folded YAML multiline block scalar to split long commands, additional indentation causes the lines to be processed as individual commands.А если по-русски, то:Если использовать сворачивание многострочного YAML-блока с помощью элемента>, то излишние отступы приведут к тому, что такие линии будут восприняты, как выделенные переносами строк, до и после линии с отступами.Поиграться и понять, как это работает, поможет сайтhttps://yaml-multiline.infoИтак, пример:script: >
  RESULT=$(curl --silent
    --header
      ""Authorization: Bearer $CI_JOB_TOKEN""
    ""${CI_API_V4_URL}/job""
  )Что, кстати, ровно то же самое, что:script:
  RESULT=$(curl --silent
    --header
      ""Authorization: Bearer $CI_JOB_TOKEN""
    ""${CI_API_V4_URL}/job""
  )Выведет ошибку из-за лишних переносов строки:$ RESULT=$(curl --silent # collapsed multi-line command
curl: no URL specified!
curl: try 'curl --help' or 'curl --manual' for more information
/bin/bash: line 149: --header: command not found
/bin/bash: line 150: https://gitlab.example.com/api/v4/job: No such file or directoryЭто можно исправить:Убрав лишние отступы.script: >
  RESULT=$(curl --silent
  --header
  ""Authorization: Bearer $CI_JOB_TOKEN""
  ""${CI_API_V4_URL}/job""Изменив скрипт, экранируя появляющиеся переносы строк.script: >
  RESULT=$(curl --silent \
    --header \
      ""Authorization: Bearer $CI_JOB_TOKEN"" \
    ""${CI_API_V4_URL}/job"")А в случае, если все переносы строк нужно сохранить, используется литерал|script: |
  echo 'это всё'
  echo 'отдельные'
  echo 'команды'Ну и разумеется, можно просто отделять отдельные команды переносами строки:script:
  echo 'так'

  echo 'тоже'

  echo 'можно'Что в итоге?Gitlab CI неспроста так популярна (возможностей море) и продолжает развиваться, обрастая новыми фичами. Некоторые из которых, возможно, баги, но что поделать. Даже о низкий порог входа можно запнуться. Главное, чтобы можно было быстро освоиться и использовать все эти широкие возможности. Надеюсь, моя коллекция вам в этом немного помогла :)Tech-команда Купера (ex СберМаркет) ведет соцсети с новостями и анонсами. Если хочешь узнать, что под капотом высоконагруженного e-commerce, следи за нами вTelegramи наYouTube. А также слушайподкаст «Для tech и этих»от наших it-менеджеров."
СберМаркет,,,Я могу один раз ошибиться! Как разработать нейронку для исправления опечаток в поиске,2024-07-08T12:47:04.000Z,"Всем привет! Меня зовут Аня Власова, и я работаю ML-инженером в Купере (ex СберМаркет), а именно — в команде поиска. Сегодня я расскажу про нашу нейросетевую модель, которая стои́т на страже корректных поисковых запросов. Вы наверняка найдете пару инсайтов в этой статье, если тоже разрабатываете сервисы поиска или просто интересуетесь языковыми нейронками.Зачем вообще автоисправление в поиске?Ошибки в поисковых запросах — очень частое явление. Иногда пользователь не знает, как правильно пишется слово, иногда — случайно задевает не те клавиши, забывает поменять раскладку и т. д. Ну вы сами знаете, как это бывает :)Учесть все возможные опечатки в SEO (поисковой оптимизации) карточки товара невозможно, поэтому без автоисправления поиск по запросу с опечаткой чаще всего не дает совсем ничего. Приходится повторно вводить запрос, а это — лишнее время и повод для раздражения. Тем временем семь из десяти товаров в Купере добавляются в корзину именно из поиска! Наша цель — помочь пользователю найти нужный товар как можно быстрее, и именно для этого требуется автоисправление опечаток.Как опечаточник устроен сейчасВсе запросы от пользователей отправляются в поисковый движок — у нас это Elasticsearch. Если по введенному запросу есть какая-то выдача, то в дополнительных звеньях нет смысла и мы просто показываем пользователю товары. Если же выдачи нет, запрос должен пройти фильтр опечаточника и затем, в модифицированном виде, повторно отправиться в поисковый движок.Если товаров нет в выдаче даже после работы опечаточника, приложение сообщает, что нужного товара нет в наличии.Подробнее о том, как мы отбираем и ранжируем товары в поиске, можно прочитать вмоей прошлой статье(кстати, у неё была номинаия на Технотекст🙃).На актуальном проде в качестве опечаточника у нас работает алгоритм SymSpell (вот ссылка на GitHub). Принцип его работы заключается в предобработке словаря:Для каждого слова создается перечень возможных опечаток — ключей, которые могут из него получиться путем удаления одной или нескольких букв.Если вводится слово с ошибкой, алгоритм генерирует ключи-удаления уже для него и идет сопоставлять их с предварительно созданным словарем опечаток.Результаты ранжируются для выбора того исправления, которое с наибольшей вероятностью окажется верным. Ранжирование учитывает расстояние Левенштейна, то есть степень близости, и популярность (частоту ввода запросов в поисковую строку).Объясню на пальцах. Для слова «торт» SymSpell сгенерирует такие однобуквенные удаления: «трт», «орт», «тот», «тор». Удаление «орт» также актуально, скажем, для слов «корт» и «борт», однако на запрос «иорт» алгоритм выдаст все-таки кондитерские изделия, потому что запрос «торт» гораздо более органичен для Купера (СберМаркета).SymSpell очень быстрый, однако качество его работы не идеально. Чтобы повысить эффективность поиска, мы задумались над альтернативой.Архитектура новой нейросетевой моделиНаше решение очень похоже на то, как решаются задачи по машинному переводу. Единственное отличие состоит в том, что у нас на входе и на выходе используется один язык.Модель работает по принципу Seq2Seq (sequence-to-sequence), то есть сопоставляет одну последовательность данных с другой. Энкодером стала двунаправленная модель GRU, декодером — GRU с механизмом внимания. Помимо GRU, мы пробовали работать с более классической для Seq2Seq моделью — LSTM, — однако она обучалась гораздо дольше, не будучи при этом более результативной.На вход мы подаем данные, разбитые на токены с помощью BPE. BPE-токенизация итеративно определяет наиболее часто встречающиеся в тексте сочетания символов. Каждый символ — отдельный токен. Частоты всех токенов подсчитываются, наиболее частотные пары токенов заменяются новыми токенами. В итоге формируется словарь токенов, который может включать как отдельные символы, так и комбинации.Чем мы кормили нейронкуДля обучения нейросетевой модели нам нужны были пары: поисковый запрос с опечаткой + корректно написанный поисковый запрос.Мы в Купере логируем только исходные запросы пользователей, и у нас нет информации о том, когда применялся опечаточник и как именно он исправлял запрос. Поэтому нам пришлось танцевать с бубном, чтобы заполучить требуемые пары.У нас было три пути сбора данных для обучения:Синтетическая генерация опечаток по названиям товаров.Синтетическая генерация опечаток по корректным запросам (которые надо было достать из базы всех запросов).Извлечение реальных пар.Сейчас расскажу о них в подробностях.Синтетическая генерация опечатокМы проанализировали поведение пользователей и выявили основные типы опечаток:замена буквы в слове на соседнюю по клавиатуре или случайную — «gолоко» вместо «молоко»;добавление лишней буквы — «молокео»;случайное удаление буквы — «молоо»;перестановка двух соседних букв слова местами — «млооко»;дублирование буквы в слове — «ммолоко»;отсутствие необходимого пробела — «пастеризованноемолоко»;двойной пробел — «пастеризованное  молоко»;орфографические ошибки: «а» вместо «о», «и» вместо «е» и т. д.Все типы опечаток генерировались по названиям товаров и корректным поисковым запросам. Также мы включали в обучение сами корректные запросы, чтобы модель обучалась их не трогать.Названия товаров брались не целиком, а частями — чтобы они больше были похожи на то, что вводят пользователи. Мы извлекали униграмму (первое слово), биграмму (первые два слова) и триграмму (первые три слова). Для товара с названием «Молоко 2,5% пастеризованное 930 мл БЗМЖ» это «молоко», «молоко 2,5%» и «молоко 2,5% пастеризованное».Чтобы извлечь корректные поисковые запросы из базы всех запросов в Купере (СберМаркете), мы прогоняли базу через открытые модели для исправления опечаток. Если большинство моделей не изменяли запрос, он маркировался как правильно набранный.Извлечение реальных парЭто самое интересное!Сначала мы собирали все запросы, которые в течение небольшого промежутка времени вводились одним пользователем в одном магазине и в рамках одного заказа. Если пользователь заходил в Купер (СберМаркет) и поочередно за пять минут вбивал «малоко», «молоко», «молоко п», «молоко простоквашино», «хлеб» и «масло»,  эти запросы мы объединяли в одну группу.Далее среди запросов нужно было найти те, которые отличались друг от друга одним или двумя символами. В нашем примере это запросы «малоко» и «молоко». Пары, где один запрос являлся частью другого (как «молоко» и «молоко п»), не рассматривались.Среди получившихся пар мы находили более популярный запрос — который вводился пользователями чаще («молоко»). Он считался кандидатом в корректно написанные. Наименее популярный запрос («малоко») считался кандидатом в написанные с опечаткой.N.B. Иногда популярный запрос — это тоже запрос с опечаткой. Поэтому результаты сравнения частотности добавлялись в датасет для обучения только после проверки на упомянутых ранее открытых моделях для исправления опечаток: наиболее популярный запрос должен был после прогона остаться неизменным, а наименее популярный — исправиться в наиболее популярный.Реальные опечатки пользователей принесли нам самый большой прирост в метриках качества нейросетевой модели.Дополнительные техники обученияТакже для улучшения качества и сокращения времени обучения мы применяли Teacher Forcing и Masked Language Modeling.Teacher ForcingОсновная идея Teacher Forcing состоит в том, чтобы использовать истинные значения последовательности для лучшей обучаемости модели. Это минимизирует ошибки, которые могут накапливаться при генерации последовательностей на основе предыдущих собственных предсказаний.На каждом шаге обучения мы уменьшали долю объектов, для которых применяли Teacher Forcing, чтобы сделать модель устойчивой к новым данным и её собственным ошибкам.Masked Language ModelingМетод предварительного обучения нейронных сетей, когда мы случайным образом маскируем часть входных токенов и просим модель предсказать замаскированные токены. В нашем случае сначала шло предварительное обучение на полных названиях товаров, а уже потом модель работала на искомое исправление опечаток.И что, действительно стало лучше?Да! Наша модель показала себя лучше, чем текущий продовый алгоритм SymSpell, причем как на сгенерированных опечатках, так и на реальных запросах пользователей. Финальная оценка, которую вы видите на скрине под этим абзацем, проводилась именно на реальных запросах.+17% качественных исправлений — это ничего себе, правда? Новая модель дает просадку по времени в сравнении с продовым решением, однако разница не критична, потому что мы ожидаем, что система целиком будет отвечать пользователю за 500 миллисекунд.Для обучения модели мы использовали данные за 2023 год, а для тестирования — за 2024-й. При таком подходе данные в обучающих и тестовых выборках могут повторяться, что соотносится с реальностью, ведь юзеры могут допускать как повторяющиеся, так и новые ошибки.Сейчас мы тестируем новую модель уже в проде. В планах все-таки поработать над сокращением времени работы и улучшить качество исправления опечаток для отдельных вертикалей (в частности для аптек, где своя специфика поисковых запросов). Также в будущих версиях мы учтем транслитерацию и неправильную раскладку клавиатуры.Надеюсь, что мой опыт окажется полезным для вас! Готова ответить на вопросы в комментариях :)Кстати, впервые об этой теме я рассказывала на митапе, посвященому DS-решения в нашем сервиса поиска. Там есть ещё две крутые темы, если инетресно — ловите:ML-команда Купера (ex СберМаркет) ведетtg-канал «ML доставляет»— там мы рассказываем, как создаём доставку будущего с помощьюDS, NLP, CV. А если хочешь больше узнать, про нашу инженерную культуру в целом, подписывайся наtg-канал Купер.техи на нашYouTube. А также слушайподкаст «Для tech и этих»от наших it-менеджеров."
СберМаркет,,,Как оценить эффективность IT-команд и с умом задебажить процессы,2024-07-04T09:58:01.000Z,"Привет, Хабр! Это Оля Муттер, руководитель IT-проектного офиса в Купере (ex СберМаркет). Жизнь заставила меня научиться настраивать процессы как боженька. Я стартовала карьеру в роли бизнес-аналитика, доросла до директора продукта в финтехе, успела побывать наставником для проджектов, создать несколько проектных офисов и центров компетенций — всего за десять лет.Сейчас я рулю проектным офисом в Купере (ex СберМаркет) — это 1300+ человек в IT-команде. Как понять, работает ли такая большая система эффективно? И что делать, если какая-то веточка этого гигантского дерева растет не в ту сторону? Об этом моя сегодняшняя статья.Спойлер:надо дебажить процессы, а для этого придется много работать с цифрами и общаться с людьми. За это у нас в компании отвечаютdelivery-менеджеры.Самые важные метрикиКак понять, эффективно ли настроены процессы в компании?Для начала я бы посоветовала собрать следующие метрики на основе данных о работе ваших команд:Lead time— время от точки принятия обязательств, когда мы сказали «Мы это сделаем», и до момента, когда эти изменения стали доступны нашим пользователям.А также время на отдельные этапы работы.Cycle Time— это Lead Time, разделённый на этапы. Для качественного планирования полезно знать, сколько времени занимает каждая часть процесса.Всегда есть соблазн просто подкрутить Lead Time и сказать: «Раньше мы поставляли 5 проектов за 60 дней, а теперь поставляем за 10». Но как это влияет на здоровье бизнеса: крепнет оно или ухудшается?Чтобы понять это нужно обратить внимание и на контрметрики.Объем поставки (Throughput)— объем работ, поставленный за определенный период времени, который несет конечную ценность для пользователей. Измеряется в количестве выполненных задач или реализованных фич.Точность поставки (Delivery on date)— процент проектов, которые завершаются в ожидаемый срок.Эффективность потока (Flow Efficiency)— как эффективно проходят задачи или элементы процесса через систему. Оценивает время, фактически затраченное на выполнение задачи, по сравнению со временем, когда задача находилась в активной обработке.Что делать с цифрамиЕсли вы собрали метрики и довольны результатами на 100%, то читать дальше не обязательно :) Но по опыту в крупных компаниях, заточенных на рост и развитие, так не бывает. Даже при достойных показателях, чтобы оставаться конкурентоспособной, компания должна совершенствоваться. Для этого существует замечательный инструмент —анализ цепочки поставок.Он представляет собойоценку каждого этапа поставки ценности до конечного пользователя, то есть всех процессов, из которых состоит производство вашего продукта или предоставление вашей услуги.Представьте людей, которые передают по цепочке мяч.Почему посередине у кого-то были заняты руки и он пропустил пас?Почему кто-то отошёл, когда должен был принять и передать его дальше?Или передал так быстро, что следующий человек не успел среагировать?Именно на такие «почему» отвечает анализ цепочки поставок. Он позволяет понять, какие проблемы возникают на каждом этапе работы и как они влияют на метрики.Дальше я подробно расскажу, как грамотно провести такую оценку.Анализ цепочки поставок по шагам#1 Определяем уровни системы и количество оцениваемых элементовС точки зрения структуры мы делимся на домены. Внутри них выделяется по несколько юнитов, а внутри юнитов — по несколько команд.Если вы хотите полностью оценить процесс на уровне  компании, лучше всего взять за единицу анализа юнит. Создаём роадмап и обозначаем, когда планируем оценить каждый из юнитов. Приоритизацию можно сделать на основе собранных метрик, где больше выбросов или показатели сильно выше таргетов.#2 Готовим шаблонСтартовый шаблон может выглядеть так:Дальше скелет обрастает мясом:Кто ответственный на каждом этапе?Какие проблемы могут возникнуть на каждом этапе?Что представляет из себя результат каждого этапа?Какие метрики мы собираем на каждом этапе?#3 Собираем метрикиДальше начинается самое интересное. В юните, скажем, пять команд. В каждой команде есть продакты, тимлиды. Нужно не только назначить встречу, но и подготовить всех: рассказать, зачем это всё необходимо и какой результат мы ожидаем. Нельзя влететь с ноги и сказать: «Всё, идём оценивать цепочку». Поэтому важно настраивать доверительные отношения со всеми командами заранее, либо же в начале встрече снять все опасения и боли.#4 Готовим почву под долгий созвон с важными участникамиПредупреждение, чёткая цель, планируемый успех. За кадром — надежда, что хоть кто-то придёт на мою фан-встречу:#5 Ставим встречи в календарьИ, соответственно, проводим! Предлагаем план встречи в пяти частях.1. В самом началестроим мостик. Люди должны понимать, что речь не о далёком будущем и не о том, чтобы просто повесить на них ещё один отчётик. Не забывайте о цели — понять, что происходит прямо сейчас в конкретных командах.2. Дальше мы, естественно,расставляем тайминги. Время каждого сотрудника ценно, да и вряд ли кто-то выдержит встречу, которая затянется больше чем на два часа.3. Начинаем собирать мнения и при этомтранслируем открытость и доверие. Мы не навязываем решения, а выявляем и записываем проблемы. Разумеется, если у кого-то есть идеи преобразований, это надо записать и включить в фоллоу-ап, но всё же мы собираемся не за этим.4.Вовлекаем всех участников.Желательно распределить время так, чтобы высказались все, кто участвует в доставке продукта. Пусть сначала активно включаются продакты, а тимлиды подсказывают, что они заметили. Потом, наоборот, активно включаются тимлиды, а подсказывают продакты. Так не нарушается динамика встречи.Собирать продактов и тимлидов по отдельности — не вариант. Это размывает анализ, не даёт получить полную картинку, а нам нужно собрать максимум проблем и максимум статистики.5. Обязательноопределяем so what. Можно бесконечно долго проводить подобные встречи, чтобы все пожаловались, получили моральное удовлетворение и выдохнули, но какой будет прок?Нормально, если вы не знаете, что будете делать с полученной информацией, когда идёте на первую встречу. В этом случае тоже можно дать прозрачную обратную связь: «Ребята, вы первый юнит, с которым я общаюсь. После вас я пообщаюсь со всеми остальными и тогда уже смогу вернуться и сказать, сколько у нас проблем, решаемы ли они и что можно сделать». Такая искренность заряжает людей, потому что они понимают, что встреча с ними будет особенно ценной для всего анализа.#6 Фиксируем и кластеризуем всю информациюВот мы провели анализ цепочки поставок по пяти юнитам одного домена, сделали красивую доску в Miro. Куда с этим идти? Явно не к вице-президенту. Эта информация нужна прежде всего нам, проектному офису.Если мы действуем на уровне компании, то нужно менять не конкретные команды или даже юниты, а вводить системные улучшения, которые уменьшат общий Lead Time без потери в Throughput и Value. Соответственно, после общения со всеми юнитами одного домена мы идём в другой, потом в третий и т. д.Когда у нас будет информация по всем доменам, её нужно кластеризовать. В кластер входят все проблемы одного типа. Целесообразно брать только те проблемы, которые значительно влияют на метрики. Помимо проблем и влияния, важно прописать ответственных и возможные решения.Допустим, мы выяснили, что средний объём поставки у одного юнита без потери Value — не более четырёх проектов в месяц, но многие юниты берут и восемь, и 16, а то и 32. То есть сотрудники явно переоценивают свои силы. Мы выделяем кластер «Планирование работ», заводим туда проблему «Смещение фокуса и частые переключения». Влияние: -10% к эффективности. Ответственность берём на себя как на проектный офис. Решение: внедрение WIP-лимитов, то есть ограничений по количеству проектов и задач, при верхнеуровневом масштабировании.Могут быть проблемы, которые вы в принципе не способны решить. Например, если они связаны с внешним влиянием. Но чётко понимать свои ограничения и их роль — тоже важно.#7. Прописываем действияВсе действия нужно разделить на проактивные, которые можно предпринять прямо сейчас, и на системные улучшения.Проактивные действия:• Работа с блокерами. Если вы ведёте какие-то проекты в Jira и не пользуетесь флагированием, рекомендую взять на заметку. Это даёт большой объём данных о том, почему работа зависает, в дополнение к ретроспективам.• Расчёт предиктивного Lead Time.• Нотификации. Много. Мой проектный офис даже работает над специальным сервисом для нотификаций. Иначе командам приходится изучать огромные дашборды, чтобы узнать, что Lead Time улетел в космос.Системные улучшения, в свою очередь:• Приоритезация проблем на основе влияния. Что хуже всего сказывается на бизнесе?• Поиск ответственных.• Выявление решений.За планом действий идут сами действия. Важный совет: системные улучшения нельзя проводить резко, они ведут за собой непомерные скачки нагрузки. В случае каждого такого улучшения я рекомендую для начала провести пилот на одной команде. Так вы сразу увидите, что может пойти не так, и лишний раз задумаетесь, действительно ли это нужно компании.Кто проводит системные улучшения?У нас в компании за это отвечает направлениеdelivery менеджеров, о них рассказывала в своейстатье про проектный подход. Они выявляет блокеры на всем процессе доставки ценности, внедряет системные улучшения и обучает команды.Надеюсь, что вдохновила вас на анализ цепочки поставок ;) Буду рада ответить на вопросы в комментариях.Tech-команда Купера (ex СберМаркет) ведет соцсети с новостями и анонсами. Если хочешь узнать, что под капотом высоконагруженного e-commerce, следи за нами вTelegramи наYouTube. А также слушайподкаст «Для tech и этих»от наших it-менеджеров."
СберМаркет,,,Как СберМаркет тестирует простые и сложные продуктовые гипотезы на реальных кейсах,2024-06-28T09:03:01.000Z,"Всем привет! Я Стас Дергунов, Head of Product в СберМаркете. В моем портфеле три направления: самовывоз из магазина и в авто, покупки в офлайне-магазинах с помощью сервиса ScanPay и новые вертикали non-food (аптеки, бытовая техника и электроника).В статье расскажу, как мы внутри компании проверяем простые и сложные гипотезына основе данных, что позволяет поддерживатьрост в 1,6 раз год к году.Чем простые гипотезы отличаются от сложных?Методы проверки простых гипотез: модерируемый и немодерируемый тесты(на примере сервиса Самовывоза в СберМаркете)Методы проверки сложных гипотез(на примере сервиса ScanPay)Как простые гипотезы вписываются в сложные(на примере сервиса ScanPay)Что такое продуктовый подход в СберМаркете?Это создание ценности, которая приносит пользу бизнесу и пользователям. В основе лежит data-driven подход, где мы фокусируемся на том, что нужно пользователям, а не только на мнении экспертов внутри компании. Для регулярности поставок ценности используем циклы discovery/delivery на основе agile/scrum.Сегодня поговорим о понятии простых и сложных гипотез и работе с ними.Сначала о терминахУровень сложности исходит из количества абстракций, которые мы хотим исследовать. Чем их значение выше, тем гипотеза сложнее.Простая гипотезаформулируется в классическом фреймворке«Если …, то …».Например, у нас есть гипотеза:Если добавить на главный экран мобильного приложения функционал выбора между доставкой и самовывозом, это сократит кол-во ошибок пользователейна этапах оформления заказа.Видим, что здесь в одном месте — одно изменение.Сложная гипотеза— это несколько уровней абстракции, которые состоят из связанных между собой гипотез. Для их проверки требуется серия исследований.Пример:Если дадим старым клиентам новый опыт покупок в офлайн-магазинах, который поможет им сэкономить время и деньги, то они начнут им пользоваться вместо обычных касс.Видим, что здесь сразу несколько гипотез в разных местах, на стыке офлайна и онлайна.Исследования мы проводим в трех случаях:Если хотимпроверить гипотезу, улучшает ли новое решение пользовательский опыт и продуктовые метрики по сравнению с текущим вариантом.Если планируемвыяснить, насколько решение отвечает потребностямпользователей. Например, если такой функции раньше не было, но она может быть полезной.Если у наснет решения, а есть запросна вдохновение или поиск потребностей и проблем клиентов.Понимая, какая гипотеза на столе, мы можем применять разные исследования, чтобы снизить риски неудачных решений. Ещё это помогает проводить исследования дешевле, например, без привлечения разработчиков для A/Б теста. О способах проверки расскажу далее.Методы проверки простых гипотезНемодерируемые тесты в количественных исследованиях. Пример — first click. Можно использовать респондентов накраудсорсинговых платформах, выдаем им задание и смотрим на тепловые карты взаимодействия с интерфейсом. Ещё можно использовать респондентов СберМаркета, фильтруя, например, по частоте заказа/специфика и пр. Свою базу мы в последнее время стали использовать намного чаще.На таком исследовании видим не только куда кликнул испытуемый, но и за какое время.Модерируемые тесты, они же глубинные юзабилити-интервью или UX-исследования. У нас есть гипотеза в виде интерфейса и задача — проверить, понятно ли решение для клиента.Немодерируемый тест — на примере самовывоза из магазинаЗадача: добавить в мобильное приложение возможность выбирать способ получения заказа (доставка или самовывоз из выбранного магазина). Мы хотели понять, в какомиз трех вариантов посетители быстрее найдут кнопку самовывоза на главном экране.Вторая задача: понять, куда кликают для оформления самовывоза, если дать несколько вариантов.Для этого мы запустили немодерируемый онлайн-опрос по базе клиентов СберМаркета.Часть 1: Тест первого кликаВ эксперименте один респондент тестировал только один вариант главной страницы.Задание звучало так:Представьте, что вам нужно заказать продукты и забрать их самовывозомиз магазина. Куда на экране вы нажмёте, чтобы это сделать?Примеры интерфейса для тестированияКакие сделали выводы?В категории «Еда»:Лучше всего работает вариант «новый 1» — там бóльшая доля людей без сомнения идет в «переключалку»: доставка или самовывоз.Текущий вариант тоже работает хорошо, но, сравнивая с «новым 1», клики людей чуть более «размазаны» по странице, хотя они все равно справляются быстро.Вариант «новый 2» самый неудачный: очень разнородные клики, людям надо искать и думать — тратят больше всего времени.В категории «Алкоголь»:В целом справляются быстрее, чем в едеУже не так важны кнопки «самовывоз», «переключалка» и пр. — большинствов любом случае «идет» через джобу «Алкоголь».Часть 2: Субъективная оценкаЗадание:Ниже вы увидите высказывания разных людей о том, как им удобнее всего оформлять заказы самовывозом через приложение. Выберите то высказывание,с которым вы согласны и которое лично вам кажется наиболее близким.1. Хочу зайти в приложение, выбрать способ получения «самовывоз», а потом уже выбирать доступный магазин и собирать корзину.2. Хочу сначала собрать корзину из нужных мне товаров, а потом уже выбирать — самовывоз мне нужен или доставка.3. Мне важно сначала выбрать конкретный магазин или точку (понять адрес и режим работы), а потом уже собирать корзину для самовывоза именно в этом магазине.4. Другое.Выше — результаты, которые мы получили. На их основе наметилось два пути.Использовать вариант «новый 1» (если наша главная цель – выстроить более однозначный сценарий для клиента).Оставить текущий вариант, т.к. он тоже удобный и подходящий, но чуть менее конкретный (видим больше «кликов в другие места страницы).Модерируемый тест — на примере UX-исследования для сервиса  самовывозаЗадача: встроить в опыт клиента товары и ретейлеры, которые работают толькопо  самовывозу. Это позволит увеличить ассортимент для покупателя, а для ритейлерабез доставки получить заказы. Целимся в основные точки касания с клиентом в поиске и выдачи.1 – поле ввода запроса, 2 – выдача для доставки, 3 – состояние с самовывозомМетод исследования: модерируемое UX-тестирование с элементами интервью. Тестировали прототип со сценарием самовывоза в поиске. Средняя продолжительность одного теста — 40 минут. Всего провели 6 UX-тестов.Тестировали на аудиторию:Активные пользователи СберМаркета — делают заказы не реже несколькихраз в месяц.Активно пользуются самовывозом — не реже нескольких раз в месяц.Возраст респондентов: от 20 до 55 летРаспределение по полу: 6 женщин, 2 мужчины.Какие рекомендации сформулировали на основе исследования:Проработать актуальность фильтров и название.Дать возможность смотреть расстояние для ближайшего магазина.Упростить работу с выбором адреса и построением маршрута.Пример результата из исследованияМетоды проверки сложных гипотез — на примере сервиса ScanPayВыше рассказал про методы проверки простых гипотез. К ним имеет смысл переходить тогда, когда понятно, что продукт нужен, и осталось понять, как сделать его лучше, быстрее и удобнее.Расскажу, как мы искали точки кратного роста для бизнеса на примере запуска нового сервиса ScanPay в СберМаркете.Для подобных кейсов требуется комплексные исследования гипотез разными методами (один из них — тест первого клика). Цель: быстро подтвердить их или опровергнуть и сделать пивот.Что такое ScanPay?Это функция внутри приложения СберМаркета, которая позволяет сканировать товарыв магазине и оплачивать без очередей в приложении, применяя программы лояльности СберСпасибо и магазина (детали поссылке).Преимущества сервиса ScanPay#1 Анализируем рынок и стратегииОбязательно погружаемся в стратегию компании, чтобы понять планы по развитиюи целями на ближайшие несколько лет. Проведя анализ, мы увидим текущие рынки,на которых играет компания и там, где она пока не представлена.Например: мы определили, что рынок Grocery состоит из двух сегментов: онлайн и оффлайн. В онлайне (e-grocery) СберМаркет — лидер рынка на 165 млрд рублей в 2023,а вот в офлайне присутствия СберМаркета нет.Оценкарынка продовольственных продуктов в 2023 году— 47,4 трлн рублей (рост годк году на 6,4%). Делаем вывод, что есть огромный рынок офлайн-аудитории и пришло время провести исследования потребностей сегмента. Эту гипотезу мы берем в работу.#2 Изучаем клиентовДля начала проводим качественные исследования, цель которых — понять потребностии проблемы клиентов в офлайн-магазинах. Кроме этого, узнаём, как сейчас клиенты решают эти проблемы, для этого задействуем интервью.Пример вопросов в ходе интервью:Как сейчас решаете задачу?Какие проблемы бывают? В каких ситуациях и как часто сталкиваетесь с ними?Насколько эти проблемы вас раздражают?Как вы справляетесь с проблемами?#3 Проверяем гипотезу количественноСледующий этап — оцифровать и приоритизировать с помощью количественных исследований, на чем именно нужно сфокусироваться среди всех потребностейи проблем. Цель: выявить критичность проблем и потребностей. Метод — опрос.По результатам этих двух исследований мы нашли интересный сегмент —омниканальные клиенты(совершают покупку в онлайн и оффлайн канале). Они иногда покупают продукты онлайн в СберМаркете, а иногда в офлайн-магазинах.Мы решили сфокусироваться на этой аудитории и сформулировали гипотезу:Если мы дадим клиентам СберМаркета новый сервис при покупках в офлайне,то увеличим кол-во покупок и средний чек у таких клиентов за счет омни-опыта,а ещё сможем быстрее и дешевле привлечь клиентов в новый сервис.Структурируем наш сегмент и строим дерево задач клиентов по методологии job-to-be-done с путем клиента по этапам в оффлайне: до магазина, в магазине, после магазина.За основу взяли качественные исследования, которые мы проводили ранее.#4 Ищем УТППараллельно провели анализ конкурентов в РФ и за рубежом. По решениямдля сегмента:в чем суть решения;какие есть плюсы и минусы;какие УТП.Это помогает сформировать насмотренность по конкурентам и их решениям.Исследование конкурентов в РФ и GlobalНа основе потребностей и проблем формируем для нашего сегмента варианты УТП или value proposition. Вот, какие получились у нас:возможность отслеживать стоимость отдельных товаров и всей корзины в онлайн-режиме;оплата в мобильном приложении без очередей;использование программ лояльности и СберСпасибо, и от ретейлера, где можно списать и начислить виртуальные рубли.#4 Тестируем концепцииПримеры двух концепций для ScanPayТеперь нужно понять, заинтересован ли клиент и готов ли он попробовать наше решение?Инструмент валидации —fake door.Это количественный метод для верификации спросаи сбора аудитории, которая оставит заявку на получения доступа к сервису.Другими словами мы делимся с клиентом продуктом, который якобы уже создан,и смотрим на его реакцию.Например, мы направили в магазины команду промоутеров, которая взаимодействовалас потенциальными клиентами, предлагая им продукт. Их задачей было установить приложение СберМаркет для новыех клиентов, а для старых — активировать режим ScanPay на главном экране.Так мы протестировали концепции клиентских ценностей, где получили нужную нам информацию:верифицировали УТП и коммуникацию с клиентами;провалидировали шаги в воронке. Например, мы поняли, что взаимодействиес промоутерами (в какой-то момент нахождения в магазине клиент узнавало продукте) непосредственно влияет на готовность воспользоваться сервисом.На выходе это позволило внести корректировки в ожидания от продукта и его позиционирование.Как простые гипотезы вписываются в сложныеА теперь давайте посмотрим пример, когда простая гипотеза вписывается в сложную через те инструменты, что обсудили выше.Например, наша цель — протестировать заметность нового варианта расположения точки входа в услугу ScanPay по сравнению с текущим вариантом.Метод исследования — немодерируемый тест на платформе Useberry, количественное исследование. Целевая аудитория: пользователи СберМаркета с разным опытом использования сервиса. Количество респондентов: по 100 человек на каждый из вариантов дизайна, непересекающиеся аудитории.Преимущества метода:Возможность количественно подтвердить, насколько предложенное решение отвечает потребностям пользователя.Если решений интерфейса несколько, до А/В-теста оценить потенциал каждого:это может быть актуально, когда точек входа в раздел несколько.Благодаря методу first click возможно найти новые решения, ранее не очевидныепри проработке продукта.Пример тепловой картыНа старой главной:Лишь треть опрошенных в первую очередь нажимали на плашку «Scan Pay | Сканировать» в нижней части экрана. Большинство пользователей (61%) ассоциировало вход в услугу с выбором магазина, в котором планируется покупка.Это дало понять, что необходимо реализовать дополнительную точку входа на главной ретейлера.На новой главной:почти половина опрошенных (46%) корректно считала точку входав услугу, при этом процент ошибочного выбора других разделов экрана значительно снизился по сравнению с текущим вариантом. Выбор услуги на странице ритейлера составил 31%.Резюмирую: мы прошли путь от поиска рынка и изучения стратегии компании, работы с клиентами на рынке и поиске проблем и потребностей, сфокусировались на омниканальных клиентах, изучили опыт конкурентов, применили количественный метод для подтверждения гипотезы. Так мы ищем для клиентов возможности в продуктов, которые клиенты пока не осознали.ВыводыЯ рассказал, как мы работаем с простыми и сложными гипотезами. Показал, как простые гипотезы вписываются в сложные и работают в связке. Рассказал про области применимости методов и ограничений.Что еще отмечу: важно в каждом исследованиисоветоваться с исследователями, потому что универсальных решений нет, и ключ успеха — правильная комбинация подходов. Поэтому, на мой взгляд, ключевая задача продукт-менеджера —повышать насмотренностьи учиться применять разный набор инструментов.Благодарю за помощь в подготовке статьи исследователя из моей команды Наташу Гурееву.Product&data команда Купера (ex СберМаркет) ведет соцсети с новостями и анонсами. Если хочешь узнать, что под капотом высоконагруженного e-commerce, следи за нами вTelegramи наYouTube. А также слушайподкаст «Для tech и этих»от наших it-менеджеров."
СберМаркет,,,Как подготовить тестовое окружение и не сойти с ума,2024-06-24T11:29:32.000Z,"Привет, Хабр! Я Александр Непомнящих, QA в СберМаркете. Мы с командой кодим программу лояльности, которая позволяет списывать в заказах бонусы «Спасибо», а также запускать различные акции с повышенным начислением бонусов.За последний год проект быстро обрастал новыми фичами. Архитектура: монолит на Ruby + микросервисы на Go. Для безопасного внедрения многие изменения активировались фича-флагами, разрасталась ролевая модель, фичей становилось ещё больше. Всё это увеличивало количество действий для подготовки тестового стенда, их количество обычно достигало 9. Да, нужно было совершитьдевять разных операций, чтобы подготовить стенд к работе с бонусами.Далее я расскажу, как мы в итоге сократили этот процесс до1 командыв Rails-консоли.А в чем, собственно, проблема?Пока проект был небольшой, вся настройка тестового окружения фактически сводилась к выдаче текущему пользователю уникального идентификатора, привязанного к определенному статусу:зарегистрирован и имеет положительный баланс;зарегистрирован, баланс 0;не зарегистрирован (доступна регистрация);заблокирован (недоступна регистрация);Проект был запущен в середине 2022 года, до этого времени пользователи могли списывать и копить бонусы только с картами Сбербанка, далее — по любым картам и с разными способами оплаты (картой онлайн, курьеру и др.) За последний год проект оброс новыми фичами:Появилась возможность списывать в определенных ритейлерах до 99% бонусами (на старте проекта было только до 10%).Особые промоакции, например:начисления на алкоголь — ранее код работал таким образом, что если в заказе есть алкоголь, любые операции с бонусами были запрещены;прогрессивный кэшбек для подписчиков — начисления увеличиваются пропорционально сумме заказа.Начисления по заказам были отложены на сутки (продуктовые заказы) либо 15 дней (электроника), чтобы уменьшить риск фрода.Для безопасного внедрения многие изменения активировались фича-флагами, разрасталась ролевая модель (некоторые настройки были доступны сотрудникам с определенными ролями), фичей становилось ещё больше. Всё это увеличивало количество действий для подготовки тестового стенда.Приведу пример: чтобы прогнать на стенде задачу «Отложенные начисления в ритейлере А», нужно было:Выдать пользователю идентификатор registered.Активировать фича-флаг delayed_accruals.Выдать пользователю нужные роли, например для проведения возвратов заказа.Создать на стенде нужные промоакции.Добавить ритейлера А в конфиг, где хранятся id ритейлеров по которым начисления будут работать отложенно.Настроить время в минутах, через которое должно сработать начисление (для прода это 1440, для стенда достаточно 2 минут).Настроить баннер на чекауте, который будет предупреждать пользователя, что в текущем заказе бонусы будут начислены через N времени.Настроить в ритейлере А списание 99%.Выдать пользователю подписку.Итого, 9 действий на каждый деплой (собственно, настройка тестового окружения). Далее нужно было создать несколько тестовых заказов:Авторизоваться на сайте.Накидать товары в корзину.На чекауте выбрать время доставки, политику замен, способ оплаты, адрес доставки и т.д.Оформить заказ (на этом этапе происходит холдирование денег и бонусов)Далее нужно было этот тестовый заказ довести до статуса «Доставлено», чтобы джоба которая отвечает за начисление бонусов, запланировалась на выполнение через N времени:сначала заказ берется в сборку;затем берутся все товары в заказе, им проставляется статус «Собрано», сборка завершается;далее происходит окончательная оплата заказа и списание бонусов;после заказ «доставляется» и запускается джоба начисления.Настройка легко могла занимать до получаса, а если в день проверяешь 3-4 задачи, то и до двух часов. Случалось, что я полчаса настраивал окружение ради десятиминутной проверки задачи.Мы с командой поняли, что жить так дальше нельзя, и было принято решение написать код, обернутый в rake task, который бы позволил автоматизировать рутинные действия.Код, решающий проблемуНаш код нужен для настройки тестового окружения, но будет лежать в продакшене, чтобы быть доступным на любом стенде любой команде. Поэтому, первым делом мы написали проверку, которая бы запрещала выполнение команды в продакшн-окружении и разрешала в тестовом:return Failure(message: ""No way you're doing it in production"") if 
Rails.env.production?Далее объявили список фича-флагов, которые нужно включить при каждом вызове команды:FEATURES_LIST = %i[
  flag1 
  flag2 
  flagN
].freezeДалее описывали методы отдельно для каждого нужного действия. Активируем фича-флаги из списка:def enable_flags
  FEATURES_LIST.each { |feature| FeatureToggle.enable!(feature) }
endВыдаем пользователю нужные роли (если смысл задачи не в проверке ролевой системы, можно выдать все существующие).def add_roles(user)
  user.roles = Role.all
endУстанавливаем пользователю идентификатор в программе лояльности (registered), предварительно удалив все старые, если они были.def update_auth(user)
  user.user_authentications.destroy_all
  user.user_authentications.create!(
    provider: 'provider_name',
    uid: external_service_id,
    payload: { uid: external_service_id }
  )
endСоздаем базовую промоакцию (и другие при необходимости):def create_basic_promotion!
  Loyalty::AccrualPromotion.find_or_create_by!(kind: 'basic') do |promo|
    promo.name = 'Базовое начисление'
    promo.calculator =
      'AccrualCalculator'
    promo.state = 'active'
    promo.percent = 1
    promo.display_name = 'Базовое'
  end
endУстанавливаем процент списания для первого ритейлера:def max_spend_percent_for_first_retailer
  ChargeSettings.create!(
    starts_at: 1.year.ago,
    percent: 99,
    retailer_id: Retailer.first.id
  )
endВ итоге код имеет следующий вид:FEATURES_LIST = %i[
  flag1 
  flag2 
  flagN
].freeze


option :user_id
option :loyalty_id


def call
  return Failure(message: ""No way you're doing it in production"") if Rails.env.production?


  user = Spree::User.find(user_id)
  enable_flags
  add_roles(user)
  update_auth(user)
  create_basic_promotion!
  max_spend_percent_for_first_retailer
  Success(message: ""Loyalty is enabled! user id = #{user_id}; service id = #{loyalty_id}"")
endКак подготовить тестовое окружение одной командойДалее я расскажу, как использовать написанный код на практике, откуда его можно вызвать, а также, как быстрее подключаться к rails-консоли.Вызвать класс EnableForTesting на тестовом окружении можно 2 способами:из rails console;из bash.Чтобы быстро подключиться к rails console или bash мы используем kubectl + скрипт kch, за который отдельное спасибо моему коллегеАлексею. Скрипт является набором алиасов для часто используемых у нас команд kubectl.Вызов команды производится в формате kch namespace pod cmd. Например, вместо длинной командыkubectl exec -n stand-0000 -i -t -c app api-pod-ffffff-0000 -- railsc можно написать короткуюkch 3003 api rails, которая будет трансформирована в вышеописанную длинную, т.е. нам не нужно знать полные имена подов приложения —  достаточно знать номер стенда и куда хотим подключиться (rails, bash, mysql).Скрипт в общем списке kubectl найдет полное название неймспейса и пода в нем по соответствующим вхождениям, подставит их в шаблон команды для вызова, после чего выполнит получившуюся команду.Вызов из bash происходит командой:rake 'loyalty:enable_for_testing[user_id, loyalty_id]'Вызов из rails console:Loyalties::Tasks::EnableForTesting.call(user_id: 12345, loyalty_id: 'abcdef')После вызова класса для пользователя с id = 12345 будут последовательно выполнены все команды по настройке тестового окружения. Второй аргумент (loyalty_id) является опциональным: если не передавать его, будет установлен loyalty_id по умолчанию (со статусом registered). Если нужен другой статус — передаём нужный loyalty_id и он привязывается к тестовому пользователю.Смотрим, как в консоли выполняются команды, ощущаем себя хакером.После выполнения видим результат:=> Success({:message=>""Loyalty is enabled! user id = 12345; loyalty_id =""abcdef""})Готово, мы только что сэкономили полчаса жизни :)ЗаключениеДля создания тестовых заказов мы также написали код, который можно выполнять в консоли (осталось только обернуть его в отдельную рейк-таску).Также обязательно пишем статью во внутренней базе знаний, как пользоваться нашими рейк-тасками, чтобы QA из других команд могли сами всё настроить, если нужно проверить какой-то функционал с бонусами. Тем самым сокращаем время на коммуникации.Теперь настройка занимает меньше минуты, а QA занимается непосредственно проверкой кода. При появлении новых фич (а значит фича-флагов и прочих артефактов) я делаю merge request в проект и добавляю нужный код, поддерживая инструмент в актуальном состоянии. Да, у нас QA тоже деплоят в продакшн :)В заключение хочу сказать, что описанный выше пример можно переложить на другой стек и бизнес-логику, там конечно будут свои особенности реализации, но сама идея использования скриптов для настройки тестовой среды на стендах остается актуальной.Поделитесь, как вы настраиваете тестовое окружение в своих командах?Tech-команда Купера (ex СберМаркет) ведет соцсети с новостями и анонсами. Если хочешь узнать, что под капотом высоконагруженного e-commerce, следи за нами вTelegramи наYouTube. А также слушайподкаст «Для tech и этих»от наших it-менеджеров."
СберМаркет,,,"Миграция ЦОД, или взгляд на переезд со стороны ИТ",2024-10-04T12:39:55.000Z,"Мы планируем переезд! Новый офис будет современнее, удобнее, красивее и больше. Хорошая новость для сотрудников, но непростая – для айтишников.  В этой статье решили разобрать ключевые вопросы переезда со стороны ИТ-оборудования и ПО: какие миграции бывают, на что нужно обратить внимание и как справляться с некоторыми трудностями. Причём предлагаем расширить тему: переезд не всегда связан с перемещением именно пользователей, персонал может остаться в том же офисе, но оборудование и системы по каким-то причинам мигрировать нужно. Вот об этом поговорим.На простом языке миграция ЦОД — это и есть переезд. Он может происходить на двух уровнях: физическом («железо») и логическом (ПО), а может объединять и тот и другой. Таким образом, все миграции ЦОД можно разделить на три типа. Мы пойдем от простого к сложному.Физические переездыПервый тип — самые простые переезды, когда мигрирует оборудование. Например, у компании есть набор оборудования на одной из площадок. Он может состоять из чего угодно, скорее всего, это комплекс: сети, серверы, системы хранения плюс какая-то обвязка. В целом, все, что нужно для ЦОД. Цель — переехать на новую площадку.Вне зависимости от основных мотиваций переезд не затрагивает напрямую самую сложную часть — перенос данных, информационные системы, которые работают на этом оборудовании. Может быть, это прозвучит грубо, но в сухом остатке оборудование само по себе никому не нужно. Никому не нужны сетевые коммутаторы, маршрутизаторы, серверы, системы хранения. Все это строится для того, чтобы запустить на них какой-то полезный софт, который реализует бизнес-функцию. Поэтому, когда мы говорим про простые переезды на уровне «железа», речь идет про такие переезды, когда не нужно продумывать, как переносить данные или системы. Так получается, если оборудование уже все выключено: например, новое оборудование приобретено, а старое (прошлое) надо перевезти на другую площадку, чтобы там развернуть его как резервный или тестовый контур. Еще встречаются ситуации, когда так устроен бизнес, что он действительно может выключить полностью все свое оборудование вместе с системами, погасить их и не задумываться о том, как они там работают и взаимосвязаны.Кстати, такое бывает реже: далеко не все бизнесы работают 24/7 и предоставляют услуги B2C, функционируют в формате цифровой дистрибуции и т.д. Главное при таком типе переезда — с пониманием дела перевезти оборудование. То есть точно нужно:правильно изучить исходные наборы оборудования и площадкутщательно обследовать целевую площадку и посмотреть, хватит ли места в стойкахнарисовать план размещения: если стойки другие по своей конфигурации, то продумать перераспределение оборудования. Исходя из плана размещения нарисовать план коммутации, потому что все оборудование нужно подключить проводами в определенные дырки: сетевые коммутаторы, SAN-коммутаторы, электричество.Важно, точнее, крайне желательно промаркировать все порты. Потому что иначе будет непонятно, как потом это все собирать. Дальше, кстати, маркировку используют на плане коммутации. Когда наступает некий час X, выключаются все информационные системы, которые там работают, всё оборудование оказывается без нагрузки. Затем специалисты корректно выключают оборудование.Оборудование не всегда легко выключить. У него есть определенные процедуры, особенно если мы говорим про какое-то серьезное High-End-оборудование. Соответственно, всё выключается, декоммутируется, все провода вытаскиваются, всё демонтируется.Тут тоже есть нюансы. Некоторое оборудование нужно демонтировать, разбирать, вытаскивать из шкафов. Некоторое оборудование, наоборот, нельзя. Надо заранее продумывать переезд: объемы машин, всякие проходы, возможности погрузки, разгрузки, работы грузчиков именно на оборудовании, смонтированном в шкафах. Там ставятся специальные транспортировочные болты — крепление, которое позволяет оборудование не разбирать. У многих «железок» есть очень сложная внутренняя коммутация. Если её декоммутировать, то потом на площадке, как правило, коммутация такого оборудования возможна только в присутствии вендоров или с их участием. Сейчас это большая проблема, потому что оборудование осталось, а зарубежные производители ушли.Особое внимание стоит уделить ещё плану старта. Есть определенные взаимосвязи внутри оборудования, они довольно очевидные для нас. Точно так же, как и включать-выключать оборудование нужно согласно процедурам. Есть ещё взаимосвязи внутри информационных систем – их, по-хорошему, должен понимать сам владелец систем: в каком порядке надо стартовать оборудование с учётом работающих на нём систем. Некоторые предпочитают лишний раз не заморачиваться, но потом при включении возникают проблемы.Физические переезды, как правило, происходят в сжатые сроки. Переезды с полным выключением оборудования чаще всего встречаются у тех компаний, у которых все системы сконцентрированы на одной площадке, нет никаких резервных площадок, резервных «плечей», контуров и т.д. Проще говоря, нет возможности перенести, например, нагрузку пользователей на другое оборудование, чтобы поработать с этим. У них обычно не остается никаких вариантов, кроме как все выключить и все включить на новом месте.Опять же, далеко не все переезды так можно организовать. Огромные объемы в сотни-сотни стоек просто не уедут за короткий срок, и, соответственно, он затянется. В большинстве случаев такие большие компании не могут себе позволить выключить свои системы надолго, поэтому здесь появляются другие переезды.Логические переездыВторой тип переездов связан с переносом данных — это ключевая задача, а конфигурации на уровне оборудования могут быть любыми. Например, вы приобрели какие-то новые «железки», пусть будут хранилища (СХД). У вас система как работала, так и работает, но нужно переехать со старой на новую. Или ситуация, когда у клиента система так работает в некоем распределенном режиме в разных ЦОДах, но снизу всегда есть место хранения. Оно почти всегда одно, потому что с точки зрения ограничений записи - всегда есть точка, которая является мастер-данными. И вот владелец хочет эти мастер-данные перенести со старой СХД. «Железка» или целый комплекс — это неважно. Нужно перенести объект как сущность, хранилище, на новое. Такая миграция может происходить внутри ЦОДа, между ЦОДами, между странами — это неважно. При таком типе переезда важно то, что чаще всего отсутствуют работы на физическом уровне с оборудованием, потому что есть старое оборудование и новое.Помимо этого, почти всегда есть требования, что переезд должен происходить вообще без прерывания сервиса и без прерывания доступа к данным. Такие ограничения присущи онлайн-сервисам, процессингу в банках, каким-то карточным историям в программах лояльности — в общем, тем, что работает 24/7. При этом данные мигрировать надо, потому что старое хранилище заканчивается, ломается или что-то ещё нехорошее с ним происходит. В таких случаях самое сложное — это продумать, как осуществить переезд с одного хранилища на другое так, чтобы системы, которые эти данные используют, ничего не заметили. И тут масса вариантов.Так, если мы говорим о базе данных, то у самой БД есть ряд инструментов, которые позволяют создать ее копию, перенести все данные, а потом в какой-то момент относительно быстро и с коротким перерывом переключить нагрузку систем на новую локацию. Такой локацией может оказаться новая «железка», новый ЦОД, новая страна. Если это какие-то абстрактные данные, то есть не конкретная СУБД, а какие-то абстрактные данные, и снизу меняется «железка» или комплекс «железок», который это все реализует, тогда продумываются истории с репликацией низкого уровня — хранилища данных.Есть пример: требовалось перевезти хранилище высокого класса, которое поддерживало active-active-репликацию. Записи проходили справа налево и слева направо одновременно в онлайн. В реальности стоит одно хранилище, где лежат все данные, и которое поддерживает одну из технологий active-active-репликации. В целевой локации ставится такое же или совместимое хранилище и объединяется в репликационную пару. Это позволяет взять локальные LUN первой СХД и растянуть их в распределенные LUN, которые в свою очередь поддерживают одновременную запись с двух сторон и двунаправленную репликацию. Выполнив все условия создания такой сущности, объединив их в некий репликационный кластер, их называют по-разному. Не важно, первое сейчас устройство работает или второе, для системы это будет прозрачно. Пока хотя бы одно из них работает, будет прозрачно: таким образом можно данные из первого устройства перенести во второе незаметно для систем, потом первое выключить, а системы продолжат работать во втором. Это называется перенос данных.На самом деле вариантов много. Так, в случае с виртуальными машинами (абстрактные сущности), существует множество технологий. Они позволяют переносить виртуальные машины (далее – ВМ) с места на место так, чтобы системы, которые работают внутри ВМ, и системы, которые взаимодействуют вокруг, ничего не заметили, пока происходит переезд в другую систему, ЦОД или страну. Существуют даже целые идеологии, которые продвигали зарубежные крупные вендоры, — follow the moon, follow the sun. Их смысл заключается в том, что если у вас транснациональная компания и есть цепочка ЦОДов по всему миру, around the globe, то вы максимально локализуете трафик. Вы можете уносить данные, системы, виртуальные машины и все остальное от пользователей максимально далеко, в ту часть земного шара, где сейчас ночь и где самое дешевое электричество, при этом люди будут работать в светлое время суток и тем самым экономить. Переносим данные — нужно исходное и целевое «железо» — это минимальное требование.Таких кейсов тоже очень много. Чаще всего эти кейсы возникают, когда у компании идут модернизации на новое «железо», в новые ЦОДы и т.д. Иногда все это используется, когда у компании проходят процедуры централизации — например, забирают информационные системы из регионов и переносят в центральный ЦОД в Москве, или, наоборот, децентрализации — когда хотят из дорогого ЦОДа в Москве уехать в более дешевую локацию.Переезды ИТ-системДанный тип переезда не так сложен с точки зрения технологических решений, как предыдущий. Потому что именно там создаются уровни абстракции, мы подкладываем репликационную логику, придумываем хитрое снизу, для того чтобы сверху для систем ничего не изменилось. А с точки зрения логики переезда, планирования переезда, подготовки к переезду, устранения последствий, вообще, влияния на бизнес, самые сложные и тяжелые — это миграции информационных систем. Например, у нас есть какой-то ЦОД, в котором работает какой-то набор информационных систем, создающих бизнес-пользу, и нужно их перенести в новый ЦОД. Предпосылки могут быть те же, что и в физическом переезде: закрывают старый ЦОД, расширяют текущий и т.д.Самые часто встречающиеся кейсы — это когда построены или арендованы новые ЦОДы и туда планируется переезд. При этом бизнес такой большой и зависимый от ИТ, что он не может выключить все на выходные или на пять дней, перевезти на физическом уровне и включить. Или, например, планируется переезд со старого оборудования на новое, но целиком. Либо нужно, чтобы информационная система, которая сейчас работает на двух серверах, переехала в новый ЦОД. Благодаря этому два сервера в основном ЦОД превращаются в четыре: 2 в ОЦОД и 2 в РЦОД (резерв). Одним словом, когда у тебя накапливается какой-то набор изменений, самый простой способ их все разом применить — это заново развернуть систему в новой инфраструктуре.Обычно в реальных проектах чаще всего встречается некий микс: что-то поедет без изменений, что-то — с большим объемом изменений, что-то — со средним объемом изменений. В итоге проект выходит большим, разнообразным и разношерстным. Самые большие сложности в таком переезде заключаются в том, что нужно в первую очередь спланировать переезд посистемно. Необходимо понимать, какая система из чего состоит, и какая система с чем связана и почему.Так, переезды из старой инфраструктуры в новую часто обусловлены отягчающими обстоятельствами, например сменой IP-адресации. Нельзя сохранить старую, потому что построено новое ядро сети. Старое ядро сети разобрать не можем, оно продолжает работать, а новое надо построить, там своя IP-адресация. Это огромная проблема для систем: меняются IP-шники, и рассыпается практически все: доступ к системам пользователей, доступ к системам извне через межсетевые экраны, интеграции между системами и даже взаимосвязи внутри каждой системы.Второй отягчающий фактор встречается, когда люди переезжают, потому что отделяются от головной компании, например, и создают собственную ИТ-инфраструктуру. В таких случаях меняется IP-адресация, домен, авторизация, порядок именования объекта, да что угодно. При этом применяется собственная техническая политика, которая определяет стандарты версии операционных систем, СУБД и т.д. Конечно, такой переезд — это, по сути, пересоздание всех систем в новых ЦОДах, в новой инфраструктуре. И подобную активность никак нельзя рассматривать как новый цикл внедрения систем. Они давно работают, в них накоплен объем настроек и данных, поэтому это именно миграция. Когда построены новые ЦОДы и есть понимание, что от всего старого оборудования нельзя отказаться, и докупается некий пул, например 30%, то освобождаются старые 30%, переносится часть своих систем, данных виртуальных машин на новые, добавляются в общий пул, выключаются старые мощности.Подобная игра в пятнашки будет продолжаться до победного. Такие переезды обычно очень долгие и занимают год-два, в зависимости от того, о каком масштабе компании мы говорим. Большие банки переезжают по два-три года. Причем это абсолютно нормальная история. Потому что у тебя сотни информационных систем, каждая нужна и в каждой нужно разобраться, подготовить необходимые планы, документации, которые опишут, как она будет меняться. Только после этого что-то можно менять, потому что это все работающие продуктивные системы.Когда готовится посистемный план миграции, необходимо подумать обо всем:из чего состоит системаиз каких серверов состоитгде эти серверы лежат, физические и виртуальныегде лежат их данныекакие есть между ними дополнительные компоненты, файловые шары и т.д.Дальше необходимо продумать интеграционный уровень: как системы общаются с другими системами, что является источником данных, а что — приемником данных, через какие интеграционные компоненты: может, через файловые шары, через почту, через интеграционные шины и т.д. Без комплексного понимания всего перечисленного план может получиться однобоким, и миграция закончится откатом в исходное состояние.Если переезд целиком на новое оборудование, надо думать о совместимости. Случается, старые версии софта не смогут подняться на новых версиях операционных систем. Это важный аспект, если ты пытаешься в рамках миграции поднять уровень операционных систем до какого-то корпоративного технологического стандарта, потому что софт там не развернется.Если забираешь старые виртуальные машины либо сервер, нужно подумать:как эти старые компоненты со старыми прошивками, со старыми версиями лягут в новую инфраструктуру;обновление до новых версий и могут ли они обновиться до новых версий;готовность держать в своей новой и чистенькой современной инфраструктуре старые куски и чем это будет грозить.Если запланирована смена IP-адресации, доменов, еще каких-то логических вещей, то это самая сложная часть. Многие системы просто не переживут переезда, существуя годами с определенными настройками, именами, IP-шниками. Системы обрастают различными атавизмами, которые жестко завязаны на эти адреса, имена. Поэтому зачастую все это попадает даже внутрь каких-то конфигураций системы либо вообще внутрь кода («захардкоженные» настройки).Все это значит, что в моменте планирования большой миграции надо подготовить запасные пути, какие-то «костыли», потому что переехать целиком на новую адресацию сразу точно не выйдет. Что делать? Сохранять связи со старым ядром, инфраструктурой, забирать маленькие кусочки сетевой адресации, забирать старое именование. Это может сильно помочь, если, допустим, появляются проблемы в именах: не всегда можно поменять имя у сервера, либо у соседнего компонента, либо у сервиса, который тоже опирается на DNS A-запись, либо не получается создать Alias, чтобы сохранить старое имя, не сохраняя старое имя, и т.д. Практически ни одна крупная компания не переехала в таком формате в новую чистую красивую инфраструктуру целиком, у всех оставались «костыли» к старой инфраструктуре.Миграция: как?Сами по себе переезды могут быть разными. Самый классический —это посистемный. Из плана переезда понятно, из чего система состоит, как она ляжет в целевую инфраструктуру, на какие серверы, хранилище и т.д. После требуется написать план, что система, находясь в состоянии А, переходит в состояние Б с помощью вот таких шагов, за каждый из шагов отвечает какой-то специалист, который понимает, как этот кусок сделать. Большинство шагов можно будет делать вне даунтайма. Как правило, это подготовительные и еще какие-то шаги, которые скрыты от самой системы и ее пользователей.Так, очень часто возникает какая-то точка переключения. Грубо говоря, система работает в ЦОД А, ты поднял новую систему в ЦОД Б, перенес настройки, скопировал данные. В результате, есть исходная система и целевая точно такая же, даже лучше. Теперь нужно, чтобы пользователи перестали ходить в систему А и начали ходить в систему Б. Даже если точка входа в систему со всех сторон абстрагирована, где-то на внешнем балансировщике создана точка балансировки, куда приходят пользователи. Потом там идет технологический редирект через несколько слоев, и только тогда пользователи оказываются на системе, ничего, конечно, не зная о ее внутренних частях. В этом случае нужно как минимум в этой точке балансировки убрать старые серверы и добавить новые. Получается короткий период недоступности систем на время перенастройки точки балансировки, и пользователи ходить не смогут. Итого: немного работ в даунтайме, часть — вне его. И так по каждой системе — классика!Есть менее классические подходы. Среди них встречаются случаи, когда приходится разбирать взаимосвязи системы, группировать их на какие-то глубоко связанные друг с другом группы — кусочки. Например, есть пять систем, которые реализуют какую-то одну бизнес-функцию. Они тесно друг с другом связаны, то есть выключение любой из них делает все остальные бессмысленными, цепочка рвется, бизнес не работает. Поэтому, если есть такие взаимосвязи, это обычно серьезная нагрузка на специалистов в самой компании, на технологов и тех, кто понимает бизнес-процессы.На практике это происходит так. Представим, что у нас в час Х не будет работать функционал кредитного конвейера, потому что мы возьмем все системы, которые участвуют в автоматическом одобрении кредитов, и их перенесем из ЦОДа А в ЦОД Б. Мы понимаем, из чего они состоят, как они друг с другом интегрируются, что нужно в них поменять в зависимости от ситуации. После этого все системы разом подготавливаются, гасятся, переносятся или подготавливаются на новом месте и переключаются. Таким образом у нас происходитперенос группами.Такой тип переезда случается реже, потому что владельцы бизнеса крайне редко знают о всех взаимосвязях. Т.е. понятно, что есть система кредитный конвейер. Но это лишь одно звено, а ведь у него много источников данных, приемников данных и т.д. Целостное знание отсутствует, поэтому переносим по одной системе.Третий способ миграции — этоперенос ЦОДами.Данный тип тоже касается больших ИТ-инфраструктур со сложной взаимосвязью с работой систем 24/7. Но почему тогда не использовать другой тип? Тут появляется еще одно условие — вся инфраструктура на момент старта переезда имеет два контура. Условно, есть системы с резервным контуром во втором ЦОД, но нет нового набора железа в целевом ЦОД для посистемного перехода. Поэтому переезд происходит целиком ЦОДом и системы едут вместе с оборудованием.Как это происходит? На двух площадках размещены полностью идентичные продуктивные контуры, и есть понятный механизм переноса нагрузки с площадки A на площадку Б и обратно. Тогда, например, если не покупать для новых площадок новое оборудование или покупать его в очень ограниченных количествах, придется использовать свою резервную инфраструктуру для переезда на уровне ЦОДов. Что это такое? Это когда есть два ЦОДа, 50–100 систем. Будем считать, что они все зарезервированы, все идеально, хотя так не бывает. Берем всю нагрузку пользователей, все подключения внешние и внутренние и переключаем, чтобы все это шло на площадку А. Убеждаемся, что площадка Б функционирует, но все ресурсы там, по сути, в пассиве. Проверяется это очень легко — нужно их выключить. Таким образом, когда все сделано, следующий шаг — это выключение и проверка, не перестали ли работать бизнес-функции и корректно ли все перенеслось. Обычно с первого раза не получается. Выключаем площадку, с которой снята нагрузка, и выходит – половина сервисов отъезжает. Выясняется: алгоритмы переключения у владельцев систем написаны не очень хорошо. Тогда включаем обратно, разбираемся, где что потеряли, корректируем алгоритмы.По опыту, со второго или третьего раза точно получается. Зато мы точно можем спокойно на какое-то необходимое время избавиться от второй площадки. Безусловно, ИТ-инфраструктура в этот момент находится в аварийном состоянии без резерва. Но это допустимое зло. Если мы из двух ЦОДов в два переезжаем, мы подготавливаем два ЦОДа, связываем их друг с другом, со старыми ЦОДами, настраиваем. Cетевое оборудование в любом случае будет дублироваться. Все готовится заранее: покупаются необходимые объемы оборудования, чтобы минимизировать этот переезд, выключается ЦОД, с которого снята нагрузка, увозится, все там размещается, так же как в физическом переезде, планы коммутации, планы размещения и т.д. Все запускается. У нас ЦОД А1, который остался в старом месте, начинает работать с ЦОДом Б2, то есть ЦОД Б, но уже на новом месте. Все работает (проверяем). Затем всю нагрузку всех систем переносим, но теперь из ЦОДа А1, старого, в ЦОД Б2, уже новый. Снова убеждаемся, что все функционировало, как надо. Потом выключаем ЦОД А1, все идет по плану: увозим в ЦОД А2, включаем. Восстанавливаем все взаимосвязи, репликации обратно, догоняем, чтобы они снова стали синхронными, или заново синхронизируем. Всё, переехали двумя ЦОДами в два новых ЦОДа — стало А2, Б2.Третий способ миграции по срокам самый короткий. Да, долгий процесс подготовки, много-много нюансов на самом деле, но в целом это намного быстрее, чем переезд по системам или группами. Все потому, что при переезде на уровне ЦОДа не нужно так глубоко разбираться в системах, в их составе и взаимосвязи. Конечно, при условии, что на момент старта работ все системы действительно симметрично зарезервированы в ЦОД А1 и Б1.В целом это всё, что нужно знать о миграциях и трудностях при переезде.ВЫВОДЫПодводя резюме, ключевое, о чём нужно помнить при миграции ЦОД:Любой переезд – это сложный технологический процессНеобходимо тщательное планированиеНеобходимо следовать плану переездаСледуя этим шагам, вы сильно облегчите себе жизнь и перевезёте весь ИТ-парк максимально бесшовно, независимо от масштабов компании. Но сроки важны всегда – чем больше времени у вас на подготовку, тем меньше рисков при переезде."
СберМаркет,,,Как подружить Zabbix с eXpress,2024-08-07T08:28:33.000Z,"Всем привет! На связи Ситуационный Центр Московской биржи. Сегодня мы расскажем, как наводили красоту и удобство в процессе интеграции Zabbix с новым корпоративным мессенджером eXpress, и покажем, как сделать так же (или еще лучше).1. ДаноВ конце 2023го года в рамках работ по импортозамещению основным корпоративным мессенджером Московской Биржи вместо MS Teams стал eXpress.На корпоративный мессенджер завязано множество интеграций и рабочих процессов, в частности - интеграция системы мониторинга Zabbix Ситуационного Центра. С помощью этой интеграции мы имели возможность оповещать инженеров в чатах о возникновении и устранении событий с помощью бота.Перед командой Ситуационного Центра Московской Биржи встала задача: перенести все настроенные интеграции, рабочие процессы и оповещения Zabbix в eXpress2. Первый подходМы решили не придумывать велосипед и воспользоваться самым простым и очевидным решением:Создали чаты по каждой из системДобавили в эти чаты заинтересованных инженеровДобавили в чаты ботаНастроили бота писать сообщения по событиям разных систем в разные чатыВыглядело это примерно так (верхнее сообщение - событие произошло, нижнее событие - событие устраненоПример сообщений в чате eXpressТехнически - задача решена, интеграция работает, инженеры оповещаются. Но что-то было не так..3. Что было не такОсновная проблема такого подхода - полное отсутствие читаемости и наглядности.Сложно сходу понять приоритет события.Сложно понять, какие события уже устранены, а какие нетДля решения этих проблем мы немного поковырялись в шаблонах и пришли к новому виду оповещений.4. Как сталоВ новом шаблоне оповещений полностью изменился формат сообщений.Появился цветовой индикатор статуса события (зеленый - хорошо, красный - плохо), который меняется при изменении статуса в заббикс.Использована маркдаун маркировкаДобавлен цветовой индикатор приоритета событияПолучили:При возникновении события:Сообщение о возникновении события по новому шаблонуПри решении редактируется сообщение. Меняется цветовой индикатор, добавляется строка с информацией о времени решенияСообщение об устранении события по новому шаблону5. Почему это хорошоПлюсы такого шаблона сообщений понятны:Повышена читаемость - сразу виден статус события, его описание, приоритетменьше спама - вместо повторного сообщения о восстановлении редактируется исходное сообщение и меняется статус с красного на зеленый6. Как сделать так же6.1. Изменить скрипт отправки·       Изменить метод с reply на editМетод отправки уже добавляет тег с ключом сообщения для того, чтобы отправлять ответное сообщение при восстановлении проблемы. Все, что нужно для того, чтобы менять текст оригинального сообщения -  endpoint для ответного сообщения//Было
url = 'api/v3/botx/events/reply_event';
//Стало
url = 'api/v3/botx/events/edit_event';·       Добавить цветовые индикаторы в тело скриптаДобавление цветовых индикаторов несколько неочевидно. Если попробовать добавить их в шаблон сообщения, то получим ошибку. Zabbix не может сохранить эмоджи в шаблон сообщения. Однако может сохранить их в скрипте отправки. Для добавления индикатора состояния достаточно просто отправлять его вместе с сообщениемif (is_problem) {
            data = {
                group_chat_id: Express.params.send_to,
                notification: {
                    status: 'ok',
                    body: ""🔴""+message  //Отправится при возникновении проблемы
                }
            };
            url = 'api/v4/botx/notifications/direct';
        }
        else {
            data = {
            payload:{
            body: ""🟢""+message  //Отправится при решении проблемы
            }
            };
        url = 'api/v3/botx/events/edit_event';Для добавления маркировки статусов потребуется немного больше изменений. Нам необходимо создать свой макрос в шаблоне сообщения и заменять его на соответствующий статус перед отправкой. Для этого сперва добавляем приоритет события в параметры скрипта макросом{EVENT.NSEVERITY}, а в шаблоне сообщения заменяем приоритет уникальной строкойSeverity: {event_sever}. После чего мы можем подставить нужное нам значение в скриптеpostMessage: function (is_problem,event_sever) {
var message = Express.params.message
var sever = [""◻Not classified"",""🟦Information"",""🟨Warning"",""🟧Average"",""🟥High"",""🟪Disaster""]
message = message.replace(""{event_sever}"",sever[event_sever])6.2. Собрать новый шаблон сообщенияШаблон сообщения с маркировкойСобытие произошло:**{EVENT.NAME}**

* Started at {EVENT.TIME}
* Host: {HOST.NAME}
* Severity: {event_sever}
* Operational data: {EVENT.OPDATA}
* Description: 
{TRIGGER.DESCRIPTION}
* Event info: [Zabbix](http://zabbix.com/zabbix/tr_events.php?triggerid={TRIGGER.ID}&eventid={EVENT.ID})Событие устранено:**{EVENT.NAME}**

* Started at {EVENT.TIME}
* Host: {HOST.NAME}
* Severity: {event_sever}
* Operational data: {EVENT.OPDATA}
* Description: 
{TRIGGER.DESCRIPTION}
* Event info: [Zabbix](http://zabbix.com/zabbix/tr_events.php?triggerid={TRIGGER.ID}&eventid={EVENT.ID})7. Заключение / ВыводыНовый вид оповещений помогает нашим инженерам быстрее и точнее обрабатывать события. Мы продолжаем изучать возможности интеграции между Zabbix и eXpress для еще большего упрощения процесса и обязательно вернемся к вам с новостями.Спасибо за внимание!"
СберМаркет,,,Автоматизация тестирования бизнес-процессов через camunda,2024-07-18T09:35:07.000Z,"1. ВведениеВсем привет! Меня зовут Ренат Дасаев и в прошлой статьеАвтоматизация Е2Е‑тестирования сквозных БП интеграционных проектов Операционного блокабыло рассказано о том, как устроено e2e‑автотестирование. Сегодня хочу рассказать о том, как используетсяcamundaв автотестировании бизнес‑процессов (далее БП). На практических примерах рассмотрим, что и как мы делаем в своих тестах.Если интересно узнать о движке camunda и о том, как он применяется в БП Московской Биржи на примере проекта ЦУП, — рекомендую ознакомиться сматериалом нашего коллеги.2. Проверки БП в camunda автотестамиПрежде чем погрузиться в техническую часть вопроса, определимся, какие проверки нам необходимо осуществить в ходе БП.Стандартный набор проверок:процесс начинается и завершается успешно;gateway, boundary events и другие элементы двигают БП в нужном направлении;на проверяемых шагах корректный контекст — присутствуют необходимые переменные процесса;корректный результат вычислений в выражениях events/gateway;успешно исполняются service‑таски и создаются user‑таски;на шагах, где есть взаимодействие с внешними сервисами — успешно отрабатывают (в e2e нет заглушек, работаем с реальными интеграциями);не только «success path», но и другие маршруты следования по схеме, в том числе, где обрабатываются ошибки в процессе;процесс не впадает в инцидент.Схемы могут вызывать подпроцессы со своим контекстом. В таком случае проверяются и порожденные процессы наравне с родительским.3. Разработка модулей по работе с bpmn – процессамиВпрошлой статьеупоминалось, что совместно с другой командой разрабатываем python-модули для работы с микросервисами, а также модули (клиенты) по работе с сервисами/движками.Для работы с bpmn-процессами разработано несколько модулей:moex-pmh-bpmn (работа с bpmn-процессами);moex-pmh-camunda-client (работа сcamunda rest api);moex-pmh-<название_БП> (работа с конкретным инстансом БП, на каждый бизнес-процесс по 1 такому модулю).Иерархия модулей:moex-pmh-<название_БП> → moex‑pmh‑bpmn → moex-pmh-camunda-client.Модули разработаны, размещены в корпоративном pypi-репозитории, и любая из команд в нашей компании может их использовать в своем проекте. При этом у команд появляется также возможность предлагать изменения в кодовой базе (новые фичи, исправление ошибок) через merge-request.На текущий момент модульmoex-pmh-camunda-clientумеет работать со следующими контроллерами:Decision DefinitionExecutionExternal taskJobJob DefinitionHistoric Process InstanceHistoric Variable InstanceIncidentProcess DefinitionProcess InstanceTaskVariable InstanceКонтроллеры добавляются по мере необходимости. В каждом методе любого контроллера на выходе получаем объект датакласса со всеми полями, которые приходят в ответе. Ниже будет показан пример использования возможностей этого модуля при решении практических задач автотестирования БП.4. Старт бизнес-процесса. Поиск запущенного процесса в camunda.В любом БП движок camunda доступен через http — https://<тестовый_домен>/<camunda>. С этим endpoint автотест и работает в ходе проверок внутри БП.Первое, с чего начинается автотестирование БП, — это его старт. В большинстве случаев запуск процесса происходит в ходе возникновения определенного рода событий:отправки сообщения в шину, например, esb (enterprise service bus);создания объекта в БД сервиса;наступления определенной даты и/или времени;получения электронных писем, файлов и прочее.После того, как БП запустился, необходимо этот старт идентифицировать и найти запущенный процесс в списке активных. Его, как правило, можно найти по уникальным переменным процесса, таким как бизнес-ключ (business_key), торговый код инструмента (securityid) или другим переменным, присущим конкретным БП. В редких случаях идентификация процесса происходит по дате и времени запуска, если нет уникальных переменных процесса.В большинстве случаев инженеру по автотестированию достаточно просто руками найти запущенный процесс в его camunda, основываясь на времени его запуска. Далее инженер анализирует переменные процесса и находит уникальные, по которым можно точно идентифицировать запускаемый экземпляр процесса.Для наглядности возьмем реальный БП «Активация выкупа ценной бумаги» (входит в один большой процесс по Регистрации, активации и деактивации выкупа). Это одна из самых маленьких схем, что удалось найти. В 99% случаев схемы гораздо больше по объему и взаимодействию с внешними ресурсами. На рисунке 1 представлен скриншот данного БП из camunda:Рисунок 1. Схема бизнес-процесса “Активация выкупа”В нашем примере поиск процесса будет производиться по переменнойsecuirtyId(торговый код инструмента). На рисунке 2 можно увидеть запущенный экземпляр процесса «Активация выкупа» со списком переменных процесса, среди которых есть необходимая переменная:Рисунок 2. Запущенный экземпляр БП «Активация выкупа» и отображение переменной securityIdНиже представлен код на языке Python, который реализует поиск среди активных процессов по переменнойsecuirtyId:def wait_for_process_instance_by_security_id(
    self,
    security_id: str,
    *,
    max_time: int = 180,
    sleep_time: float = 2.0,
) -> CamundaProcessInstance:
    with allure.step(
        f'Проверка того, что был запущен процесс ""{self.pretty_name}"" '
        f'со значением переменной securityId=""{security_id}""',
    ):
        return poll_by_time_first_success(
            fn=lambda: self.get_process_instance_by_security_id(security_id),
            checker=lambda p: p is not None,
            max_time=max_time,
            sleep_time=sleep_time,
        )Производятся попытки в течениеmax_time(120 секунд по умолчанию) с шагом в 2 секунды (по умолчанию) в цикле (поллинг) найти процесс, в котором обнаружилась необходимая переменная (get_process_variable_instances) и её значение:def get_process_instance_by_security_id(
    self,
    security_id: str,
) -> Optional[CamundaProcessInstance]:
    with allure.step(
        f'Поиск процесса ""{self.pretty_name}"" '
        f'со значением переменной securityId=""{security_id}""',
    ):
        processes = self.camunda.get_process_instances()
        for process in processes:
            variables = self.camunda.get_process_variable_instances(process.id) or {}
            if 'securityId' not in variables:
                continue
            v_security_id = variables.get('securityId').value
            if v_security_id == security_id:
                return processЕсли процесс нашелся, то возвращается объект датаклассаCamundaProcessInstance,либоNone.ПримерCamundaProcessInstanceобъекта:id='007f5c67-44c4-11ef-b669-b2d4f57bb43f'
rootProcessInstanceId='007f5c67-44c4-11ef-b669-b2d4f57bb43f'
superProcessInstanceId=None
superCaseInstanceId=None
caseInstanceId=None
processDefinitionName='Подтверждение активации режима выкупа'
processDefinitionKey='bp-offer-activation'
processDefinitionVersion=7
processDefinitionId='bp-offer-activation:7:b99a97ae-17e5-11ee-971d-7ef2aaea4619'
businessKey='bpms/offersreg/1/1/RUTEST48KTEP/5623'
startTime=datetime.datetime(2024, 7, 18, 8, 10, 8, 440000)
endTime=None
removalTime=None
durationInMillis=None
startUserId=None
startActivityId='StartEvent_1'
deleteReason=None
tenantId=None
state=<ProcessInstanceState.ACTIVE: 'ACTIVE'>Если в процессе поллинга процесс не обнаружился за промежуток времениmax_time, то порождается исключениеTimeoutErrorи автотест завершается на этом.5. Мониторинг активностей по bpmn-схемеВажной функцией является поиск активностей по camunda-схеме. Это могут быть разные шаги:определенные вычисления в gateway/boundary/events;сервис-таски (service-tasks);пользовательские задачи (user-tasks);взаимодействие с базами данных/шинами/почтовыми ящиками/сайтами и прочее.На примере нашего БП по активации выкупа попробуем определить, что создалась пользовательская задача «Выполнить сверку ЕКБД с ASTS» (ЕКБД– Единая Клиентская База Данных,ASTS— Automated Securities Trading System).Для начала нужно вычислить id данного шага на схеме в camunda. Загрузим bpmn-файл данного БП вcamunda modelerи выделим необходимый шаг (см. рисунок 3):Рисунок 3. Выделенный элемент (пользовательская таска “Выполнить сверку ЕКБД с ASTS) в camunda modeler”В списке атрибутов находим id = compare-offers-manual. Это и будет тот идентификатор, который необходимо найти в списке активностей нашего тестируемого БП.Для поиска активности используем функцию с поллингом (waitforprocess_activity):def wait_for_process_activity(
    self,
    process_instance_id: str,
    *,
    activity_name: Optional[str] = None,
    activity_id: Optional[str] = None,
    max_time: int = 30,
    sleep_time: int = 2,
) -> None:
    moex_asserts.assert_true(
        expr=(activity_name or activity_id) and not (activity_name and activity_id),
        msg='Должен быть указан один из аргументов [activity_name, activity_id]',
    )
    kwargs = (
        {'activity_name': activity_name}
        if activity_name else {'activity_id': activity_id}
    )
    with allure.step(
        f'Проверка того, что процесс ""{self.pretty_name}"" '
        f'с id ""{process_instance_id}"" дошел до активности '
        f'""{activity_name or activity_id}""',
    ):
        poll_by_time_first_success(
            fn=lambda: self.find_process_activity(
                process_instance_id=process_instance_id,
                **kwargs,
            ),
            checker=lambda a: a is not None,
            max_time=max_time,
            sleep_time=sleep_time,
            msg=(
                f'Процесс ""{self.pretty_name}"" с id ""{process_instance_id}"" '
                f'не дошел до активности ""{activity_id or activity_name}""'
            ),
        )В которой на каждой итерации получаем список активностей (find_process_activity):def find_process_activity(
    self,
    process_instance_id: str,
    *,
    activity_name: Optional[str] = None,
    activity_id: Optional[str] = None,
) -> Optional[CamundaActivity]:
    moex_asserts.assert_true(
        expr=(activity_name or activity_id) and not (activity_name and activity_id),
        msg='Должен быть указан один из аргументов [activity_name, activity_id]',
    )
    with allure.step(
        f'Поиск активности ""{activity_name or activity_id}"" '
        f'процесса ""{self.pretty_name}"" с id ""{process_instance_id}""',
    ):
        for activity in self.get_process_activities(process_instance_id):
            if activity_name:
                if activity.activityName == activity_name:
                    return activity
            elif activity_id:
                if activity.activityId == activity_id:
                    return activityВнутриfind_process_activity()вызываетсяget_process_activities()(внутри используется camunda api -/process-instance/{process_instance_id}/activity-instances) и ищем среди них наш активити поactivity_id/activity_name:def get_process_activities(self, process_instance_id: str) -> List[CamundaActivity]:
    with allure.step(
        f'Поиск активностей процесса ""{self.pretty_name}"" '
        f'с id ""{process_instance_id}""',
    ):
        base_activity = self.camunda.get_process_instance_activities(
            process_instance_id,
        )
        child_activities = self.__get_process_activities(base_activity)
        return child_activitiesСоответственно, необходимо передать в аргументахwait_for_process_activityлишь:process_instance_id=<id_процесса> (вычисляется из функции поиска старта процесса);activity_id=’ compare-offers-manual’ (илиactivity_name).Как только процесс дойдет до этого шага в указанныеmax_time(обычно 60 секунд), то поллинг это отловит, произведет проверку и, если все успешно, передаст управление следующей функции в тесте. Иначе получимTimeoutError.После того, как тест нашел activity сactivity_id=’ compare-offers-manual’в camunda, необходимо вычислить id пользовательской задачи (user-tasks), чтобы потом с этим идентификатором найти задачу уже в Менеджере Задач платформы ЦУП.Для вычисления id пользовательской задачи в camunda разработали функциюwait_for_process_user_task():def wait_for_process_user_task(
    self,
    process_instance_id: str,
    *,
    activity_name: Optional[str] = None,
    activity_id: Optional[str] = None,
    max_time: int = 30,
    sleep_time: int = 2,
    post_await_time: int = 3,
) -> CamundaTask:
    moex_asserts.assert_true(
        expr=(activity_name or activity_id) and not (activity_name and activity_id),
        msg='Должен быть указан один из аргументов [activity_name, activity_id]',
    )
    kwargs = (
        {'activity_name': activity_name}
        if activity_name else {'activity_id': activity_id}
    )
    with allure.step(
        f'Проверка того, что по процессу ""{self.pretty_name}"" с id '
        f'""{process_instance_id}"" была создана пользовательская задача по джобе'
        f'""{activity_id or activity_name}""',
    ):
        task = poll_by_time_first_success(
            fn=lambda: self.find_process_user_task(
                process_instance_id=process_instance_id,
                **kwargs,
            ),
            checker=lambda a: a is not None,
            max_time=max_time,
            sleep_time=sleep_time,
            msg=(
                f'По процессу {self.pretty_name} с id ""{process_instance_id}"" '
                'не была создана пользовательская задача по джобе '
                f'""{activity_id or activity_name}""'
            ),
        )
        time.sleep(post_await_time)
        return taskКак и в поиске необходимой активности, необходимо передать идентификаторы процесса и активити. Внутри поллинга используется функцияfind_process_user_task():def find_process_user_task(
    self,
    process_instance_id: str,
    *,
    activity_name: Optional[str] = None,
    activity_id: Optional[str] = None,
) -> Optional[CamundaTask]:
    moex_asserts.assert_true(
        expr=(activity_name or activity_id) and not (activity_name and activity_id),
        msg='Должен быть указан один из аргументов [activity_name, activity_id]',
    )
    with allure.step(
        'Поиск пользовательской задачи по джобе '
        f'""{activity_id or activity_name}"" процесса '
        f'""{self.pretty_name}"" с id ""{process_instance_id}""',
    ):
        if not activity_id:
            activity_id = self.find_process_activity(
                process_instance_id=process_instance_id,
                activity_name=activity_name,
            ).activityId
        tasks = self.camunda.get_process_instance_tasks(
            process_instance_id=process_instance_id,
            activity_id=activity_id,
        )
        return tasks[0] if tasks else NoneВнутри функции используется POST-метод/task. В случае успеха (нашлась таска в нужном процессе и с нужным активити) возвращается список объектов датаклассаCamundaTaskсо всеми полями:id='00977769-44c4-11ef-b669-b2d4f57bb43f'
name='Добавить ""TEST-H9XQ8"" (RUTEST48KTEP) на режим ""Выкуп: Адресные заявки""'
assignee=None
created='2024-07-18T08:10:08.598+0300'
due='2024-07-18T23:59:59.647+0300'
followUp=None, delegationState=None
description='5623'
executionId='007f5c67-44c4-11ef-b669-b2d4f57bb43f'
owner=None
parentTaskId=None
priority=50
processDefinitionId='bp-offer-activation:7:b99a97ae-17e5-11ee-971d-7ef2aaea4619'
processInstanceId='007f5c67-44c4-11ef-b669-b2d4f57bb43f'
caseExecutionId=None
caseDefinitionId=None
caseInstanceId=None
taskDefinitionKey='compare-offers-manual'
suspended=False
formKey=None
camundaFormRef=None
tenantId=NoneИз данного объекта нас интересуетid='00977769-44c4-11ef-b669-b2d4f57bb43f'это и есть идентификатор задачи, который будет такой же и в ЦУП. Далее необходимо будет отправить запрос на получение активных задач в ЦУП в Менеджер Задач, найти её в выдаче и далее выполнить над этой задачей необходимые действия («Выполнить» или «Отклонить»).Если необходимо идентифицировать прохождение определенного шага в схеме, но этот шаг проскакивает очень быстро — достаточно в поллинге в sleep_time (задержка перед следующей итерации) задать низкое значение, например, 0.3 секунды.Если же поллинг по активным процессам не позволяет вовремя его обнаружить, то стоит воспользоваться поиском в уже завершенных процессах.Для получения информации об уже завершенном процессе достаточно передатьprocess_instance_idв  функциюget_historic_process_instance():def get_historic_process_instance(
    self,
    process_instance_id: str,
) -> Optional[CamundaHistoricProcessInstance]:
    url = f'{self.__url_prefix}/history/process-instance/{process_instance_id}'
    resp = self.__get(url=url)
    return CamundaHistoricProcessInstance(**resp) if resp else NoneЕсли же при этом нужно найти в уже завершенном процессе какой-то активити, то можно использоватьget_process_instance_historic_activities():def get_process_instance_historic_activities(
    self,
    process_instance_id: str,
) -> List[CamundaHistoricActivity]:
    url = f'{self.__url_prefix}/history/activity-instance'
    resp = self.__get(url=url, params={'processInstanceId': process_instance_id})
    return [CamundaHistoricActivity(**a) for a in resp]и в списке всех активити найти нужный.В случае если нужно найти переменную в завершенном процессе, то можно использоватьget_historic_process_instance_variables():def get_historic_process_instance_variables(
    self,
    process_instance_id: str,
) -> Optional[Dict[str, ProcessInstanceHistoricVariable]]:
    url = f'{self.__url_prefix}/history/variable-instance'
    params = {
        'processInstanceId': process_instance_id,
        'deserializeValues': 'false',
    }
    resp = self.__get(url=url, params=params)
    return {
        variable['name']: ProcessInstanceHistoricVariable(**variable)
        for variable in resp
    } if resp else None6. Взаимодействие с таймерамиСуществуют БП, где в схеме запрограммированы таймеры. С теми процессами, что имели дело, таймер, как правило, срабатывал на следующие события:наступление определенной даты и времени;поступление сигнала от внешней системы.Чтобы проверить, что БП встал на таймере также по camunda modeler, находим id этого шага и ищем этот идентификатор в списке активити по процессу (более детально в предыдущей главе). Часто бывает так, что необходимо не просто найти активный таймер, но и произвести взаимодействие с ним (симуляция наступления времени), если таймер очень долгий (> 30 сек).Для взаимодействия с таймерами разработана функцияpass_timer():def pass_timer(
    self,
    *,
    process_instance: Union[ProcessInstance, CamundaProcessInstance],
    timer_id: Optional[str] = None,
    job_type: Optional[str] = None,
) -> None:
    if isinstance(process_instance, ProcessInstance):
        process_instance_id = process_instance.processInstanceId
    elif isinstance(process_instance, CamundaHistoricProcessInstance):
        process_instance_id = process_instance.id
    else:
        process_instance_id = process_instance.id
    timer_id = timer_id or self.TIMER_ID
    job_type = job_type or self.TIMER_JOB_TYPE
    with allure.step(
        f'Проброс таймера с параметрами timer_id={timer_id} '
        f'и job_type={job_type} для процесса ""{self.pretty_name}"" '
        f'с id ""{process_instance_id}""',
    ):
        timer_job = self.get_timer(
            process_instance=process_instance,
            timer_id=timer_id,
            job_type=job_type,
        )
        self.camunda.execute_job_by_id(timer_job.id)
        time.sleep(self.TIMER_AWAIT_TIME)В качестве аргументов необходимо передать:idпроцесса;timer_id(id активити по схеме в camunda);job_type(по дефолту используется ‘timer-intermediate-transition’).Так как таймер является весьма специфичным активити, то была разработана отдельная функцияget_timer(),который, помимо стандартныхprocess_instanceиtimer_id(по сутиactivity_id), есть еще иjob_type, который может варьироваться от типа реализации таймера(timer-intermediate-transitionилиtimer-transition):def get_timer(
    self,
    *,
    process_instance: Union[ProcessInstance, CamundaProcessInstance],
    timer_id: Optional[str] = None,
    job_type: Optional[str] = None,
) -> Optional[CamundaJob]:
    if isinstance(process_instance, ProcessInstance):
        process_instance_id = process_instance.processInstanceId
        process_definition_id = process_instance.processDefinitionId
    elif isinstance(process_instance, CamundaHistoricProcessInstance):
        process_instance_id = process_instance.id
        process_definition_id = process_instance.processDefinitionId
    else:
        process_instance_id = process_instance.id
        process_definition_id = process_instance.definitionId
    timer_id = timer_id or self.TIMER_ID
    job_type = job_type or self.TIMER_JOB_TYPE
    with allure.step(
        f'Получение таймера с параметрами timer_id={timer_id} '
        f'и job_type={job_type} для процесса ""{self.pretty_name}"" '
        f'с id ""{process_instance_id}""',
    ):
        job_definition = self.camunda.get_job_definitions(
            params={
                'activityIdIn': timer_id,
                'processDefinitionId': process_definition_id,
                'jobType': job_type,
            },
        )[0]
        timer_job = self.camunda.get_jobs(
            params={
                'jobDefinitionId': job_definition.id,
                'processInstanceId': process_instance_id,
            },
        )
        return timer_job[0] if timer_job else NoneКак нашли таймер, то исполняем егоexecute_job_by_id()— внутри зашит camunda-метод‘/job/{job_id}/execute'.7. Идентификация завершенного БП по camunda, удаление инстансов процессов до и после автотестированияВажным шагом в тестировании БП является проверка, что процесс завершился без ошибок. Для этого разработана функцияwait_completed_process():def wait_completed_process(
    self,
    process_instance_id: str,
    *,
    max_time: int = 30,
    sleep_time: int = 2,
) -> None:
    with allure.step(
        f'Проверка того, что процесс ""{self.pretty_name}"" '
        f'с id ""{process_instance_id}"" завершен',
    ):
        poll_by_time_first_success(
            fn=lambda: self.__get_process_instance_with_incidents_check(
                process_instance_id,
            ),
            checker=lambda p: p is None,
            max_time=max_time,
            sleep_time=sleep_time,
            msg=(
                f'Процесс ""{self.pretty_name}"" с id ""{process_instance_id}"" '
                f'не был завершен за max_time = {max_time} секунд'
            ),
        )Используется поллинг по методу __get_process_instance_with_incidents_check()проверяется:что в процессе с указаннымprocess_instance_idнет инцидентов (внутри используется GET метод/incident);что процесс с указаннымprocess_instance_idисчез из списка активных процессов, что является показателем успешного завершения процесса в camunda.Если определяется, что в процессе обнаружился инцидент, то выбрасываетсяRuntimeError. Найденный инцидент добавляется в отчет о тестировании, чтобы автоматизатор смог сразу увидеть причину падения теста. На рисунке 4 представлен фрагмент отчета с инцидентом процесса:Рисунок 4. Пример информации об инциденте БП в allure-отчете о тестированииИногда требуется проверить, что процесс вопределенный промежутоквремениНЕзавершился. Для этого используем обратный механизм, что при поллинге поиска активных процессов — проверяется, что процесс возвращается из camunda на протяжении необходимого времени.Часто требуется перед тестом (setup) или после него (teardown) удалить активный процесс (классический пример — процесс выпал в инцидент и «висит» в camunda). Для удаления активных процессов разработана функцияdelete_process_instance_by_business_key()(удаление по business_key):def delete_process_instance_by_business_key(
    self,
    business_key: str,
) -> None:
    with allure.step(
        f'Удаление процесса ""{self.pretty_name}"" '
        f'с бизнес-ключом ""{business_key}""',
    ):
        self.camunda.delete_process_instance(
            process_instance_id=self.get_process_instance_by_business_key(
                business_key,
            ).id,
        )Вdelete_process_instance()используется DELETE метод /process-instance/{process_instance_id}.Есть аналогичные функции с параметрами, отличные от business_key. Также имеется в арсенале функция по удалению всех активных процессов внутри определенногоprocess_definition.8. ЗаключениеВсе больше и больше команд в нашей компании подключаются к тестированию БП. Часть из них используют тот же самый подход, что и мы, в том числе через использование модулей, которые рассмотрели в данной статье. Надеюсь, что кому‑то материал поможет и даст отправную точку, чтобы начать тестирование БП через camunda. По крайней мере 5 лет назад, когда мы только начинали выстраивать автотестирование БП (не e2e), подобных материалов с подходами в тестировании camunda не встречали.Конечно, это не полноценный how‑to или туториал от начала и до конца, как выстроить процесс автотестирования бизнес‑процессов, а лишь рассмотрение базовых возможностей в наших процессах. В наших модулях множество функций и описать их все в одной статье проблематично, да и смысла в этом не особо много. Очень много специфики в покрываемых нами автотестами БП.Спасибо всем, кто дочитал статью до конца. Если остались вопросы, пишите их в комментариях — с радостью ответим! До новых встреч!"
СберМаркет,,,Немного кода и вы опционный трейдер: API Опционного Калькулятора,2024-07-04T11:33:07.000Z,"Всем привет! На связи Срочный рынок Московской биржи. Будем вместе исследовать увлекательный мир на стыке финансов и технологий. Независимо от того, являетесь ли вы профессионалом в области финансов, энтузиастом технологий или хотите быть в ""модной"" теме инвестиций, знакомство с новым сервисом Московской биржи будет полезно.Опцион — инструмент, который дает покупателю право купить или продать базовый актив по заранее установленной цене в будущем. Как он работает и для чего нужен – читайте внашей статье.Опционный калькуляторпомогает рассчитать прибыль / убыток, комиссию от торговли опционами и даже моделировать собственные торговые стратегии.Что умеет калькулятор?Показывать краткую сводку по опциону с возможностью моделирования значения опциона, которое зависит от рыночной ситуации.Показывать доску опционов и кривую волатильности.Моделировать портфель инструментов на текущий момент времени, момент исполнения опционов и на произвольную дату.Функционал калькулятора позволяет рассчитывать показатели на сайте, однако прямое получение данных для построения автоматизированной торговой системы было недоступно, постоянно требовался контроль со стороны человека.Перед нами встала задача – расширить функционал опционного калькулятора и дать клиентам возможность автоматически напрямую получать рассчитанные показатели для создания собственных стратегий. Теперь это стало возможным с нашим новым сервисом – бесплатнымAPI опционного калькулятора.Что умеет API?Предоставлять перечень всех доступных опционов и их базовых активов. Трейдерам больше не нужно часами изучать сайт в поисках нужного инструмента.Давать краткую сводку по опциону, в том числе теоретическую, последнюю и      расчетную цены, информацию о комиссиях и коэффициентах чувствительности опциона, известных как ""греки"" (Дельта, Гамма, Вега, Тета, Ро), и предполагаемой волатильности (Implied Volatility).Передавать доску опционов и данные по опционным сериям: объем торгов и открытый интерес по опционной серии, теоретическую цену (в рублях и пунктах), последнюю цену, внутреннюю и временную стоимость опционов, коэффициенты чувствительности (те самые ""греки"").Предоставлять набор точек для построения графика волатильности для серии опционов и расчетных показателей портфеля, что позволяет отслеживать возможные изменения расчетных показателей.Посчитать суммарное гарантийное обеспечение по любому набору инструментов и с учетом неттирования.И, наконец, собрать портфель из опционов и их базовых активов с расчетом прибыли, убытков и комиссий.А можно пример?Конечно! Показываем, как выглядитзапросдля начала работы и получения списка базовых активов:import requests

BASE_URL = ""https://iss.beta.moex.com/iss/apps/option-calc/v1""

# возвращает массив объектов Asset
def get_assets(asset_type: str = None, asset_subtype: str = None):

    url = f""{BASE_URL}/assets""

    if asset_type and asset_subtype:

        url = f""{url}?asset_type={asset_type}&asset_subtype={asset_subtype}""

    elif asset_type:

        url = f""{url}?asset_subtype={asset_type}""

    elif asset_subtype:

        url = f""{url}?asset_subtype={asset_subtype}""

  response = requests.get(url)
  assert response.status_code == 200, ""Error while asset list requesting""

    return response.json()Кому будет полезно API опционного калькулятора?Трейдерысмогут использовать инструмент для принятия торговых решений на      основе детальной информации о параметрах и тестирования различных сценариев.Программистымогут интегрировать данные и расчетыдля разработки торговых систем и других финансовых приложений.Новички на срочном рынкемогут познакомиться с определениями всех показателей и терминов, необходимых для понимания опционов.Полезные ссылки:Документация API опционного калькулятораSwagger APIОтветы на вопросыОставляйте в комментариях вопросы, на все мы дадим ответ навебинаре, посвященному новому API, на базе Школы Московской Биржи. Видеоконференция пройдет 5 июля на платформе МТС Линк (бывш. Webinar.ru), а запись будет доступна на YouTube.Больше полезных статей в нашемTelegram, например, рассказываем,как появились фьючерсыи причем тут рис."
СберМаркет,,,Реинжиниринг управления лицензиями (SAM) в Группе MOEX,2024-06-20T11:53:46.000Z,"Современное управление лицензиями на софт (Software Asset Management, SAM) становится все более сложным и требовательным процессом - финансовые ресурсы на лицензирование программного обеспечения активно расходуются, но пропадает уверенность в том, что темпы этих расходов эффективны. Ситуацию также усложняют факторы:количество софта растет из-за роста организации;регуляторные требования предъявляют новые контроли, которые требуют новых расходов;качество связности баз учета (Configuration Management Data Base, CMDB) с нематериальными активами в бухучете падает с каждым годом;стоимость расходов на программное обеспечение начинает приближаться к стоимости ИТ-инфраструктуры.Последствия также определить несложно:ИТ-подразделения тратят постоянные усилия на отработку запросов руководства;возникают шоу-стопперы для процессов закупки;зависают внедрения новых решений и двигаются проектные сроки;согласование закупок затруднено в условиях отсутствия качественных данных текущего учета;внутреннее неприятие уместности запросов в конкретный момент времени.Группа MOEX давно определила для себя, что нужно выстраивать эффективный процесс с максимальной автоматизацией, и в целом для реинжиниринга процесса SAM появились некоторые предпосылки:Основными мы считаем геополитические факторы, необходимость обеспечения технологического суверенитета, требования системы управления рисками в части контроля использования имеющихся прав на ПО и цифровую трансформацию;Дополнительно точечные проверки подсвечивали переплату за неиспользуемые лицензии, недостаточное качество учета, длительный процесс выдачи лицензий, отсутствие актуальных цифр и данных по утилизации лицензий и повышенные трудозатраты на их подготовку.Из опыта MOEX, серьезные процессные изменения занимают в среднем до 7 месяцев – это значительный срок, который многим не по нраву, но в то же время он позволяет правильно подготовиться к изменениям и закрепить их в операционной деятельности.Однако мы не стали тратить много времени на описание процесса «как сейчас», а почти сразу начали со списка согласованных проблематик и описания «как будет». Это позволило последовательно и с минимальными трудозатратами продвинуться на пути к образу результата, когда правильный процесс будет давать нам правильные результаты. В рамках реинжиниринга процесса выделили два неочевидных момента, которые должны найти свое отражение в целевом ИТ-процессе управления лицензиями:Что является объектом процесса - запрос или учет? Каждый из них приносит одинаковый результат, но усилия, трудоемкость и достоверность полученных данных могут значительно отличатьсяПотребители и поставщики данных по лицензиямОсобенный акцент мы делали на децентрализованный процесс учета в рамках единой системы, чтобы не создавать зависимости от отдельных специалистов:- Подразделения обеспечивают определение потребности, бюджетирование, сопровождение закупок и договоров, управление использованием лицензий;- Руководители обеспечивают принятие управленческих решений, управление рисками и контроль процесса в зоне ответственности подразделений.Ответственность за исполнение и управлениеМы добавили автоматизацию вокруг CMDB и в комплексе с организационными изменениями принципиально изменили концепцию – теперь вместо «данные по запросу» стало доступно «данные для всех в любое время».В результате появилась дорожная карта процесса SAM, которая в нашем случае входила в планы развития процесса управления ИТ-активами в целом (IT Asset Management, ITAM) и была плотно интегрирована с планами развития процесса управления доступностью и мощностями (Capacity management). Дорожная карта SAM включала в себя:простые и доступные инструкции;децентрализацию зон ответственности;внедрение учета лицензий на тестировании и временном использовании;работу с шоу-стопперами при использовании лицензий в обход процесса;унификацию всех справочников;регулярное обучение участников процесса;все в одном месте – данные по договорам, поставщику, утилизации, НМА (нематериальные активы) из бухучета;средства инвентаризации для рабочих групп.По факту реализации SAM, можно отметить, что наибольший эффект в распространение культуры и знаний о процессе оказывают:Ежемесячные рабочие группы по лицензиям, где обсуждаются любые вопросы от вовлеченных в учет специалистов;Внедрение новых, простых и понятных UI-форм при запросе лицензий, что позволило ориентироваться на показатели утилизации в момент заказа;Контрольные процедуры на данных текущих средств дискаверинга, что позволило резко поднять качество учета;Трехуровневая структура учета – программные продукты, лицензионные пулы, лицензии;Включение в развитие процесса мнений и опыта финансовых и рисковых подразделений Группы MOEX, что позволяет заранее спланировать и реализовать любые корректировки.На чём мы построили SAM?В центре системы учета лицензий находится платформа Omnitracker / Наумен.Дашборды для руководителей и ТОП-менеджмента реализованы в BI.Реинжиниринг процесса позволяет нам в ближайшей перспективе значительно повысить уровень зрелости с минимальными трудозатратами для поддержки процесса, а именно реализовать:BI-отчетность в разрезе систем;интеграцию с репозиторием программ;разработать дашборд с метриками и показателями импортозамещения;начать детальный учет параметров техподдержки лицензий.При трёхлетнем горизонте планирования развития процесса мы понимаем, что самое главное – сохранить свою способность обеспечивать перемены. Она, в свою очередь, обусловлена:Навыками решения проблем взаимодействия;Ориентацией на бизнес-результат;Умением правильно обозначать миссию и ценности с использованием прикладной статистики;Способностью взрастить из посевных зернышек «Культура в процессах» полноценное дерево;Готовностью брать ответственность за решения;Высокой внутренней мотивацией;Ну и немного импровизации :)Надеемся, что опыт Группы MOEX в части ускорения принятия управленческих решений пригодится средним и крупным организациям, которые постоянно думают над изменениями и повышением зрелости ИТ, эффективному использованию своих ИТ-ресурсов.Валерий Ивлев, Хакимзянов Тимур,Управление развития ИТ-процессов Центра компетенций ITG"
СберМаркет,,,Распределённый BPMS. Опыт Московской Биржи,2024-05-22T09:45:01.000Z,"Всем привет!Меня зовут Сергей Максимов и я руковожу разработкой в Центре Управления Процессами (ЦУП)Московской Биржи. В статье я хочу рассказать о нашем опыте автоматизации бизнес-процессов (БП), когда система должна быть не только удобной бизнес-пользователям снаружи, но и надежной внутри.Бизнес Биржи, с одной стороны, похож на обычный банковский финтех, но имеет ряд важных особенностей. Чтобы лучше представить специфику нашей работы, я приведу метафору. Представьте, что каждое утро с вашего корпоративного космодрома в космос отправляется ракета. В течение дня космический корабль автономно выполняет работу на орбите, а вечером возвращается на базу. В полёте связь с кораблем очень ограничена и успех его полёта на 99% определяется качественной подготовкой. Всё должно отработать точно и в срок. Досрочный спуск корабля с орбиты технически возможен, но влечет за собой огромные репутационные потери с отчетом регулятору и новостями в федеральных СМИ.Немного историиПочему для автоматизации выбрали BPMN?Когда в 2018 году система ЦУП только проектировалась, остро встал вопрос выбора общего языка для обмена информацией. В виду наличия большой экспертизы у наших бизнес-пользователей нам была важна возможность регулярного контроля с их стороны и внесения дополнений в ходе разработки. Были рассмотрены несколько нотаций UML, ArchiMate, IFDEF, но все они страдали достаточно узкой специализацией. Свой выбор мы остановили наBPMN 2.0, тем более на рынке к тому моменту уже был ряд приложений, позволяющих работать с этой нотацией напрямую. Сначала нотацией овладели аналитики и разработчики, постепенно и пользователи подключились к работе по согласованию и модификации схем процессов. На данный момент BPMN-схемы стали неотъемлемой частью повседневной аналитической работы.Архитектура движка BPMSBPMS в облаке. Микросервисный подходКогда с языком описания бизнес-процессов определились, перешли к поиску промышленных BPMS-систем. Подавляющая часть BPMS тяготела к монолитному развертыванию и имела свои специфические механизмы версионирования, раскатки и моделирования. Этот подход был хорошо документирован и обеспечен инструментом от вендоров, но налицо были ограничения по надежности и масштабируемости. Нам хотелось максимально использовать свою экспертизу в Java-разработке, как в части процесса, так и в части инструментов. От корпоративной архитектуры поступило требование к внедрению микросервисного подхода и на основе популярной BPMS-библиотекиCamunda 7мы начали разработку собственного распределённого движка.Архитектура распределённого движка BPMSСредой функционирования BPMS-движка было выбрано частное облакоKubernetes. Частный (on-premise) вид облака продиктован требованиями по безопасности и непрерывности работы инфраструктуры. Решение на базе проекта Kubernetes с открытым исходным кодом минимизировало зависимость от конкретного вендора. База данных Postgres развернута отдельно от облака в виде гео-распределенного кластера.В центре нашего распределенного BPMS-движка находится брокер сообщений. Изначально брокером был выбранApache ActiveMQ Artemis, главным образом из-за наличия экспертизы в смежных подразделениях. Брокер обеспечивает маршрутизацию команд и событий между сервисами.Для построения персональной ленты задач для каждого пользователя нами был разработан сервис «Менеджер задач». В отличие от встроенных средств управления задачами от Camunda, наш сервис умеет бесшовно соединять в единый список задачи из разных сервисов и управлять доступом пользователей на базе корпоративного AD-домена. Это достигается за счет использования шаблоновCQRS(command-query responsibility segregation) иEvent Sourcing.OLTP-схемы данных Camunda технически имеют возможность хранить исторические данных о ходе завершившихся процессов, но практически использовать эти данные для аудита очень трудно. Запросы выполняются долго и отнимают ресурс на исполнение у активных бизнес-процессов, тем самым существенно снижая их быстродействие. В результате исследования внутри команды мы пришли к гибридной модели: данные завершенных процессов хранятся в исторических таблицах 90 дней, а затем убираются служебным скриптом. За указанное время данные о всех процессах успевают скопироваться в витрину на основеElasticsearch 7, чтобы быть доступными для быстрого поиска. Таким образом размеры оперативной базы данных бизнес-процессов остаются достаточно небольшими и удобными для обслуживания.Практические сценарии использованияДля иллюстрации работы нашей платформы расскажу о решении некоторых практических бизнес-задач.Интеграция с системой бизнес-мониторингаДля эффективного управления повседневной операционной работой требуется обеспечение прозрачности и предсказуемости глобальных бизнес-процессов компании. Для этого разные системы компании, участвующие в информационном обмене, маркируют критические этапы исполнения функций и отправляют события в дашборд для руководства.Трансляция потока событий BPM в события бизнес-мониторингаВ рамках решения данной задачи мы добавили дополнительного подписчика к имеющемуся потоку событий бизнес-процессов. Отдельный сервис на базеLogstashосуществляет фильтрацию и преобразование выбранных событий для публикации их в дашборде. Такой подход позволяет максимально переиспользовать имеющиеся информационные потоки с минимальными доработками уже использующихся в промышленной среде бизнес-процессов.Чек-лист ежедневных операций маклераРабочая смена маклеров операционного департамента сопряжена с выполнением большого количества ежедневных регламентных операций. Контроль качества и сроков выполнения раньше осуществлялся вручную через отдельную страницу в Confluence. Когда список операций стал превышать 100 элементов, заказчиком было принято решение по созданию автоматического средства контроля. Командой ЦУП был разработан сервис «Чек‑лист», который в назначенное время запускает проверки выполнения операций. Результат всех проверок за день выводится таблицей на экран, где все записи имеют цветовую индикацию в зависимости от своего статуса. Каждая автоматическая проверка представляет из себя небольшой бизнес-процесс на нашем BPMS-движке, результатом которого будет актуальный статус операции. В результате внедрения сервиса «Чек‑лист» удалось автоматизировать более 200 ежедневных операций и добиться сокращения рабочей смены почти в 3 раза (с 16 до 6 человек).Одним из важных направлений развития платформы является упрощение взаимодействия пользователей с ИТ-системами. Нашей командой был разработан комплекс бизнес-процессов, осуществляющих синхронные действия в нескольких торговых системах. Ранее некоторые регламентные процессы требовали голосовой синхронизации и согласованных действий внутри нескольких изолированных АРМов (автоматизированных рабочих мест). Это могло приводить к ошибкам ручного ввода и несогласованности действий по времени. Разработанная нами автоматизация позволяет в режиме одного окна выбрать все необходимые действия и указать точное время выполнения этих действий, далее система организует всё взаимодействие автоматически. Таким образом было существенно снижено влияние человеческого фактора на регламентные процессы, а время их выполнения сократилось с 40 минут до 3, то есть более чем в 10 раз.Планы дальнейшего развитияПланы дальнейшего развитияКоманда ЦУП Московской Биржи продолжает развивать платформу распределенного движка BPMS параллельно с решением актуальных бизнес-задач. На данный момент архитектура системы показала хорошие показатели по отказоустойчивости и производительности. Однако пространство для улучшений всё ещё есть:Надо улучшать возможности мониторинга и трассировки проблем в ходе выполнения бизнес‑процессов. Исследуется вопрос применения стандарта Open Telemetry.При заинтересованности со стороны бизнеса обеспечить интеграцию с BI и Process/Task Mining системами с целью быстрого анализа и оптимизации сложных бизнес‑процессов.Исследовать вопрос преимуществ замены связующего брокера с Artemis ActiveMQ на Apache Kafka.На этом у меня всё. Надеюсь, было интересно 🙂"
СберМаркет,,,Эволюция системы разработки на SQL,2024-02-16T06:27:04.000Z,"Мы — SQL команда Срочного рынка Московской Биржи, занимаемся разработкой и сопровождением бэкофиса торгово-клиринговой системы Spectra с момента ее возникновения. Срочный рынок Московской Биржи — это более 500 фьючерсных и 30000 опционных инструментов, несколько миллионов сделок в день.Торгово-клиринговая система Срочного рынка (ТКС Spectra) изначально строилась на основе MS SQL, и за пару десятков лет прошла сложный путь от нескольких серверов БД до огромной системы с сервис-ориентированной архитектурой. Долгое время вся бизнес-логика системы разрабатывалась в программном слое на серверах MS SQL: и матчинг заявок, и расчет обеспечения, и управление клиентами были реализованы на T-SQL.На сегодняшний день весь высоконагруженный функционал вынесен в отдельные сервисы, но в базах данных остаются сотни таблиц и тысячи программных объектов. Особенностью кода является высокая когнитивная и цикломатическая сложность. Управлять этим кодом с учетом всех требований по надежности и быстродействию – очень интересная задача.В этой статье мы хотим рассказать об эволюции нашей системы разработки на SQL.Как все начиналосьПару десятков лет назад, когда не было ни спринтов, ни сборочных конвейеров, ни модульных тестов, эталон программного кода находился в боевых базах, а для разработки использовались копии боевых БД с обезличенными данными.  Каждый разработчик снимал скрипты необходимых объектов с сервера, вносил в них изменения, собирал набор скриптов по своим задачам. Согласование итогового комплекта для заливки выполнялось в ручном режиме. Набор скриптов модификации объектов и данных передавали в тестирование, в этот же набор вносили исправления обнаруженных багов и в релиз заливали в продакшен БД. С ростом числа задач и разработчиков сложность этого процесса многократно возросла, а с ней возросла и вероятность ошибки.Первый подход к версионированию, Jira, спринтыТак дальше жить было нельзя, и мы перешли к версионированию SQL скриптов на основе SVN. Для мастер-ветки с продакшена были сняты скрипты всех баз, под каждую новую версию заводился бранч копированием мастер ветки. Со временем мастером стала текущая ветка в продакшене и уже от нее бранчевались версии для разработки.Теперь эталонный код хранился не в БД, а в SVN. Для каждого объекта появилась история изменений, упорядочилась совместная разработка, но сам по себе переход на версионирование не решал проблему создания набора релизных скриптов. Распространенный в то время подход автоматизированного сравнения структур старой и новой версий БД и генерации скрипта заливки всех различающихся объектов нам не подходил, требовалось сохранить определенный разработчиком порядок внесения изменений в данные, схему и программный код. Кроме того, у нас было (и остается по сей день) довольно много кросс-серверного кода, который необходимо заливать поочередно на разные сервера.  Решением стал переход к инкрементальной заливке изменений: выкладывать пронумерованные скрипты модификации структуры и данных в каталоги по датам.Новый процесс разработки выглядел так:Взять объекты для модификации из SVN, внести изменения, залить на свой рабочий сервер, проверить работоспособность.Закоммитить изменения в бранч.Выложить скрипты модификации данных, схемы и манифест со списком скриптов для одного или нескольких серверов БД и описанием задачи в каталог ToTest с текущей датой. Перевести задачу в тестирование.Тестировщики заливают к себе скрипты согласно манифесту и проверяют задачу.Ошибки при заливке исправляются в выложенном скрипте, замечания и баги - в новых файлах.В релиз последовательно накатываются все инкрементальные скрипты из бранча версии, предварительно заливаемость комплекта скриптов проверяется на интеграционных полигонах.После релиза текущая ветка в SVN становилась эталонной, с боевых серверов снимались скрипты для сравнения с репозиторием.Через некоторое время к системе был добавлен Atlassian Crucible, а к практикам - требование обязательного ревью кода при коммитах в релизную ветку. Вскоре после введения код ревью пришлось зафиксировать и стандарты написания кода, для предотвращения многодневных холиваров внутри команды.Новый подход позволил перейти к разработке по спринтам, повысить качество кода и снизить число багов. Разумеется, у него были недостатки: необходимость выкладывания одного и того же кода в версионированные и в инкрементальные скрипты, длительная заливка в релиз (один и тот же объект мог перезаливаться десятки раз), непрозрачность изменений в данных, которые можно было отследить только по инкрементальным скриптам.Но самое главное - пришла эра автоматизации.Автоматизируй всёDevOps подход за короткое время изменил мир разработки ПО. Трансформация практик захлестнула и Московскую Биржу: одна за другой команды переходили на git, автоматизированную сборку, отчитывались о покрытии юнит-тестами и собирали билды в конвейере. Чтобы сделать шаг в прекрасное будущее, нам, прежде всего, надо было научиться разворачивать все базы с нуля к любой заданной версии и наполнять их консистентными данными, но инкрементальные скрипты для этой задачи категорически не подходили. И тогда мы снова всё переделали.Требовалось решить много интересных задач:Заменить SVN на GIT. Разделить хранящийся в SVN sql-ный монолит на подпроекты и вести каждый в отдельном репозитории.Добиться четкого версионирования как всей системы в целом, так и отдельных объектов.Интегрироваться с системой общей сборки. Исключить необходимость ручной работы при генерации скриптов для заливки.Упростить параллельную разработку нескольких версий и перенос кода между ними.Получить возможность создания БД с нуля и заполнения набора справочников, необходимого для функционирования системы.Реализовать систему юнит-тестов.Рассчитывать метрику покрытия кода.Использовать статический анализатор.Автоматически генерировать документацию.DDL, DML, DCL - версионируй всёВерсия БД — это определенное состояние схемы, данных и программных объектов. Для перехода от версии N к версии N+i необходимо однократно применить все изменения схемы и однократно модифицировать данные, а программный код можно перезаливать без ограничений. Например, тривиальная задача «добавить поле A в таблицу B, заполнить его в соответствии с некоторым правилом   и выдать это поле в API-вызов C» реализуется тремя скриптами: alter table A add B.. ; update A set B=..; create or alter procedure C..;. Изменения любой сложности в итоге можно разделить на эти три потока: схема, данные, объекты.Эта несложная идея и стала основой новой системы хранения и версионирования.Тип объектовПринятое обозначениеТипы объектов БДПересоздание или  инкрементальный подход. Способ хранения.СхемаDDL (Data Definition)RolesSchemasSequencesServiceContractsServiceMessageTypesServiceQueuesServicesTablesXmlSchemaCollection* индексы считаются составной частью таблицы и хранятся вместе со скриптом создания/модификации таблицыНельзя пересоздавать,   необходимы инкрементальные изменения.Каждый   объект хранится как каталог с набором инкрементальных скриптов модификации   структуры.ДанныеDCL(1)(Data change)Инкрементальные   скрипты модификации данных, сгруппированные по соответствующим таблицам.Можно   пересоздавать некоторые справочники, в остальном необходимы итеративные   изменения.Можно   перезаливать только статичные данныеПрограммные   объектыDML (Data manipulation)CheckConstraintsDefaultsForeignKeysStoredProceduresTriggersUserDefinedFunctionsUserDefinedTableTypeViewsМожно   пересоздавать без ограничений.Хранятся в   отдельных файлах.Сложные случаиAFTЛюбые изменения,   которые невозможно декомпозировать до основных типов.Должны   выполняться однократно.При заливке версия и хэш каждого объекта, в том числе и версия модификации данных, сохраняется в служебной таблице. Дерево проекта выглядит примерно следующим образом:Для автоматизации сборки и развертывания был написан комплект утилит на Python. Сборщик проекта запускается с указанием версии, обходит дерево проекта и генерирует пронумерованные заливочные скрипты:Удаление всех программных объектов.Каждому объекту добавляется обвязка: сверка номера версии с сохраненным в заливочной таблице, расчет и сравнение хэша. Эта сверка обеспечивает строго однократное применение изменений схемы и данных: после заливки счетчик версий DDL/DCL объекта в заливочной таблице обновляется и при повторном накате скрипт той же версии будет пропущен.Для DDL и DCL последовательно применяются все версии, т. е. изменение схемы таблицы на версию 1.1 - изменение данных на версию 1.1 - изменение схемы на версию 1.2, и т. д.DML собирается в один файл для каждой базы в порядке view - function - stored procedure, с учетом списка исключений: иногда требуется залить функцию раньше, чем представление, или внутреннюю функцию раньше внешней.Модификация данных, для которой требуется полная заливка программного кода. Например, обновление данных на значение, рассчитываемое функцией, или вставка данных в таблицу с триггером.Скрипты создания джобов SQL Agent-а.Служебные операции.Для самого крупного проекта сейчас генерируется почти 800 файлов.Теперь полученный набор необходимо проверить, и в дело вступает утилита заливки. Скрипты накатываются на указанный в конфигурации сервер БД, это может быть машина разработчика или сборочная машина. Один и тот же комплект скриптов может быть залит на любую версию, от ""нулевой"" без БД до полной версии с данными, и после заливки мы получаем состояние, совпадающее с эталоном.Хеши кода объектов сохраняются не для красоты. В strict режиме заливка первым делом проверяет соответствие существующих объектов их хэшам. Если обнаружено расхождение, это признак случайного или преднамеренного изменения кода, с которым необходимо разбираться.СборкаПосле решения проблем версионирования интегрироваться в общесистемный сборочный конвейер оказалось достаточно просто. Про сборки подробно рассказывается в одной изпредыдущих статей(MOEX DevOps, вы крутые!).  Пайплайн, собирающий определенную версию системы, забирает код sql-проектов, генерирует скрипты, заливает их на сборочные инстансы MS SQL, прогоняет unit-тесты, сравнение интерфейсов, автогенерацию кода взаимодействия с другими сервисами, и, если всё прошло успешно, пакует весь сгенерированный код и публикует артефакты в Nexus.Да, кстати, мы покрыли unit-тестами большую часть огромного легаси монолита.Unit-тестыВыбирая решение для модульных тестов, мы с легкой завистью смотрели в сторону Java- и С-команд. У коллег имелся широкий выбор прекрасных систем, интегрируемых в IDE и в автоматизацию, а для MS SQL ничего удобного и подходящего под наши задачи подобрать не получалось. Да, было тяжеловесное решение от Microsoft, был tSqlt, но для сложных процедур с десятками параметров создание тестов в этих фреймворках грозило превратиться в бесконечную монотонную ручную работу. Необходимо было охватить тестами сотни объектов, зачастую содержащих сложную логику, покрываемую несколькими наборами входных параметров, при этом Arrange для теста порой требовал вставки консистентных данных в десяток таблиц.Хочешь сделать хорошо - сделай сам, и мы написали свой фреймворк на Python.Сильные стороны нашего фреймворка - автоматизация, унификация тестов и сокращение объема кода тестового модуля. Вся логика Arrange/Act/Assert пишется на SQL.Перед прогоном модульных тестов на базу накатывается DCL с общими данными, создаются несколько клиентов, торговых инструментов, заполняются сессионные справочники. Записи хранятся в формате JSON, для добавления новых строк нетрудно снять данные с любого полигона разработки.При разворачивании модуля тестирования в базе создаются служебные функции получения часто используемых эталонных данных. При написании теста не требуется изобретать велосипед, чтобы получить ""образцового клиента"".У tSqlt был позаимствован подход FakeTable, в котором выбранная таблица на время прогона теста пересоздается без констрейнтов. Это позволяет добавить только необходимые для теста данные, не беспокоясь об остальных полях.Все тесты параметризованы. Перед прогоном тестов передаваемые параметры в виде JSON строк заливаются в таблицу, по которой строятся вызовы тестовых модулей. Проверка одной процедуры с десятью наборами параметров — это написание одного модуля и добавление 10 строк в файле параметров.Для проверки скалярной функции достаточно добавить одну строку в DCL обобщенного теста: параметры, выходное значение, код, выполняемый перед запуском.Табличные функции и процедуры, возвращающие датасет, также могут (но не обязаны) проверяться единым тестом. Для созданной функции добавляем одну запись с json-ами входных и выходных наборов параметр/поле - значение.Сложные случаи, не подпадающие под паттерн сверки входных и выходных значений, оформляются в виде отдельных параметризованных модулей.Прогон тестов осуществляется в два этапа. На первом генератор скриптов соединяет модули и параметры, и для каждой пары создает скрипт теста, добавляя заголовок и обработку ошибок. На втором этапе заливщик накатывает на базу полученные скрипты (сейчас их почти 1800 только в одном из проектов) и передает лог выполнения наружу. Если хотя бы один тест зафейлен, поднимается красный флажок, и сборка останавливается.ПокрытиеТесты пишутся и выполняются, пайплайн зеленеет, а сколько кода у нас покрыто? Для оценки метрики покрытия взяли опенсорсный SQL Cover, доступный под лицензией Apache 2.0.  Перед запуском тестов он подключается к серверу, считывает выполняемый код через Extended Events, сравнивает с объектами в базе и выдает итоговый процент покрытия.Статический анализДля статического анализа SQL кода нашлось прекрасное коммерческое решение - SQL Enlight. Он поставляется с обширным набором правил, которые можно настраивать и кастомизировать. У нас он установлен на машине каждого разработчика для проверки кода на лету и в сборке для проверки диффов.SQL Enlight сигнализирует о нарушении правилАвтодокументирование«Вот теперь всё красиво», - сказали нам на планерке, — «а автогенерации документации не хватает». В очередной раз с завистью посмотрев на джавистов, взяли Python и сделали утилиту, которая пробегает по хранимым процедурам, генерирует описание интерфейсов по заголовкам и комментариям и выкладывает артефакты в Nexus.Интеграция и сравнение интерфейсовПо мере усложнения архитектуры ТКС возросло число модулей, с которыми надо взаимодействовать, и не всегда получалось синхронизировать изменения в одном цикле разработки. Расхождение обнаруживалось только на этапе интеграции, отнимало время и портило метрики. После того, как в ежедневную сборку добавили выкачку артефактов других модулей и сверку сигнатур/схем, несовпадение интерфейсов стало обнаруживаться и исправляться в течение нескольких часов.А что дальше?А дальше нас ждет много увлекательных и интересных задач, связанных с переходом на другую СУБД и продолжение декомпозиции монолита на модули. Усилия, вложенные в собственный тестовый фреймворк, уже оправдались: он с легкостью мигрирует на альтернативную СУБД и переносимый код с самого начала будет покрыт тестами. Одни кирпичики нашего процесса будут постепенно заменяться другими, но сам по себе стабильный, предсказуемый и удобный процесс разработки, надеемся, останется с нами надолго и упростит переход на новую систему."
СберМаркет,,,Как мы разрабатывали бота в мессенджере eXpress для 1-й линии поддержки,2024-01-25T08:57:19.000Z,"Привет, уважаемые читатели Хабра!На связи Лаборатория инноваций Московской биржи.Хотим поделиться с вами нашим опытом разработки чат-бота для 1-й линии поддержки на базе корпоративного мессенджера eXpress.Расскажем о нашем опыте, ошибках, сделанных выводах и поделимся полезными ссылками для желающих заняться разработкой ботов для eXpress.Примечание: учтите, что доступ к ботам в eXpress предоставляется только корпоративным клиентам.Итак, приступим.В компаниях с 0 толерантностью к риску, таких как Московская биржа, даже самые простые инициативы требуют специального подхода, внимательного рассмотрения и согласования со Службой информационной безопасности (далее по тексту - СИБ) каждого этапа.Нашей команде поступила задача купить/придумать/разработать простой инструмент, который позволил бы бизнес-пользователям, у которых нет прямого доступа к корпоративному тасктрекеру (далее по тексту ТТ), получать статус по своим заявкам без необходимости дополнительного обращения к первой линии поддержки. Изначально мы рассматривали возможность использования телеграм-бота, но выяснилось, что такое решение не подходит по ряду технических причин:Оно должно быть доступно из контура, откуда нет свободного выхода в интернет.Информация в заявках может быть ""чувствительной"" и хорошо бы, чтобы она оставалась в пределах компании.Вместо этого мы приняли решение разработать бота в eXpress. Их бот-платформа схожа по архитектуре с Телеграмом. Учитывая популярность языка Python и его обширное использование в различных областях, для написания бота мы выбрали именно его. Мы также рассчитывали на активное сообщество специалистов и доступный бюджетный фонд, ведь нашей целью была реализация MVP и передача бота заказчику в течение нескольких недель.Однако результат показал, что мы сильно заблуждались.После нескольких переговоров с заказчиком мы утвердили схему бота (ниже), но столкнулись с серьезными техническими трудностями.Схема ботаТ.к. мы хотели напрямую из бота ходить по API к нашему ТТ и забирать данные о заявках, это стало серьёзным ограничением как с точки зрения СИБ, так и с точки зрения самой реализации. В ходе переговоров и чтения документации выяснилось, что API ТТ не готов отдавать нам всё, что было нужно для данного кейса, и требовалось потратить несколько недель на выполнение доработок. Нас это не устроило и поэтому, на смену работы с ТТ по API, пришло решение работать через представления в БД (оказалось, это куда проще и безопаснее сделать) и работу с представлениями было принято вести не напрямую из бота, а через специальный сервер внутри контура, который будет выполнять роль фильтра.Во-первых, он будет авторизовывать запрос от бота, и это даст своеобразную гарантию, что к сервису нельзя будет получить доступ другим способом.Во-вторых, проверять, что у обращающегося от имени бота есть доступ к данному сервису.В процессе диалога с коллегами из испытательной лаборатории по информационной безопасности мы разработали интеграционную схему и отказались от некоторых решений, которыми первоначально хотели воспользоваться.Первая версия интеграционной схемы выглядела так:Но СИБ такую схему не утвердили, т.к. в ней недоставало деталей. На последнюю версию схемы мы нанесли названия сетей, подсетей, систем, IP-адреса и порты и сетевую схему взаимодействия. К тому же, доступ к нашему Application REST API реализован не через Gravitee, а внутри VPN тоннеля посредством прямого подключения по HTTPS, что тоже нашло отражение на новой схеме.После всех этапов согласований и утверждения плана реализации мы приступили к написанию кода для самого бота. При написании использовали библиотеку для создания чат-ботов и SmartApps для мессенджера eXpresspybotx(т.к. она  простая в использовании  и легко интегрируется с асинхронными фреймворками).  Также у ребят из eXpress есть шаблон, в котором основной код для работы с библиотекой pybotx уже написан. В шаблоне используется fastapi, sqlalchemy, redis для конечных автоматов, psycopg для интеграции с PostgreSQL и библиотека  для работы конечных автоматов. Шаблон разворачивается с помощью библиотекиcopier, чем мы и воспользовались. В результате первая часть - сам бот, который развернут на бот-сервере в Express, готова. Осталась вторая часть сервиса - бэкенд бота, который развернут на внутреннем сервере Биржи. В нем будет происходить вся бизнес-логика: обработка команд, проверка прав, соединения с другими сервисами.Первой трудностью, с которой мы столкнулись при разработке бэкенд части, стала банальная версионная несовместимость. В разработке бэкенда мы хотели использовать последний python, но не получилось, т.к. все нужно брать из корпоративного Nexus, а некоторых библиотек там вообще не оказалось, например fastapi-users. Пришлось остановиться на python версии 3.8, менять все версии библиотек в pyproject.toml под него, пересобирать poetry.lock и писать самим аутентификацию пользователей с JWT и хешированием паролей (спасибо документации FastAPI за подробныйгайд)Следующим фактором, с которым боролись не один день, стала сложность свместимости одновременно синхронного и асинхронного подключения к двум разным БД. Для работы с данными по заявкам на поддержку нам необходимо иметь соединение с базой данной MSSQL, находящейся на другом сервере, а для хранения пользовательских данных нужно соединение с базой данных PostgreSQL, находящейся рядом с приложением на одном сервере. Так как эндпоинты в приложении асинхронные, мы использовали асинхронное соединение с PostgreSQL с помощью create_async_engine в SQLAlchemy и библиотекиasyncpg. А вот для соединения с БД MSSQL в Nexus не оказалось асинхронной библиотеки, была только синхронная -pymssql.Мы попробовали использовать обычный create_engine для соединения с MSSQL и засунули его в Depends:К сожалению, это не сработало, при использовании параметраmssql_dbв коде возникала ошибкаcannot unpack non iterable coroutine object. Как мы поняли, проблема может быть связана с тем, что FastAPI ожидает, что все обработчики зависимостей (dependencies) будут асинхронными в асихронном эндпоинте.Потом мы решили использоватьrun_in_executorиз библиотекиasyncio, чтобы запуститьсинхронный код в асинхронной функции:И это сработало! Мы получили то, чего добивались.В конечном итоге, после нескольких месяцев совместной усердной работы с коллегами, согласований и разработки мы получили работающего бота:На весь проект ушло 4 месяца, вместо пары недель, на которые мы рассчитывали xD.В ближайшие пару месяцев планируем проверить работоспособность бота на тестовой группе сотрудников Московской биржи: посмотрим, кто и как будет его использовать, какие запросы чаще всего будут отправляться. Далее, если бот будет пользоваться спросом - будем думать, как его масштабировать на всех сотрудников Группы MOEX.Результатами поделимся с вами в следующих статьях.ЗаключениеЭтот опыт показал нам важность рассмотрения всех технических аспектов разработки, правильного планирования сроков, когда в проект вовлечены внешние вендоры и есть процессы согласования с несколькими подразделениями, внимательного подхода к интеграции с существующими системами и тщательного управления рисками. Мы уверены, что собранный опыт будет полезен другим разработчикам, приступающим к подобным задачам.В завершение несколько общих советов (которые, кстати, можно применить к любому ИТ-проекту):закладывайте время на возможные срывы сроков, форс-мажоры и уход ключевых разработчиков с проекта;в планировании сроков реализации важное место отводите этапу согласования с СИБ;продумывайте «план Б» на случай, если по основному плану всё пойдёт не так;выявляйте и анализируйте требования заказчика в форме прецедентов использования;описывайте предполагаемый бизнес-процесс;моделируйте взаимодействия прецедентов и бизнес-процессов;обращайте внимание на детали, фиксируйте всё письменно.Благодарим за внимание и желаем удачи в ваших проектах!P.S.А для тех, кто решит разрабатывать ботов в Express, список статей и материалов по их разработке ниже:Статьи:https://docs.express.ms/chatbot-and-smartapp/developer-guide/getting-started/what-is-chatbot-and-smartapp/https://hackmd.ccsteam.ru/s/4IHk-iFDRБиблиотеки Бот-сервисов:https://github.com/ExpressApp/pybotxhttps://github.com/ExpressApp/async-boxhttps://github.com/ExpressApp/pybotx-fsmhttps://github.com/ExpressApp/pybotx-smartapp-rpchttps://github.com/ExpressApp/pybotx-smart-loggerhttps://github.com/ExpressApp/pybotx-smartapp-smart-loggerhttps://github.com/ExpressApp/smartapp-bridgehttps://github.com/ExpressApp/smartapp-sdk"
СберМаркет,,,Как мы развиваем Платформу Цифрового Опыта,2023-09-25T07:28:23.000Z,"Цифровой опыт (Digital Experience) играет ключевую роль в современном мире бизнеса.Клиенты и пользователи ожидают быстрых, интуитивных и персонализированных взаимодействий с цифровыми каналами, будь то приложения, сайты, телефония, почта или мессенджеры. Пользователи могут в любой момент переключаться между ними, при этом бизнес через оператора или автоматизацию должен находиться всегда в контексте и знать весь накопленный клиентский путь и потребности.Для оперативной адаптации приложений в таких каналах нам нужны подходящие инструменты. Если их множество и функциональность может стать базовой для прикладных задач, неизбежна платформизация.Разработка платформ (Platform Engineering)  – тема, которая всего за какие-то пару лет превратилась из хайпа в устойчивый тренд. Совсем недавно многие о ней даже не слышали, а на сегодняшний день существуют сотни профильных сообществ. Даже несмотря на пандемию, первая конференция PlatformCon собрала тысячи участников.Однако несправедливо утверждать, что платформенная инженерия – абсолютно новое направление. Впервые это понятие появилось в Thoughtworks Tech Radar в 2017 году. Затем в 2019-м концепцию более подробно раскрыли Matthew Skelton и Manuel Pais в книге Team Topologies. А в 2023 году аналитики Gartner включили Platform Engineering в топ-10 стратегических технологических трендов. На сегодняшний день все больше и больше компаний из разных областей выводят платформенную инженерию на первое место среди направлений развития.До сих пор ведутся дискуссии, к какой именно области отнести Platform Engineering. Многие связывают ее с методологиями DevOps и SRE, кто-то фокусируется на дизайне и UX, а кто-то – на разработке. Однако платформенная инженерия не замыкается на продукте и отдельных этапах его создания. Platform Engineering – это целостный подход по созданию базовой экосистемы, которая способна упростить внедрение и применение лучших практик и инструментов для решения повседневных задач компании.В данной статье я расскажу про платформу цифрового опыта (Digital Experience Platform, DXP), ее состав и роль в производстве ценностей для компании.DXP – это комплексное решение,которое объединяет различные технологии и инструменты создания, управления, оптимизации и контекстуализации цифрового опыта для клиентов, сотрудников и пользователей.Компонентная схема DXPРазвивая DXP в Группе ""Московская Биржа"", наша команда решает множество интересных задач, мы постоянно исследуем инновационные решения, строим на них прототипы, которые развиваются в компаниях до проектов и сервисов.Слаженная команда является ключевым фактором успешной реализации сложных архитектурных решений. Работа по созданию и поддержке DXP обычно включает в себя множество технических и организационных аспектов, и только благодаря слаженному сотрудничеству и вкладу каждого члена команды можно достичь успеха.Наша командаСуровцев ВладимирЛидер платформы,@vvsurovtsevОтвечает за платформу цифрового опыта в Группе компаний, ее состав, концептуальное и стратегическое развитие.Кузнецов ВиталийАрхитектор платформыОпределяет архитектуру и формирует компонентную базу платформы.Асламов ВладиславЭксперт по платформизацииРазрабатывает и проводит экспертизу технических решений в области платформ и отвечает за их распространение по продуктовым командам.Петряков АлександрАрхитектор систем управления контентомОтвечает за создание единого технического решения для публичных сайтов Группы компаний.Гублин ДенисАрхитектор персонифицированных сервисовРазвивает личные кабинеты, упрощает жизнь пользователей и разработчиков.Хейлык ИльяЛидер направления развития личных кабинетовОтвечает за клиентские сервисы для участников торгов и эмитентов. Внедряет автоматизацию в процессы взаимодействия с клиентами.Елисеев АлексейИсследовательВнедряет инновационные технологии, планирует, ищет решения, отвечает за результат.Николаева ВероникаДизайн-лидерРазвивает направление дизайн-системы, унифицирует дизайн и компонентную базу UI Kit.Лушкин ВиталийЛидер направления ЭДОРазвивает два продукта: «Электронный архив документов» и «Единая система электронного документооборота Группы компаний».Козлов АлексейЛидер направления Low-CodeВоплощает в жизнь магию Low-Code/No-Code-инструментов, которые дают возможность легко и эффективно решать задачи бизнеса.Засименко АлександрЭксперт по клиентским интерфейсамОсуществляет полный цикл работ – от проработки до внедрения продуктов в платформу цифрового опыта и распространения их в продуктовые команды.Воронин ВсеволодСистемный аналитик платформыСобирает и анализирует требования к инструментам платформы, прорабатывает концепции решений, осуществляет документирование и управление базой знаний.Интересно, что в какой-то момент мы стали ассоциировать себя с DXP и теперь уже сложно понять, где грань между платформой и командой 😉. Словно это живой организм – свойства платформы становятся свойствами команды, и наоборот.Попробую описать:Многопрофильность.DXP обеспечивает совокупный цифровой опыт для различных типов пользователей, таких как клиенты, сотрудники, партнеры и другие. Это требует разработки различных модулей и функциональности с учетом разных потребностей каждой аудитории.В то же время команда обладает разнообразными навыками, что способствует полному и комплексному пониманию потребности в цифровом опыте.Интеграции.DXP постоянно интегрируется с существующими корпоративными системами.Слаженная коммуникация между членами команды позволяет учесть разные точки зрения, согласовать и обеспечить совместимость решений.Композитная архитектура. Хорошо спроектированная композитная архитектура позволяет легко добавлять и изменять инструмент/функциональность в платформе, что облегчает масштабирование и адаптацию под разные бизнес-потребности.Для этого в команде есть четкое определение ролей и обязанностей и каждый делает то, в чем лучше всего разбирается.Аналитика и оптимизация. Важной функцией является сбор и анализ данных о пользовательском опыте.Команда работает над инструментами аналитики и оптимизации для улучшения цифрового опыта. Мы проводим исследования и адаптируемся под потребности.Многоканальность.Для обеспечения единообразного цифрового опыта через различные каналы взаимодействия команда обладает исчерпывающей экспертизой в разработке пользовательских интерфейсов.В платформенной команде выделены направления, соответствующие целостности компонента, которые являются входными точками для развития данного компонента и прикладной функции на основе него.Кастомизация и персонализация.DXP предоставляет возможность для персонализации опыта разных пользователей и групп пользователей.При этом команда разрабатывает инструменты для управления контентом и функциональностью, решает сложные задачи, преодолевает архитектурные трудности и кастомизирует платформенные компоненты под прикладные требования.Новые технологии и лучшие практики.Технологии и требования в области DXP постоянно меняются.Члены команды активно обмениваются знаниями и учатся друг у друга. Формируется созидательная атмосфера, способствующая творчеству и поиску инновационных решений.Melvin Conway сформулировал такую зависимость полвека назад.Поэтому, когда говорим о сложных архитектурных решениях, важно уделять внимание выстраиванию и поддержке слаженной команды, которая способна работать сообща, чтобы достичь поставленных целей.Развивая тему платформы цифрового опыта и ее ключевых компонентов, стоит подробнее рассмотреть, как эти элементы взаимодействуют между собой и как они способствуют улучшению общего цифрового опыта.Рассмотрим ключевые компоненты платформы:Дизайн-система и наборы компонентов пользовательского интерфейсавсегда в тесной связке, не только обеспечивая красивый и интуитивно понятный дизайн, но и повышая производительность разработки.Используя вместе готовые компоненты пользовательского интерфейса, дизайнеры и разработчики создают единый узнаваемый пользовательский опыт, сосредоточившись на реализации бизнес-функций, исключив затраты на согласование содержания UI-компонентов и примитивов.Всё это задает высокую планку дизайна и разработки интерфейсов, посредством единых стандартов. А где есть стандарты, там и централизованные знания, опыт, принципы, стили, наработки. Это приводит нас к унификации каналов, улучшению цифрового бренда, упрощения онбординга новых специалистов, повышает эффективность рабочего часа и уменьшению количества ошибок в дизайне.Системы управления контентомобеспечивают надежное хранение, управление и распространение контента по разным каналам. Они позволяют создавать, редактировать и публиковать контент через визуальный и программный интерфейсы.В нашем IT ландшафте CMS имеет интеграцию с производственной платформой, что позволяет парой кликов подготовить DEV-окружения для новых прикладных задач. Она использует дизайн-систему и набор компонентов, обеспечивая быструю и согласованную публикацию информации.Конструкторы форм и страницдополняют системы управления контентом, предоставляя возможность бизнес-пользователям создавать и настраивать страницы и формы без необходимости обращаться к разработчикам. Это увеличивает скорость реагирования на изменения в бизнес-процессах.Конструкторы форм и страницХранилище цифровых активовявляется единым ресурсом для хранения типизированных документов Группы Московская Биржа. Эти активы могут использоваться в любых приложениях и системах группы.Платформа работает с несколькими репозиториями, что позволяет обеспечить безопасное хранение как внутренних документов, так и документов клиентов.Конфигуратор электронного архиваСейчас система активно используется 19-ю командами, в следующем году будет осуществлена интеграция с Единым ЭДО и дочерними компаниями. В этом году был реализован No-Code конфигуратор типов документов на базе платформы DXP, что позволяет управлять типами документов и их атрибутами без привлечения разработчиков.Графический редактор и корпоративные плагины к немурешают сразу несколько разноплановых задач:Создание и редактирование графического контента, что особенно важно в цифровом опыте, где визуальный аспект играет значимую роль,Является площадкой для обсуждения и груминга концепций и получения обратной связи для всех участников команды, что повышает эффективность коммуникаций и выводит их на новый уровень,Обеспечивают прямую бесшовную передачу дизайн-макетов в код, что сокращает избыточные операции для фронтовой верстки,Ускоряет и увеличивает производительность труда дизайнеров, с помощью разнообразных плагинов.Графический редактор и корпоративные плагиныИнтересно, что графический редактор стал у нас ключевым инструментом для генерации, трансформации и материализации идеи.Библиотеки и шаблоныпредоставляют разработчикам готовые протестированные решения для ускорения разработки, помогают соблюдать правила и стандарты, установленные в дизайн-системе. Это обеспечивает единообразие в разработке и снижает вероятность возникновения ошибок.Данное решение включает в себя готовые модули, блоки и даже целые шаблоны приложений.Технологические стандартыобеспечивают согласованность и безопасность всей архитектуры, упрощают разработку, помогают выдерживать качество и облегчают интеграцию.Они определяют лучшие практики, архитектурные принципы и рекомендации по разработке ПО. Тех. стандарты помогают нам сократить время и усилия, потраченные на принятие решений по проектированию и разработке, снижают время на онбординг новых сотрудников, а также помогают гарантировать, что разработанный софт соответствует метрикам качества, регуляторным требованиям и регламентам информационной безопасности.Создавая в платформе компонент по единым технологическим стандартам, мы гарантированно получаем ускорение в интеграции его с существующей и будущей компонентной базой.Система управления платформой– это уникальное решение в Группе, задуманное как сервис для быстрой и удобной доставки клиентам создаваемой платформой цифрового опыта ценности, с фокусом на потребности команд разработки, устранении их болей и снижении нецелевых активностей.DXP ManagerНа основе системы управления мы сделали ""цифровую витрину"" компонентов платформы - со всеми доступными управленческими функциями и артефактами по ним.Я постарался раскрыть наиболее развитые компоненты платформы, некоторые мы ввели только в этом году, некоторые уже эволюционировали. В любом случае это множество интегрированных инструментов и их набор не конечен. При таком объеме возникает вопрос, как нам сократить издержки на развитие платформы и производство прикладных задач, ответ – берем готовые инструменты и минимизируем собственную разработку.С готовыми инструментамиподбираем Open Source с достойным сообществом или работаем с поставщиками инструментов, обеспечиваем себя лицензией и качественной поддержкой, не стесняемся близкого партнерства, когда вы можете влиять на backlog поставщика и выносить часть функций в базовое ПО.Говоря о минимизации собственной разработки,мы начинаем затрагивать Low-Code-парадигму, которая зачастую ходит парой с No-Code:Low-Code-решенияпредоставляют инструменты для быстрого создания приложений с минимальным кодированием за счет визуальной среды и преднастроенных компонентов, что упрощает и ускоряет создание цифрового опыта;No-Code-решенияидут еще дальше, предоставляя возможность создавать приложения без написания кода вообще, используя визуальные интерфейсы и автоматизированные инструменты для разработки приложений.Как Low-Code/No-Code влияет на архитектуру платформы цифрового опыта, расскажу в следующей статье.В завершение текущей хотелось бы пояснить: как вы заметили, в отличие от предыдущей статьи проэлектронный архив документов, в этой я ни слова не сказал про ПО, которое используется под капотом. Это намеренное решение, чтобы не сбивать вас с вашего собственного пути платформизации. Получайте удовольствие от экспериментов, конструируя и прототипируя новые функции вашей платформы.Инвестирование в платформу цифрового опыта сегодня является не просто инновацией, а стратегической необходимостью для многих компаний, стремящихся развить всеобъемлющий опыт (Total Experience), включающий в себя как пользовательский, так и клиентский опыт, опыт сотрудника, повысить гармонию услуг, располагая к себе клиента и добиваясь доверия на всех этапах взаимодействия с ним."
СберМаркет,,,Автоматизация Е2Е-тестирования сквозных БП интеграционных проектов Операционного блока,2023-09-20T13:31:00.000Z,"Всем привет! Меня зовут Ренат Дасаев. Являюсь руководителем направления интеграционного автотестирования в компании MOEX и сегодняшний рассказ будет посвящен истории процесса внедрения E2E-автотестов в тестирование бизнес-процессов Московской Биржи. В статье расскажу про наиболее важные аспекты, фичи и сервисы нашего направления. Забегая вперёд, скажу, что сейчас это одно из самых востребованных направлений тестирования в Группе компаний.Для начала вкратце разберемся, что такое E2E-автотест. Это вид тестов, который проверяет бизнес функционал от момента его начала до завершения.Похожие тесты уже, безусловно, существовали в нашей компании, но они, как правило, покрывали функционал, который не выходил за пределы одного департамента (подразделения). У нас же была задача покрыть автотестами функционал, проходящий через несколько департаментов. Теперь поговорим, откуда взялась такая потребность.Предпосылки внедрения E2E-автотестированияМногие компании, в том числе и наша, сталкиваются с дорогостоящим тестированием бизнес-пользователей на стендах UAT (User Acceptance Testing):Поддерживать такой стенд — это большая задача, так как здесь представлены абсолютно все интеграции, как и на промышленном контуре. Полигон обновляется каждый день множеством версий различных микросервисов от разных продуктовых команд;бизнес-пользователям необходимо выделять время на проведение тестирования в отрыве от их основной работы - операционной деятельности в промышленном контуре. А представьте себе, что для проверки бизнес-процесса (далее БП) необходимо участие нескольких подразделений и каждое подразделение должно последовательно выполнять шаги, то есть на синхронизацию между подразделениями также уходит время. А если где-то случается ошибка в интеграции/данных/конфигурациях, то нужно снова всё начинать сначала после исправления ошибки;помимо тестирования нового функционала, бизнес-пользователям нужно проверять ещё критичные регрессионные кейсы, которые могли быть затронуты внедряемым изменением.Для сокращения расходов на тестирование на UAT руководством нашей компании было принято решение о внедрении автоматического тестирования, которое позволило бы снять нагрузку с операционных департаментовпо части регрессионных интеграционных критичных кейсов на UATи сосредоточить их силы на тестировании нового функционала. В дополнение выполнялись бы проверки готовности данного полигона к пользовательскому тестированию, возможность непрерывно запускать тесты на множественные обновления микросервисов в разных системах, снижение сроков доставки кода в промышленный контур.КомандаДля достижения выше поставленных задач была создана команда (в скобках поясняем, чем занимается на данном направлении):начальник управления (анализ потребностей бизнес-заказчика, административный контроль за показателями в команде);руководитель направления (техническая проработка потребности, делегирование задач в команде, участие в разработке автотестов и CI/CD процессов);ведущий разработчик автотестов (разработка автотестов, развитие CI/CD процессов);начинающий разработчик автотестов (разработка автотестов).С недавнего времени нам также помогает бизнес-аналитик из нашего Управления с анализом функциональных задач, полученных от заказчика.Направление E2E-автотестирование стартовало в феврале 2022 года.  Далее расскажем, как мы отбираем задачи для покрытия автотестами.Процесс отбора функционала для E2E-тестовКритериев для отбора бизнес-процессов на покрытие e2e-автотестами несколько:На ручную проверку функционала бизнес-пользователирегулярнотратят много своего времени, которое они могли бы направить в более ценное русло;БП проходит через несколько систем;Есть функциональное задание, из которого можно понять, как в целом устроен процесс, из каких частей состоит и пр.После определения трудозатрат и внесения БП в план на автоматизацию, заказчик приступает к написанию ТЗ (техническое задание) и описывает сценарии тестов, если их еще нет. Эту часть работы мы зачастую выполняем вместе с заказчиком для более глубокого погружения.В ТЗ описываются сценарии тестов с отражением всех используемых параметров. Как правило, автоматизации подлежат всего 1–2 сценария, в основном позитивные (негативные практикуются тоже, но это скорее исключение).Почему так мало сценариев тестирования:Данные тесты не имеют под собой цели проверять отдельно взятую функциональность, так как проверка корректности работы отдельных функций осуществляется автотестами продуктовых команд;Сценарии повторяют те действия пользователя, которые он делает в рамках регрессионных интеграционных тестов, а таких сценариев обычно не много и их в целом достаточно для проверки взаимодействия систем, сетевой связанности и т. д.Теперь перейдём к технической части вопроса.Особенности процесса разработки E2E-тестовПри разработке автотестов стараемся придерживаться следующих положений:Таблица 1. Список положений и эффект от их примененияПоложениеЭффектБольше использовать в тестах rest API, меньше UI.Это продиктовано болеенадежнойибыстройработой самого теста. Разработка и поддержка такого теста существенно   дешевле, чем прохождение БП через UI. Но   все же без использования UI в тестах не   обошлось. В нашей компании сохраняется небольшое число крупных систем в виде   монолита, у которых нет своих rest-api. В этих системах проводится крупный рефакторинг   (перевод на микросервисы), но процесс этот не быстрый.Генерация моделей на основе swagger-документов.Генерируем модель сервиса на основе его swagger. На выходе получаем python-объект (со всеми методами из swagger), с которым работаем уже в тесте.Совместная с другими командами разработка python-модулей и использование их в тестах.Обсудили со смежной командой этот подход и решили   его придерживаться. На каждый микросервис, с которым взаимодействуем в тесте,   пишем python-модуль (пакет), помещаем его в pypi-репозиторий (nexus). Если необходимо написать тест на БП, в которых   участвуют микросервисы, для которых уже существует python-модули, то задача сводится лишь к тому, чтобы   собрать конструктор из этих модулей (загрузив их из биржевого pypi-хранилища) и описать тестовый кейс.Генерация уникальных тестовых данных runtime.Получаем независимые тестовые данные, которые не   пересекаются с пользовательскими. Есть небольшой процент тестов (менее 10%), где   мы используем уже созданные сущности ввиду сложности их создания в   автоматическом режиме.Никаких зависимых тестов – все необходимые данные   готовим фикстурами перед каждым тестом. Там, где нужно – подчищаем за собой   после теста.Все тесты атомарные, можно запустить любой из них   когда угодно.Детальное логирование всех запросов (http, sql, ftp, smb…).Легко найти запрос и ответ в консоли выполнения   теста. Особенно полезно при выяснении причин падения теста.Маркировка тестов списком микросервисов и БП.Легко запускать группы тестов по маркировке: бизнес-процесс/микросервисСбор всех инцидентов в БП.Помогает в разборе падения тестов. Инциденты   собираются на основе маркировок.Сбор всех логов микросервисов при падении автотеста.Помогает в разборе падения тестов. Логи собираются   на основе маркировок.Иметь все необходимые системы, сервисы и интеграции   на стенде разработки автотестов.Этот стенд мы поддерживаем своими силами, по   интеграциям получаем консультации от внешних команд. Таким образом, мы знаем,   как эти системы деплоятся, обновляются, настраиваются. Используем все те же   инструменты и подходы в развертывании и конфигурировании сервисов, что и на   стендах далее (приемочное тестирование, промышленная эксплуатация).Более   подробно о стендах расскажем далее.Проведение ежедневных митингов.Решаем технические проблемы.Использование практики дежурстваПоочередно каждый из исполнителей назначается на   неделю дежурным. В его задачи входят:- разбор ночных   прогонов на QA/UAT;- заведение   артефактов на актуализацию автотестов;- заведение   артефактов на ошибки/улучшение в сервисах;-  актуализация   конфигов сервисов; поддержка тестового контура.ПолигоныВ нашей компании представлено много полигонов. Мы используем несколько из них:QA – полигон для внутреннего тестирования. Для целей E2E развернуто два k8s-кластера с микросервисами, которые участвуют в БП, покрываемых тестами. Внутри этих кластеров есть множество неймспейсов (пространство имен), которые используются как инженерами по автоматизации тестирования смежных команд, так и инженерами функционального тестирования. На данном полигоне мы разрабатываем и отлаживаем тесты;UAT (stage) – полигон приемочного тестирования. Данный полигон повторяет промышленный как по данным (на определенную дату), так и конфигурационно. На нем тестируются текущие боевые версии систем и перспективные релизы. После разработки автотеста на QA мы включаем новый тест в сборку для прогона уже на UAT. Это целевой полигон для наших тестов, т. к. здесь же тестируются заказчики. На данном стенде раскатываются уже проверенные сборки сервисов с других стендов QA, проверенные конфигурации и пр. Таким образом исключаются нестабильности и прогон тестов очень показателен. Особенно важен прогон автотестов за день до крупных релизов (участие в quality gates). Перед крупными релизами к нам обращаются продуктовые команды с целью прогнать тесты на тот или иной функционал, чтобы убедиться:критичный функционал работает, как и ранее (код, который не менялся, т. е. регрессионное тестирование);конфигурации и сетевая связанность сервисов в рабочем состоянии и корректны;время ответов от сервиса находится на приемлемом уровне при обычной рабочей нагрузке.PROD – полигон промышленной эксплуатации. На данном полигоне мы пока не запускаем E2E-автотесты на регулярной основе, но в планах на текущий год такая задача имеется.ИнструментыКак и в любом деле, без хороших инструментов нельзя выполнить эффективно поставленные задачи. Поэтому мы разработали под свои нужды сервисы, которые успешно помогают нам в процессе автотестирования, а также используем мейнстримовые инструменты для разработки и деплоя. Далее расскажем о них.Тестовый фреймворкразработка автотестов и фреймворка ведется на языке программированияPython3.8 (в текущем году планируется переход на 3.11);в качестве тест-менеджера используетсяPytest;для работы с backend в основном используются:rest-api: обертка надrequests;databases: ORM (SQLAlchemy),fdb(Firebird engine),cx_oracle(Oracle engine),psycopg2(postgresql);UI: обертка надselenium;исходный код автотестов и фреймворка хранится в git.CI/CDНаш CI/CD – конвейер состоит из:Jenkins– запуск автотестов;GitlabCI– работа с конфигурациями, исходным кодом, создание базовых образов сервисов и тестов;Vault– хранение конфигов для микросервисов;Nexus– хранилище артефактов различного типа (docker, pypi и др.).Как правило, конвейер завершает свою работу:генерацией образа и отправки его в nexus;и/или разворачиванием deployment вk8sчерезhelm(где запускаются сервисы/тесты).Ниже (см. рисунок 1) схематично изображен базовый pipeline по ночному регрессионному автотестированию в нашей команде.Рисунок 1. Общий план выполнения ночного pipeline по подготовке стенда (-ов) для запуска E2E-тестов.Под стендом подразумевается совокупность всех k8s – кластеров, баз данных, торговых систем, шин и прочего, что покрывается интеграционными автотестами в рамках БП.Запуск тестовВ запуске автотестов участвует самописный скрипт, который обходит папку tests и составляет списки тестов со всеми маркировками (в том числе своими). На выходе получаем несколько списков тестов:которые можно запускать только в одном потоке;которые можно запускать в несколько потоков (через multiprocessing. pool).Также в каждом из этих списков учитывается время, пришедшее отсервиса статистики.Затем, в зависимости от числа исполнителей, выполняется распараллеливание тестов (см. рисунок 2)Рисунок 2. Запуск автотестовE2E-автотесты могут запускаться:по расписанию каждую ночь на QA/UAT-полигонах посредством нашего CI/CD – сервера Jenkins;при изменениях в версиях микросервисов, которые участвуют в покрытых БП (на основании маркировок) в течение дня (GitlabCI + Jenkins);ручной запуск с рабочей станции или непосредственно из задания в Jenkins.Результаты тестовВажно не только прогнать автотесты, но и получить отчет, по которому можно легко анализировать:сколько тестов упало;места падений;сообщения об ошибках;логи микросервисов, скриншоты.У нас используется:allure framework: для построения отчетности на уровне инженеров автоматизации тестирования непосредственно в Jenkins;allure test ops: для построения отчетности на уровне руководства.Среди инструментов имеются полноценные сервисы (backend + frontend), которые мы разработали с нуля своими силами.Сервис статистикиРисунок 3. UI сервиса статистики.Сервис, позволяющий выгружать, обновлять и удалять статистику о времени прохождения тестов. Данная информация служит в дальнейшем для более эффективного распределения тестов при их запуске. Таким образом, самые длинные тесты запускаются в начале сборки, а ближе к концу самые короткие (более подробно в следующем разделе).В данном сервисе есть разделение тестов по типу запуска – regress (как правило весь набор тестов)/smoke, а также debug (одиночный запуск)/multiple (с распараллеливанием). В зависимости от того, с какими параметрами запустилась сборка на CI/CD, из сервиса статистики запрашивается та или иная информация (формируется json на стороне тестового фреймворка и отправляется в сервис).Данный сервис установлен как на QA-полигоне, так и на UAT-полигоне.Основные параметры:сервис написан на Python 3.8 с использованием библиотек:Django,Django REST Framework,Marshmallow;для Django используются вспомогательные библиотеки:django-bootstrap4,django-crispy-forms,django-filterиdjango-tables2;в качестве БД используетсяSQLite;для тестирования используется библиотекаPyTest;на Gitlab CI используются библиотеки:Coverage,Coverage-BadgeиAnyBadge.Сервис дашборды для руководстваСервис, позволяющий агрегировать информацию с баг-трекинговой системы и сервиса статистики, строить статистику с данными. Какие это данные?артефакты, заведенные командой E2E в разрезе проектов, типов и времени:o   можно с легкостью узнать, сколько и каких типов артефактов завели в любой месяц;агрегировать число артефактов за определенный период в любом проекте;соотношение числа открытых и закрытых артефактов по месяцам.численные параметры прогонов тестов на UAT, из которых можно легко узнать:с каким результатом выполняются прогоны (число падений);как меняется число автотестов со временем;как изменяется время исполнения тестов.Прежде всего этот инструмент разрабатывался для руководства, чтобы оценивать метрики эффективности команды E2E.Данный сервис установлен только на UAT-полигоне.Основные параметры:сервис написан на Python 3.8 с использованием библиотек:Dash,Gunicorn,Plotly,Pandas, самописные модули по работе с jira и сервисом статистики;на Gitlab CI используются библиотеки:Coverage,Coverage-BadgeиAnyBadge.Теперь посмотрим на сервис дашборды в картинках (см. рисунок 4-8) чтобы лучше понимать в каком виде мы визуализируем информацию.Рисунок 4. Сервис дашборды: общая информация по заведенным артефактам командой E2E.Рисунок 5. Сервис дашборды: статистика по jira-артефактам в разрезе проектов.Рисунок 6. Сервис дашборды: статистика по открытым/закрытым артефактам с разбивкой по месяцам.Рисунок 7. Сервис дашборды: статистика по прошедшим сборкам автотестов за последние 7 дней.Рисунок 8. Сервис дашборды: статистика по продолжительности/количеству автотестов с разбивкой по месяцам.Сервис, помогающий в дежурствеСервис по формированию различных отчетов по мониторингу окружений с CI/CD процессов позволяет получить ответы на следующие вопросы:какие сервисы и версии установлены на тестовом стенде;насколько успешно работают процессы по автоматическому обновлению сервисов в течение дня и запуска после их обновлений смоук-тестов;есть ли различия в конфигурациях сервисов между несколькими стендами.Данный сервис установлен только на QA-полигоне. На UAT у нас нет доступа к системе управления версиями микросервисов. Но у нас есть доступ (как и подписка) к проекту в баг-трекинговой системе, через который проходят все артефакты, и поэтому мы в курсе, какие версии ушли на данный полигон.Основные параметры:сервис написан на Python 3.8 с использованием библиотек:Flask,Werkzeug,Gunicorn, самописная библиотека по работе с gitlabci;на Gitlab CI используются библиотеки:Coverage,Coverage-BadgeиAnyBadge.Традиционно ниже представляем скриншоты данного сервиса (рисунок 9-13).Рисунок 9. Сервис, помогающий в дежурстве: главная страница сервиса.Рисунок 10. Сервис, помогающий в дежурстве: списки установленных и неустановленных микросервисов.Рисунок 11. Сервис, помогающий в дежурстве: отчет по запущенным пайпланам автодеплоя.Рисунок 12. Сервис, помогающий в дежурстве: отчет по запущенным пайпланам автодеплоя.Рисунок 13. Сервис, помогающий в дежурстве: отчет по различиям в конфигурациях волта между разными стендами.Дашборды для квартального планирования деятельности направления E2EТакже у нас есть доска, созданная в wiki, которая позволяет увидеть общее планирование всех задач на нашем направлении по кварталам со всеми оценками по трудоемкости и другими атрибутами.Рисунок 14. Доска в wiki для планирования E2E-задач.Результаты направления за год и планы на будущееПочти 1,5 года пролетели незаметно, и мы можем подвести промежуточные итоги нашей работы по направлению E2E-автотестирования бизнес-процессов:бизнес-процессы:покрыто42 интеграционных БП Операционного блока, что высвободило у бизнес-пользователей 25,2 человеко-дня при проведении всего того регрессионного тестирования на UAT, что покрыто автотестами. При этом запуски Е2Е тестов проводятся не несколько раз за релиз (как руками бизнес-пользователей до этого), а каждую ночь или при каждом обновлении сервиса, покрытого е2е тестом. Релизный график больших информационных систем в нашей компании как правило раз в полтора месяца. Есть системы, которые выпускаются на еженедельной основе;выстроен процесс отбора и согласования задач на автоматизацию E2E;заведено более278 артефактовв баг-трекинговую систему;процессы автоматизации:испробован новый подход с разделяемыми модулями, и он полностью себя оправдал. На данный момент со смежными командами (привет команде автоматизации тестирования проекта ЦУП/ЦРММ ?) разработано более 140 модулей по работе с rest-api микросервисов, базами данных фондового и валютного рынка, взаимодействующих с внешними сервисами, утилитами;разработаны 3 сервиса (сервис статистики/дашборды/дежурства), которые предоставляют очень нужную информацию как для инженеров по автоматизации тестирования, так и для руководства;выстроено множество CI/CD процессов по деплою сервисов, запуску автотестов, зачистке данных и др.Руководство позитивно оценило нашу работу и ставит новые амбициозные цели, например:увеличение числа покрываемых систем и БП интеграционными тестами;верификация релизных установок в промышленный контур путем запуска E2E-автотестов.ЗаключениеУ нас молодое и перспективное направление. За первый год существования многое было сделано: заложен фундамент по тому, как мы взаимодействуем с бизнес-заказчиками, разработаны все необходимые инструменты, развернуто несколько больших систем на тестовых стендах. У нас есть понимание, как строить эффективные E2E-автотесты со спецификой, которые необходимы в нашей компании. Есть свои нюансы и шероховатости, которые нужно улучшить. Но об этом как-нибудь в другой раз ?Спасибо всем, кто дочитал эту статью до конца. Если остались вопросы, пишите их в комментариях – с радостью ответим!Особую благодарность хочу выразить своему начальнику управления - Дмитрию Чугую за помощь в старте данного направления и подготовке материала для публикации."
СберМаркет,,,Ускоряем разработку: 5 необычных фич DevOps-платформы,2023-09-11T15:35:35.000Z,"Всем привет! Меня зовут Лиза Петровская, я инженер в команде DevOps-платформы MOEX и сегодняшний рассказ будет посвящен именно ей. В статье я расскажу про пять наиболее любопытных фич и сервисов платформы, которые помогают ускорять процесс разработки и облегчают жизнь инженеров и разработчиков в компании.Давайте кратко пройдемся по фундаментальным вопросам определения платформы. Мы в MOEX рассматриваем платформу как внутренний продукт, и в ходе разработки ориентируемся главным образом на “боли” и потребности команд. Platform engineering, или развитие платформ в нашем понимании – это процесс создания и предоставления удобных сервисов самообслуживания командам и инженерам, с целью ускорения потока создания ценностей, снижения объема нецелевых активностей команд, а также повышения качества, надежности и безопасности создаваемых решений. При этом важно придерживаться принципа добровольности, при котором команды приходят на платформу без принуждения, когда сами того захотят. Конечно, это сложно и достигается путем постоянного сбора и анализа обратной связи от наших многочисленных и гетерогенных команд. Задача платформенной команды развивать и улучшать свой продукт, добавлять новый функционал, исходя из потребностей пользователей, которые мы выявляем как раз в ходе непрерывного сбора обратной связи, аудита команд, опросов, разговоров у кулера и прочее, прочее.Немного предыстории: платформа в компании начала активно развиваться примерно в одно время с инженерной трансформацией (о которой можно прочитатьздесь). В рамках этой большой трансформации сборки и пайплайны модифицировались и приводились к единым стандартам для всех команд. Вырабатывался унифицированный подход по CI/CD, наводился порядок в инфраструктуре и инструментах. Мы активно выводили инженерный слой из серой зоны и делали первые шаги к созданию платформы. Про это можно послушать в докладе нашего руководителя платформы Карапета Манасяна на DevOpsConf22:В статье об инженерной трансформации подробно описан путь, который проходит артефакт от DEV до PROD окружения, и как удалось этот путь упростить. Однако, в реальности одного конвейера недостаточно, особенно если мы говорим про огромные цифровые продукты. Команды все равно сталкивались с рядом каждодневных задач, например, узнать все различия между версиями приложений, компонентов, артефактов, сервисов на окружениях. Вот тут в игру вступает фича платформы, которая позволяет экономить часы и за пять секунд узнать в реальном времени все отличия между окружениями.Квест. Найди 10 ОтличийИтак, первая из нашего сегодняшнего топа, необычная, и уже незаменимая для команд фича - сравнение окружений от dev до prod.Только представьте, у вас есть полигон, которым пользуется команда разработки, полигон для автотестов, полигон для интеграционных тестов, для нагрузочных, пре-прод, и, наконец, сам prod. В процессе поставки зачастую членам команды нужно быстро сравнить окружения. Какие есть варианты? Зайти в кубера и сравнить глазами. Да, но уйдет очень много времени. Второй вариант – сравнить yaml-манифесты репозиториев, являющихся частьюGitOps-процесса. Да, но там не вся необходимая информация, а только версии helm-чартов. В любом случае, уйдет много времени на нецелевую работу команды. Эту проблему мы решили с помощью инструмента для мониторинга и сравнения окружений. Почему не использовали готовое решение? Инструментов для мониторинга k8s множество, но вот настолько кастомных нет. Наиболее часто используется именно сравнение по версиям сервисов, но также есть возможность сравнить версии любых объектов k8s (deployment, ingress, replicaSet и т.д.), helm-чартов. При этом мы не показываем секреты или увствительную информацию, а лишь отмечаем наличие расхождений.Кстати, само сравнение можно проводить как в режиме «помониторить один полигон», так и сравнивать неограниченное количество полигонов между собой. А отличия подсвечиваются, чтобы точно не пропустить.Сравнение тестовых полигоновМультитул для тимлидаВ MOEX разрабатываются сильно отличающиеся друг от друга продукты. Например, высоконагруженные торговые системы и их ядра с особенно высокими требованиями к производительности разворачиваются на железных серверах, а микросервисные Финуслуги разрабатываются в куберах. Тем не менее разнородность ИТ-ландшафта, кардинальные различия в подходах и технологическом стеке от команды к команде не мешают нам делать одну платформу, удобную для всех. Платформа строится вокруг абстракции продуктов и проектов, а не процессов и инструментов.Поясню, платформа - единая точка входа к множеству услуг, сервисов и инфраструктуре, но работа на платформе начинается с выбора/создания продукта.Заводим продукт, добавляем компоненты, добавляем зависимости, интеграции, тестовые полигоны, команду. И метрики у нас сюда подключаются, и дашборды из графаны рисуются.Дашборд продуктаCI/CD метрики по продуктуВот так получаем продуктовое observability, как для инженера, так и прозрачность разработки и ИТ-процессов для бизнеса.Микросервисы на кончике кликаНе будем уходить далеко от разработки. Тут речь пойдет о шаблонах для быстрого создания микросервисов. Это удобно, эффективно, избавляет от ручных рутинных действий. Как я писала выше, команды и продукты на Бирже сильно отличаются друг от друга, поэтому на платформе есть как общие шаблоны по языкам программирования (наподобие тех, что предоставляет Gitlab), так и специальные шаблоны для команд, которые создаются и добавляются по обращению и в ходе совместной проработки шаблона. Большинство шаблонов параметризованные. Вот пример шаблона для определенной команды, с требуемыми им настройками.Создание микросервиса из шаблонаПольза очевидна - не тратятся часы на копирование кода из старого проекта, все готово, как говорится, из коробки. Процесс создания микросервиса, уже преднастроенного под конкретную команду, занимает от силы три минуты. Дальше только бизнес-логику писать.Мерж реквесты. Важен каждый голосПлатформа дает нам площадку и подспорье для экспериментов. Так что по просьбам пользователей мы сделали на платформе возможность настроить кастомные правила для Merge Request (MR) на ветку/группы веток в проекте. Вдохновлялись тем, как это устроено в Gitlab Premium, но добавили кое-что от себя, например, регулярные выражения для настройки правил и более гибкие политики для указания конкретных «аппруверов».Правило представляет из себя:ветку или несколько ветокминимальное количество «аппрувов», требуемых для разрешения MRгруппу людей, чьи «аппрувы» будут учитываться (инициатора MR можно учитывать или нет)Как это выглядит:Мерж-реквесты на платформеНа платформе транслируется информация из Gitlab:отображается статус MRколичество «аппрувов»наличие конфликтоввозможность автоматического вливания MRСо стороны Gitlab это выглядит как дискуссия, блокирующая MR:Безопасное проникновение в контурВот уже более полутора лет мы живем в условиях новой реальности и предпочитаем находиться в позиции недоверия: доступ в публичный интернет ограничен из внутренней сети. Процесс получения зависимостей, артефактов из интернета, обновления библиотек у нас был довольно сложным с точки зрения времени ожидания и количества задействованных людей.Там, где раньше было «docker pull», теперь «для проекта N просьба скачать докер-образ NN для использования в сегменте NNN».Радости такой процесс, конечно же, не приносит, силы отнимает, поэтому на платформе мы сделали сервис безопасной загрузки артефактов из интернета.Пользователь выбирает тип артефакта, заполняет заявку в диалоговом окне, после чего запускается процесс загрузки пакетов в карантин. Строится дерево зависимостей, проверяется наличие файлов во внутренних репозиториях, осуществляется проверка контрольных сумм. После чего заказчик подтверждает, что ему нужны именно эти артефакты. Далее скачанные файлы и отчет отправляются на проверку команде информационной безопасности.Заявка на получение артефактовСозданные заявки отображаются на платформе, можно посмотреть, проверить статус, отменить.Заявка на получение артефактовЭтим сервисом мы постарались снять нагрузку с дежурных, разгрузить ИБ. Сам по себе процесс ускорить, исключив, где возможно, человеческий фактор, ожидание, автоматизировав ручные действия. Избавились от ситуации с повторными запросами пользователей, когда не догрузили транзитивные зависимости, проект не собирается, человек возвращается и по новой ждет проверки новых пакетов.Может возникнуть вопрос, почему этот сервис располагается на платформе с UI, а не вызывается только через API с ПК разработчика. Мы считаем, что, заполняя сперва диалоговую форму, человек сильнее задумывается о вопросах безопасности и необходимости использования того или иного пакета. То есть он понимает свою причастность к тому или иному выбранному артефакту. А когда этот процесс полностью скрыт от него, то ответственность смещается на всех, кроме самого разработчика.В планах по развитию этого сервиса прикрутить к процессу скачивания и первичной проверки различные DevSecOps инструменты.Платформа - это также про постоянное улучшение DevExp, про прозрачность и избавление от узких мест в виде процессов и людей.Конечно, у нас были ошибки. Во-первых, в самом начале пути мы выпускали фичи, которые, как нам казалось, будут наиболее полезны для команд, вместо проведения аудита и сбора реальных запросов и потребностей. А также мы начинали разрабатывать новый функционал сразу в «чистовом виде» без PoC. Тратили много ресурсов, не имея уверенности, взлетит фича или нет. Так что, хозяйке на заметку, самое главное - решать реальные проблемы, устранять реальные боли разработчиков, то есть общаться, спрашивать, проводить аудит, собирать обратную связь и статистику использования.Нет единого рецепта для всех, нет двух одинаковых платформ, но определенные подходы и идеи можно и нужно пробовать внедрять. Это были пять наиболее любопытных и необычных возможностей и сервисов нашей внутренней инженерной платформы, которыми хотелось поделиться с миром. Главное, что эти фичи экономят нам время, силы, ресурсы и сокращают TTM. И самое-самое главное, ключевой фактор успеха - это наша большая сплоченная команда настоящих профессионалов, без которых ничего этого невозможно было бы построить и развивать!Мы искренне уверены, что инвестиции в команду и в людей – это основной фактор успеха.В следующих статьях расскажем о других возможностях нашей платформы, а пока пишите в комментариях, если стоит подробнее осветить какую-то из представленных фич."
СберМаркет,,,Как на Московской бирже работает система генерации отчетов для участников рынка,2023-05-23T12:29:48.000Z,"«Любой программист после пары минут чтения кода обязательно вскочит и произнесет, обращаясь к себе: переписать это все нафиг. Потом в нем шевельнется сомнение в том, сколько времени это займет, и остаток дня программист потратит на то, что будет доказывать самому себе, что это только кажется, что переписать это много работы. А если взяться и посидеть немного, то все получится. Зато код будет красивый и правильный» (с)RSDNНаверное, многим знакомо ощущение, когда приходит время переделывать хорошо отлаженный продукт. Он работает, стабилен, предсказуем и знаком, но что-то изменилось в воздухе, выросла сложность решаемых задач, разрослась инфраструктура, появились новые вызовы, и вот приходится решительно садиться и все переделывать. В этой статье мы расскажем про эволюцию нашей системы генерации отчетов aka Отчетницы, которая прошла долгий путь за последние 15 лет. Хочется похвастаться тем, что, невзирая на соблазн взять и переделать все с нуля, вопреки эпиграфу и благодаря ряду удачных решений, развитие системы оставалось эволюционным. Участки системы, ответственные за бизнес-логику работы нашей промышленной системы, сохранялись, и это спасало человеко-месяцы и годы тестирования для другой полезной работы.Разберемся с тем, что из себя представляют отчеты для участников рынка. Ежедневно на Московскую Биржу отправляются десятки миллионов торговых заявок, совершается несколько миллионов сделок. По результатам торговых и клиринговых сессий наши участники получают больше двух сотен отчетов об итогах торгов, оценке обеспечения, выполнении обязательств маркет-мейкеров и др. Отчеты бывают в основном торговые и клиринговые, и, соответственно, несут информацию о торгах или клиринге. На каждом из наших рынков они отличаются, хотя структурно могут хранить информацию одного типа. Отчеты рассылаются отдельно каждому участнику торгов\клиринга (включая самого главного из них – Банк России). На заре времен отчеты представляли из себя текстовые файлы с таблицами, нарисованными в псевдографике. Сейчас это XML файлы со схемами и стилями на нескольких языках. Кстати, забавный факт –  по историческим причинам один из наших отчетов до сих пор отправляется в псевдографике. Ради привычки наших клиентов, которым данный формат наиболее удобен – функционал отрисовки таблиц пришлось заботливо пронести сквозь все метаморфозы системы подготовки отчетов. Впрочем, не будем забегать вперед.В начале (примерно в 2008 году) немногочисленные отчеты генерились с помощью хранимых процедур БД (тогда еще исключительно Firebird) и настольной СУБД Corel Paradox. Это продолжалось недолго. Возня с форматированием псевдографики по мере расширения числа отчетов отнимала все больше и больше времени, так что мы переехали с псевдографики на XML. Возможность предоставлять данные отдельно от представления хоть и стоила дополнительных ресурсов на хранение структуры документа в памяти, но значительно упростила подготовку отчетов.Вдобавок к смене формата к Paradox прикрутили продукт Altova MapForce, чтобы ввести уровень абстракции при работе с данными. Процесс подготовки отчетов стал выглядеть примерно, как на картинке (схема не наша, но концепт тот же). Маппинг данных из источника в готовый отчет теперь элегантно настраивался в несколько кликов мышки, по крайней мере на тот момент. Увы, время шло, семейство отчетов продолжало разрастаться, структура данных базы усложняться, а энтропия не убывать. Наконец настал день, когда стало ясно, что графический подход к формированию маппинга себя исчерпал. Невооруженному человеческому глазу стало сложно справляться с творящимся на экране хаосом, после чего мы сделали очередной эволюционный переход.Стильный графический маппинг пришлось отложить в пользу удобства редактуры и повторного использования кода. Собственно маппинг, бизнес логику и сопутствующие вычисления перенесли в скриптовые файлы *.scx. Оглядываясь назад, решение оказалось исключительно удачным. Легкость редактирования скрипта и реализованный чуть позже простой, но гибкий формульный язык позволил в дальнейшем сохранять бизнес-логику формирования отчетов при значительных модификациях других частей системы. К скриптам добавили интерфейс работы с БД (все еще только Firebird), написанный на Delphi, который завернули в отдельную dll. В нее же запаковали функционал для формирования выходных данных в XML. Вокруг .scx файлов написали простые обертки на bash, чтобы можно было задать параметры вызова. Обертки используются для отладки и тестирования, а также в качестве резервного варианта подготовки отчетов при отказе каких-либо вспомогательных систем.В таком виде система оказалась очень удобной. Здесь сказалась как гибкость Paradox, так и произведенные архитектурные доработки. Сравнительно безоблачно Отчетница провела следующие несколько лет. На их протяжении в API DLL добавились интерфейсы работы с Oracle и MS SQL, для решения ряда локальных задач появилась возможность генерировать отчеты не только в XML, но и в CSV. Проводились многократные локальные оптимизации *.scx скриптов. В рамках экспериментов по улучшению и развитию системы был сформирован уже упомянутый формульный язык, но, в целом, концепция системы изменений не претерпевала.График роста числа обработки заявок в секунду, абсолютный и относительный к прошлому годуЗа несколько лет стабильности число отчетов увеличилось вдвое, объемы торгов тоже подросли. Наша торгово-клиринговая система (тогда еще ASTS), отвечая вызовам времени, постоянно повышала производительность (об этом можно почитать подробнее в наших прошлыхстатьях). Пыталась не отставать от нее и Отчетница. Стали сказываться проблемы, вызванные тем, что система, по сути, была клиентским приложением на рабочей машине ответственного за процедуру подготовки отчетов сотрудника (маклера). Дело в том, что по мере роста числа заявок и сделок, в какой-то момент размер xml файлов стал достигать нескольких гигабайтов и время подготовки отчетов перестало укладываться в установленные регламентом рамки. Одна машина хорошо, а три лучше - подумали мы и распараллелили процедуру подготовки отчетов. Забегая вперед, это оказался путь в никуда. По распределенным группам участников торгов отчеты формировались на 3-х, потом 4-х, потом 6-ти машинах. Это ускорило процесс подготовки, но сильно усложнило жизнь маклера. Если не вдаваться в подробности, теперь от него требовалось вдумчиво, с осознанием всех рисков, глазами и руками контролировать параллельный процесс подготовки отчетов в 6-ти терминальных вкладках.Жить так было невозможно. Маклеров было необходимо спасать, тем более, число отчетов и заявок\сделок продолжало расти. Команда бэк-офиса взялась писать многопоточный сервер подготовки отчетов, с интеллектуальными автоматизациями в виде графов задач с вызовом скриптов по узлам (практически Application Manager), масштабируемостью, кластеризацией и аналогом BPN. И что характерно, система была написана, но в промышленную эксплуатацию не пошла. Сказался потенциально зашкаливающий объем приемочного тестирования. К сожалению, уже готовую систему пришлось отложить и вернуться к поиску решений и коробочных аналогов.Новая архитектураВ поисках подходящих технологий мы пришли к микросервисной архитектуре и платформе Camunda. Технологии выглядели перспективными, но, в процессе реализации, мы уперлись в проблемы с производительностью библиотеки взаимодействия с БД (API dll). Чтобы их разрешить, библиотеку пришлось переписать с Delphi на .Net. Корнем всех проблем была однопоточность библиотеки, так что в новой версии она превратилась в многопроцессный и многопоточный инструмент. Это сходу уменьшило время генерации отчета примерно в три раза. Напомним, что в богатые на рыночную активность дни размер xml файлов отчетов по заявкам достигал десятков гигабайт. Таким образом, после продолжительного тестирования время формирования отчетов сократилось с более чем 60 до 20-25 минут.После того как проблемы с .dll были решены, преград на пути обновления Отчетницы не осталось.Теперь она представляет из себя набор микросервисов, развернутых в кластере Kubernetes. В каждый набор связанных между собой сервисов входят бэкэнд и фронтэнд сервиса формирования отчётов, бэкэнд и фронтэнд сервиса рассылки отчётов, Camunda для управления бизнес-процессом. Для реализации бэка и фронта мы выбрали .Net и React соответственно. Каждый из наборов развернут в шести экземплярах в зависимости от целевой группы (фондовый \ валютный рынок, торговые \ клиринговые операции + 2 экземпляра под внутренние нужды). Экземпляры между собой отличаются только настройками..SCX скрипты остались в качестве унифицированного формата описания запроса и форматирования данных. Декларативное конфигурирование обеспечивает простоту добавления новых отчетов и настройки старых. Каждый отчет по-прежнему описывается одним xml документом. Есть робкая надежда, что функционал легко расширяется без изменений в коде сервиса.Новая архитектура существенно расширила наши возможности в автоматизации процесса подготовки отчетов. Так появились различные режимы запуска формирования отчетов: в полностью автоматическом режиме (по расписанию планировщика без участия маклера-операциониста), в полуавтоматическом (операционист в нужное время запускает формирование группы отчетов) и для проблемных случаев – полностью ручной вариант. Стало возможным формировать отчет по условию, например, по факту готовности другого отчета, либо создавать отчеты, для которых требуется подтверждение двух независимых операторов. Значительно расширились возможности аудита работы сервиса. Стал приятнее интерфейс. Все это заметно сократило трудозатраты операционистов, технической поддержки и, естественно, самих разработчиков. Существенно снизились риски совершения ошибок при ручном сопровождении процесса.Старый интерфейсНовый интерфейсЛюбопытным побочным эффектом новой архитектуры стала возможность использовать Отчетницу в качестве сервера технологических операций, позволяющего подключать любые операции, выполняющиеся в течение дня, автоматически запускать группы этих операций и т. п. То есть технически ее можно использовать как инструмент для автоматизации запуска всех действий, выполняемых операционными подразделениями (на трёх рынках Мосбиржи).Подводя итогиЗа пятнадцать лет с момента создания наша Отчетница прошла долгий путь. Ограничения рождают форму и наш продукт, сталкиваясь с постоянным ростом объемов данных, увеличением сложности структуры БД, расширением бизнес-требований выкристаллизовался в масштабируемую устойчивую структуру с современным стеком технологий и высоким уровнем эргономичности. Важной заслугой на этом пути мы считаем эволюционный подход к приложению: несмотря на то, что система множество раз дорабатывалась, нам удавалось сохранить неизменными части системы с бизнес-логикой, что позволило избегать масштабного регрессионного тестирования, как в процессе разработки, так и в приемочном тестировании. Нельзя не сказать спасибо команде бэк-офиса, которая на протяжении этих пятнадцати лет с заботой и любовью совершенствовала и шлифовала напильником систему в бесконечной погоне за производительностью и удобством эксплуатации. На этом, впрочем, история Отчетницы не заканчивается. В планах дальнейшее развитие автоматизации подготовки отчетов. Запланировано импортозамещение некоторых компонент и другие работы."
СберМаркет,,,Практика использования BPMS в бэк-офисных процессах биржи,2023-04-05T15:07:08.000Z,"Где и зачем мы используем BPM процессыЦентральной частью биржи являютсяторговые системы и их торговые ядра, в которых происходит matching заявок участников рынка и формируется order_logИменно к торговому ядру (вернее к шлюзу) подключаются участники торгов. При этом, чтобы торговая система (ТС) могла работать, в неё к началу ежедневной торговой сессии должны попасть описания торговых инструментов и описания клиентов и их параметров подключения к ТС.Этим у нас занимаются бэк-офисные процессы, в исполнение которых вовлечены несколько различных подразделений Московской Биржи. Если в двух словах описать эти процессы: они долгие и сложные.Почему так?Приведем пример – допустим, пришла к нам мошенническая фирма, выпустила облигации, участники рынка купили эти облигации, а фирма вместо того, чтобы исполнять обязательства перед держателями облигаций, перевела все полученные от рынка средства на офшор на Каймановых островах и была такова. Кто за это ответит? Кого будут сажать в тюрьму?Чтобы избежать подобных ситуаций, как при допуске к торгам финансовых инструментов, так и при допуске к торгам участников этих торгов, выполняются KYC проверки и тех и других, причем в зависимости от роли в торгах и  от рынка (например, валютный, фондовый, рынок деривативов) – процессы, по которым биржа будет проверять участника торгов или эмитента финансовых инструментов, будут свои и будут состоять из разных шагов и разных наборов проверок.Все эти процессы проходят в бэкофисных системах биржи. Над KYC-проверкой работают несколько подразделений MOEX Group, и мы возлагаем оркестровку и мониторинг этих процессов на исполняемые BPMS схемы, а логику следования оформляем в нотации DMN.Это удобно по следующим причинам:Средства, которыми мы пользуемся, достаточно зрелые, в них решены задачи обеспечения стабильности и производительности.Инструментарий сопровождения удобный и наглядный, легко передаётся в руки соответствующих служб сопровождения.Разработку можно вести быстро, а также расширять при необходимости инструментарий.Бизнес у нас уникальный: банков в России сотни, а бирж -  единицы, поэтому заметная часть разработки является эксклюзивной и её приходится вести собственными силами под себя, при этом мы активно ориентируемся на opensource инструменты.Начали двигаться в сторону BPMS ориентировочно с 2017г, ИСХОДНОЕ СОСТОЯНИЕ НА 2017-18ГГ: оркестрация процессов между сотрудниками выполнялась через обмен email-ами и заполнение форм в приложениях. Некоторая часть бэк-офисных систем была написана на устаревших 4GL инструментах вплоть до Paradox и FoxPro, часть веб приложений – в качестве монолитных Java приложений со своими, написанными под частные задачи, state engine или мини-workflow.Сейчас у нас есть своя бэкофисная платформа, состоящая из следующих элементов:Фреймворк управления производственным процессом, метриками и качеством.Производственная платформа, включающая ci/cd, фреймворки конфигурирования и мониторинга.Централизованная авторизация, аутентификация.Интеграционные шаблоны.Единые технические сервисы (нотификации, бизнес-логирование, мониторинг процессов, планирование отложенных и регулярных задач).В бою работает более 100 BPMS схем процессов.  Любая ценная бумага российского фондового рынка попадает в торги через эти процессы. Любой отчет по итогам торговой сессии с 2022 г. на фондовом и валютном рынках оркестрируется также нашими системами.В качестве BPMS инструмента у нас неплохо прижился движок Camunda, как один из самых зрелых и широко используемых движков семейства Activity. Преимущественно используется v7 ввиду прозрачной склейки с Java  (для реализации machine task просто пишется Java класс).Как мы получаем описания процессовБизнес-аналитики, разбирая задачу или проект, общаются с заказчиком сразу с использованием BPMN моделей, в большинстве случаев рисуя их в camunda modeler.На диаграмме сразу видно, с чего стартует процесс, кто в какой последовательности и что делает, какие есть варианты завершения.Помимо диаграмм пишут поясняющий текст, описывают данные и все прочее, что важно. Для меня, как для представителя разработки, это вполне пригодный к обработке результат, т.к. разрешается основной в корпоративной среде вопрос - кто за что отвечает. Все это пишут в wiki по принятым шаблонам, всё удобно, по всей базе аналитики работает полнотекстовый поиск.От аналитической BPMN схемы до исполняемой - один шаг... но не всегда:Если делаем процесс ""в пределах одной системы"" или ""в пределах одного сетевого сегмента""- то можно взять логическую схему и доработать до исполняемой;Чаще процесс интегрирует и оркестрирует цепочку систем и тогда делаем      каскадирование процессов(управление уходит в подпроцессы) и тогда разработчики,      глядя в документы и диаграммы, поступившие от бизнес-аналитиков,      разрабатывают физические исполняемые схемы процессовКакие вопросы и задачи решаем, с чем сталкиваемсяBPMS-Процессы ,которые мы разрабатываем и обслуживаем, являютсясложными,проводитьв ходе релизов ручное регрессионное тестирование большинства процессов практически нереально. По этой причине активно используется как UNIT-тестирование исполняемых схем, так и автоматизированное тестирование интегрированного приложения через API и пользовательский интерфейс.По возможности, стараемся уходить от долго живущих процессов, чтобы не нести трудозатраты, связанные с миграцией или поддержанием параллельных версий.Несколько раз решали оригинальные технические задачи, такие, как разработка самодельных плагинов к camunda cockpit.Причина:необходимость поддержки тестирования, задание контекстных параметров процессов или активация таймеров в BPMS схемах по решению тестировщика, а не по наступлению планового времени.Пользовательские задачи из всех бизнес-процессов собираем в единый менеджер задач: он выдает пользователю задачу ровно в тот момент, когда процесс до задачи дошел и она назначена на исполнителя (или группу исполнителей). Менеджер задач никакой бизнес-спецификой не обладает, он просто отображает текущие активные задачи пользователя, при этом за то, как будет выглядеть и вести себя конкретная задача, отвечает разработчик бизнес-процесса. Т.е. сам менеджер задач один, а процессов, которые могут вывести в нём задачу, много и их разработка идет независимо друг от друга.В своей работе мы активно используем «цикл Plan – Do – Check - Act» и, с некоторой периодичностью, после реализации какого-то количества функциональности, по требованиям наших заказчиков задаёмся вопросами:Насколько используемые нами практики разработки позволяют быстро и эффективно реализовывать требования?Не являются ли какие-то из наших подходов нашими ограничениями?Что мы можем привнести в нашу инфраструктуру и практики разработки, чтобы разрабатывать быстрее?По результатам анализа принимаем решения, как можно упростить и ускорить.В настоящее время на уровне корпоративной архитектуры работаем над определением высокоуровневой бизнес‑архитектуры сквозных процессов Биржи, в которой процессы будут максимально перестроены для работы в формате STP: обработка стандартных операций выполняется автоматически, задача человека обработка ошибок или «нестандартных» ситуаций."
СберМаркет,,,"Реальная инженерная трансформация: от команд и метрик до культуры, конвейеров и инфраструктуры",2023-02-22T13:03:20.000Z,"Привет, Хабр! На этапе выбора темы статьи было много идей: написать про DevOps или про платформы, а может про продуктовые команды или про практики SRE? Но пришли к выводу, что нет ничего интереснее, чем реальная увлекательная история трансформации. Мы, команды платформы разработки MOEX и экосистемы Финуслуги.ру, в лице Карапета Манасяна, Александра Барыкова, Антона Квашёнкина и Юлии Лутковской, расскажем практически про весь путь изменений и про их подводные камни. Важно отметить, что в статье затрагивается довольно много тем, местами даже будут блоки со скриптами ?. Поехалиии!Оглавление:Про Финуслуги.ру и портфель продуктов1. Как было до изменений?1.1 Предпосылки1.2 Состояние “до”: метрики1.3 Состояние “до”: конвейер производства и CI/CD-процесс2. Выбор траектории трансформации2.1 Целевая топология и как менялись команды2.2 Как создавали сообщества2.3 Создание модифицирующей или change-команды2.4 Целевой конвейер и какую стратегию выбралиОписание самого CI/CD-процессаa) Почему мигрировали с Jenkins на Gitlab CI в части CDb) Про структуру CIc) Про версионирование в целевом подходеd) CD: почему выбрали ArgoCD2.5 Что изменили в архитектуре продуктов3. Планы на развитие4. ВыводыПолезные ссылки и выступленияПро Финуслуги.ру и портфель продуктовФинуслуги— первая платформа личных финансов. Мы создали нашу платформу, чтобы сделать финансовые продукты доступными для любого жителя нашей страны вне зависимости от региона проживания. Одна регистрация на нашей платформе открывает широкие возможности по выбору и оформлению онлайн банковских и страховых продуктов. Без визита в офис, в удобное для вас время. Без каких-либо комиссий.Вклады— один из самых простых инструментов, который поможет сохранить и приумножить сбережения. Когда открываешь вклад, важно найти лучшую ставку по депозиту, чтобы получить максимальную доходность. Но отследить изменение процентной ставки по вкладам вручную бывает сложно, потому что условия в разных банках постоянно меняются. Чтобы вы могли на одной странице видеть актуальные ставки по депозитам, мы создали индекс доходности вкладов, в котором сравниваем условия вкладов крупнейших банков. Наши специалисты регулярно мониторят предложения в топ-50 банков и размещают актуальные предложения. Благодаря этому вам не нужно самостоятельно идти в банки или на их сайты в поисках самой высокой ставки по депозитам. На нашей платформе можно сравнить предложения и перейти к открытию вклада. Оформить депозит на Финуслугах можно полностью онлайн без посещения офиса банка. Для этого нужно только авторизоваться через Госуслуги и один раз встретиться с представителем платформы в удобное время.Облигации— эксклюзивно на нашей платформе мы предлагаем новый простой инструмент с ежедневным начислением дохода. Народные облигации — инвестиции без рыночного риска. Они не торгуются на бирже, а продать их можно в любой момент по цене покупки плюс накопленный доход. При покупке и продаже облигаций вы не платите никаких комиссий.Страхование— на нашей платформе автолюбители могут оформить электронный полис ОСАГО с выгодой до 78%. Без комиссии, без посещения офиса и без встречи с курьером. Наша платформа рассчитывает цены в более чем 15 страховых компаний и находит самые выгодные предложения для вас. У нас удобный онлайн-калькулятор, который позволит быстро рассчитать стоимость полиса. Оформленный полис мы направляем по email и отображаем в нашем мобильном приложении, чтобы полис всегда был под рукой. Также вы можете подобрать полис КАСКО. А если у вас оформлена ипотека для вас у нас онлайн-оформление страховки.Кредиты— отправьте одну заявку сразу в несколько банков, выберите самое выгодное предложение и получите кредит. У вас есть возможность получить кредит как без визита в банк, так и в офисе банка. А бесплатный кредитный рейтинг поможет вам понять вероятность одобрения кредита. Чем выше рейтинг, тем больше предложений вы получите.1. Как было до изменений?До начала трансформации мы жили в компонентном подходе. Использовали, так называемую, модель ""водопад"", при которой время от старта разработки до вывода в прод занимала в среднем 8 недель. Организационная структура в компонентном подходе предполагала четкие зоны ответственности по направлениям: аналитика, тестирование, архитектура, DevOps, а разработка делилась на фронт и бэк.Бизнес в данной модели был практически изолирован. Задача от него ставилась при старте разработки и по истечению 8 недель осуществлялась бизнес-приемка функционала.В организационной структуре, приведенной ниже, глава поставок отвечал за скоуп и даты релизов, а также за все quality gates в рамках релиза (приемочное тестирование, регресс, нагрузка и тд). Фактически он был релиз-менеджером на большую программу (всех команд). Он являлся в некотором смысле арбитром между ИТ и бизнесом, что в свою очередь влияло на эффективность поставки и создавало ""bus factor"". Под главой поставок были все команды, как компонентные, так и сервисные (аналитика, архитекторы, DevOps, тестирование). При этом нужно подчеркнуть, что существовала отдельная линия сопровождения (команда Оps), которая структурно была в другом подразделении. Фактически, были команды разработки, DevOps и 2 линия (Оps) обособленные друг от друга, что создавало в свою очередь фактор ""перекидывания через стенку"". Каждый отвечал в рамках своей зоны ответственности.1.1 ПредпосылкиЛюбые трансформации должны иметь предпосылки. Во всех остальных случаях – это проявление фактора “слепых улучшений”. Все предпосылки были определены в коллаборации разных направлений: бизнеса, разработки, тестирования, конвейера доставки, инфраструктуры, HR, коммуникаций и тд. Это позволило добиться максимально объективной картины текущего состояния.Можно выделить следующие основные предпосылки:мы медленно поставляли функционал до пользователей: пользователи и бизнес требовали быстрой доставки изменений. Ввиду того, что скорость поставки была медленной мы теряли клиентов. А с другой стороны конвейер поставки был устроен настолько сложно, что для доставки изменений подключались много специалистов, которое увеличивало сильно стоимость поставки;страдало качество поставок: этому тоже были четкие объяснения. Не была описана методология управления качеством, отсутствовали явные quality gates в конвейере. Да, формально в нем был статический анализ, но это был всего лишь информационный этап, без явных блокировок;была несогласованность между бизнес и ИТ на всех этапах, и мы часто не оправдывали ожидания друг друга;блокирующие проблемы возникали на поздних этапах, что регулярно приводило к сдвигу релизов;несмотря на достаточное количество ресурсов и высокую экспертность, конвейер не удовлетворял потребностям команд: команды всегда хотели понимать и управлять конвейером самостоятельно, но ввиду того, что он был сложным и непонятным, они по любой проблеме, даже самой незначительной, обращались к ""devops-команде"";необходимость в ускоренной реакции на потребности рынка или уменьшение TTM: в экосистеме Финуслуги.ру существуют много гипотез и они требовали быстрых проверок. Из-за высокого TTM возникали сложности в их проверках;отсутствовала ответственность за продукт: часто звучали такие фразы как ""это все девопсы"", ""тестировщики не дотестили"" и тд. Простой инженер и разработчик продукта не брали на себя ответственности за конечный результат целиком. Причина была, конечно же, не в конкретных ребятах, а в самой инженерной культуре и формулировках миссий продуктов;большое количество коммуникаций;потребность в быстром масштабировании с повышением качества, безопасности и доступности сервисов для клиентов;отсутствовала культура практик проверки гипотез: делали-делали что-то большое, выкатывали в прод, но потом оказывалось, что это уже не нужно рынку;релизы ставились ночью. Это очень сильно выматывало команды и приводило к выгоранию.1.2 Состояние “до”: метрикиЧтобы понять куда идти, надо понять, где мы сейчас. Здесь нам как раз и помогают правильно определенные метрики, исходящие из предпосылок и целей бизнеса. Правильно определенные – это значит метрики, которые помогают находить проблемы, а не только рисует красивые мигающие лампочки. Мы сразу четко обозначили, что метрики не предназначены для того, чтобы хвалить или наказывать людей.Некоторые квадранты радара метрик приведены ниже:Технологические метрики и метрики потока:Time to MarketВремя от создания (open) до релиза (lead time), дни, историиЧастота релизов (release frequency) в месяц, без хотфиксовВремя от принятия в работу до релизов (cycle time), дни, программаКачествоПроцент внеплановых Hotfix-релизов (Change Failure rate)Кумулятивный поток дефектовКачество заведенных дефектов (% rejected)Среднее время жизни дефекта, дниДефекты конвейера, к-воДолг по автоматизации тестированияПроцент дефектов, найденных до продуктива, от общего числа дефектовЭффективностьЭффективность потока (время кодирования к общему TTM)Rework (время на исправление дефектов ко времени разработки)Метрики потокаТТМ в управлении продуктом, дниTTM User stories в ИТ, дниЭффективность потока (время написания кода), %Затраты на переделываниеДоступность, %Change failure rate, %Метрики разработкиТехнический долг (часы)Покрытие unit-тестами (%)Покрытие код-ревью (%)Метрики ScrumVelocity в story pointsПлан / факт, %Scope creep / рост задач во время спринтаКлиентские метрикиИндекс удовлетворённости клиентов, NPSTTM, от идеи до прода, дни% user stories, по которым высказана и проверена гипотеза (получена обратная связь на ценность)Бизнес-метрикиНовые продукты, к-воНовые партнеры, к-воProduct-Market FitВыручка, руб.Продажи фин. продуктовВремя открытия и пополнения вкладаДоступность, %Организационно-культурные метрикиКоличество коммуникаций и встречКачество информации и задач, %ТекучкаПереключение контекстаОткрытость к инновациямЛояльность к ошибкам и неудачамМы также ""померили"" сервисные команды, которые, в некотором смысле, являлись ""серой зоной"", т.е. сервисные команды не измеряли качество предоставляемого сервиса и удовлетворенность своих клиентов. Это не давало возможности ретроспективно посмотреть на свои процессы и внести туда корректировки и улучшения.Нужно подчеркнуть, что мы рассматривали каждый показатель только в определенном контексте, в связке друг с другом, а также с другими метриками, исходя из бизнеса. Более того, мы связывали эффективность работы с обратной связью от конечных пользователей и их удовлетворенностью. Это давало возможность принимать максимально верные управленческие решения по изменениям и ходе трансформации.1.3 Состояние “до”: конвейер производства и CI/CD-процессДо всех изменений, можно сказать, что конвейер и связанные работы с ним были непрозрачны, а команды вообще не понимали как он устроен. Для них была доступна только минимальная информация — в какой репозиторий залить код и кому написать, если возникнет красная лампочка.Структура конвейера была следующей:CI-процесс был построен на базе Jenkins и плагинаJenkins templating-engine. Под каждый стек были написаны библиотеки для тестов, сборки и тд.Стандартная сборка артефактов в виде докер-контейнера и Helm-чарта с последующей публикацией в хранилище артефактов Nexus.Прежде чем рассказывать про процесс CD, стоит рассказать про окружения или, как у нас принято называть, ""полигоны"".Полигоны разделяются по функционалу:Командные полигоны— полигоны, которые используются только продуктовыми командами. Под каждую команду – отдельный полигон, на котором развёрнуты все необходимые сервисы работы конкретного продукта. Технически, командный полигон представляет из себя отдельный namespace в Kubernetes-кластерах dev-окружения.Полигон для запуска тестов— отдельный полигон, который используется исключительно для запуска автоматизированных API и UI-тестов. Используется в основном командой QA. Также, как и командные полигоны, располагается в отдельном namespace в Kubernetes-кластерах dev-окружения.Интеграционный полигон— полигон, который подключен к реальным интеграциям с партнёрами, банками и т.д. На данном полигоне также проводится регрессионное тестирование. Аналогично другим полигонам в отдельном namespace.Полигон для проведения нагрузочного тестирования— отдельные Kubernetes-кластеры, на которых команда НТ запускает нагрузочное тестирование. Максимально приближен к конфигурации с продом.Хотфикс —через этот полигон прокатывались хотфиксы на прод.Препрод-полигон— окружение, на котором проводятся финальные проверки перед публикацией в прод. Также максимально похож на продакшен в части конфигурации.Продакшен Полигон— прод).Теперь вернёмся к CD-части нашего рассказа. Доставка также была построена на базе Jenkins, но самое интересное в том, что процесс отличался в зависимости от полигона. И это порождало немало проблем. Пробежимся по каждому из полигонов.Командные полигоны: деплой на командные полигоны выполнялся в Jenkins в последнем шаге пайплайна. По сути, шаг представлял из себя выполнение команды helm install с определённым набором параметров в виде версии артефакта и т.д. Креды доступа в виде kubeconfig-файлов к кубер кластерам dev-окружения были добавлены в настройки Jenkins.Полигон для запуска тестов: деплой выполнялся по шедулеру несколько раз в день запуском аналогичных команд, что и на командных полигонах.Интеграционный полигон: деплой выполнялся только после прохождения основной джобы API/UI-тестов на QA-полигоне, в которой запускались тесты по всем продуктам. После успешного завершения тестов деплою давался зелёный свет.Препрод-полигон: вот как раз здесь крылось основное различие в процессах по сравнению с описанными выше полигонами. После прохождения регрессионного тестирование на интеграционном полигоне в игру вступала отдельная джоба в Jenkins, в которой запускался специально написанный тулинг (на Go) для снятия так называемой ""релизной карты"". Релизная карта представляла из себя готовый helmfile.yaml (helmfile) с версиями сервисов, которые были продеплоены на интеграционный полигон в момент снятия карты. Далее запускался процесс ""переноса релизной карты"" — джоба по переносу артефактов из DEV Nexus в TEST Nexus согласно слепку версий сервисов в ранее снятой релизной карте. И финальный этап - джоба деплоя, которая под капотом запускала утилиту helmfile с ранее подготовленным файлом.Продакшен Полигон: процесс аналогичен TEST. Различия в том, что для переноса артефактов осуществлялся из TEST Nexus в Prod Nexus.За процесс деплоя на TEST/PROD была ответственна отдельная команда сопровождения прод-окружения (ops-ы, devops-ы). Они, в том числе, занимались поддержкой/доработками всевозможного тулинга, описанного выше. Продуктовые команды в процессе деплоя никакого участия не принимали. Сейчас, смотря на это ретроспективно, видно, что это был ориентированный, в первую очередь, на команду Оps инструментарий, а не клиентоориентированный набор платформенных сервисов продуктовым командам.2. Выбор траектории трансформации2.1 Целевая топология и как менялись командыКак было сказано выше, первым делом мы определили метрики и померили их. Основное, что мы поняли — многие проблемы исходили из организационной структуры. Именно с нее начался наш первый шаг к трансформации, где мы сделали акцент на потокоцентричные команды. Команды, главная миссия которых — это создание ценностей для клиентов, посредством сосредоточения вокруг регулирования скорости потока, а также качества, безопасности, доступности и удобстве для пользователей. А их цель — привлечение аудитории, которая согласится купить предлагаемое решение ее проблемы, посредством постоянного совершенствования потока создания ценностей. Формула непростая, но все по порядку.Организационная структура была разделена на три вертикали:уровень программы (CPO, СPM, главный Scrum Master (SM), СTO, главный архитектор, глава интеграционной команды). У каждой роли своя зона ответственности и фокус:SM — повышение эффективности команды, процессы и их оптимизация;CTO — как обеспечить выполнение бизнес=задач, опираясь на текущие технологии и современные тренды;Глава интеграционной команды — быстрый и надежный конвейер, доставка релизов в прод;Главный архитектор — выбор оптимальных технологий для решения задач бизнеса;CPO/CPM — какой продукт востребован на рынке, что принесет нам прибыль и т. д.уровень продукта;уровень команды.Мы детально проработали новые роли и зоны ответственности, что позже было отражено в виде плейбука.Плейбук— это инструмент, который позволяет получить исчерпывающие ответы на вопросы по процессам, ролям и инструментам. В тоже время, это некоторая их стандартизация и спецификация команд.Центром вселенной стала продуктовая/потокоцентричная команда, а роль менеджера изменилась — от позиции командования и контроля к лидеру и «слуге». Такие команды мы укомплектовали всеми необходимыми ролями, чтобы они могли независимо работать. Это был кардинально новый подход к работе, который требовал от членов команд также терпения и готовности измениться. В свою очередь, платформа производства или DevOps‑платформа MOEX аккумулировала в себя инструменты, компетенции и платформенные решения, которые предоставлялись командам в виде сервисов. Важной особенностью платформы являлась ее независимость от потокоцентричных команд. Она никак не участвует в производственных процессах самих команд, а только лишь отвечает за свою «платформу как продукт». Конечно, эта прекрасная и идеальная картина случилась не сразу. Команда платформы усердно работала над сбором потребностей команд, их обратной связи и developer experience (опыт разработчика). Именно это позволило нам создать полностью удовлетворяющую потребности разработчиков платформу.2.2 Как создавали сообществаПродуктовые команды стали более изолированы и появилась потребность сохранять и выращивать экспертизу, делиться опытом между командами. Сначала мы определили, что есть ""сообщество"". Это объединение людей, имеющих общие интересы для обмена опытом, создания комфортной среды и решения текущих проблем. Концепция ИТ-сообществ в MOEX – это поиск и внедрение практик по развитию производственных процессов и инструментов для ускорения принятия решений и сокращения времени создания ценностей.На тему сообществ и их создания существуют много докладов во внешней среде, поэтому мы сделали акцент на то, какие сообщества создавались в рамках Финуслуги.ру, и с какими проблемами мы столкнулись.Было принято решение cоздать сообщества по каждому направлению:архитектуры,тестирования,разработки,DevOps и процессов разработки.Сложности, с которыми столкнулись:Не всегда легко найти лидера сообщества. Без лидера/ов сообщества, как правило, не функционируют. Поэтому наша задача была найти мотивированных ребят, готовых лидировать и собирать вокруг идеи других инженеров. Признание стало главной составляющей мотивации лидера сообщества. Их драйвила причастность к созданию крутого сообщества и того, что они стоят у истоков этих событий. На уровне всей компании лидеры стали известными и признанными экспертами. Казалось бы, мелочь, но нам этот фактор очень сильно помог.Конфликт с бизнес-задачами. Эта проблема была ярко выраженной в начале пути. Когда процессы работы сообщества устаканились, задачи сообщества стали для команд обычным делом.Выделять человека только под задачи сообщества слишком дорого.Сопротивления нововведениям. Эта сложность решалась не принуждением или ""спусканием сверху"". Мы создавали модифицирующие/change-команды, которые помогали другим осознать свои проблемы, создавать прототипы, воспринимать явную пользу от нововведений. Далее эти процессы и технологии масштабировались на другие команды.Давайте рассмотрим работу сообщества на примереFinuslugi QA Community:встреча проходит раз в 2 недели, открытое мероприятие, любой участник команды может подключиться;повестка встречи определяется заранее с помощью голосования и насущных проблем;отдельное пространство в таск-трекере, куда можно создавать свои пожелания и потребности.Ответственность лидера сообщества:анализ ключевых метрик сообщества и проработка мер в случае отклонения;помощь с поиском новых сотрудников (собеседование, онбординг, матрица компетенций и т.д.);проведение обучающих сессий и митапов;формирование стратегии тестирования, работы по её развитию;проведение встреч гильдии QA;создание общих регламентов и стандартов;участие в проведении post-mortem-ов;содействие развитию инструментов тестирования.2.3 Создание модифицирующей или change-командыСерьезные инженерные трансформации не заканчиваются лишь внедрением CI/CD и k8s. Основные проблемы изменений кроются в инженерной зрелости команд, которая банально может стать преградой для внедрения модных-молодежных практик. Поэтому наряду с разработкой стандартов, целевых архитектур и конвейеров, мы начали процесс создания модифицирующей команды. Задача такой команды заключалась в максимальной помощи и содействии процессам трансформации и, в первую очередь, командам, которые хотят измениться, но им сложно или они не знают с чего начать, куда идти. Модифицирующую команду мы создали из максимально лояльных, сеньёрных и проактивных ребят, которые четко понимали свою миссию и цели (разумеется, они были представлены всем командам). Она создавалась как временная команда и состояла, как из ребят команды DevOps-платформы, так и Финуслуги. Работа была организована по классическому скраму: планирование, дейли, ретро, ревью и тд. Здесь мы ничего нового не придумывали. Но успех был неизбежен, за счет удаления всех стен между членами команд и создания атмосферы доверительной работыбез руководителей, но с сильными лидерами?. Никаких ограничений на межкомандные коммуникации, никаких искусственных согласований с ""непосредственным руководителем"" и, конечно, никаких формальных ""работаю только по таскам с детальным описанием и ТЗ"". Каждый инженер понимал, что, находясь в модифицирующей команде, ему нужно включать голову и, засучив рукава, разобраться, описать и решить общую задачу, чтобы нанести непоправимую пользу.В успех модифицирующей команды многие не верили. Но мы уж слишком были уверены в своих силах. Во-первых, мы не только занимались проектированием светлого будущего, но и решали задачи, стоящие здесь и сейчас у команд, и, во-вторых, любые изменения и модификации представляли из себя сначала прототип, который в ""сыром"" виде обсуждался и тестировался с командами. Так мы уменьшали риски дальнейших переделываний, а конечные внедрения имели высокий уровень качества. Кстати, после каждого спринт-ревью модифицирующей команды, мы собирали обратную связь от команд, на горячую. Как правило, это были вопросы на измерение основных метрик: удовлетворённость модифицирующей командой и их решениями, насколько понятны объясняемые кейсы и решения, чего не хватает в рассказе, какие сложности возникают и т.д. Вопросы зависили от конкретного контекста спринта.2.4 Целевой конвейер и какую стратегию выбралиГлавной целью целевого потока и конвейера являлось создание возможности независимой доставки компонентов продуктов до конечных клиентов. Мы хотели создать такой процесс, где разработчик сможет сам доставить изменения до прода. Да-да, вы не ослышались, именно сам и до прода. И у нас это получилось сделать. Самая основная преграда, существовавшая на пути к этому – была жесткая разделенность между Dev и Ops. Разработчики самостоятельно ничего не могли доставлять в прод без обращения к Ops-ам. Эта позиция аргументировалась всегда тем, что ""разработка не должна иметь доступа в прод"". Мы, как модифицирующая команда, с этим полностью были согласны. И предложили абсолютно новый концепт конвейера и CI/CD, который был ориентирован на четко определенных Quality Gates (QGs) и Security Gates (SGs), полное прохождение которых давало возможность разработчику и команде довести свои изменения до прода, не обращаясь ни к кому. Здесь важно еще раз подчеркнуть тот фактор, что мы создали процесс, при котором разработка прямого доступа до прода не имеет (доступ до серверов, k8s и тд),но, ввиду полной прозрачности процесса доставки, видимости и наблюдаемости окружений, разработчик фактически получил все необходимые ему инструменты, чтобы ""девопсить по фэн-шую"" и нести ответственность за свой продукт. У него фактически пропала возможность говорить ""виноваты все, кроме меня, локально все работает"". Здесь нам также помогла созданная доверительная атмосфера в командах, где за неудачи не ругают, а видят в них зоны роста.Чтобы вы лучше поняли наш процесс, для начала нужно разобраться с терминами и что мы под ними понимаем:QGs и SGs (quality and security gates)- Проверки и тесты на различных этапах. Такие как SAST, юнит, UI, API, регрессионные, контрактные, ручные, интеграционные, агентские и т.д. Несоответствие значения QGs/SGs эталонному блокирует конвейер до исправления ошибок.PROMOTE ARTIFACTS- Перенос артефактов из нижестоящего (с разработческого контура в интеграционный. Это разделенные сегменты) NEXUS в вышестоящий. На самом полигоне при этом ничего не меняется и не запускается. Это всего лишь декларирование факта, что артефакт соответствует всем QGs и SGs и тем самым получил визу, чтобы перейти в другой NEXUS.DEPLOY- Выкатка нового кода на полигон. При использовании feature toggle новый функционал не включен, включение происходит отдельным процессом. То есть deploy только выводит новый код на полигон, но пользователь никаких изменений не замечает.PROMOTE VERSION- Коммит версии приложения в репозиторий с deploy-map (это репозиторий gitops с состоянием полигона. Более подробное описание см. в п4 ""Описание самого CI/CD-процесса""), для PROD открывается MR в защищенную ветку.RELEASE- Включение нового функционала с помощью feature toggle. Именно в этом случае пользователь видит новую фичу или изменения.Чтобы наша концепция работала, мы определили, что:команда должна быть готова выкатывать каждый сервис отдельно и независимо;должно быть реальное покрытие тестами кода на уровне не менее 80%;должна быть возможность запуска API и UI тестов для отдельного сервиса.Описание самого CI/CD-процессаa) Почему мигрировали с Jenkins на Gitlab CI в части CDВ качестве CI-инструмента в новом конвейере стали использовать GitLabCI. Он был выбран как платформенный инструмент в MOEX. Про опыт выбора инструментов можно говорить долго - мы  напишем отдельно про это статью. Тут хочется лишь описать основные моменты.Единая точка хранения кода ПО и кода деплоя.Хорошая интеграция с различными системами (Jira, Teams).YAML для описания пайплайнов, не нужно использовать полноценные ЯП.Большая популярность для поиска и онбординга новых сотрудников.Возможность использования шаблонов (широкие возможности по переиспользованию и распространению новых фич).b) Про структуру CIШаблонизация легла в основу всех пайплайнов. В GitLab есть замечательный функционал, который позволяет включить в описание вашего пайплайна внешние YAML-файлы. Выглядит это так:include:
  - project: platform/templates
    file: ci/templates/java/java-gitlab-ci.yml
variables:
  VAR1: ""var1""
  VAR2: ""var2""Мы сделали единый репозиторий с шаблонами, доступный всем командам. Он состоит из общих шаблонов CI и шаблонов CD. В них содержатся отдельные ""кирпичики"", из которых строятся пайплайны. Также в репозитории есть директория с командными шаблонами. В ней содержатся шаблоны команд, в которой объединены общие шаблоны +специфика команд.Такая концепция позволяет разработчикам почти моментально ставить свои сервисы на общие рельсы. Для этого добавляем в наш gitlab-ci.yml include шаблон и пару переменных – все готово. Также мы можем легко изменять процесс одновременно для всех команд, сделав необходимые правки в командном шаблоне. Последним мы сделали шаблон проверки docker image c помощью инструмента trivy, протестировали, добавили в общий командный шаблон, и теперь все разработчики могут просмотреть уязвимости в своих сборках, при необходимости их устранить.В новом процессе особую роль мы отвели понятию QGs. Так как мы предоставили разработчикам возможность доставки своего кода до прода, нам нужно было их подстраховать и дать инструмент контроля качества изменений. QG стоят на разных этапах пайплайна и, если не достигнут целевой уровень, то он блокирует дальнейшее продвижение пайплайна.Что в себя включает Quality Gates (QGs) и Security Gates (SGs):Ручной CodeReviewСинтаксический анализ кода (code-style/lint)Статический анализ кодаСтатическое тестирование безопасности приложения (SAST)Запуск unit-тестовДинамическое тестирование безопасности приложения (DAST, при необходимости)Запуск API, UI, manual, smoke=тестовЗапуск интеграционных, нагрузочных, smoke-тестовПроверка артефактов на уязвимости и лицензии.Для всех Gates с командами разработки были согласованы уровни прохождения. На переходный период были выставлены уровни меньше целевых и смотрели на новый код, чтобы команды успели подтянуть свои сервисы до общего целевого уровня.Как мы уже писали, разработчики не могут иметь доступа в прод. Но как же им тогда доставить свои изменения? Мы дали возможность разработчикам, при условии прохождения всех Quality Gates (QGs) и Security Gates (SGs) переместить артефакты, полученные в результате сборки, с нижестоящего полигона на вышестоящий. Таким образом, команда разработки может полностью контролировать, когда и на каком полигоне появятся их изменения. И останется только их задеплоить. Также нам приходилось решать определенные трудности изолированности сред и возможности доступов. По нашей концепции мы можем иметь доступ только с вышестоящих полигонов в нижестоящие. То есть прод → тест можно, наоборот – нет. В выбранном нами в качестве CI инструменте GitLab всю полезною работу выполняют runners. При этом они постоянно отслеживают, есть ли для них задание на GitLab, основываясь на заданном для них теге. Соответственно, в каждом полигоне развернуты runner-ы для выполнения шагов в соответствующем полигоне.c) Про версионирование в целевом подходеПри проектировании целевого версионирования были выработаны следующие подходы:Версионирование должно быть автоматическим в рамках конвейера. Избавиться от ручного инкремента минорных версий.Для версионирования сервисов используется SemVer строго формата X.Y.Z без добавления beta и т.д. Пример: 1.4.0.Источником правды для версий служат git tags как более универсальное решение в отличие от билд-номеров CI-систем.Majorверсия берется для каждого языка из разных мест и полностью контролируется разработчиками:NodeJS – файл package.jsonmaven – pom.xmlДалее во время сборки осуществляется автоматическая корректировкаminorиpatchверсии:Дляtrunk(developbranch) изменяетсяminorверсия;Дляhotfixизменяетсяpatchверсия;Дляfeatureветокне генерируетсяверсия ине создаютсяартефакты.Правила изменения версий компонентов:Дляmajorверсии нумерацияначинается с 1, изменяется на+1командой разработки, ответственной за компонент, только в случае, когда невозможно поддержать обратную совместимость с предыдущей версией компонента. Происходит по итогам разработки story перед вливаниемfeaturebranch вdevelopbranch (trunk). Черезпоследнийcommit в feature branch.Дляminorверсии нумерацияначинается с 0, изменяется на+1 автоматически при каждом push в trunk. Сбрасывается в 0 в случае изменения major версии.Дляpatchверсии нумерация такженачинается с 0, изменяется на+1 автоматически при каждом push в hotfix. Сбрасывается в 0 в случае изменения minor или major версии.Техническая реализацияДля реализации версионирования, согласно принятым правилам, был выбран инструмент GitVersion.GitVersionпозволяет выполнять bump версии без написания дополнительной логики. Необходим толькоGitVersion.ymlконфиг с описанием логики:mode: ContinuousDelivery
no-bump-message: ^\b$
continuous-delivery-fallback-tag: ''
commit-message-incrementing: Disabled
branches:
  trunk:
    regex: ^master$|^main$|^develop$
    tag: ''
    increment: Minor
    track-merge-target: true
    source-branches: []
  hotfix:
    regex: ^hotfix[/-]?
    tag: ''
    increment: Patch
    track-merge-target: true
    source-branches:
    - develop
    - main
    - master
    tracks-release-branches: false
  main:
    regex: ^\b$
  develop:
    regex: ^\b$
  feature:
    regex: ^\b$
  release:
    regex: ^\b$
  pull-request:
    regex: ^\b$
  support:
    regex: ^\bОсновная логика:Считывание Major версии из файлов проекта (в зависимости от каждого языка) и сохранение в переменную окружения$NEXT_MAJORЗапуск утилитыдля определения новой версии:/tools/dotnet-gitversion /config /tools/GitVersion.yml /nofetch /nocache /showvariable MajorMinorPatch /overrideconfig next-version=$NEXT_MAJORСохранение результата в build.env, для использования в последующих шагах в пайплайнеПосле успешной сборки создание git tag с соответствующей версией и пушем тэга в транкПример Set Version в пайплайне:Set version:
  image:
    name: <private repo>/cicd/gitversion:${GITVERSION_IMAGE_TAG}
    entrypoint: [""""]
  stage: version
  cache: []
  variables:
    GIT_DEPTH: ""0""
  before_script:
    - |
      NEXT_MAJOR=$(grep '<version.major>' pom.xml | cut -d'>' -f2 | cut -d'<' -f1)
      echo ""INFO - Next major: ${NEXT_MAJOR}""
  script:
    # run gitversion for additional information
    - /tools/dotnet-gitversion /config /tools/GitVersion.yml
    - APP_VERSION=$(/tools/dotnet-gitversion /config /tools/GitVersion.yml /nofetch /nocache /showvariable MajorMinorPatch /overrideconfig next-version=$NEXT_MAJOR)
    - 'echo ""INFO - Next version: ${APP_VERSION}""'
    - 'echo ""INFO - Check if a git tag with version ${APP_VERSION} does not exist""'
    - |
      if [ $(git tag -l ""${APP_VERSION}"") ]; then
        echo -e ""ERROR - git tag for ${APP_VERSION} already exists in repository.\nRun pipeline with variable PIPELINE_SKIP_BUILD for skipping build new version."" && exit 1
      fi
    - echo ""APP_VERSION=${APP_VERSION}"" >> build.env
  artifacts:
    reports:
      dotenv: build.envd) CD: почему выбрали ArgoCDВ качестве инструмента CD мы используем ArgoCD, который является также платформенным инструментом в MOEX. Основным плюсом ArgoCD для нас является декларативный подход, следования подходу GitOps и красивый UI, который очень нравится разработчикам ?. Для создания приложений мы выбрали паттернapp-of-apps- создавать приложения и проекты в ручном режиме у нас запрещено. Под каждое окружение dev/test/prod у нас развёрнуты отдельные инстансы ArgoCD, которые располагаются также в отдельных, так называемых, management-кластерах под каждое окружение (в этих кластерах только инфраструктурный тулинг). Для авторизации в ArgoCD используется интеграция с корпоративным Keycloak+LDAP. Также для каждого окружения созданы отдельные гит-репозитории, в которых декларативно описано состояние приложений и проектов для данного окружения. У себя мы также называем эти репозитории deploy-map.Deploy-map, по сути, представляет собой репозиторий, в котором находится Helm-чарт и набор values-файлов. Для удобства поддержки нескольких репозиториев deploy-map под каждое окружение мы написалиLibrary Chart, с помощью которого можно гибко создавать приложения в ArgoCD.Структура размещения values-файлов имеет следующий вид:.
    └───clusters
    │   └───<название кластера>
    |       ├───<название полигона>.yaml
    └...Пример файла описания дев-полигона:autoSync: false
cluster: dev
environment: dev
namespace: dev
project: dev
selfHeal: false
valueFile: values-dev.yaml
valueFileCommon: values-common-dev.yaml
 
applications:
- name: foo-1
- name: foo-2
- name: foo-3
- name: foo-4
- name: foo-5Далее в ArgoCD создается приложение в парадигме app-of-apps, которое смотрит на гит-репозиторий и создаёт новое приложение автоматически. Поскольку deploy-map находится в гите, то добавление приложения сводится к созданию MR, код-ревью и успешному прохождению пайплайна с линтингом. Продуктовые команды активно добавляют приложения на окружения именно таким образом.Все это позволило нам деплоить сервисы на наши полигоны одинаково и единообразно, чего раньше не было.Общая архитектура конвейера:2.5 Что изменили в архитектуре продуктовВажной частью инженерных изменений была архитектура продуктов и их связанность. Ввиду того, как было сказано ранее, что наша цель была обеспечить доставку микросервисов обособленно, мы начали активно работать над созданием стандарта к микросервисам и формировать план приведения всей архитектуры к этому стандарту. Все полученные стандарты были заложены в функционал платформы разработки -""Микросервис по кнопке"". Это фича в портале разработчика, которая позволяет за несколько кликов получить готовый микросервис по стандарту, подключенный к единому CI/CD, инструментам мониторинга и логирования, внутренним репозиториям и т.д.Наряду с этим, был организован переход на trunk-based подход. Ранее у нас были проблемы с ветками- под каждый релиз отводились новые ветки, что-то забывали подливать, теряли изменения...Кратко, про сам подход:у нас одна ветка develop, т.е. все живут в единой стратегии ветвления;разработка ведется по принципу branching by abstraction;подход поддерживает фича-флаги, что дает нам возможность гибко управлять изменениями и отключать функционал, если он не готов или нашлась ошибка в последний момент.Однако у данного подхода, наряду с плюсами, есть свои особенности:удаление старых фича-флагов и, в целом, управление ими;quality gates и security gates должны быть на очень высоком уровне;необходимо инвестировать ресурсы в автотестирование, и этот процесс должен быть прозрачен бизнесу;требуется обучение новому подходу разработки branching by abstraction.Для перехода были организованы технические спринты. Это были итерации, направленные только на технологические задачи, связанные с изменением ветвления и конвейера. В этих спринтах мы сначала наращивали покрытие тестами. После проставляли quality gates (QGs) в нужных местах конвейера, неспеша переходили на новые рельсы и стабилизировали все последующие процессы.3. Планы на развитиеРабота над улучшениями – непрерывный процесс, поэтому хочется поделиться нашими некоторыми планами на 2023 г. Наряду с описанной выше трансформацией, мы решили из линии сопровождения сделать полноценную SRE-команду со всеми необходимыми компетенциями. Что уже успели сделать:команда активно работает над повышение отказоустойчивости и надежности системы,на порядок увеличена наблюдаемость прод-окружения,перешли на спринты,расширен функционал инфраструктурной платформы с фокусом на потребителей, то есть на продуктовые команды. Инфраструктурную платформу начали рассматривать как полноценный продукт.Что предстоит:способность системы автоматически восстанавливаться (применение паттернов при построении архитектуры),усовершенствование конвейера доставки новыми проверками,добавление новых фич на платформу разработки для продуктовых команд,развитие сообществ и увеличение количества совместных активностей.4. ВыводыМы хотели показать, что изменения и трансформации охватывают много различных аспектов: как культурного характера, так и команды, технологии и инструменты. Поэтому статья получилось большой, но, надеемся очень интересной. Хочется подытожить рассказ следующими тезисами:Для трансформации должны быть определены инвестиции. Такие истории невозможно реализовывать только на голом энтузиазме, и в этот процесс должны быть вовлечены все стейкхолдеры.Подходов к трансформации у нас было несколько, но самый последний и масштабный, который затронул все вышеперечисленные аспекты, занял около 6 месяцев. Нужно подчеркнуть, что дальше следовали процессы постоянного усовершенствования, основные задачи трансформации служили толчком для последующих положительных перемен.Переходы на новые практики и процессы осуществлялись постепенно. Модифицирующая команда брала один микросервис одной продуктовой команды, переводила на новые рельсы в виде прототипов, делала демо для самой команды, и,когда пилот был успешным, команда самостоятельно переходила на целевые процессы. Модифицирующая команда всего лишь была рядом и могла сориентировать или подсказать.Если продуктовая команда четко понимает, зачем ей нужны изменения и какие возможны бенефиты, то она сама приложит максимальные усилия для реализации изменений. Но для этого сервисным и модифицирующим командам нужно включить навыки продажников и маркетологов, чтобы продать новую концепцию. Никакие принуждения к изменениям не работают. Да, возможно, инструмент и процесс вы поменяете, но команда не поймет, зачем это нужно было...До трансформации публикации в прод были только в ночное время с плашкой о технических работах. Трансформация позволила реализовать стратегию бесшовной публикации в любое время без простоя и влияния на пользовательский путь.Очень важно не увлекаться сверхулучшениями. Инженерам это нужно хорошо осознавать, в противном случае инженерные процессы превращаются в ""полигон для оттачивания навыков"" о новомодные инструменты, не нужные бизнесу.Конвейер производства должен быть построен на нативных и простых инструментах. Нужно обращать внимание на порог входа в инструмент и саму структуру пайплайнов. Важно понимать, чем занята инженерная команда, отвечающая за построение инфраструктуры производства и насколько принятые там решения/паттерны соответствуют целям продуктов.Описанная выше трансформация позволила решить все обозначенные проблемы, исходящие из предпосылок. Залог успеха – правильно сформулированные миссии и конечные образы результатов.Процессу трансформации Финуслуги.ру дала сильный импульс общегрупповая стратегия DevOps-трансформации. Многие аспекты инженерной культуры, паттернов и инструментов там уже были описаны и отлично себя зарекомендовали к тому моменту.Идеальный DevOps случается тогда, когда тот, кто разработал продукт, его доставляет и поддерживает.В конце, хочется поблагодарить всю нашу крутую команду, которая реализовывала процессы трансформации и продолжает совершенствовать продукты. Мы уверены, что люди - это главное, а инвестиции в развитие команд, лидеров и культуры могут изменить мир к лучшему.Полезные ссылки и выступления:Доклад Карапета Манасяна про стратегию DevOps-трансформации;Первое сообщество про platform engineering – Platen, который ведет Карапет. Подписка, колокольчик и пальчик вверх приветствуются ?"
СберМаркет,,,Про поставки и релизы в мобильной разработке,2022-09-01T05:10:54.000Z,"Конвейер готовится к поставке.Благодаря старанию мобильного сообщества сейчас есть много классных источников информации про то, как писать код, или про то, как устроена мобильная ОС.  Намного меньше источников, из которых можно узнать, как выстраивать процессы поставки в командах и как связывать это с процессом разработки. А это тоже по-своему важно. Прозрачность и общее понимание процесса релиза и доставки позитивно влияет на эффективность команды - помогает каждому лучше планировать деятельность.В жизни мобильного разработчика может настать день, когда в зоне его ответственности, кроме задач по верстке экранов и походов в сеть, появляется работа с выстраиванием релизных процессов, инфраструктурой и непрерывной поставкой (CI/CD). Когда мне первый раз предложили взять роль релиз-инженера, я не знал, что мне нужно делать. У меня не было четкого понимания, как работать с версионированием приложения, и как оно связано с релизным циклом. Как правило, вопросами поставки занимаются отдельные люди в команде, в то время как большинство пишет код - так было и у нас.В этом посте я постарался систематизировать свои знания, связанные с процессами поставки мобильных приложений, а также поделиться практическими рекомендациями.Большинство из них будет связано с нативной Android разработкой, но основная информация актуальна для разработки мобильного приложения на любом языке и платформе.Disclaimer: Пост полезен тем, кто впервые попал на проект, в котором поставка работает на непрерывном потоке, и еще больше тем, кто впервые начинает новый проект и хочет поставить на рельсы “релизный поезд” с учетом будущего масштабирования.Все описанное ниже является результатом личного опыта автора и его коллег. Любые примеры и рекомендации в этом посте не являются эталонными и могут не подойти вам или вашей команде. Однако все, что описано дальше, практикуется в известных продуктовых компаниях.ВерсионированиеБольшинство Android разработчиков слышали про versionCode и versionName (для iOS - Version и Build), часть из них точно помнит, в чем их отличие, и только часть из них знает, когда и по какому правилу их менять. Давайте разбираться.Как известно, у любого программного продукта есть версия. Сегодня практикуется так называемоесемантическое версионирование. Наверняка вы видели название версии в формате Major.Minor.Patch. Рассмотрим что означает каждая цифра в классическом описании (которое более справедливо для разработки backend).Major. Увеличивается, когда сделаны обратно-несовместимые изменения API.Minor. Увеличивается, когда сделаны обратно-совместимые изменения API.Patch. Увеличивается, когда вливаются исправления багов.В общем случае мобильные приложения не предназначены для того чтобы предоставлять API, зато они часто зависят от какого-то базового API. В таких случаях релизный цикл приложения может быть завязан на общий релизный цикл продукта.Однако приложение может не зависеть от стороннего API. Либо продуктом команды может быть библиотека. Тогда справедлив такой подход:Major. Увеличивается, когда приложение после обновления стало для пользователя принципиально новым (вследствие серьезных изменений в логике работы, интерфейсе). Для библиотеки увеличивается, когда сделаны обратно несовместимые изменения, вынуждающие переписать код, использующий библиотеку.Minor. Увеличивается, когда добавлены изменения, условно не отменяющие, а дополняющие существующий функционал. Для библиотеки увеличивается при доработках с учетом обратной совместимости.Patch. Увеличивается, когда вливаются исправления багов.Важное отличие Patch от Major и Minor - увеличения Patch версии заранее не запланированы, они выпускаются вдогонку, когда уже состоялся релиз Major.Minor.0. Другими словами, Patch поднимается в релизной ветке, а не в основной (про ветвление поговорим ниже).Особенности в Android:Android Gradle Plugin дает возможность работать с версионированием черезversionCode (int) и versionName (string).VersionName нужен, чтобы пользователь/тестировщик отличал версию приложения; versionCode - чтобы версию сборки приложения отличала ОС и Google Play. Каждая новая версия должна содержать versionCode выше предыдущего. Это позволит Google Play предлагать более свежую сборку, а системе запретить (или предупредить) установку более старой версии вместо новой.Фактически нет связи между этими параметрами - вы можете как угодно повышать первый и заполнять второй.Мне нравится следующая формула, позволяющая не ломать голову над связью параметров и значением versionCode (трудности могут начаться, когда помимо основной git-ветки будут параллельно жить релизные). Его ограничение в допустимых значениях, введенное для избежания коллизий versionCode: значения minor и patch - [0, 99). Приводится фрагмент скрипта из build.gradle.kts в директории app-модуля:/* Допустимые значения major - [0,21000); minor и patch - [0, 99) */
val major = 3
val minor = 7
val patch = 0

defaultConfig {
  versionCode = major * 10000 + minor * 100 + patch
  versionName = ""${major}.${minor}.${patch}""
}Релизный цикл и модели ветвленияЧтобы понимать последовательность действий, необходимых  для доведения своего кода до пользователя, разработчику, в первую очередь нужно понимать, какая модель ветвления используется в команде. Модель ветвления отвечает на вопрос “что и когда делать”: когда и откуда отводить рабочие ветки, когда создавать merge request, как делать hotfix; для каких веток настраивать процесс CD (continuous delivery).Есть прямая связь между моделью ветвления и релизным процессом в команде, так как перечисленные выше действия, в общем случае, делаются не в кодерском вакууме, а в ходе взаимодействия с командой и с известными сроками этапов процесса.Самые известные модели на сегодня это Gitflow, Feature Branch Workflow и Trunk-Based Development. Рассмотрим их отличия в упрощенном варианте (опущены моменты работы с release-ветками).Git-flow- подразумевает наличие master и develop ветки + долгоживущих feature-веток. В ветке master хранится история релизов, а ветка develop служит для интеграции новых фич. Релизные ветки отводятся от develop и по окончанию релиза вливаются в master.Feature Branch Workflow- подразумевает наличие stable-ветки + долгоживущих feature-веток, над которыми может трудиться несколько разработчиков. По мере готовности feature-ветка вливается в stable. Тестировщики работают со сборками из feature веток и разрешают вливать feature-ветку в stable. Релизные ветки отводятся от stable ветки.Этот же подход часто называют Github flow, что приводит к путанице с описанным выше Git-flow. Подробнеетут.Trunk-Based Development- подразумевает наличие stable-ветки (trunk) + коротко-живущих feature-веток, разрабатываемых как правило одним разработчиком и содержащих атомарные изменения. Тестировщики не влияют на влитие feature-веток и работают только со сборками из trunk.Количество stable-ветокСрок жизни feature-ветокКогда вливается feature-веткаFeature Branch Workflow1ДлинныйПосле завершения разработки и тестированияGit-flow2 (master и develop)Trunk-Based Development1Короткий (меньше 1 дня)После каждой “атомарной” итерацииПоследний подход также предполагает обязательное наличие фича-тоглов (feature-toggle).Feature-toggle- это флаг (в самом простом виде boolean) конфигурации приложения, скрывающий не готовый к релизу функционал. Они могут быть захардкожены, храниться в памяти устройства или прилетать из облака. Обычно во внутренних debug сборках добавляется технический экран, из которого можно включать фича-тоглы.Пример экрана управления feature-togglesФича-тоглы могут использоваться командой в любой модели ветвления, но лишь в последней он обязателен, так как в trunk-ветке содержится код незаконченной фичи, который должен быть скрыт.Я считаю наиболее подходящим для мобильной разработки подход с использованием с Trunk-Based Development. У этого подхода есть два весомых преимущества перед другими: он простой, как табуретка: в любой ситуации нужно соблюдать только одно правило - в trunk-ветке всегда должен быть самый свежий и актуальный код (все фиксы сначала вливаются в trunk); из-за короткой жизни feature-ветки почти забываются долгие разруливания merge-conflict-ов при вливании feature-ветки в trunk. Но перечисленные плюсы рождают недостатки: повышенные требования к качеству и безопасности вливаемого кода, так как влитые “ломающие” изменения будут тормозить работу всем; необходимость выделять ресурсы на внедрение и использование фича-тоглов, особенно в начале пути.Но есть и другое мнение от коллег из hh, которые перешли на github flow, его можно послушать вэтом видео.А про связку trunk-based-development и фича-тоглов можно послушать ввидеоот коллег из QIWI.Этапы релизного цикла и что делает релиз-инженерЯ начинал пост с того, что часто первое назначение разработчика релиз-инженером может ввести его в недоумение.Хорошая новость в том, что никаких специальных знаний для релиз-инженерства не нужно - достаточно уметь работать с git. Плохая новость в том, что нужно следить и управлять тем, когда один этап переходит в другой (рядовой разработчик обычно избавлен от этого).В разных командах этапы релиза и обязанности инженеров отличаются, могут быть перемешаны местами, идти параллельно или вовсе быть упущены. Но я постарался обобщить.ПланированиеТут все понятно: прежде чем делать, команде нужно иметь понимание, что делать. Я бы включил в этот этап написание аналитики и изучение документации, а также подготовку дизайна и вообще все, что идет перед следующим этапом.Разработка + тестированиеВключил в один этап, так как тестирование может идти параллельно с разработкой.Фича-фриз и включение фича-тогловОстанавливается разработка нового функционала. Релизный инженер отводит релизную ветку, включает фича-тоглы выпускаемого условно-готового протестированного функционала. Через инструменты CD (continuous delivery) происходит сборка и раскатка релиз-кандидата на тестировщиков.Регресс и исправление баговТестируется весь функционал, который мог быть затронут в релизе. В случае обнаружении ошибок разработчики исправляют ошибки (bug fixing), но не добавляют новый функционал. MR с исправлением ошибок сначала вливается в trunk и только потом делается cherry-pick в релизную ветку (правило “код в trunk самый актуальный”). После каждого исправления через инструменты CD (continuous delivery) происходит сборка и раскатка обновленного релиз-кандидата на тестировщиков.Код-фриз и выкатка релизаПо результатам регресса принимается решение, какой функционал готов выпускаться на пользователей, для остального релиз-инженер отключает тоглы обратно. Запускается сборка и раскатка релизной сборки в магазин приложений на бета-тестировщиков или сразу на всех пользователей.Более полноевидеопро так называемый “релизный поезд” сделали те же коллеги из hh.Релизы и версионирование руками разработчикаНа схеме ниже я попытался привести пример общей схемы работы с релизными ветками и версиями с использованием модели Trunk-Based Development.Вариант версионирования в Trunk-Based Development.Здесь видим известные Android-разработчикам логотипы - Google Play и Firebase. Обычно приходится работать с обоими сервисами или их аналогами. Первый, как известно, распространяет новые версии приложения пользователям, а также позволяет проводить beta-тестирование на ограниченном круге пользователей. Второй (а точнее Firebase App Distribution) позволяет распространять промежуточные сборки для внутреннего тестирования.Сборки в Firebase App Distribution поставляются как можно чаще - все заинтересованы в том, чтобы новый функционал получил обратную связь как можно раньше. Сборки в Google Play, наоборот, попадают в идеале один раз в релиз или чуть больше, если пришлось срочно исправлять критическую ошибку (hot fixing).Обратите внимание на изменения версий в примере: minor версия поднимается в момент отведения релизной ветки; patch версия поднимается после каждого исправления (или набора исправлений) в релизной ветке. В разных компаниях это может отличаться, но этот подход кажется мне наиболее логичным и соответствующим семантическому версионированию. Сборки конфигурируются через build variants, которые являются комбинациями build flavors и build types. Через них можно задавать и комбинировать различные константы, правила сборки и подписи  в зависимости от назначения сборки. Подробнее о них можно почитать вдокументации.Зачем подписывать приложения и что нужно помнить?Любая сборка для Android, apk или app-bundle имеет электронную подпись (даже если вы просто нажимаете на зеленый треугольник в  Android Studio - в таком случае используются ключи подписи по-умолчанию, сгенерированные на вашем устройстве). Подпись - это неотъемлемая часть безопасности ОС для пользователя. Например, с помощью нее устройство не позволит установить приложение с тем же package name, но другой подписью (без отдельного разрешения от пользователя). Здесь мы не будем подробно раскрывать этот вопрос, почитать можно также вдокументации.Но всем нужно понимать важность безопасного хранения ключей подписи для production сборок и паролей к ним - они не должны храниться и генерироваться локально у кого-либо (да, в том числе у тимлида). Желательно генерировать их в облаке и хранить там же в зашифрованном виде, соответственно, релизные сборки могут собираться и подписываться только в облаке.Как сделать, чтобы при сборке в приложение не попадали лишние константы и библиотеки?Нужно следить за тем, чтобы в сборку попадало только то, что для нее нужно. К примеру, любопытствующему, исследующему обфусцированный код вашего production приложения, ни к чему показывать адреса develop-стендов вашей компании.flavorDimensions(""sign"")
productFlavors {

    /**
     * Строки в buildConfigField должны быть обернуты в кавычки
     */
    fun buildConfigFieldStringValue(source: String): String = ""\""$source\""""

    create(""dev"") {
        isDefault = true
        // разные подписи для dev и prod сборок
        dimension = ""sign""
        signingConfig = signingConfigs.findByName(""dev"")

        buildConfigField(
            ""String"",
            ""BACKEND_URL"",
            buildConfigFieldStringValue(""https://mybackend-dev.url"")
        )
    }

    create(""prod"") {
        // разные подписи для dev и prod сборок
        dimension = ""sign""
        signingConfig = signingConfigs.findByName(""prod"")

        buildConfigField(
            ""String"",
            ""BACKEND_URL"",
            buildConfigFieldStringValue(""https://mybackend-prod.url"")
        )
    }
}Таким образом, во время сборки будет сгенерирована строковая константа BACKEND_URL с правильным значением (скрипты для запуска сборки приведены ниже).Это же относится к библиотекам. Например, если ваше приложение будет распространяться через несколько экосистем и потребует разных зависимостей для разных типов сборок. В примере показано, как “подсунуть” разные имплементации библиотек приема push-уведомлений в сборки для Google и Huawei (рекомендую делать сразу так, это избавит вас от переписывания, когда до вас сверху дойдет решение публиковаться в новые магазины).// app/build.gradle.kts
// Создается новый flavorDimension, отвечающий за провайдера сервисов
flavorDimensions(""sign"", ""provider"")
productFlavors {

    create(""google"") {
        dimension = ""provider""
        isDefault = true
    }

    create(""huawei"") {
        dimension = ""provider""
        isDefault = false
    }
}

dependencies {
		// Создается 1 api модуль и 2 модуля impl, содержащие имплементацию api
    implementation(project("":core:push-messaging:api""))
    ""googleImplementation""(project("":core:push-messaging:firebase-impl""))
    ""huaweiImplementation""(project("":core:push-messaging:hms-impl""))
}Более подробно про этот подход можете послушать, например в этомвидео.Тогда разные имплементации будут “подсовываться” во время сборки, для этого в команде сборки нужно указать dimension:// Запускается сборка aab-артефакта с dimentions prod и google 
./gradlew finuslugi-app:bundleProdGoogleRelease
// Запускается сборка aab-артефакта с dimentions prod и huawei 
./gradlew finuslugi-app:bundleProdHuaweiReleaseКак отличать частые сборки, которые прилетают в Firebase для внутреннего тестирования?Для сборок в Firebase можно делать синтетический versionName для того, чтобы отличать частые сборки друг от друга - в примере добавляется –dev- и дата сборки.flavorDimensions(""sign"", ""provider"")
productFlavors {
    create(""dev"") {
        val buildDate = java.text.SimpleDateFormat(""dd.MM.yy-HH:mm"").format(Date())
        versionNameSuffix = ""-dev-$buildDate""
    }
}Также к сборкам в Firebase можно прикреплять changelog. Обычно это описание недавно влитых MR, его делают на основе тэгов (git-tags). Ниже прикрепляю более простой вариант сбора списка влитых MR, влитых за последний день (настраиваемо). На практике этого хватает, чтобы тестировщики могли ориентироваться в сборках.git log --pretty=format:""%an : %s"" --since=""1 day ago"" --no-merges >> release-notes.txt || trueЧто лучше не полениться сделать с первых дней?Рекомендую с первых дней сделать для сборок в Firebase другой packageName, чтобы позже была возможность на одно устройство ставить production и develop сборки. (см. applicationIdSuffix вдокументации)Также лучше сделать разные проекты для production и develop сборок в Firebase с добавлением разных конфигурационных файлов с разными credentials для разных buildTypes. Это позволит без дополнительных костылей отделять события события аналитики, крашей (Firebase Crashlytics) и тоглы в Firebase Remote Config.Для develop сборок можно хранить сгенерированный ключ подписи прямо в проекте - это будет гарантировать, что вне зависимости от источника сборки (локальная машина разработчика на первых порах или CI) переустановка приложения на устройстве тестировщика не снесет сохраненные данные предыдущей установки.ЗаключениеНадеюсь, этот пост окажется полезным тем, кто хочет осознанно участвовать в поставке приложения, и избавит от неловкости в момент осознания, что его зона ответственности стала шире окна редактора IDE, как когда-то было и у меня. Конечно, тема поставок шире и глубже, чем то, что я тут охватил.Что можно изучать дальше?Тем, для кого погружение в тему непрерывных поставок в мобильной разработке актуально прямо сейчас, рекомендую следующие источники:- Уже несколько раз упомянутый канал коллег изhh.ru“Охэхэнные истории”- Интереснейшее видео с конференцииPodlodka“Детектим и автоматизируем рутинные задачи в Android” от Сергея Боиштяна.Благодарю Сергея Боиштяна за обратную связь, которая помогла сделать этот пост (надеюсь) более последовательным и понятным."
СберМаркет,,,Как мы создали единый электронный архив документов MOEX Group на основе open source,2022-08-09T06:26:06.000Z,"Приветствую! Меня зовут Владимир Суровцев, я руковожу управлением развития внешних клиентских сервисов на Московской бирже. В нашем управлении созданы и развиваются:сайты интернет-представительства, главный их них -https://www.moex.com;личные кабинеты корпоративных клиентовhttps://cabinet.moex.com;образовательная платформа, с помощью которой была создана Школа Московской Биржи -https://school.moex.com;платформа цифрового опыта (DXP);low-code платформа для построения UI;платформа управления цифровыми активами (ECM/DAM), на базе которой мы создали электронный архив документов - о нем и пойдет сегодня речь.До появления единого электронного архива документов, в компании развивались локальные хранилища в разных технологических решениях с разными зависимостями как от внутренних ресурсов, так и от внешних. Такой подход был распространен повсеместно: по мере развития систем, работающих с документами, рядом с ними (а то и в них), обязательно появлялось место для прихранивания бинарников. Не говоря уже о том, что все атрибуты по документам были доступны только в системе, которая первая стала работать с ним. Более того, разные подразделения могли запрашивать и обрабатывать одни и те же документы от клиентов, что способствовало ухудшению не только employee, но и customer experience. Понятно было, чем дольше мы с этим живем, тем больше накапливаем долги, с которыми нужно будет разбираться. А самое главное - эти долги можно продолжать накапливать, пока мы не будет готовы затронуть customer experience. В итоге отрицательное влияние на опыт клиента оказалось точкой, с которой что-то нужно было делать.Перед нами встала задача свести все документы компаний группы в одном месте для оперативного доступа к одной единице цифрового образа документа. Ну, раз к одной единице, понятно, что нужно устранить и боль с разъехавшимися дублями, запрашиваемыми от одних и тех же корпоративных клиентов в разные точки группы. Плюс к тому, обеспечить сквозной поиск, историю изменений, вложенности, уровни доступа и хранения, и конечно, расширяемый атрибутный состав.Вроде всё, да нет: компания существует уже давно, накоплен огромный цифровой опыт, объем цифровых активов, образы существующих документов, программного обеспечения по разным подразделениями, которые их обрабатывают, новые типы документов, которые стоят в очередь на перевод в электронный вид.И вся эта история приходит ко мне в формате: а не посмотрел бы ты на это? Кстати, если ты готов, нужна жизнеспособная концепция через 2 месяца на архкоме.У меня даже не было мыслей, что ответ может быть каким-то другим. Непонятная, жутко амбициозная задача с короткими сроками - конечно, готов ?Итак, как мы создали электронный архив документов в компании Московская Биржа.Поиск решенийКлючевым фактором успеха оказалась поддержка со стороны команды: нашлись такие же энтузиасты, и нам хватило пары недель для четкого понимания, как это должно быть.Dream TeamПервым делом, мы сами прошерстили рынок таких решений. На этом этапе познакомились с таким классом ПО как Enterprise Content Management (ECM), системами управления корпоративным контентом. Посмотрели на решения и российского рынка, и зарубежного. Поняли, что существует множество проприетарных решений и совсем небольшое число открытых.Мало того, большинство российских поставщиков использовали в своих системах открытый зарубежный софт Alfresco. Да-да, люди из этой компании сделали огромный вклад в развитие корпоративных систем по всему миру. Помимо того, что на основе их решений появилась львиная доля российских компаний в секторе ЭДО, практически в каждой компании стоит платформа автоматизации бизнес-процессов (BPM) Camunda, форк от системы BPM Activiti, которая до сих пор развивается в Alfresco.Ну да ладно, хватит расхваливать Alfresco - тем более, что мы не взяли их решение за базис ?. Конечно, мы собрали, установили и протестировали Alfresco ECM Community версию, но все-таки это скорее промо, чем реальное решение для корпорации. Для наших целей нужна была Alfresco ECM Enterprise по подписке, где поддерживалось горизонтальное масштабирование. И мы стали смотреть прямых конкурентов Alfresco - Hyland, Nuxeo.Nuxeo тоже предлагала рынку решение с открытым кодом, при этом версии Community и Enterprise оказались одинаковыми Единственная разница в том, что в Enterprise-подписке новая версия оказывалась быстрее на 3 месяца, также появлялась возможность использования low-code платформы Nuxeo Studio.При этом, когда мы протестировали под нагрузкой Alfresco ECM Community и Nuxeo ECM Community, решение от Nuxeo оказалось устойчивей и быстрее в разы. Единственный недостаток Nuxeo - не с кем поговорить ?. В России нет поставщиков и контента по этой платформе. Но все же количество плюсов явно перевешивало,Два прекрасных решения ECM от HYLANDпоэтому мы отрисовали концепцию, представили ее на архкоме и...Концептуальная схема ECM в компанииНам поверили! С осторожностью, с ограничениями, но поверили! Договорились, что мы можем использовать это ПО при первичной поддержке со стороны самого производителя, можем разрабатывать прототип и параллельно вести переговоры по получению Enterprise подписки.Прототипирование и MVPДля прототипа нам необходимо было максимально быстро и дешево развернуть ПО в ландшафте IT с минимальными серверными ресурсами. Мы взяли легковесную, компактную архитектуру ECM с использованием Redis-а:Компактная архитектура ECM NuxeoИмейте в виду, что такая архитектура годится только для прототипов и MVP, здесь вы не получите истории сообщений в системе и потеряете текущие стримы при сбоях из-за ограничений работы с оперативной памятью.Получив рабочий прототип, мы стали подключать к нему внутренних клиентов и поставщиков документов, чтобы добиться минимального жизнеспособного продукта. А когда добились успеха, получили интерес к этой услуге со стороны других подразделений.На полноценную реализацию (oт прототипа до результатов по MVP) у нас ушло порядка трёх месяцев Все это время мы планово вели переговоры с производителями Nuxeo ECM по получению Enterprise подписки (а она складывается очень индивидуально). При этом постепенно осознавали, что справляемся и без нее (мы сразу пошли путем сборки образов в инфраструктуре компании из исходных кодов, что сыграло нам на руку в текущих реалиях).В итоге Nuxeo вошла в состав холдинга Hyland, Alfresco - тоже, а услуги компании Hyland теперь вообще не могут быть предоставлены в нашем регионе. А мы к этому времени уже запустили процесс установки распределенной архитектуры ECM под целевой Единый электронный архив ???Мы оказались на развилке: продолжать проект в текущей конфигурации или закрыть от греха подальше и начать всё заново? Сначала поэмоционировали, но все опасения исчезли, как только мы задали себе вопрос: что мы теряем и/или что мы недополучаем?Что теряем?Ничего. Лицензия Apache 2.0 дает ту свободу использования, которая нам нужна.Проверка и сборка исходников внутри компании дает гарантии безопасности и того, что собранный софт будет работать до тех пор, пока будет надлежащая поддержка внутри.Что недополучаем?1. Поддержку по подписке от производителя на 2 года.Да, наращивать экспертизу в компании становится сложнее, но все-таки возможно - особенно с учетом того, что за время работы с прототипом и MVP мы уже начали наполнять багаж знаний. Достаточно минимально расширить команду, чтобы ускорить наполнение. Да, появляется задержка с доступом к новой версии (версия, которая появляется по Enterprise подписке, становится доступной в Community только через 3 месяца). Для нас это может быть существенно только с точки зрения безопасности, но поскольку мы планово проходим penetration тесты и у нас всегда есть состав для устранения выявленных уязвимостей, задержка в обновлении не критична.2. Доступ к low-code платформе Nuxeo Studio. Да, теряем удобный drag-and-drop интерфейс для обогащения бизнес-функциями ECM, но базовые сценарии и настройки для работы с бизнес-потребностями уже внесены в корпоративный репозиторий в виде конфигурируемых файлов. Так что мы можем добавить описание и конфигурировать через инженера.Проведя оперативно RFI, по результатам которого мы не обнаружили новых интересных предложений на рынке, а стоимости текущих проприетарных российских решений удивили своей быстрой реакцией на уход компании Hyland (Alfresco) с российского рынка, мы решили продолжить развитие электронного архива до целевого уровня с использованием ECM Nuxeo.Неубиваемый единый электронной архивДля разворачивания неубиваемого единого электронного архива под нужды всех подразделений компании, мы взяли за основу распределенную архитектуру с Kafkой и ZooKeeperом:Распределенная архитектура ECM NuxeoИ получили конечный набор расширяемых функций:конфигурируемые метаданные;вложенность документов и их образов, папок, типов объектов;версионирование всего и вся;интеграция с текущими ПО компании:операторские интерфейсы;криптосистемы;корпоративные сервисы аутентификации;текущие системы ЭДО;станция сканирования;логирование и аудит в соответствии с требованиями ИБ;хранение данных в разных сетевых сегментах;сквозной поиск.К маю этого года мы провели всевозможные тесты, завершили миграцию всех MVP и предложили внутренним подразделениям целевое решение как сервис - бери и пользуйся без дополнительных инвестиций. Любые объемы, уровни поддержки и дальнейшее развитие уже заложены в сопровождение системы.Немного статистикиСейчас в Электронном архиве с такой конфигурацией обрабатывается порядка 10 тысяч документов в день, пиковая нагрузка была 88 тысяч документов. Всего в архиве хранится порядка 7 млн документов от различных департаментов, и это всего 0,5 Терабайт. На ближайшие полгода у нас уже есть заказы на перенос из legacy решений документов объемом 3,5 Тб. Т. е. на конец года мы выходим на50 млн документов, и это только начало для всей Группы компаний.При этом каждое событие системы логируется в ELK и передается в подразделение инфо безопасности. Вам может показаться фантастическим, но за год эксплуатации по системе не было зарегистрировано ни одного инцидента.Главный выводза 1,5 года реализации единого корпоративного электронного архива: opensource-продукты потенциально несут в себе большую ценность, но, чтобы ей воспользоваться, нужно инвестировать время команды в исследования, отстаивать их результаты и верить в успех.И о планахПо сути, все только начинается ?У нас в планах:Собственный Low-code конфигуратор (аналог Nuxeo Studio).Продуктовый подход к разработке интерфейса.Автоматизация процесса распознавания и проверки подписи документов.Тесная интеграция с DXP платформой и платформой DevOps.Трансформация Электронного Архива документов в корпоративную платформу управления цифровыми активами.Обязательно расскажем о новых результатах, а сейчас с удовольствием ответим на ваши вопросы.А если у вашей компании появится (а может, уже появился) интерес к такому решению, готовы поделиться накопленным опытом более подробно. Ждём в комментариях или в личке :)"
СберМаркет,,,"Как создать пирамиду из мороженки, если надежды нет",2022-03-30T13:46:03.000Z,"Для организации разработки и тестирования сегодня принято выстраивать пирамиду тестов, это считается мейнстримом. Существуют десятки, если не сотни, вариаций пирамиды, опубликовано много докладов и статей о том, как она должна выглядеть. И почти все эти материалы помогут ответить на вопрос «Как мне построить пирамиду тестирования в новом проекте?».Но что делать, если вы приходите в проект, в котором исторически применялся подход «мороженки» тестирования, когда основную часть проверок закрывали ручным тестированием? При этом компания проходит трансформацию, и от вас ждут, что вы приведёте процессы в соответствие современным практикам и ускорите их?Меня зовут Максим Бугров, я больше 8 лет работаю в тестировании ПО. На Московскую биржу я пришел летом 2021 года на позицию начальника отдела тестирования. Наш департамент преимущественно разрабатывает софт, который связывает клиентов и торгово-клиринговые системы Биржи. И я расскажу, как мы начали превращать мороженку в пирамиду — нас ждал огромный ледник задач.Пирамида тестированияДля начала кратко расскажу, что такое пирамида тестирования и для чего она нужна. Теории я коснусь только вскользь, потому что на эту тему написано достаточно материалов.Пирамида тестирования — это абстракция, которая показывает соотношение видов тестов приложения. Основная цель пирамиды — помочь обнаруживать баги на ранних стадиях, когда их исправление значительно дешевле, чем на поздних.Ключевые утверждения, которые справедливы для пирамиды.Наибольший объем тестов должен быть сконцентрирован на нижних уровнях.Чем выше поднимаемся по пирамиде, тем дороже и медленнее будут тесты.Тяжелых Е2Е-тестов должно быть минимальное количество.В построении пирамиды должны участвовать и разработчики, и тестировщики.Пример пирамиды, которую мы хотим у себя видеть:Исходные данныеТеперь расскажу о ситуации, которая сложилась к началу превращения нашей мороженки в пирамиду, и какими ресурсами мы обладали.Летом 2021 года у нас было:огромный монолит, которому уже больше 10 лет. Активно распиливается на микросервисы;микросервисы для новой функциональности и выпиленные части монолита;5 ручных тестировщиков, которые работали в четырех смежных проектах;30 разработчиков;отсутствие автоматизации тестирования;почти полное отсутствие юнит-тестов для монолита и микросервисов;нехватка ресурсов.То есть, ситуация выглядела так:Классическая мороженка тестирования, когда основной объем тестов сконцентрирован на поздних стадиях, преимущественно в виде ручного тестирования. При этом со стороны бизнеса и пользователей все чаще звучали пожелания ускорить разработку, повысить качество и надежность нашего софта. Чтобы выйти из текущей ситуации, первым делом нужно было разработать стратегию.Первые шаги к светлому будущемуПервые шаги по превращению мороженки в пирамиду были такими.Сосредоточиться на найме дополнительных тестировщиков в команду.Начать выстраивать автоматизацию.Договориться с разработчиками по развитию юнит-тестирования.С наймом тестировщиков больших проблем не было, а вот над двумя другими пунктами пришлось поломать голову.Юнит-тестирование для начинающихС юнит-тестами ситуация обстояла так.Для монолита тесты отсутствовали.Для новых сервисов тесты писали, но мало. При этом у разработчиков не было общего подхода к написанию тестов и выработанных стандартов тестов.Не было возможности автоматически измерять покрытие юнит-тестами.На написание юнит-тестов нужны были дополнительные ресурсы разработки.С монолитом все было, в принципе, понятно – покрывать его юнит-тестами большого смысла не было, потому что в ближайшие два года монолит планировали полностью распилить на микросервисы. Но все-таки новая функциональность еще добавлялась и в монолит. Мы решили, что сосредоточимся на покрытии юнит-тестами нового кода. Так получится сохранить ресурсы и рассудок разработчиков и начать движение в правильном направлении.Чтобы решить задачу с измерением покрытия, мы подключили статический анализатор кода во все наши проекты.Основной проблемой стал вопрос, где найти ресурсы на написание тестов. Нельзя же просто так увеличить время разработки на 20–30 %, бизнес не пойдет на это. К решению  мы подошли с точки зрения общего количества ресурсов, которое требуется на вывод проекта в эксплуатацию. Отсутствие юнит-тестов приводит к тому, что большинство багов мы пропускаем на стадии разработки. Эти баги в дальнейшем съедают ресурсы тестировщиков, девопсов, поддержки, бизнеса и пользователей. Поэтому, если мы будем тратить ресурсы на написание тестов, то на более поздних этапах мы их будем экономить. Идею поддержали, в том числе, разработчики, поэтому мы продвинулись в этой истории.Также мы запустили серию митапов по юнит-тестам, чтобы выровнять навыки всех разработчиков по подходу написания тестов. И к концу 2022 года планируем добиться покрытия нового кода юнит-тестами на 80%.АвтоматизацияПроблемыОсновные проблемные зоны в выстраивании автоматизации были такими.Нужна была команда автоматизаторов, желательно с лидом.Отсутствовал план работ. Существующие тестировщики занимались срочными задачами, им было не до тестовой документации.Первое время все попытки автоматизации не приносили и видимого результата.НаймЯ начал с поиска лида по автоматизации. Стэк был выбран близкий к разработке: Java. Удалось за несколько месяцев найти лида (привет, Саша :)). Вместе с ним определились, что первое время автоматизаторы, которых наймем, будут работать в одной платформенной команде, чтобы они находились на одной волне: вырабатывали общие практики, проверяли код друг друга и были знакомы со всеми частями приложения.Параллельно успешно нанимали ручных тестировщиков и лидов проектных команд, поэтому появилась возможность создать план работ по каждой части приложений. К концу 2021 года количество ручных тестировщиков удалось довести до 20. Даже с учетом двукратного роста команды разработки это соотношение тестировщиков к разработке можно считать приемлемым в нашем случае. Найм автоматизаторов тоже хорошо пошел к концу года, с октября по конец декабря нам удалось набрать 9 человек.Уровни пирамидыПо-хорошему, автоматизацию нужно выстраивать на основе фундамента в виде юнит-тестов, но мы не можем ждать год, чтобы только потом начать работу. Поэтому начали активно покрывать автотестами критичную функциональность каждой из проектных команд.Для начала стали собирать smoke-набор со всех команд, который в дальнейшем объединили в общий smoke нашего любимого монолита. Поскольку это монолит, то уровень интеграционных и API тестов для нас ограничен, поэтому мы преимущественно написали UI-автотесты. Там, где это возможно, для функциональности микросервисов наибольшую часть smoke покрыли API-тестами, а оставшаяся часть пошла на UI-тесты.Теперь поясню, что я имею в виду под интеграционными тестами. Это тесты, которые проверяют конкретную интеграцию сервиса с одной внешней зависимостью (в крайнем случае, с несколькими). Все остальные зависимости мокаются. Из-за глубокой связанности кода внутри монолита и микросервисов (правильней будет назвать их микромонолитами) уровень интеграционных тестов в пирамиде нам дается сложнее всего. Здесь нам начинают помогать разработчики, которые вместе с юнит-тестами первое время будут брать на себя и интеграционные тесты. В дальнейшем команда тестирования также будет подключаться к написанию интеграционных тестов.Стоит немного рассказать и про уровень Е2Е-тестов и наши системы. Бизнес-процессы Биржи могут протекать через множество колоссальных по размеру систем, за которые порой отвечают разные департаменты. Если говорить про автоматизацию тестирования таких бизнес-процессов, то для этого у нас есть выделенная команда, которая сейчас активно покрывает Е2Е-тестами несколько процессов. Часть этих тестов будут собирать из smoke-тестов конкретных систем, в том числе и нашей. Сейчас у нас автоматизировано несколько бизнес-процессов поменьше, которые затрагивают не более двух-трех систем.Где мы находимся сейчас и дальнейшие планыТекущую ситуацию можно изобразить так:Юнит-тестыМы включили quality gate в нашем CI для всех наших сервисов. Активно начали покрывать новый код юнит-тестами. До конца первого квартала 2022 планируем довести покрытие нового кода юнит-тестами до 25 %. А к концу года, как говорил выше, планируем довести до 80 %. Также мы уже провели первый внутренний митап по модульному тестированию, который понравился аудитории.АвтотестыУ нас уже автоматизирован smoke половины команд, который преимущественно состоит из API- и UI-тестов. Критичную новую функциональность также автоматизируем. К концу первого квартала у нас будет автоматизирован smoke всех команд, и мы начнем подступаться к почти неподъемному регрессу!Впереди у нас еще много работы по выстраиванию пирамиды. Нужно начать автоматизировать менее критичную функциональность, расширить покрытие регрессионными тестами, довести покрытие юнит-тестами до планируемых значений. Но мы уже сейчас начинаем извлекать пользу из всех этих активностей: сокращаем наш TTM (time to market) без потери в качестве.Lessons learnedКогда ты смотришь на все эти привлекательные картинки разных пирамид тестирования, то кажется, что вот оно — мы нашли то, что нам нужно. Выглядит всё логично, складно. Ты закатываешь рукава и говоришь себе: «Давайте сделаем это!» А потом ты сталкиваешься с суровой реальностью. Например, юнит-тесты, это, конечно, круто, но кто их будет писать для legacy-монолита, в котором полмиллиона строк кода? А как быть с API-тестами? Можно всё покрыть UI-тестами, но тогда у нас мороженка превратится в автоматизированную мороженку, что имеет право на жизнь, но совершенно не то, что нам хотелось бы получить после изучения best practices. Тебя мало кто поддерживает, часто людям просто не до тебя. А твой опыт постройки пирамиды с нуля лишь отчасти поможет в перестраивании мороженки.Не нужно строить иллюзий, что всё будет легко и просто, раз вы изучили крутые доклады, прочитали модные статьи и сходили на несколько конференций. Признаюсь честно, что такие иллюзии у меня были, и где-то на середине пути, когда они развеялись, я был на пороге выгорания. Благо хороший отпуск, сильная мотивация и переосмысление ситуации всё смогли поправить ?Стоит учитывать, что чем крупнее организация, тем сложнее и дороже даются любые развороты и смены направления. Поэтому старайтесь трезво оценивать свои возможности, возможности компании и объективную реальность. В конце концов, пирамида тестирования — это всего лишь рекомендация, к которой стоит стремиться, но совершенно необязательно соответствовать ей на 100%.ПослесловиеМы еще только в начале своего пути, впереди новые проблемы, но мне кажется, что с самыми сложными мы уже справились! Если в первое время наша команда была настроена пессимистично, то теперь концентрированный оптимизм витает в наших чатиках и митингах.У каждой компании, проекта и команды будут свои уникальные проблемы при попытке выстраивания пирамиды тестирования. Главное, чтобы в ней были люди, которые не готовы мириться с текущим положением дел. Начать этот путь можно даже в одиночку. Если ваше дело правое, то у вас быстро появятся сторонники. Всем благ!"
СберМаркет,,,Теплый резерв Jira и Confluence (на пороге импортозамещения),2022-03-21T12:23:06.000Z,"За почти четырнадцатилетнюю историю использования Jira и Confluence на Московской бирже в них накоплен огромный объем данных: у нас более 350 проектов в Jira и более 200 пространств в Confluence. Не будет преувеличением сказать, что в этих продуктах сейчас работает вся Биржа, а не только айтишники. Оперблок ведет в Confluence чеклисты регламентных операций, бизнес и аналитики пишут и согласовывают функциональные задания. В Jira недавно перевели проектный портал, которым заведует Проектный офис. Фактически продукты Atlassian у нас используются в режиме, приближенном к 24*7. Поэтому вопросы резервного копирования, восстановления в случае сбоя и времени вынужденного простоя уже давно стояли для нас весьма остро. В прошлом году мы сделали теплый резерв Jira и Confluence буквально на коленке, о чем и расскажем в этой статье. Ничего уникального, но тем выше шанс, что наш подход принесет пользу кому-то еще – увы, Atlassian уже начала отзывать лицензии, и неизвестно, что будет дальше.Как былоНа рисунке ниже представлена прошлая архитектура развертывания и механизмы резервирования и восстановления:Как известно, Jira и Confluence хранят пользовательские данные в СУБД и на файловой системе. В случае проблем с основным ЦОДом для запуска Jira или Confluence в резервном ЦОДе нужно было скопировать базу данных и архив с данными на файловой системе на соответствующие сервера в резервном ЦОДе, развернуть базу, распаковать архив, запустить сервис. У такого решения есть недочеты. Первый – можно потерять боевые данные за сутки, если сбой в основном ЦОДе произойдет непосредственно перед снятием копий с базы и/или файловой системы Jira или Confluence. Второй недочет – в случае необходимости восстановления сервиса в резервном ЦОДе счет времени неработоспособности сервиса идет на часы, так как резервная копия базы данных Confluence у нас более 100 ГБ, а архивы данных на файловых системах Jira и Confluence – под 50 ГБ, только копирование и распаковка архивов занимает 2-3 часа. То есть нужно было как-то решить проблему оперативной доставки обновлений данных в резервный ЦОД.Как сталоНа следующем рисунке представлена текущая архитектура развертывания и механизмы резервирования и репликации:Репликация данных, хранящихся в СУБД, сделана стандартными средствами MS SQL Server (см. например). Для репликации файловых систем была выбрана утилита lsyncd – демон, который следит за изменениями файлов в дереве локальной директории, и раз в какое-то настраиваемое время отправляет изменения в удаленную примонтированную локально директорию. В настройках lsyncd можно указать, какие поддиректории игнорировать (например, логи нет смысла копировать). Про lsyncd можнопочитать тут. В части репликации файловых систем мы пошли немного дальше и реплицируем не только директории с данными (аттачменты и т. п.), но и директории, в которые установлены сами Jira и Confluence. Таким образом, обновление на новую версию продукта достаточно сделать только в основном ЦОДе, в резервный все скопируется автоматически. Старые механизмы резервного копирования раз в сутки также работают, этими резервными копиями мы пользуемся при необходимости обновить данные на тестовом контуре. В результате применения данной схемы данные реплицируются в резервный ЦОД в пределах минуты. Подъем сервиса в резервном ЦОДе занимает 5-10 минут, в чем мы убедились во время проведения DR-тестирований в июле 2021 года и в январе 2022 года. Сценарий запуска сервиса в резервном ЦОДе на примере Confluence следующий.Cредствами MS SQL Server на сервере БД в резервном ЦОДе останавливаем репликацию данных.На сервере приложения в резервном ЦОДе прекращаем сетевой доступ к отшаренной папке, где установлен Confluence.Убеждаемся, что в файле confluence.cfg.xml на сервере в резервном ЦОДе указан IP-адрес сервера БД в резервном ЦОДе.Запускаем Confluence.В DNS переключаемся на Confluence в резервном ЦОДе и идем смотреть, что там приключилось в основном ЦОДе.В итоге мы получили возможность перевода сервисов Jira и Confluence в резервный ЦОД в случае проблем в основном ЦОДе за 5-10 минут практически без потери данных. Решение не потребовало материальных вложений на закупку специализированного ПО или железа, все было сделано имеющимися средствами. Теперь нам предстоит решить вопрос с импортозамещением обоих продуктов.  С учетом того, что таких проработанных продуктов в реестрах отечественного ПО нет, задача выглядит непростой. Будем делиться изысканиями и успехами на этом пути."
СберМаркет,,,Финуслуги: продолжение революции,2022-02-16T08:52:22.000Z,"Когда вы в последний раз совершали революцию? Мы — в ноябре 2021 года, когда внедрили кредиты на платформе личных финансовhttps://finuslugi.ru/. Впервые у россиян появилась возможность взять кредит полностью онлайн, не посещая офис банка. При этом пользователь заполняет одну заявку и отправляет её сразу в несколько банков, а затем выбирает лучшее предложение — и не важно, есть в его регионе офис этого банка или нет.Почему Московская биржа занимается маркетплейсом для «физиков», мы немного говорилиздесь. Если коротко: мы давно больше, чем биржа, и успешно работаем на внебиржевых рынках  — для этого у нас хватает и знаний, и технологий. Плюс к тому, одна из наших задач — развивать финансовую культуру страны. Вот мы и развиваем — создавая продукты, которые меняют представление об управлении личными финансами. Иметь несколько вкладов на разные цели, между которыми можно перекладывать деньги в пару кликов, а если хочется инвестировать, тут же можно прикупить облигаций регионов — круто же?Но мы отвлеклись. Сегодняшний материал будет о продукте «Кредиты». О том, как создавались кредиты на Финуслугах, с какими вызовами мы столкнулись и как с ними справились, рассказывают ребята из продуктовой команды: Андрей Кителёв (тимлид команды), Алёна Садовская (лидер разработки продукта) и Павел Кряженков (директор департамента разработки электронных платформ). Спойлер: будет и про командную работу, и про knowledge-management, и про чистую архитектуру.Выбор продуктаИз всей линейки кредитных продуктов первым мы выбрали именно потребительский кредит, и вот почему:у каждого банка уже есть такой продукт;потребительский кредит максимально понятен клиентам;потребительские кредиты популярны, и на их основе можно будет быстрее внедрять новые продукты: кредитные карты, рефинансирование, кредиты под залог.Преимущества потребительского кредита на Финуслугах:Банки:могут обслуживать клиентов из любого региона России;не тратятся на физическое обслуживание клиентов (например, во время подписания документов в офисе);могут сосредоточиться на предложении лучших условий по продукту вместо разработки креатива и концепции продвижения;могут получить большой поток клиентов, которые генерирует сам маркетплейс.Клиенты:могут получить кредит не в том банке, который рядом, а в том, в котором выгодно;могут заполнить одну заявку, отправить её сразу в несколько банков, получить от всех них решение и выбрать наиболее подходящее предложение;могут получить сумму кредита на счёт в любом банке.Начало проектаСтарт работ по кредитам на Финуслугах совпал у нас с переходом от монопродуктовой к мультипродуктовой разработке и с созданием продуктовых команд. Проект был амбициозным, необходимо было из одной большой команды, разрабатывавшей депозиты на Финуслугах, выделить:отдельную продуктовую команду для разработки линейки «Сбережения»;сервисные команды для работы над кросс-продуктовой функциональностью;а также создать новую команду, которая менее чем за год смогла бы запустить MVP, позволивший бы оформлять кредит в любом банке через сайт Финуслуг.MVPПервой ключевой задачей команды было определение функций MVP. Мы собрали рабочую группу из различных участников и партнёров проекта Финуслуги, и по результатам её деятельности команда «Кредиты» разработала универсальный процесс онлайн-кредитования, который учитывает все потребности каждого клиента, соответствует законодательной базе РФ и подстраивается под особенности любых банков. Сначала проанализировали процессы получения кредитов у банков из рабочей группы, после чего провели исследования первых прототипов на реальных клиентах и коридорные исследования с работниками смежных подразделений Московской биржи. В результате сформулировали основные этапы процесса:заполнение короткой анкеты;получение нескольких предварительных предложений от банков;выбор лучшего предложения и заполнение полной анкеты для его подтверждения;получение финального предложения от понравившегося банка;подтверждение финального предложения;выдача денежных средств и отправка отчёта в РФТ (Регистратор Финансовых Транзакций).Попытка купить готовое решение (кредитный конвейер)Сначала мы делали ставку на приобретение готового кредитного конвейера и встраивание его в архитектуру Финуслуг. Изучили рынок, но решения, которое полностью удовлетворило бы нашим нуждам, просто не существует, потому что каждое из них адаптировано под свои определённые цели и не позволяет создать тот универсальный продукт, который был в наших планах. В итоге решили делать самостоятельно — это позволило нам разработать универсальный сервис и не зависеть ни от кого при его дальнейшем развитии. Мы сами принимаем решения по его доработке, в том числе по запросам партнёров и пользователейТехнические решения по ходу разработкиДля нового продукта нам понадобилось разработать несколько новых микросервисов:Обработка кредитных заявок клиента и банковских предложений.Обработка кредитных договоров.Интеграционный шлюз для работы с банками.API для UI-приложения.UI-приложение.Также внесли небольшие доработки в общие платформенные сервисы маркетплейса (уведомления, словари, продуктовый каталог и прочее).О технологическом стеке проекта Финуслуги мыписали год назад, и с тех пор он существенно не изменился. Использование того же стека помогло минимизировать затраты на поддержку нового продукта, а также позволило применитьпрактики InnerSourceдля кросс-командной разработки общих платформенных сервисов и тем самым ускорить внедрение новых фич.Для разработки кредитных микросервисов старались использовать best practices проектирования ПО, а также опирались на опыт разработки существующих сервисов внутри экосистемы Финуслуг, чтобы поддерживать общую целостность и придерживаться единых стандартов в проекте.РазработкаМы ставили перед собой цель обеспечить не только высокое качество продукта, но и простоту его поддержки. Большую роль в этом сыграл подход к написанию документации. Мы уделили ей особое внимание с самого начала проекта. В вики описывалось всё — от верхнеуровневого представления процессов до правил маппинга полей при обработке каждой конечной точки. Периодически мы сомневались, целесообразно ли тратить столько сил на документацию, но по мере усложнения процессов приходило понимание, что оно того стоило:Низкий порог входа для новых участников команды, которые быстрее разбирались в продукте и начинали приносить пользу.Более качественная реализация задач.Минимизация ошибок и оперативное их устранение.Упрощение межкомандного взаимодействия за счёт описанных процессов.А сейчас, на этапе поддержки продукта, в пользе документации мы убеждаемся ещё больше.Также разработчики придерживались принципа коллективной ответственности за кодовую базу. Все merge request’ы (МРы) в обязательном порядке проходили код-ревью, и только после получения минимум двух одобрений код заливали в основную ветку. На проверку каждый день выделяли по полчаса утром и вечером. При выявлении ошибок программирования мы не искали виноватых на основании истории изменения исходного кода, потому что влитый код означал, что команда приняла эти изменения коллективно. Такой принцип стимулировал разработчиков старательно проверять код и не одобрять его просто так.1. ТехтолкиБольшую роль в повышении качества кодовой базы сыграла и практика регулярных встреч-техтолков командой разработчиков. На встречах обсуждали любые вопросы и сложности, возникающие при работе с кодовой базой, а также принципы написания кодовых конструкций, минимизирующие вероятность ошибок за счёт улучшения читаемости и поддерживаемости.Примеры тем:Правила именования и внутренней структуры тестовых методов.Принципы работы с null-объектами, использование классаjava.util.Optional.Способы написания миграционных скриптов БД (Liquibase DSL или SQL).Выбор фреймворка для маппинга DTO.Все договоренности мы записывали в вики на отдельной странице, которая служила разработчикам хорошим руководством при написании кода и значительно уменьшала когнитивную нагрузку, не относящуюся напрямую к решению бизнес-проблемы.2. Проба чистой архитектурыКроме вопросов написания конкретных фрагментов кода на техтолках обсуждалась также и архитектура сервисов. Изначально все сервисы мы писали по стандартному принципу создания веб-приложений из трёх условных слоёв: веб, сервисы и доменная модель. Но по мере усложнения бизнес-логики стало всё труднее расширять существующую функциональность. На помощь пришла концепция чистой архитектуры, о которой написан не один десяток книг. Она нам очень понравилась, хотя опыта работы с ней практически не было.Посовещавшись, решили испробовать новую концепцию на свежем микросервисе для работы с контрактами. В качестве руководства использовали книгу""Get Your Hands Dirty on Clean Architecture""— пожалуй, самое практическое руководство по чистой архитектуре. Автор не настаивает на конкретном способе реализации архитектуры и не позиционирует его серебряной пулей, которая решит все проблемы. Вместо этого он призывает сохранять баланс между архитектурной сложностью и гибкостью решения в зависимости от поставленной задачи.На создание основы сервиса по новому подходу ушло больше времени, чем обычно. Для сервисов со старой парадигмой мы используем Maven-архетип, который позволяет за пару минут создавать заготовку сервиса. А с чистой архитектурой так уже не получалось: доработка под новую концепцию заняла бы ещё больше времени, чем создание сервиса с нуля. Также необходимо было определиться с модульной структурой сервиса, выбрать наиболее подходящую стратегию маппинга и понять, как это всё сочетается с использованием ORM-фреймворка. Не будем сейчас углубляться в подробности, иначе статья превратится в отчёт об использовании clean architecture вместо обзора проделанной нами работы, но с удовольствием ответим на все вопросы в комментариях ?Результат себя оправдал: в период активной разработки, когда над сервисом работало одновременно три разработчика и каждый добавлял новую функциональность, МРы не конфликтовали между собой. Этого удалось достичь явным отделением доменной модели и бизнес-логики от остальной логики — обработки HTTP-запросов, работы с БД, маппингов и т. д. Таким образом, каждый компонент имелтолько одну причину для измененийи при параллельном добавлении новых фич старый код изменялся минимально или вообще оставался нетронутым. Также за счёт уменьшения ответственности и межкомпонентных зависимостей проще писать и поддерживать тесты, что тоже повышает качество продукта.Однако в сравнении со стандартным архитектурным подходом у чистой архитектуры выше порог вхождения (особенно для разработчиков с небольшим опытом работы), но это решается проведением митапов внутри команды и совместным обсуждением всех принимаемых решений.ТестированиеОсновным принципом было тестирование не сервисов или отдельных фич, а продукта в целом. Большое внимание мы обращали на пользовательский опыт. Проверяли функциональность, максимально имитируя эксплуатационную среду: к изменению БД и сбросу состояния клиента приходилось прибегать только в случае крайней необходимости. В тестировании принимала участие вся команда, от разработчиков до владельца продукта, что способствовало большой вовлечённости в процесс создания.Бизнес-мониторингТак как мы планировали выпустить продукт, который будет целевым в ближайшие годы, то уже к моменту его запуска сформировали требования к мониторингу.Первая задача — своевременное отслеживание и реагирование на возможные ошибки со стороны системы для контроля качества её работы с точки зрения клиентов. Мы в любой момент времени видим на дашбордах состояние системы, получаем оповещения при технических сбоях и узнаем о проблемах клиента чаще всего ещё до того, как он обратится в поддержку.В качестве одного из основных инструментов для бизнес-мониторинга мы используем Grafana. Она помогает нам визуализировать показатели системы в различных бизнес-процессах, наличие отклонений в воронке и прочую важную для поддержания качества информацию.Также мы с самого начала стали изучать сильные и слабые стороны нашего бизнес-процесса с помощью детального анализа действий наших клиентов на сайте, результатов их заявок и причин выбора тех или иных продуктов. Это позволяет нам строить дальнейшие планы по развитию.Еще одной важной задачей мониторинга является контроль выполнения SLA — времени ответа со стороны наших партнёров на различных этапах бизнес-процесса. Ведь под капотом оформления заявки происходит взаимодействие с подключенными к маркетплейсу банками, благодаря чему система в онлайне позволяет получить несколько реальных кредитных предложений в рамках одного кредитного продукта каждого банка.Планы по развитиюМы двигаемся итерационно: формируем гипотезы, проверяем их с помощью различных инструментов, и в случае положительного решения ставим в бэклог разработки.В ближайших планах:оптимизировать клиентский путь, минимизировав требуемые от клиента действия и ввод данных;подключить Цифровой профиль, который позволит повысить уровень одобрения и качества предложений от банков;расширить совместно с банками перечень потребительских кредитов;расширить линейку неплатформенных продуктов: предоставить клиенту ещё больше вариантов возможных кредитов.В списке долгосрочных задач — запустить новые типы кредитных продуктов, создать собственный кредитный рейтинг пользователей, разработать подборщик продукта, наиболее точно отвечающий потребностям клиента, и многое другое.Хотите посмотреть, что у нас получилось? Переходите поссылке."
СберМаркет,,,Как быстро писать в Apache Ignite,2022-01-26T08:07:40.000Z,"Что такое квадриллион? Это единица с 15-ю нулями, численность популяции муравьев на планете или 100 световых лет в километрах. А еще этообъем торгов в рублях на Московской бирже за 2021 год.Чтобы достичь такого результата, компания должна быть очень технологичной, очень надежной и очень быстрой. Поэтому более 50% штата Биржи – айтишники, работающие с передовым набором технологий, уровень надежности наших ИТ-систем стабильно составляет 99,99%, а еще мы постоянно разгоняем наши системы и процессы. Об одном из примеров такого ускорения рассказывает Григорий Доможиров, разработчик сервиса Data Grid.– В компании есть куча систем-источников данных с одной стороны и систем-потребителей этих данных с другой. Я разрабатываю сервис, в котором эти данные сохраняются, предоставляя потребителям универсальный интерфейс доступа. Входящих данных генерируется много и происходит это быстро, а мы сохраняем их на скорости свыше 500 тысяч записей в секунду на пике.Каким должно быть хранилище, чтобы выдерживать такие скорости? Во-первых In-memory, чтобы не тормозить о диск, во-вторых, – распределённым, чтобы распараллеливать запись. Такой класс решений называется In-Memory Data Grid, и яркий его представитель — Apache Ignite. Но просто установить его мало – чтобы добиться от него максимальной производительности, нужно правильно его «приготовить». И сейчас я расскажу, как.СодержаниеЗадачаВыбор APIPutData StreamerPutAllВлияние параметров потока, кеша и кластераSQLЗапись с серверного узлаРазмер POJOРазмер batch’аКоличество бэкаповРазмер кластераBinary ObjectДополнительные советыРаспараллельте запись и подготовку данныхИспользуйте CacheEntryProcessorИтоговая памяткаЗадачаБудем складывать в key-value кеш входящий поток записей. Казалось бы, типичная задача и базовая операция, однако есть как минимум три способа это сделать, и рекомендуемый — не всегда самый быстрый. Неважно, откуда поступают данные (у нас это Apache Kafka), важно, что мы хотим обрабатывать максимально большой поток — то есть, добиться максимального количества записей в секунду при ограниченной задержке. Будет обзор API, подводные камни, бенчмарки влияния множества параметров, а также практические рекомендации на основе опыта разработки и года эксплуатации в проде.Выбор APIФормализуем задачу. Пусть данные на входе представлены объектом некоторого класса с рядом полей, для определенности таким: «Value Object» или «POJO».public class Data {
    private byte[] payload;
    private int int1 = 0;
    private int int2 = 0;
    private int int3 = 0;
    private int int4 = 0;
    private int int5 = 0;
    private int int6 = 0;
    private int int7 = 0;
    private int int8 = 0;
    private int int9 = 0;
...
}Тип, имена и значения полей не так важны. Важно, что они примитивного типа, в реалистичном количестве, а объем в байтах можно варьировать. Будем реализовывать такой интерфейс:write(List<Entry<Integer, Data>> data, IgniteCache<Integer, Data> cache);и добиваться максимальной скорости выполнения. кеш будет in-memory (не персистентный), атомарный (не транзакционный) ипартиционированный. Запускать всё будем наGridGainCommunity Edition 8.8.3 – это доработанный аналог Apache Ignite 2.8.3,заявляемыйкак production-ready. В версии Community Edition также бесплатный.PutПервая попытка в лоб: есть жеIgniteCache.put(...)! Заполняли бы мы, например, HashMap — так бы и делали. Создадим кеш по умолчанию:ignite.createCache(""benchmark"");и переложим в него данные:data.forEach(entry -> cache.put(entry.getKey(), entry.getValue()));Каждый объект наполним 200 байтами данных и запустим бенчмарк на кластере из 5 машин (Intel® Xeon® E5-2640 v3 @ 2,60 ГГц с виртуализацией, сеть 1000 Мбит/сек.). Высокопроизводительная распределённая система выдаст… 2555 записей/сек. ?Это никуда не годится, что-то мы делаем не так.Data StreamerВ предыдущих версиях документации было явно написано — не используйте put, используйтеIgniteDataStreamer:Что же, получаем IgniteDataStreamer:final IgniteDataStreamer<Integer, Data> dataStreamer = ignite.dataStreamer(cache.getName());и пишем через него те же данные, не забывая, если нужно, периодически flush’ить. Например, каждые 1000 записей, или настроивIgniteDataStreamer#autoFlushFrequency(long autoFlushFreq):data.forEach(entry -> {
    dataStreamer.addData(entry.getKey(), entry.getValue());
    if (...) {
      dataStreamer.flush();
    }
  });Получаем 228 493 записей в секунду. Другое дело! Задача решена? В общем случае, оказывается, нет. Если в кеше уже содержится значение для записываемого ключа, то новые данные сохраненыне будут.При этом, предложенныйallowOverwrite(true)мало полезен, т.к. вjavadocк IgniteDataStreamer скрывается крайне важный нюанс, умолченный в документации:Это значит, что более свежее значение для ключа может быть перезаписано более старым значением!Более того, вероятно, вы захотите сделать ваш кеш терпимым к потере одного узла кластера настроив 1 бэкап, то есть однократную репликацию.final CacheConfiguration<Integer, Data> cacheConfiguration =
new CacheConfiguration<Integer, Data>(""benchmark"")
.setBackups(1)
.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC)
.setAtomicityMode(CacheAtomicityMode.ATOMIC)
.setCacheMode(CacheMode.PARTITIONED);
ignite.createCache(cacheConfiguration);CacheWriteSynchronizationMode.FULL_SYNCзаставит Ignite считать запись оконченной только когда она завершена на всех узлах, включая реплики. Будьте внимательны, режим по умолчанию —PRIMARY_SYNC. CacheAtomicityMode.ATOMICиCacheMode.PARTITIONED— явно указанные значения по умолчанию.В таком режиме (наличие бэкапов и включенныйallowOverwrite) Data Streamerначинает работатьчерез тот самый медленныйput. Скорость в таком бенчмарке составила ничтожные 23 623 записи в секунду!Тут можно было бы пофантазировать профильтр Блумаи интеллектуальный вызовflush(), но реальность проста: не используйте Data Streamer при неуникальных ключах. А при уникальных ключах — это эффективный API, можно использовать.PutAllТретий способ записи —IgniteCache.putAll(). Как и раньше, для ограничения задержки будем нарезать входящий поток на пачки (batch-и) и писать их последовательно.data.forEach(entry -> {
 final TreeMap<Integer, Data> batch = batchHolder.get();
 batch.put(entry.getKey(), entry.getValue());
 if (...) {
   cache.putAll(batch);
   batchHolder.set(new TreeMap<>());
 }
});Влияние размера пачки мы рассмотрим ниже. А пока запустим привычный тест и сравним со скоростью записи через Data Streamer при различных параметрах, включая частоту повторения ключей. При неуникальных ключах, как мы помним, использовать Data Streamer нельзя, поэтому значения для Data Streamer в таких случаях отмечены серым и приведены исключительно для сравнения.Как видите, при уникальных ключахputAllдаёт меньшую, но сравнимую скорость. При увеличении частоты перезаписи по ключу скорость объяснимо резко растёт: из всех записей с одним ключом в рамках batch’а будет записана только одна — самая свежая. Остальные «схлопнутся» в локальной HashMap. Фактически при 50 %-ном повторении ключей в Ignite будет записана половина от входящих POJO, что и даёт примерно вдвое более высокую скорость записи. Полезное свойство, у нас оно особенно активно проявляется при запуске системы.Обратите внимание, что я использовал именно TreeMap. При использовании HashMap вы получите ценное предупреждение влогах:LT.warn(log, ""Unordered map "" + m.getClass().getName() +
   "" is used for "" + op.title() + "" operation on cache "" + name() + "". "" +
   ""This can lead to a distributed deadlock. Switch to a sorted map like TreeMap instead."");При параллельном запуске несколькихputAllдействительно легко получить deadlock, а в трассировке стека серверных узлов — увидеть вереницу блокировок. Не рассчитывайте на то, чтоputAllникогда не будет вызван параллельно, тем более что это может быть другой процесс на другом компьютере, работающий на том же кластере.Итог таков: если вы пишете данные с уникальными ключами — используйте Data Streamer, иначе —putAll. Или всегдаputAll, если не хотите заморачиваться ;)Влияние параметров потока, кеша и кластераSQLIgnite позволяет настроить SQL поверх кеша и делать к нему SQL-запросы, как будто это таблица. Мы, кстати, этим пользуемся. Дает ли это избыточную нагрузку? Давайте посмотрим.Настроить SQL можно через аннотации в POJO или в конфигурации кеша при его создании, cпособы эквивалентны. Мы используем второй из них, это позволяет применять универсальные классы POJO, не зависящие от Ignite. Включаем SQL:Кодfinal LinkedHashMap<String, String> fields = new LinkedHashMap<>();
fields.put(""payload"", byte[].class.getName());
fields.put(""int1"", Integer.class.getName());
fields.put(""int2"", Integer.class.getName());
fields.put(""int3"", Integer.class.getName());
fields.put(""int4"", Integer.class.getName());
fields.put(""int5"", Integer.class.getName());
fields.put(""int6"", Integer.class.getName());
fields.put(""int7"", Integer.class.getName());
fields.put(""int8"", Integer.class.getName());
fields.put(""int9"", Integer.class.getName());
cacheConfiguration.setQueryEntities(Collections.singleton(new QueryEntity()
       .setValueType(Data.class.getName())
       .setTableName(""Data"")
       .setFields(fields)));Замеряем при разном размере кластера, методе записи, количестве бэкапов, размере batch’а, объёме данных и использовании Binary Object (обсудим ниже):Таблица с результатамиКак видите, избыточная нагрузка сильно различается: от практически нуля до более чем двух раз. В реальности, скорее всего, вы включите SQL только в том случае, если он вам нужен. А с возможным overhead останется только смириться.Запись с серверного узлаНемного теории. Все узлы кластера Ignite могут быть одного из трех типов: клиентский, серверный или тонкий клиент. Тонкий клиент долгое время имел ограниченный API, его я рассматривать не буду. Полагаю, производительность на запись будет близка к «толстому» клиенту. Тип узла задается в конфигурации Ignite при его запуске:Ignition.start(new IgniteConfiguration().setClientMode(true) ...)Основное отличие клиентского узла от серверного в том, что он не хранит данные. Все данные кеша хранятся на серверных узлах.Из этого свойства следует потенциальное ускорение при записи с серверного узла: не нужно лишний раз гонять данные по сети с клиента на сервер. Все вышеприведённые примеры выполнены с клиентского узла. Если вы захотите писать с серверного узла, то советую посмотреть наsingleton serviceиcompute jobs.Результаты бенчмарков при различных параметрах кеша и данных:Таблица с результатамиВидим незначительное ускорение на кластере из пяти серверных узлов и более существенное на кластере из двух.Размер POJOОчевидно, что чем больше весит каждая запись в байтах, тем дольше она будет передаваться по сети, потенциально замедляя процесс записи. Запускаем бенчмарки и получаем:Таблицы с результатамиНе стоит сравнивать линии между собой, важно, что вне зависимости от способа записи и наличия бэкапа по мере роста размера объекта скорость записи ожидаемо падает. При небольших значениях разница особенно существенна.Размер batch’аОдин из параметров — размер записываемой пачки в штуках. Для Data Streamer — это частота flush’а, дляputAll— размер накапливаемой Map. Увеличивая размер пачки мы жертвуем задержкой в угоду вероятного повышения скорости. Давайте посмотрим, действительно ли это того стоит:Таблицы с результатамиТут есть необъяснимая просадка при 10 000, а в остальном скорость растёт, особенно сильно при увеличении размера пачки до 1 000.Количество бэкаповОдин из параметров кеша — количество бэкапов, то есть дополнительных узлов кластера, на которые будет записана реплика каждой партиции данных. Количество бэкапов определяет, сколько серверных узлов кластера может одновременно его покинуть, например, «упасть». Если узлы будут покидать кластер последовательно, то бэкапы будут перераспределяться по оставшимся узлам, чтобы поддерживалось заданное количество.Очевидно, что чем больше бэкапов, тем больше нужно будет выполнить записей, что замедлит процесс записи в кеш.Таблицы с результатамиЗдесь мы видим примерно линейную зависимость дляputAll. Для Data Streamer — более сложную с аномалией на четырёх бэкапах, которую я объяснить не могу.Размер кластераПришло время проверить Ignite на горизонтальную масштабируемость. Поскольку записываемые данные распределяются по всем серверным узлам кластера, увеличение его размера будет распараллеливать запись, теоретически увеличивая скорость. Проверяем:Таблица с результатамиПризнаем наличие горизонтального масштабирования, хоть и с не очень большим коэффициентом.Binary ObjectIgnite хранит данные в своём бинарном формате. Когда мы пишем в кеш объекты нашего класса, они будут в него сериализованы с помощью reflection. Можно ли помочь в этом Ignite’у? Да. Есть механизмBinary Object— обёртки над бинарным представлением. Выглядит вот так:final BinaryObjectBuilder builder = Ignite.binary().builder(Data.class.getName());
builder.setField(""payload"", payload, byte[].class);
...
builder.setField(""int9"", int9, Integer.class);
final BinaryObject binaryObj = builder.build();Чтобы записывать полученный Binary Object в кеш, нужно получить его бинарное представление:IgniteCache<Integer, BinaryObject> binaryCache = ignite.cache(""cacheName"").withKeepBinary();Фактически мы вместо reflection говорим Ignite, что мы пишем объект вот такого класса (указываемString), и в нём есть поля вот с такими названиями (указываемString) с вот такими значениями (передаемObject). Другое полезное свойство Binary Object — вам не обязательно даже иметь класс для ваших данных! Например, можно взятьResultSetкакого-нибудь SQL-запроса и переложить поля из него сразу в Binary Object. Какое название класса передать, на самом деле, не важно. Он не обязан быть вclasspathни у клиента, ни у серверного узла, ни даже существовать в принципе. Существование класса понадобится при чтении из кеша, если вы захотите читать его в виде объектов этого класса, а не через Binary Object или SQL. Отмечу, что признак.withKeepBinary()—это свойство представления кеша. В один и тот же кеш можно писать как через POJO, так и через Binary Object.Настоятельно рекомендую использовать именно сигнатуру с указанием класса третьим аргументом:<T> BinaryObjectBuilder setField(String name, T val, Class<? super T> type)вместоBinaryObjectBuilder setField(String name, Object val)Почему? Дело в том, что Ignite хранит и распространяет по кластеру метаинформацию об известных типах и их полях. Когда для какого-то типа (его название вы передаете вIgnite.binary().builder(typeName)) вы впервые вызовете BinaryObjectBuildersetField(String name, Object val)с новымname, то Ignite выполнитval.getClass(), чтобы запомнить, к какому классу относится полеname. Еслиvalокажетсяnull, это станет невозможным и Ignite будет считать это поле классаObject, что станет ошибкой. ДальнейшиеsetField с val != nullне помогут: изменение типа полей невозможно. кеш будет сломан до перезапуска кластера.Быстрее ли писать через Binary Object? Если сначала переводить объекты Data в Binary Object и потом их писать:final List<Entry<Integer, Data>> data = ...;
final Stream<Entry<Integer, BinaryObject>> binaryStream = data.stream().map(...);
binaryStream.forEach(entry -> {
 dataStreamer.addData(entry.getKey(), entry.getValue());
 if (...) {
   dataStreamer.flush();
 }
});то нет:А вот если подготовить Binary Object заранее:final List<Entry<Integer, BinaryObject>> data = ...;
data.forEach(entry -> {
 dataStreamer.addData(entry.getKey(), entry.getValue());
 if (...) {
   dataStreamer.flush();
 }
});то будет быстрее:Ускорение до полутора раз! Рекомендую, мы всё пишем через Binary Object. Читайте следующий раздел, чтобы узнать, как использовать второй подход.Дополнительные советыРаспараллельте запись и подготовку данныхВыше мы рассматривали скорость записи уже готовогоList<Dаta>илиList<BinaryObjеct>. В реальности его нужно откуда-то получить, вероятно, десериализовать и/или создать BinaryObject, что требует времени. Следовательно, подготовку данных нельзя делать последовательно с вызовомputAll(...), который блокируется до окончания записи. Распараллелить подготовку данных можно с помощью небокируемогоputAllAsync(...), возвращающегоIgniteFuture<Vоid>. ВызывайтеputAllAsync, и пока Ignite трудится над записью, готовьте следующую пачку, чтобы, как только запись завершится, начать новую, не давая Ignite’у простаивать. Примерно так:private IgniteFuture<Void> currentWriteFuture;
public void pollAndWriteLoop() {
  while (!Thread.currentThread().isInterrupted()) {
    final Iterable<byte[]> serializedPojos = pollSerializedData();
    final Map<Integer, BinaryObject> batch = deserializeAndCollect(serializedPojos); //CPU expensive
    currentWriteFuture.get(); //throws IgniteInterruptedException extends RuntimeException
    currentWriteFuture = cache.putAllAsync(batch);
  }
}Обратите внимание, что операцияIgniteFuture.get()ожидаемо блокирующая, но вместо контролируемогоjava.lang.InterruptedExceptionбросает uncheckedorg.apache.ignite.IgniteInterruptedException. Не забывайте про него.Используйте CacheEntryProcessorБывает, что вы хотите для определенного ключа обновить только некоторые поля записи, не изменяя остальные. Может быть, вы даже не имеете полной записи чтобы записать ее любым описанным выше методом. Ни в коем случае не пытайтесь прочитать запись, обновить ее на клиенте и записать целиком обратно. Это и не атомарно, и очень долго. Используйте специальный механизм IgniteEntryProcessor, он позволяет вместо записи отправить в Ignite код, который выполнится на серверном узле, хранящем запись для указанного ключа. Этому коду будет доступно текущее значение и возможность его обновить. Примерно так:public class PojoUpdateProcessor implements CacheEntryProcessor<Integer, Data, Void> {
public PojoUpdateProcessor(byte[] newPayload, int newInt3) {
...
}

@Override
public Void process(MutableEntry<Integer, Data> mutableEntry, Object... objects) throws EntryProcessorException {
  final Data currentPojo = mutableEntry.getValue();
  ...
  mutableEntry.setValue(...);
  return null;
}}Вызывается при помощиIgniteCache.invoke(K key, CacheEntryProcessor<K,V,T> entryProcessor, Object... arguments)и подобных. Ваш класс, реализующийCacheEntryProcessor, должен находится в classpath серверных узлов, либо необходимо включитьpeer class loading. Вне зависимости от того, как вы пишете в кеш, вEntryProcessorможно использовать BinaryObject. Это позволяет, например, написать универсальныйEntryProcessor, который изменяет любые указанные поля любого кеша:Кодpublic class NoArgBinaryObjectMutator implements CacheEntryProcessor<Object, BinaryObject, Object> {
 private static final long serialVersionUID = 1L;

 private final Map<String, Object> updatedNonNullFields;
 private final Map<String, Class<?>> updatedToNullFields;
 private final Map<String, Class<?>> notAffectedFields;
 private final String typeName;

 @SuppressWarnings(""unused"") @IgniteInstanceResource
 private Ignite ignite;

 public NoArgBinaryObjectMutator(Map<String, Object> updatedNonNullFields, Map<String, Class<?>> updatedToNullFields, Map<String, Class<?>> notAffectedFields, String typeName) {
   this.updatedNonNullFields = updatedNonNullFields;
   this.updatedToNullFields = updatedToNullFields;
   this.notAffectedFields = notAffectedFields;
   this.typeName = typeName;
 }

 @Override
 public Object process(MutableEntry<Object, BinaryObject> entry, Object... arguments) throws EntryProcessorException {
   final BinaryObject value = entry.getValue();
   final BinaryObjectBuilder objectBuilder;
   if (value == null) {
     objectBuilder = ignite.binary().builder(typeName);
     notAffectedFields.forEach((fieldName, fieldClass) -> objectBuilder.setField(fieldName, null, fieldClass));
   } else {
     objectBuilder = value.toBuilder();
   }
   updatedNonNullFields.forEach(objectBuilder::setField);
   updatedToNullFields.forEach((fieldName, fieldClass) -> objectBuilder.setField(fieldName, null, fieldClass));
   entry.setValue(objectBuilder.build());
   return null;
 }
}Важная особенность: как я писал выше, когда вы впервые вызываетеsetField(...)для какого-то поля, Ignite распространяет по кластеру мета-информацию о новом поле в указанном типе. Это очень небыстрый процесс, замедляющий запись. Поэтому лучше с первой же записью передать в BinaryObject все ожидаемые поля класса. Для тех, которые пока проставлять не хотите, — со значениемnull.Итоговая памяткаГлавный вывод: самый быстрый способ записи в Ignite —IgniteDataStreamer, но его можно использовать,толькоесли ключи в вашем кеше уникальны, то есть заведомо не может быть обновления записи для уже существующего ключа. Иначе используйте толькоIgniteCache.putAllAsync, распараллеливая подготовку и запись данных — производительность сравнима сIgniteDataStreamer. А также:Пишите черезBinaryObjectвместо POJO своего класса. При этом используйте методsetFieldс сигнатурой из трех аргументов(String, T, Class<? extends T>), а не двух. При первой записи в кеш обязательно добавьте в объект BinaryObject все поля класса, включая те, что равныnull.Пишите настолько большими batch’ами, насколько можете себе позволить.Настройте кеш с умом: включенный SQL, большой backup factor и лишние или тяжёлые поля уменьшают скорость записи.Запись можно ускорить, выполняя её ссерверногоузла. Может помочьsingleton service.Если вам нужно поменять лишь несколько полей из многих, то используйтеEntry Processor.У вас есть возможность горизонтального масштабирования при помощи увеличения количества машин в кластере."
СберМаркет,,,Томас Петерффи. История крестного отца электронного трейдинга,2024-09-22T09:00:42.000Z,"«Эмигрант из социалистической Венгрии сколотил баснословное состояние в США» — любой сразу подумает, что речь идет о великом и ужасном Джордже Соросе. Но нет! Хотим сегодня рассказать о его легендарном земляке, «перевернувшем игру» в торговле ценными бумагами и ставшем родоначальником электронного трейдинга в том виде, в котором мы знаем его сегодня — Томасе Петерффи.Сначала он переучился на айтишника лишь бы не учить английский язык, а затем внедрил электронный трейдинг с помощью предтеч планшетов для трейдинга, работающих на его ПО (это в начале 80-х!)Совесть — неважная замена жвачкеТомас Петерффи появился на свет посреди Второй мировой войны, после которой Венгрия присоединилась к Восточному блоку. Но и посреди социалистической реальности он умудрился обнаружить в себе талант коммерсанта! Так, будучи школьником, он выкупил у вернувшегося из Австрии приятеля дефицитную жевательную резинку и перепродал одноклассникам. Маржа превысила 500%, и глава школы взывал к коммунистической совести маленького спекулянта, но, как Петерффи рассказал спустя годы, в тот момент он физически не понимал, о чем идет речь.Шли годы. В 1956 году, после провала Венгерской революции отец Томаса эмигрировал в США. Дотянув до 21 года, в 1965 Петерффи-младший последовал его примеру, бросив учебу на инженера и рванув за океан навстречу американской мечте. Однако отец не ждал Томаса с распростертыми объятиями и довольно быстро тот обнаружил себя с сотней долларов в кармане один на один со стремительно меняющимся миром 60-х.Дело осложняло то, что он не мог связать по-английски двух слов. После полосы мытарств Томас тем не менее прибился чертежником к нью-йоркской компании, проектирующей хайвеи. Страна нуждалась в новых автомагистралях, и в какой-то момент инженерные расчеты стали немыслимы без ЭВМ. Так в офисе Петерффи завелся итальянский аппарат, известный как Olivetti No.1. Загвоздка была только в одном: чтобы от дьявольской машины был прок, требовались некие загадочные программы, которые никто из сослуживцев Томаса не умел писать, да и не горел желанием учиться это делать.Венгерский доброволец стал идеальной кандидатурой для экспериментов по переучиванию на программиста. Попробуйте представить себе сегодня специалиста, который освоил бы сложную айтишную специальность с той мотивацией, что это избавит его от необходимости учить английский язык. Но таковыми были реалии Америки 60-х, и таковым был Томас Петерффи – человек, впоследствии произведший революцию на фондовых рынках.Вынашивание волшебного алгоритма и первые ЭВМ на Уолл-стритПрошло совсем немного времени, прежде чем Томас оставил проектирование автобанов и посвятил себя разработке софта для Уолл-стрит. Отточив свое мастерство, в какой-то момент Петерффи на все свои сбережения приобрел место на Нью-Йоркской фондовой бирже и начал вести торговлю как индивидуальный опционный маркетмейкер.Если вы смотрели старые фильмы про финансистов тех времен, то перед мысленным взором у вас сейчас могут пронестись залы, набитые брокерами, выкрикивающими что-то на фоне грифельных досок с котировками и тикерные аппараты. Томас Петерффи вступил в игру на стыке эпох и сразу же задумался о том, как использовать мощь транзисторов для автоматизации процессов торговли. По словам Томаса, во время торгового дня он писал код прямо у себя в голове, а по ночам обкатывал его на доступном железе. Около девяти месяцев венгр вынашивал авторский алгоритм определения стоимости опциона, схожий с известной моделью Блэка-Шоулза – и не прогадал. Сделав ставку на эту модель, Петерффи вызвал ажиотаж среди трейдеров, что в начале 80-х позволило ему популяризовать идею компьютеризации торговых залов и по сути стать отцом электронного трейдинга.Дополнительным толчком к компьютеризации послужил человеческий фактор. После регистрации собственной конторы, скромно названной TP&Co (Thomas Peterffy and Company), Томас обнаружил, что ни один из четырех сотрудников, нанятых им для отслеживания рыночных преимуществ при торговле опционами на акции, не справляется с работой по придуманным им алгоритмам.Чтобы покорить мир ценных бумаг, требовалось обрабатывать данные на недоступном для человеческого мозга уровне. Проектировкой компьютеров, отвечающих амбициям Петерффи, занялись инженеры его новой фирмы Timber Hill Inc. Р. В результате на свет появились предтечи планшетов для трейдинга, которые можно увидеть в руках современных биржевиков. К 1983 году Томас пролоббировал использование этих мобильных устройств всеми брокерами, работающими с написанным им программами. Результат был ошеломляющим.Если в прежние времена переоценка опционов осуществлялась несколько раз в день и фиксировалась в аналоговых таблицах илимаксимум выводилась на электронные табло, то планшеты Петерффи позволяли сравнивать значения стоимости опциона с биржевой ценой в реальном времени, что позволяло сотрудникам венгра с легкостью обставлять любых конкурентов. Это был первый фурор, произведенный компьютерами на фондовом рынке – и за ним быстро начали вставать очертания цифрового будущего.«Робота надо кормить» или массовая цифровизация фондовых площадокГлавный вызов, стоящий на тот момент перед Петерффи, заключался в том, как централизовать ценообразование и управление риском для портфелей с деривативами на акции, если они находятся в разных штатах. К 1985 году инженеры Timber Hill Inc решили эту задачу, создав систему, моментально взятую на вооружение Нью-Йоркской, Чикагской и Филадельфийской биржами. А когда доходность трейдеров, использующих эту разработку Томаса, составила 430% за 1986 год, уже никто не мог усомниться том, что на фондовых биржах произошла революция.Хотя к концу десятилетия компьютеризация бирж шла уже полным ходом, не обходилось и без курьезных ситуаций. Так, хотя NASDAQ (National Association of Securities Dealers Automated Quotation) и позволяла использовать компьютеры, трейдеры были обязаны вводить данные в них вручную. Для Петерффи, в распоряжении которого была аппаратура, автоматически ведущая торговлю по алгоритмам Timber Hill Inc, это выглядело анахронизмом и в какой-то момент он тайно подключил свой компьютер к терминалу NASDAQ.Число операций и их прибыльность моментально взлетели вверх. Ходит анекдот, будто биржа резко отреагировала на отсутствие оператора, и Томас поручил своим инженерам собратьавтоматон, который резиновыми пальцамипроворно выстукивать все необходимые команды на клавиатуре. Формально это не противоречило правилам биржи, и представители NASDAQ смогли лишь развести руками.Следующий шаг Петерффи заключался в создании единой сети, позволяющей трейдерам в реальном времени мониторить позиции друг друга. Для этого Timber Hill Inc всего за несколько лет реализовала беспрецедентную по своим масштабам программу по созданию системы передачи на машины брокеров данных с терминалов Петерффи. Скорость транзакций взлетела до небес, а трейдеры, первыми подсевшие на эти инновации, озолотились.Вслед за «Большим яблоком» к ногам Томаса упали торговые площадки Старого Света: не прошло и нескольких лет, как его компьютерные системы начали монтироваться на торговых площадках Лондона, Цюриха и Берлина.Эта экспансия, занявшая примерно 11 лет, принесла Петерффи более 200 млн долларов. И он не только озолотился, но и вошел в историю торговли ценными бумагами. В 90-е большая часть мира оценила плюсы машинных алгоритмов и перешла на компьютеризированную торговлю.Осень патриарха1993 год стал точкой отсчета для перехода Томаса Петерффи из рядов миллионеров в миллиардеры. Основанная им в этом году брокерская фирма заработала для своего создателя баснословные деньги, продавая трейдерам инновационные инструменты для торговли. В какой-то момент компанию хотел выкупить Goldman Sachs (один из крупнейших инвестиционных банков в мире), но сделка не заинтересовала Томаса. Вместо этого в 2007 он сам провел IPO, выручив не менее 1 млрд долларов.Как это часто бывает, в какой-то момент патриарх перестал успевать за вызванным им к жизни прогрессом. Так, когда мир финансовых рынков захлестнул высокочастотный трейдинг, позволяющий совершать сделки с ценными бумагами за доли секунды, доходы компании Петерффи стали стремительно падать, а позиции – ухудшаться.Томас сумел сохранить чистую прибыль своей компании, насчитывающей сотни миллионов долларов в год, однако его личное отношение к новым технологиям стало куда более консервативным. Так, в Сети можно найтиинтервью, где он проводит параллели между современной гонкой за скоростью торговли и гонкой вооружений времен Холодной войны, едва ли не впервые задаваясь вопросом о том, несет ли это хоть какую-то пользу для общества.В этом году Томас Петерффи отметит восьмидесятилетие в своем имении в Палм-Бич, Флорида. Хотя отец цифрового трейдинга давно сам не пишет программ, он по-прежнему владеет 75% акций основанной им компании, а его чистое состояние оценивается Forbes в 25,3 млрд долларов, что делает его богатейшим венгром на планете.Далеко не худший закат жизни для рожденного под звуки бомбежек."
СберМаркет,,,Compute — волшебная пилюля?,2024-09-20T13:07:22.000Z,"При разработке flutter‑приложения может возникнуть задача, в рамках которой придется выполнять какую‑то «тяжелую» операцию над большим объемом данных. Если потратить на нее больше 16 миллисекунд (или 8, если говорим о 120 fps), то пользователи могут заметить небольшое подлагивание при скролле или анимациях. Во фреймворке подготовлена удобная функция compute, которая выполнит нужную операцию в отдельном изоляте в фоновом режиме.Казалось бы, вот оно идеальное решение — как только начинаются проблемы, оборачиваем вызов в compute и продолжаем жить без проблемдальше. В большинстве случаев — да, но иногда этого может быть недостаточно.Это статья будет полезна тем, кто использует отдельные изоляты для объемных вычислений, а особенно тем, кто передает между ними огромные объемы данных. Чтобы разобраться в ситуации, давайте взглянем надокументациюк методу для отправки данных между изолятами.The send happens immediately and may have alinear time costto copy the transitive object graph.Также частично эта история былазатронута в недавней статье, где упоминались разные подходы при передаче данных в изолят:для примитивных типов выполняется сериализация в любом случае (при использовании TransferrableTypeData передается указатель, из которого выполняется копирование в память нового изолята);для heap‑объектов в пределах группы изолятов выполняется копирование внутреннего представления и выделение памяти в памяти нового изолята (без полной сериализации);в случае несовпадения групп изолятов выполняется полная сериализация объекта и повторная материализация его в контексте изолята получателя порта.Если мы говорим о непримитивных типах, точем больше объект, который мы передаем в изолят,тем больше временина негозатратится.А теперь давайте посмотрим насколько большим должен быть объект, чтобы можно было увидеть разницу в частоте кадров, и можем ли мы как‑то повлиять на затраченное время.О способе замеровСкрытый текстВ первой части для всех замеров использовался Macbook Pro 13” M1, 2020, 16 GB RAM. Сборка проводилась на версии dart 3.3.4 командой dart compile exe …. В каждом замере создавался долгоживущий изолят через Isolate.spawn, потом заполнялись данные для передачи разными способами. Перед стартом отправки данных вывожу текущее время в консоль, а первым делом внутри колбэка для изолята вывожу время получения. В каждом замере было по 50 попыток. Количество элементов в коллекциях — 1024 * 1024 * 100 — такое большое число подойдет для того, чтобы показать разницу между способами передачи.Во второй части замеров использовалось стандартный counter app, в котором был добавлен CircularProgressIndicator. Сборка проходила в режиме profile, видео с экрана записывалось с Samsung S21Создание данных производилось до замера следующими способами:final size = 1024 * 1024 * 100;
 1) final list = List.filled(size, 0, growable: false);
 2) final list = List.filled(size, 0, growable: true);
 3) final list = List.generate(size, (i) => 0, growable: false);
 4) final list = Int64List(size);
 5) final list = Uint16List(size);
 6) final list = Uint8List(size);
 7) final rawList = [Int64List(size)];
    final list = TransferableTypedData.fromList(rawList);
 8) final list = '0' * size;
 9) final list = generateObject(3);Результаты замеровВ первой группе выступают обычные целочисленные списки List.filled, List.generated.Среднее, мсМедиана, мсМинимальное, мсМаксимальное, мс95-й перцентиль, мсList.filled(growable: false)714615,534115161347,25List.filled(growable: true)624,78577,53721008980,2List.generated(growable: false)627,14576,53691210943,2Среднее значение, потраченное на передачу таких списков имеет один порядок. Это не удивительно: способ создания списка остается за рамками сегодняшних замеров, а с точки зрения передачи данных результаты сравнимы. Забегая вперед, скажу, что это самый медленный способ передачи из рассмотренных в статье.Во второй группе идутразличные TypedData:Среднее, мсМедиана, мсМинимальное, мсМаксимальное, мс95-й перцентиль, мсInt64List232,32224,5117454319,2Uint16List33,730227457,95Uint8List16,7814115829,1Напомню, что на нативных платформах в int входят 64-битные числа, то есть напрямую List<int> уместно сравнивать с Int64List. Среднее время на передачу у него в 3 раза меньше по сравнению с первой группой. Если ваши данные можно преобразовать к Uint16 или Uint8, то результат будет уже на порядок лучше.В третьей группе будут TransferrableTypedData и String:Среднее, мсМедиана, мсМинимальное, мсМаксимальное, мс95-й перцентиль, мсTransferrableTypedData0,760064String0,040010TransferrableTypedData позволяет передавать readonly данные между изолятами. Они могут быть прочитаны только один раз, но зато их передача O(1), потому что передается только ссылка. По ссылке также передаются неизменяемые объекты (например, строки). Конечно, эта группа безусловный победитель в том, что касается времени на отправку данных между изолятами.Предлагаю ещё посмотреть на смешанный пример. Пусть наш объект будетсодержать следующие поля:class MixedObject {
  final String s;
  final int i;
  final double d;
  final List<String> listS;
  final List<int> listI;
  final List<double> listD;
  final Map<String, int> mapS;
  final Map<String, List<int>> mapSl;
  final List<MixedObject> innerState;
  
  //
}Заполним его случайными значениями, каждый уровень кроме последнего будет содержать по 10 потомков, всего уровней будет 4.MixedObject generateObject(int maxDepth, [int depth = 0]) {
 if (depth >= maxDepth) {
   return MixedObject(
     s: 'abcd' * 10,
     i: 123456789,
     d: 2345.345678,
     listS: [for (int i = 0; i < 750; i++) 'asdfg' * i],
     listI: List.filled(100, 1),
     listD: List.filled(100, 1.0),
     mapS: {'a': 1, 'b': 2},
     mapSl: {'a': List.filled(100, 1), 'b': List.filled(100, 1)},
     innerState: [],
   );
 }

 final innerStates = [
   for (int i = 0; i < 10; i++) generateObject(maxDepth, depth + 1)
 ];

 return MixedObject(
   s: 'abcd' * 10,
   i: 123456789,
   d: 2345.345678,
   listS: [for (int i = 0; i < 750; i++) 'asdfg' * i],
   listI: List.filled(100, 1),
   listD: List.filled(100, 1.0),
   mapS: {'a': 1, 'b': 2},
   mapSl: {'a': List.filled(100, 1), 'b': List.filled(100, 1)},
   innerState: innerStates,
 );
}Если запустим инструменты разработчика и посмотрим на вкладку с памятью, то увидим там следующее:Объем занимаемой памяти при создании такого объектаОбъем занимаемой памяти обычного dart-приложения, без создания объектаПри отправке такого объема в dart-приложении получим в среднем 7мс, а при отправке данных в самом простом flutter-приложении СounterApp уже около 10мс.Среднее, мсМедиана, мсМинимальное, мсМаксимальное, мс95-й перцентиль, мсState(3), macos7,76422723State(3), android10,841062622,1Если добавим несколько анимаций, то этот долгий кадр уже можно заметить визуально.Конечно, я специально растянул размер передаваемого объекта до того, что на телефоне был бы виден этот junk. Но и ваше приложение может делать в рамках этого кадра множество других задач, а у пользователей могут быть не самые новые телефоны, что в итоге может привести к просадкам fps и на меньших объемах данных.ВыводыОтвечая на вопрос в заголовке: да, если вы не передаете большие объемы данных. Ответственность за передачу любых объектов между изолятами лежит на отправляющей стороне. Если отправка идет из главного потока, то вы рискуете получить junk при стечении нескольких факторов: большой объем данных, множество других операций в рамках кадра, слабое железо пользователя.По результатам выше однозначно видно, что пара TransferrableTypedData и String справилась с отправкой лучше всех. То есть если стоит задача максимально быстро передать данные в другой изолят, то эти два варианта — идеальное решение. Вам очень повезло, если ваши данные уже представлены этими типами, но если нет, то задача уже сводится к тому, как максимально быстро конвертировать данные в эти типы. Создание TransferrableTypedData занимает O(n), создание строки, из которой потом можно будет десериализовать объект, тоже не бесплатно.Поэтому если всё‑таки есть потребность передать большие данные, то их стоит разбивать на части, чтобы дать возможность flutter отрисовать новый кадр."
СберМаркет,,,Возвращаться в офис или оставаться на удаленке в 2024 году: опыт «Финама»,2024-09-12T09:56:20.000Z,"Пандемия ковида многих убедила в эффективности удаленного формата работы. Однако у каждой организации остается свое отношение к удаленке: кто-то свободно строит международные команды, кто-то допускает гибридный формат, а кто-то до сих пор только отпускает поработать в пятницу из дома, и то со скрипом. И у обеих сторон, конечно, есть свои аргументы.У нас в «Финаме» обе эти стороны представлены. И мы решили поделиться своей историей внедрения удаленного формата работы, построения ее культуры, а также нашими аргументами за и против.Эта статья будет интересна как руководителям, так и обычным читателям:Руководители узнают, почему организация удаленной работы стоит того и с какими подводными камнями придется столкнуться.А обычные читатели смогут увидеть удаленную работу и гибридные форматы глазами руководителей и лучше определить какой вариант подходит им самим.В первую очередь разговор пойдет про финтех и про организацию работы постоянных сотрудников с трудовым договором; т.е. не про фрилансеров или самозанятых.Формат удаленной работы был в «Финаме» всегда. Однако до ковида рассматривался как вспомогательный к основному: поработать из дома в пятницу, что-то посильное доделать при болезни; но все-таки офис считался эффективнее.В компании не было ни готовой к удаленке инфраструктуры, ни культуры: если бы в 2018-м кто-то предположил, что через пять лет компания будет так активно использовать удаленку, то над ним просто посмеялись бы.Но была (и остается) и другая важная причина не работать удаленно:безопасность. Для «Финама» крайне важны вопросы безопасности данных и устойчивости системы, обеспечить которые проще и надежнее всего было простым заведением всей работы в рамки безопасного периметра.Впрочем, некоторые случаи удаленной работы все-таки были: когда нужно было собрать удаленные команды из офисов разных городов. У нас были команды из Екатеринбурга, Новосибирска, Калининграда — и, хотя все их участники сидели в офисах, сам формат удаленного взаимодействия ими понемногу обкатывался.Непонятно, какие еще потрясения могли бы вытащить нас из пресловутого безопасного периметра, но…Пандемия изменила всеМассовый переход сотрудников на удаленную работу стал для компании настоящим стресс-тестом, а никакого другого выбора у нас просто не было. Как еще помнят читатели, карантин был введен не сразу, он «наступал» волнами. Поэтому у нас просто оставалось какое-то неопределенное время, чтобы оценить масштаб грозящих изменений и быстро подготовиться к ним.Мы организовали выдачу необходимой техники и настроили удаленный доступ для массового перехода на работу из дома. При этом треть сотрудников сами выразили желание работать в офисе — это, правда, было еще до объявления жестких ограничений.Но 30 марта грянул тотальный локдаун. Мы к нему уже были готовы, но потребовалось также подготовить и людей, облегчив им переход.1)    Ключевым инструментом стал, конечно, интранет. Мы открыли на внутреннем сайте коммуникационный раздел, а также подготовили справочные материалы для сотрудников:a)    Разместили подробную инструкцию общения с техподдержкой, оставили ссылку на доступный снаружи портал, внутренний и городской телефонные номера, почтовый адрес.b)    Наиболее подробно мы осветили телефонию: организацию звонков и конференций. На тот момент мы пользовались зумом, скайпом, софтофонами — позже весь этот набор будет унифицирован. Но тогда с помощью инструкций с сайта их можно было изучить и настроить для работы дома.c)    Сделали целую коллекцию материалов для удаленного подключения: советы по настройке нескольких мониторов и сглаживанию шрифтов, решение проблем с паразитной раскладкой клавиатуры, ну и, конечно, отдельный инструкции для WIN 7 и 10, а также мака.d)    Там же расположили инструкции на разные неожиданные случаи: что делать, если аннулируется рабочий пропуск, как менять пароль учетной записи, как переадресовывать почту...2)    Изменение формата работы было стремительным. А мы не хотели, чтобы оно привело к падению эффективности. Поэтому мы предложили сотрудникам отмечать свое ежедневное настроение — и оперативно реагировали на низкие оценки (ниже 4 по пятибалльной шкале). Если становилось понятно, что кто-то унывает — с ним связывались, чтобы решить его проблему. HR-отдел чутко следил, чтобы все хорошо перенесли переезд.3)    Открыли «Кладовую знаний» — раздел с лекциями, записями семинаров и образовательными материалами; ну и развлекательный раздел с болталкой и конкурсами.Коллеги очень хорошо вовлекались в онлайн-активность, а параллельно центральный офис не закрывался — на работу выходили дежурные сотрудники.Ну и какие же выводы мы сделали? Получилось ли сохранить эффективность работы при переходе на удаленку?На самом деле некоторым подразделениям удалосьдостигнуть исторического пикапривлечения клиентов и активов. Это был кратковременный эффект мобилизации времени и сил, зато очень показательный! Во-первых, что мы действительно справились с переходом на карантин. И, во-вторых, что удаленка — вполне приемлемый, эффективный формат работы.Владислав Кочетков, президент–председатель правления«Сначала массовый уход из офиса, потом массовый возврат. Потом понимание, что, во-первых, мы выросли, и в офис все уже не помещаются. И, во-вторых, что работа исключительно из офиса уже устарела. Поэтому взяли курс на развитие гибридных форматов, под которые выделили несколько этажей, на которых организован коворкинг с возможностью онлайн-бронирования рабочих мест.Хотя мне самому комфортнее работать в офисе, для меня тут меньше отвлекающих факторов. Плюс много встреч именно по моему профилю: крупные клиенты предпочитают вживую общаться».Конечно, после выхода обратно в офис перевод многих коллег на удаленку и в гибрид был уже гораздо проще. Ковид породил в нашей организации и культуре необратимые изменения. И на фундаменте пандемийной инфраструктуры можно было свободно строить новый формат организации команд в компании.Удаленная работа и офис: дискуссияКовид был толчком, перевернувшим менталитет многих компаний. После него массовый возврат в офис хоть и случился, но не до конца: выяснилось, что компания выросла и больше в него не помещается. Это тоже пришлось решать: кого-то отправили обратно на удаленную работу, кого-то «пересадили» на гибрид, а несколько этажей офиса выделили под коворкинг с бронированием рабочего места заранее.Теперь большинство наших коллег так или иначе работают либо удаленно, либо в гибридном формате, посещая офис 1-3 дня в неделю.У нас удаленный и офисный форматы работы не слишком различаются ни по уровню дохода, ни по доступности большинства офисных вакансий. Есть, конечно, такие, куда удаленно не возьмут (не возьмешь же на удаленку кассира), но это скорее исключения.Екатерина Грипась, руководитель управления по работе с персоналом«Сейчас это уже обязательный минимум любой ИТ-компании в России. Это уже такая база, что идет фактически по умолчанию. Либо возможность гибрида, либо возможность дистанционно работать — один из главных приоритетов при выборе ИТ-работодателя. Ну а для нас это возможность расширить географию поиска специалистов. Мы теперь не ограничены Москвой и крупными городами, а можем сосредоточиться на поиске самых подходящих кадров».Впрочем, рабочие процессы в компании организованы так, что работающие в офисе сотрудники получают преимущество в вертикальном росте. Неформальная коммуникация играет свою роль — и, хотя полностью удаленный эксперт может отлично развиваться, большинство управленцев работают в офисе.В остальном же в «Финаме» существует ряд аргументов pro et contra. С одной стороны:1.    Удаленка или наличие гибридного формата — этостандарт отрасли, которого от ИТ-компании ожидают по умолчанию. Не предоставить его — значит потерять возможных ключевых коллег.2.    Без нее пропадет возможностьпривлечь к работетех, кто не готов к релокации в города с нашими офисами. А так можно находить лучших людей по рынку и, соответственно, предлагать им комфортные условия работы.3.    Она помогаетоптимизировать затратына офисные площади и время работников (за счет того, что им больше не нужно добираться в офис).На самом деле из-за удаленки возрастает и роль личных встреч. Личное общение теперь гораздо сильно влияет на сотрудников и мотивирует в работе. И мы каждый раз отмечаем, что встречи и офлайн-мероприятия дают мощный толчок инициативности, креативу, взаимодействиям, возможности личного общения.С другой стороны, удаленка также порождает и вызовы, которые приходится решать. Ее сторонники сами признают, что:1.    Удаленная работа — это вызов дисциплине и культуре работника: и чтобы не отвлекаться, и чтобы не перерабатывать. Помните мы написали про взлет показателей у некоторых подразделений? После него случился незначительный, но спад. Удаленка все-таки может быть чревата выгоранием и истощением, поэтому она выдвигает требования кдозировке нагрузок.2.    Это также ивызов линейному менеджменту. Управлять человеком, который сидит от тебя в двух метрах, гораздо проще, чем тем, которого ты почти никогда не видишь. Поэтому удаленка подразумевает увеличение количества точек контакта руководителя со своими сотрудниками. Дополнительные встречи, которые при работе в офисе кажутся избыточными, необходимы в случае удаленки. Важно, чтобы команда ощущала себя как команда, чтобы люди видели друг друга.3.    И, конечно, с большим количеством людей на удаленке приходитсясчитаться с совсем другими рисками. Нам пришлось создавать новые меры безопасности, чтобы справляться с совершенно другим масштабом угроз: перестройка инфраструктуры, новые сервисы и меры безопасности.Сергей Загарский, ИТ-директор«Удаленка — это история про людей. И первый вызов, с которым сталкиваются люди — это вызов к собственной зрелости, к ответственности человека. Потому что когда человек переходит на удаленную работу, то сам начинает управлять своим временем, и управление его эффективностью тоже ложится на его плечи. И от того, насколько человек готов соблюдать трудовой ритм, зависит и результат. В целом для людей с правильным майндсетом удаленка, вопреки традиционному заблуждению, приводит к тому, что человек начинает больше работать. И это уже другой вызов — такое злоупотребление приводит к выгоранию».Ну и, наконец, остаются сторонники офисного формата работы, которые даже в постковидную эпоху не отказываются от своего мнения. Аргументы их таковы:1.    Офисный формат означаетобеспеченную оперативность. Когда возникает проблема, достаточно просто подойти к заинтересованным лицам и заказчикам, обсудить детали и найти решение.2.    Он не допускаетразрыва коммуникаций. Когда работники все в офисе, сразу видно, кто свободен, кто имеет нужные компетенции и может помочь в тот или иной момент. Этот моментальный доступ к знаниям и опыту друг друга невозможно заменить виртуальными встречами.3.Моментальный обмен информацией. Новая информация, изменения в планах или полезные советы распространяются в офисе моментально.4.    Реагирование навнештатные ситуации. В офисе любые перестановки происходят без задержек, без необходимости вызванивать десяток ответственных лиц.5.Онбординг новых сотрудников. Введение новых сотрудников в коллектив происходит быстрее и качественнее. Они видят, как принято работать, учатся на месте, а не через экран монитора.6.Лояльность сотрудников. Работая вместе в офисе, мы становимся настоящей семьей. Это формирует лояльность к компании, которая ценнее любых материальных благ.Николай Саратовский, руководитель DataOffice""Офис – это наша крепость, наша база, где мы работаем как одна большая семья. Здесь каждый чувствует поддержку и участие в общем деле. Только вместе мы сможем достичь великих высот».РезюмеВ локдаун все ушли на удаленку, а потом вернулись — чтобы развить гибридный формат. Сейчас многие возвращают строгую работу в офисе, хотя удаленка остается скорее стандартом отрасли.При этом, удалённая работа хоть и помогает утолить кадровый голод большой организации, но также выдвигает ряд вызовов, с которыми придется справляться: и чтобы ее организовать, и чтобы она эффективно работала.Ну а часть руководителей все равно остаются верны работе строго в офисе, и чем больше времени проходит со времени локдауна, тем больше людей вернется в офис. Хотя прежней, до-ковидной картины мы уже не увидим.А каков ваш опыт?Интереснее/удобнее вам работать удаленно или в офисе? При этом, быть может, вам близки и аргументы противоположной стороны?Только зарегистрированные пользователи могут участвовать в опросе.Войдите, пожалуйста.Какой вариант работы для вас наиболее предпочтительный?66.67%Удаленный4410.61%Офис731.82%Гибридный21Проголосовали 66 пользователей.   Воздержались 5 пользователей."
СберМаркет,,,Роботы до электромеханики: от кузницы Гефеста к мастерской Вокансона,2024-09-08T09:00:34.000Z,"Сегодня ассоциации со словом «робот» отдаляются от зловещей или услужливой человекообразной машины – в конце концов, так называют и бестелесных чат-ботов, и промышленных монстров, далекие от людского облика. Однако начиналось все именно со схожести роботов с представителями вида Homo Sapiens.Так, чешский прозаик и драматург Карел Чапек, впервые использовавший это понятие в 1920 в пьесе «Р.У.Р.», описывал даже не металлические тела, а биологических людей, собираемых на конвейере Россумской фабрики из разных органов для изнурительного труда (отсюда и название: по-словацки robota – это, по сути, каторга). Считается, что авторство термина Карел делит со своим братом Йозефом: графиком и фотографом, объединившем в своем творчестве кубизм с фольклорной эстетикой.Как легко догадаться идея витала в воздухе ревущих 20-х, и в емком слове «робот» братья обобщили и привели к общему знаменателю огромные культурные пласты, связанные с искусственными человекоподобными созданиями. К моменту выхода «Р.У.Р.», мир уже несколько десятилетий зачитывался «Франкенштейном или современным Прометеем» Мэри Шелли и «Островом доктора Моро» Уэллса, однако корни представлений об искусственных людях уходили гораздо глубже.Сцена восстания роботов из постановки пьесы «Россумские универсальные роботы» Карела ЧапекаИз красной глиныСюжеты о «вторичных созданиях» – рукотворных имитаторах тех или иных свойств человека – уходят в глубины веков. В одних преданиях эти существа создавались самими людьми, в других – богами и другими сверхъественными сущностями. Роднило их одно: искусственность происхождения, отсылающая чаще к ремеслам, чем к деторождению.Так, у древних эллинов олимпийцы вдыхали жизнь в Галатею, высеченную из слоновой кости скульптором Пигмалионом. Герой Кадм, а затем и вождь аргонавтов Язон, в считанные минуты разживались армиями воинов-спартов, растущих из драконьих зубов, которыми надо было лишь вовремя засеять поле. Бог Гефест и вовсе беззастенчиво ковал из золота служанок-подмастерий для работы в кузнице, а на пике карьеры смастерил бронзового гиганта Талоса, которого Зевс впоследствии подарил Европе для защиты Крита.В германо-скандинавской мифологии что-то похожее выкинул великан Хрунгнир, слепив из глины гиганта Меккуркальви – согласно «Младшей Эдде», эта машина убийства должна была помочь ему в битве с асом Тором. Впрочем, согласно «Пятикнижию» и «Корану» из глины был сотворен и Адам, а первые люди у скандинавов Аска и Эмблу были вырезаны из ясеня и ивы, что наводит на определенные мысли.Намеки на искусственных людей можно встретить и в египетских, аккадских, шумерских, африканских и индийских мифах. Важно, что в большинстве из них «роботы» созданы в прагматических целях, в основном как прислуга или «чемпионы» на поле брани, не имеющие собственных желаний.В средневековой Европе этот архетипический образ ярче всего выразился в предании о Големе – глиняной фигуре, оживленной каббалистической магией Йехуди Бен Бецалелема. По преданию раввин хотел с помощью Голема защитить еврейскую общину от козней горожан, и здесь интересно то, что рукотворный помощник намеренно создавался свободным от людских условностей и табу, что в итоге привело к восстанию против создателя – звучит как история в духе пионеров научной фантастики, не правда ли?Голем Йехуди Бен Бецалелема – одна из достопримечательностей современной ПрагиПружинные гомункулыДо изобретения транзисторов и генной инженерии оставалось еще несколько веков, и вопрос создания «роботов» дрейфовал из мифопоэтической в прикладную плоскость по причудливым траекториям. Так, создание големов именно в качестве технологии, впервые было описано в Clavicula Salomonis – одном из главных магических гримуаров XVI века. Рука об руку с эзотерическим знанием шли научные представления той эпохи – анимакулькизм Левенгука и Парацельса, а также овизм Бонне подарили нам образы гомункулов, выращиваемых в ретортах с помощью химических, а впоследствии и магнетизационно-электрических воздействий на кровь и сперму.Если подобные эксперименты и увенчивались успехом, то история донесла это до нас лишь в художественной форме: гейдельбергском романтизме Арнима, страхе перед двойниками Гейне, алхимическом гомункуле Гете, «Големе» Густава Майринка и т.д. Однако, помимо оккультно-мифологических преданий и метафор писателей, есть и реальные данные о попытках создания человекообразных машинах разной степени сложности – автоматонах.Судя по всему, пытливые умы грезили идеей создания искусственных помощников едва ли не с момента возникновения первых цивилизаций. Если верить физику-ядерщику Д.Р. Хиллу, работавшему над Манхэттенским проектом, еще в XII веке арабский ученый Аль-Джазари построил каботажный кораблик с командой из механического арфиста, флейтиста и двух набатчиков. Идея оказалась заразительной и, уже в XIII веке некий юноша Фома Аквинский случайно испортил «железного человека», созданного его учителем, схоластом Альбертом Магнусом, по образу и подобию арабского квартета – история якобы вдохновившая Л.Ф. Баума, автора «Озма из Страны Оз» на создание меднотелого Тик-Тока.К той же эпохе относятся зарисовки Виллара де Оннекура – пикардийский архитектор в своем альбоме «Legiere Poupee» изобразил механического Христа, который после взвода пружины, мог передвигаться по улицам, повторяя благословляющий жест на манер деревянных кукол манэки-нэко в средневековой Японии. Пройдет еще два века и эти наработки получат развитие в набросках Леонардо да Винчи, посвященных механическому фехтовальщику.К XVIII развитие классической механики усилиями таких титанов как Галилей, Ньютон, Эйлер, Бернул и Лагранж привело к распространению пружинного привода Петера Хенляйна и маятниковых часов Христиана Гюйгенса. Именно эти механизмы позволили изобретателю Жаку де Вокансону стать отцом многочисленных автоматонов: от юноши-флейтиста, до утки, «переваривающей» еду.Хотя некоторые из предтеч роботов сразу влились в промышленность в качестве суппортов для токарно-копировальных станков и регуляторов уровня воды в котлах, большинство автоматонов служили сугубо развлекательным целям: танцевали, рисовали, играли на музыкальных инструментах. В музее Истории Искусств в Невшателе сегодня можно увидеть «роботов» из той эпохи: Музыкантшу, умеющую играть на органе пять сольных партий, Художника, способного написать три картины и Каллиграфа, освоившего алфавит из 40 символов. Каждый из этой троицы был изготовлен из нескольких тысяч деталей основателем Пьером Жаке-Дро, сегодня более известным в качестве создателя марки швейцарских vip-часов «Jaquet Droz». В некоторых источниках имя сына часовщика, юного дарования Анри Дро, упоминается как давшее начало слову «андроид». Хотя эта версия, скорее всего ошибочная (и из-за нюансов французской фонетики, и из-за того, что слово Androides фигурировало в «Циклопедии» Эфрейма Чэймберса, изданной еще в начале XVIII) совпадение любопытное.XIX век ознаменовался созданием перфокарт (в 1808 они начали применяться в ткацких станках Жозефа Мари Жаккара) и благодаря им конструкторы автоматонов твердо встали на путь, который в ХХ веке подвел их к основам робототехники. Александр Белл, Александр Попов, а также заклятые враги Никола Тесла и Томас Эдисон заложили основы электроники и радиотехники, позволившие создать вначале электронное реле, а затем и ЭВМ. Мир захлестнула автоматизация и фантазии о големах стали принимать современные очертания – однако это уже совсем другая история.Даже спустя 250 лет троица Пьера Жаке-Дро (Каллиграф, Музыкантша и Художник) продолжает творитьЗловещие долины романтизмаХотя автоматоны пользовались популярностью и у аристократов, и у простолюдинов в качестве занятных диковинок, многие относились к механическим людям настороженно и даже враждебно.Разбираясь в причинах, семиотик Юрий Лотман обращал внимание, что автоматон больше похож на человека, чем обычная кукла, и что именно эта схожесть делает различия более тревожными и зловещими. По сути, речь шла о феномене, известном сегодня благодаря Масахиро Мори под именем «uncanny valley» – подспудном отвращении или неприязни, которые вызывают фигуры, не сильно отличающиеся внешне от человека, но и не имеющие полного сходства. Однако причина неприязни к роботам крылась не только в этом.С конца XVIII по начало ХIХ в Европе регулярно вспыхивали бунты против внедрения машин в промышленность, что к 1811 году вылилось в возникновение движения луддитов. Если страх перед контактами с неживой материей, замаскированной под человека, еще можно было назвать плодом коллективного бессознательного, то опасность лишиться из-за роботов своего хлеба насущного стала для представителей многих профессий вполне реальной.В результате, согласно все тому же Лотману, автоматоны объединили в себе «древний миф об оживающей статуе и новую мифологию мертвой машинной жизни», став «воплощенной метафорой слияния человека и машины». Культура пристально следила за этими трансформациями умов, и тщательно фиксировала руками художников и поэтов усиливающийся конфликт между естественным и искусственным, природой и техникой, подлинником и копией. Так, вторя визионерству ХVIII Уильяма Вордсворта и, отчасти, Уильяма Блейка, одним из центральных образов романтизма века XIX стала механическая кукла – ее тень нависает и над творчеством Владимира Одоевского и Александра Грина и многих других.Эрнст Теодор Амадей Гофман же в рассказах «Автомат» (1814) и «Песочный человек» (1816) и вовсе создал негласный канон изображения роботов в литературе и поднял вопрос о том, можно ли влюбиться в антропоморфный механизм, если он будет неотличим от человека (история юного Натанаэля и автоматона «Олимпия»). Этот мотив продолжает будоражить умы миллионов людей и в новом тысячелетии.Автоматон Олимпия в опере Жака Оффенбаха «Сказки Гофмана»Горизонт событийК XX веку круг замкнулся. Когда Карел Чапек впервые написал слово «робот» на странице своей рукописи, искусственные создания мыслились уже почти неотличимыми от людей, а люди на фабриках – стали уподобляться машинам, больше похожим не на очаровательные автоматоны эпохи просвещения, а на выращенных в ретортах рабов-гомункулов.Гефестианская кузница вернулась в облике исполинских цехов с конвейерными лентами, однако суть за прошедшие тысячелетия не изменилась. Создание искусственных слуг и воинов прочно переплелось с устремлением к богоподобию, как возможности разжигать огонь разума в неживой материи. Но никуда не исчезли и страхи: что творение рано или поздно восстанет против творца, и, займет его место, унаследовав Землю.Современная робототехника и технологии искусственного интеллекта все еще отстают от фантазий как паникеров, так и мечтателей, но людям не свойственно сдаваться так легко."
СберМаркет,,,Летняя практика в «Финаме»: чему обучали студентов и что усвоили сами,2024-09-04T14:00:29.000Z,"Этим летом у нас в компании стартовала новая активность: мы провели пилотный проект летней практики со студентами из МИФИ ИАТЭ. Цели были поставлены институтом: поделиться знаниями о работе в современных ИТ-компаниях, показать, как у нас выстроены процессы и с какими инструментами мы работаем. Ну а нам самим хотелось получить опыт организации совместной работы со студентами и молодыми специалистами, а их познакомить с реальными рабочими процессами.Поэтому мы решили отойти от шаблонных практических заданий и предложить студентам реальную, сложную для их уровня задачу.Для нас было важно предложить такой формат практики, который дал бы обучающемуся в вузе студенту возможность поучаствовать в рабочих процессах, близких к настоящим и стал бы переходным звеном между обучением и реальной работой в ИТ. А также новые знания и навыки командной работы! Ну и, конечно, привел бы новых молодых специалистов в наши собственные команды, помог им определиться с будущим, а нам — научиться помогать этим будущим экспертам становиться на ноги.Ход практикиВся практика проходила в удаленном формате — через чаты и конференции. На дворе 21 век — работа в основном строится именно так, ну и нам хотелось приблизить опыт ребят к действительности. Практика была разделена на два этапа.Первая неделябыла посвящена ряду обзорных лекций по основным направлениям: разработка, QA, аналитика и эксплуатация. Отдельными блоками были даны материалы про использование AI, карьерное консультирование и навыки самопрезентации.В студенчестве может быть сложно определиться с тем, куда дальше прокладывать профессиональный путь. Поэтому мы постарались сориентировать ребят о возможном развитии и перспективах.Вторая и третья неделипрактики были посвящены практическим навыкам. Мы решили не следовать стандартным шаблонам проведения летней практики, а сосредоточиться на реальной задаче с использованием актуальных инструментов. И, конечно, познакомить студентов с опытом наших технических специалистов.Команда разделилась по специальностям: тестировщики, аналитики и разработчики. Задача должна была быть сложной, чтобы не превратиться в легкую прогулку, но и доступной для выполнения за отведенное время. Поэтому мы поставили задачу создания веб-сервиса для поиска финансовых инструментов (скринера). И тут всплыли и сложности, и баги, и даже сломанный сервис перед самой защитой перед менторами.Ребята развернули сервер, параллельно разрабатывая автотесты. Когда сервер настроили, тестировщики отправили нерабочий автотест, после чего сервер заново был поднят с неактуальными данными до внесения нужного коммита.Так как в скрипте деплоя не был указан флаг сборки контейнеров, то при внесении дальнейших изменений в код эти действия не давали никакого эффекта. Ситуацию вынесли на обсуждение, разработчики нашли ошибку (был пропущен флаг --build) и исправили. Сервис продолжил работу уже в автономном режиме.Радостно было видеть, что при нештатном ходе практики студенты быстро включились и решили проблему, обратившись к наставникам. Ну и при этом на реальном опыте узнали, что ошибки и вопросы могут прилетать и ночью, и в выходные. Теперь они будут к этому готовы!Однако, несмотря на все препятствия, студенты с задачей справились, добавив очков к навыкам работы в команде и софт-скиллам.Итоги — для нас и для студентовДля любой компании студенческая практика — это инвестиция в будущее.Во-первых, в свое: специалисты, проводившие практику, сами проделали большую работу: написали лекции, подготовили презентации. Многие из них потом использовали готовый материал для конференций и профессиональных выступлений. Сама же летняя практика позволила студентам осознать, что они готовы с нуля организовать работу команды.«Для себя я еще раз подтвердил, что у меня получается организовать работу команды, выстроить процесс с нуля. Эта практика создает впечатление о бренде компании. Ведь ребята запомнят, что в «Финаме» было прикольно, и друзьям расскажут...А еще такие проекты по работе с молодежью сокращают бюджет на поиск новых сотрудников. У нас в тестировании часто открываются новые позиции, и, в том числе, с помощью практикантов их можно закрывать. Это очевидный плюс, на мой взгляд».Владимир Першуков, Lead QAВо-вторых, в будущее всей отрасли и грамотных специалистов. Уже на третьем курсе ребята смогут понять, как действительно работается по профессии, на что обращать внимание, чтобы получать востребованный опыт. А еще — приобретут софт-скиллы и сориентируются на карьерной лестнице, смогут понять, как им развиваться и в какую сторону.«Летняя практика — классный проект как для нас с точки зрения бренда и привлечения молодежи, так и для студентов.Для студентов очень полезно слушать не только теорию на лекции, но и попробовать на практике сделать кейсы, послушать о финансовой грамотности от лучших спецов в этой области, на практика изучить QA и многое другое.Помимо опыта и знаний, большой стресс для любого студента – выйти за пределы университета и найти работу, а как ее искать, что делает, куда смотреть никто не рассказывает и показывает. Мы для ребят провели HR консультирование, которое сможет помочь им найти быстро свое первое место работы))Нам как компании такое направление очень понравилось, мы много готовились и хотели бы классных студентов забрать к себе и сделать практику в ИТ постоянной и расширить границы по формату, образовательным учреждениям и набирать к себе как можно больше молодежи)И да, есть ощущение, что от этой практики получили удовольствие и студенты, и наши руководители!»Лера Кузьмина, HRBPНу и, в-третьих, в будущее компании. Студент, познакомившийся с нашими процессами и вместе с нашими специалистами решивший реальные рабочие задачи, с большей вероятностью придет в «Финам» уже в качестве стажера, потом молодого специалиста, эксперта, руководителя, ключевого сотрудника и т.д. Летняя практика сегодня — новые грамотные специалисты и прибыльные проекты завтра.«Я никогда не программировал на Java и не занимался веб-разработкой. Благодаря практике освоил этот язык на среднем уровне, а также:1) разобрался, что такое контроллеры и научился их писать;2) узнал, что такое сервисы в Spring Boot и тоже понял, как их писать;3) благодаря swagger узнал про АРI запросы и попрактиковался с ними;4) научился писать документацию для swagger.5) узнал как устроен локальный кэш в Redis и Spring Boot.В заключение скажу, что у меня не было никогда ТАКОГО командного опыта. Для меня это было все в новинку: не один разработчик, а целых 6, это удобно, интересно и, самое главное, практично. И не могу не сказать про Postgres, Docker и Postman, я тоже с ними не был знаком, но теперь я умею этими вещами пользоваться».Мнение одного из студентов, пожелавшего остаться неизвестнымВсе довольны новым форматом. Студенты – потому что погрузились в рабочий процесс и попробовали свои силы в почти настоящих задачах. Их наставники – потому что получили важный организационный и преподавательский опыт. Ну а компания – потому что появился новый формат взаимодействия с вузами по привлечению молодых специалистов. Теперь, когда летняя студенческая практика у нас получилась, мы собираемся масштабировать формат и взаимодействовать с большим количеством вузов.Так что скоро обязательно повторим!"
СберМаркет,,,Про уродов и людей,2024-08-25T09:00:12.000Z,"В 1932 году студия MGM выпустила фильм ужасов «Уродцы» (Freaks) — историю про артистов бродячего цирка. Фильм шокировал зрителей не только сюжетом, но и документальностью — роли в нем сыграли лилипуты, сиамские близнецы, микроцефалы, бородатая женщина, человек-скелет и другие цирковые звезды Америки.Фильм собрал всего $164 000 при бюджете $300 000. Провал фильма стал символом начала конца столетней эпохи популярности в США цирков с живыми диковинами и бизнеса на людях с физическими недостатками и отклонениями в развитии.Сегодня нам кажется непонятной и даже отвратительной тяга зрителей середины XIX — начала ХХ века к грубым шоу, но сам феномен внимания толпы к отклонениям от нормы заслуживает внимания и размышления. Как начинался шоу-бизнес на людях и куда ушла популярность?Куда уехал цирк? Он был ещё вчера…Первые передвижные «шоу уродов» появились на средневековых ярмарках, а «золотым веком» для этой индустрии стал период 1840-1940 гг. Символом этого расцвета стал бизнес американца Финеаса Тейлора Барнума, чье имя воспринимается синонимом мистификации и агрессивного маркетинга. Впрочем, великий Барнум занимался не только цирком. Барнум «вкатился» в шоу-бизнес в 1835 году, купив у промоутеров-шоуменов 80-летнюю чернокожую рабыню Джойс Хет за $1000 — сумму очень солидную по тем временам. Полгода балаган Барнума катался по США, представляя старушку как 161-летнюю няню Джорджа Вашингтона. Слепая и почти парализованная женщина рассказывала публике истории про маленького Джорджа, зарабатывая для шоумена от $1500 в неделю.Сомнения публики развеивал внешний вид Джойс — истощенная старушка, согласно рекламе, весила всего чуть более 20 кг. Сам Барнум впоследствии рассказывал, что поил Джойс виски до бесчувствия, чтобы удалить ей зубы — без зубов она выглядела еще старше. Интерес подогревался рекламой, а когда аудитории поднадоел аттракцион, Барнум отправил в газеты анонимку, рассказав, что Джойс на самом деле искусная кукла, а говорит за нее чревовещатель. Толпы людей приходили на представление еще раз, чтобы выявить обман. И после смерти Джойс продолжала зарабатывать деньги для Барнума — более 1500 человек заплатили по 50 центов за право присутствовать на вскрытии тела «няни Вашингтона». Вскрытие подтвердило, что Джойс человек, а не кукла, и ей не более 80 лет. Барнум тут же запустил слух, что тело подменили, а настоящая Джойс жива и путешествует по Новой Англии. Всего Барнум заработал на Джойс порядка $100 000.Успех с Джойс вдохновил шоумена на новые предприятия, и в 1841 году открывается Американский музей Барнума.Американский музей Барнума, Нью-Йорк, 1853 г.,из цифровых коллекций Нью-Йоркской публичной библиотекиМузей Барнума стал главным развлекательным центром Америки — он сочетал в себе зоопарк, аттракционы и театр. Чего здесь только не было — диорамы и панорамы, макет Ниагары, интерактивные выставки с научными приборами, тир и блошиный цирк, конкурсы красивых детей, дрессированные медведи и тюлени, знаменитая Фиджийская русалка (на деле — чучело обезьяны с пришитым рыбьим хвостом), устричный бар, экзотические животные, театральные представления, индейцы, фокусники, чревовещатели, аквариум с белухами… А гордостью музея была «коллекция» людей-уродов.В музее Барнума гастролировали сросшиеся в области грудины близнецы Чанг и Энг — именно они и дали название термину «сиамские близнецы». Здесь выступала жемчужина паноптикума — карлик Генерал Том Там, талантливый комик, танцор и певец, исполнявший пародии на известных людей.Сиамские близнецы Чанг и ЭнгТолпа валила поглазеть на негров-альбиносов и «Живых ацтеков», «Швейцарскую бородатую леди», чернокожего карлика «Булавочная голова», говорившего на таинственном языке (изобретенном, разумеется, Барнумом), великаншу Анну Свон, «Татуированного человека» Джорджа Костентенуса, на теле которого насчитывалось 338 художественно выполненных татуировок, и другие диковинки.Музей погиб в пожаре в 1865 году. За 24 года он принес Барнуму около $10 млн (примерно $320 млн на сегодня), музей посетило порядка 38 млн человек, при том, что все население США на то время составляло менее 32 млн человек.Бизнес на людях приносил неплохие деньги. Как грибы после дождя, в Америке во второй половине XIX в. появлялись «десятицентовые музеи» — дешевые заведения, в которых всего за 10 центов посетителям показывали экспонаты вроде русалок или следов снежного человека, а гвоздем программы были «странные люди».  Здесь демонстрировали себя как люди с врожденными отклонениями, так и те, кто уродовал себя намеренно, «дикари-каннибалы» из Африки, и выступали артисты низкого, считавшегося уродливым, жанра — гипнотизеры, огнеглотатели, заклинатели змей.Уродские законыОдной из причин популярности фрик-шоу в США исследователи считают принятие во второй половине XIX в. «законов о нищих», которые в 70-х годах ХХ в. получили название «уродских законов» (ugly laws). Эти законы запрещали калекам показываться на публике и тем более попрошайничать, но позволяли участвовать в цирковых представлениях. Современному человеку эти запреты кажутся жестокими и отвратительными, но в них была своя логика. Первый «уродский закон» приняли в 1867 году в Калифорнии, где после «золотой лихорадки» резко выросло количество нищих и сошедших с ума золотоискателей-неудачников. Кроме того, свою роль сыграли и последствия Гражданской войны 1861-1865 годов, когда в города стали стекаться инвалиды и обездоленные. Власти ответили запретами, пытаясь не допустить социальных взрывов. Считалось, что нищие вызывают чувство вины у здоровых, а кроме того, в обществе тогда царили опасения, что, вступая в браки, инвалиды будут портить генофонд.Крао ФариниВпрочем, влияние «уродских законов» на популярность фрик-шоу было, скорее, косвенным. Вторая половина XIX века стала периодом медицинских и научных открытий, колониальной экспансии и «странные люди» вызывали любопытство широкой публики, так и ученых. Вместе с тем, с началом индустриализации люди стали переезжать в города, росло количество рабочих. У людей появились деньги, им требовались развлечения, которые бы хоть ненадолго отвлекли их от тяжелой работы. Шоу человеческих диковинок выполняло одновременно и развлекательную, и просветительскую функции, знакомя простой народ с медициной и наукой и рассказывая о дальних странах.Так, выступавшая в шоу Барнума девушка из Лаоса Крао Фарини позиционировалась как «недостающее звено эволюции» между обезьянами и людьми, подтверждая теорию Дарвина. Фарини страдала гипертрихозом — аномальным ростом волос на теле, и обладала поистине обезьяньей ловкостью.Шоу должно продолжатьсяИтак, для людей с отклонениями от нормы выступления в цирках становились возможностью стать полноценными членами общества. Упоминавшиеся выше сиамские близнецы Чанг и Энг стали гражданами США, богатыми плантаторами, купили рабов, женились на сестрах, у них был 21 ребенок.И хозяева цирков, и артисты считали, что лучше зарабатывать деньги, чем гнить в богадельне или больницах. Вопреки мнению публики, которая считала, что шоумены обращаются с артистами чуть лучше, чем с животными, на деле многие из выступающих были куда богаче тех, кто приходил на них поглазеть. Помимо гонораров за представления, артисты зарабатывали на продаже коллекционных карточках, прибылью от которых они ни с кем не делились. При этом сами гонорары за выступления достигали внушительных сумм.Звезды шоу нередко становились миллионерами, зарабатывая больше хозяев цирка, которым приходилось нести расходы на бизнес. Впрочем, хозяева тоже не оставались внакладе. На момент смерти Барнума в 1881 году его состояние оценивалось примерно в $15 млн (около $500 млн на сегодня).Конец уродливой эпохиК 1890-м годам популярность шоу «странных людей» начала падать. То, что считалось развлечением, стало восприниматься неприличием, в обществе росло движение за право инвалидов на нормальную жизнь. «Дикари», которые еще недавно вызывали любопытство, как диковинка, теряли свою экзотичность — с развитием транспорта люди стали больше путешествовать самостоятельно, и могли своими глазами увидеть недоступные ранее страны.Развивалась медицина, и теперь бородатая женщина считалась не чудом, а патологией — врачи ставили диагноз и предлагали лечение.С окончанием Первой мировой войны с фронта вернулись солдаты, получившие увечье в боях — именно к ним теперь было приковано внимание общества. С появлением радио, кино, а потом и телевидения фрик-шоу окончательно утратило популярность.В 1930-х годах, к выходу фильма «Уродцы» общественное мнение было уже переформатировано, публика словно стыдилась, что еще недавно с удовольствием посещала цирки и глазела на уродов — это, пожалуй, главная причина провала картины в прокате. Общество словно заявляло, что не хочет больше платить за бизнес на человеческих несчастьях.Пройдет еще тридцать лет, и мнение поменяется еще раз. Фильм откроют заново, найдут в нем новые смыслы, и он вдохновит Д. Линча на картину «Человек-слон». «Уродцев» назовут новаторским фильмом, критики увидят в нем не отвращение, но исключительное уважение режиссера к так называемым уродам, которые сыграли в фильме. В этой картине именно люди с ограниченными возможностями проявляют человечность, а здоровые физически персонажи оказываются в итоге моральными уродами.Безусловно, за век общество стало гуманнее. Но любопытство к человеческим странностям не исчезло, а приобрело сегодня другую форму. Есть спрос — есть и предложение, и не бродячие цирки, а интернет и телевидение являют нам постмодернистское возвращение былого развлечения."
СберМаркет,,,Великое мужское отречение или модный приговор,2024-08-11T09:00:21.000Z,"«Красавчик Браммелл, Браммелл, ну кто его не знает, красавчик Браммелл — стиляга с Джермин-стрит» — возможно, именно так звучал бы хит московской группы, доведись ей посетить Лондон рубежа XVIII - XIX вв.Для современной публики денди — синоним модника. Но модный гардероб был лишь частью философии дендизма — коктейля из социального протеста, романтизма, иронии, напускного равнодушия и капельки цинизма. Денди — это порождение революционных 1790-х годов, социальный маскарад выходцев из среднего класса, это «виртуальные» аристократы, которые сохранили кодекс поведения высших кругов, но отменили родовитость.Денди назначили себя борцами с пошлостью, став контркультурой в Англии конца XVIII- начале XIX века. Философия денди — культ собственных эмоций на фоне контроля самообладания. Отсюда и правила денди — «ничему не удивляться», «сохраняя хладнокровие, поражать неожиданностью», «уходить, как только достигнуто впечатление». Отсюда и выражение собственного превосходства через легкий цинизм и иронию. Дендизм оказал огромное влияние на мировую культуру. К денди себя причисляли Байрон, Диккенс, Теккерей, Оскар Уайльд, Бальзак, Стендаль, Бодлер, Марсель Пруст, а в России —  Пушкин, Лермонтов, Чаадаев. «Лишние люди» в русской литературе — Онегин и Печорин — тоже денди, которые «вышли из фрака Джорджа Браммелла».А вот кем был Джордж Браммелл, почему его называли «премьер-министром элегантности» и о каком великом мужском отречении идет речь — сейчас расскажем!Принц и дендиИтак, перенесемся в Лондон рубежа XVIII-XIX веков. Завершается промышленная революция. Машины вытесняют ручной труд, становится доступнее одежда и другие блага цивилизации. Король Георг III впал в безумие. Принц-регент Уэльский, будущий король Георг IV, пользуется репутацией «первого джентльмена Европы», его обожают дамы, письма которым он подписывает именем шекспировского персонажа — «Принц Флоризель».Широкая жизнь принца обходится казне в 120 тыс. фунтов в год, не считая долгов. Георг предпочитает благочестию общество хорошеньких женщин, банкеты и выпивку — и разгульная жизнь вскоре начинает сказываться на его талии. Он стремительно тучнеет — в 1797 году вес 35-летнего принца достигает 111 кг, а к концу жизни перевалит за 130 кг. Поэтому принц вводит в моду темные цвета в одежде, полагая, что черный цвет стройнит его фигуру. Подражая принцу, мужчины носят высокие воротники с шейным платком — такой элемент одежды позволяет Георгу скрывать двойной подбородок.И вместе с тем, будущий король остается дерзким щеголем, вводя в моду элементы на уровне троллинга. Георг носит огромную пряжку для туфель, которая с обеих сторон достает до пола, и облачается во фрак с пуговицами размером с яйцо — и придворные тут же подражают наследнику престола.А еще принц командует гусарским полком, в который в 1794 году зачисляют корнетом 16-летнего Джорджа Брайана Браммелла.Младший офицер с первой же встречи впечатлил принца-щеголя безукоризненными манерами и вкусом. Эта встреча станет началом долгой дружбы, которая завершится так нелепо… Впрочем, не будем забегать вперед.Джордж Брайан БраммеллДжордж не мог похвастать родовитостью. Его дед служил камердинером и сдавал комнаты, но отцу Джорджа удалось сделать карьеру, став личным секретарем главы казначейства. Больше всего на свете Браммелл-старший хотел, чтобы Джордж стал джентльменом, и потому отправил его в Итонский колледж. Но путь Джорджа в высший свет лежал через гвардию. Благодаря покровительству принца Уэльского, Джордж всего за пару лет, к великой зависти сослуживцев, проходит путь от корнета до капитана.В 1798 году полк направили в Манчестер для усмирения бунтов. Тут-то Браммелл решил, что с него хватит военной службы — он подает в отставку и остается в Лондоне.Дружба с принцем Георгом продолжается, и тот вводит Джорджа в свет. Отец Джорджа умер, оставив ему около 30 000 фунтов (примерно $2,5 млн на сегодняшний день). На эти деньги Браммелл снимает дом на Честерфилд-стрит — одной из самых дорогих улиц Лондона, и вскоре зарабатывает репутацию «арбитра моды», и прозвище Бо (от французского Beau — «Красавчик»). Его зовут на все приемы, его совета спрашивают, ему подражают.Великое мужское отречениеВ 1930 году психолог Джон Флюгель выпускает труд «Психология одежды», в которой исследовал моду с точки зрения психоанализа. Отмечая перелом в мужской моде на рубеже XVIII-XIX веков, Флюгель назвал этот период «Великим мужским отречением». Взгляните на портреты аристократов первой половины XVIII века — мужчины щеголяют в одеждах ярких, иногда «вырвиглазных» цветов, носят высокие каблуки, украшают себя кружевами и драгоценностями, наносят макияж и щедро пудрят парики. «Мужчина соперничал с женщиной в великолепии своих одежд, единственной прерогативой женщины были декольте и другие формы эротической демонстрации тела», —  отмечает Флюгель. Собственно, в природе самцы часто окрашены ярче самок, а люди тоже часть живой природы.Но что же произошло? Почему мужчины отказались от павлиньих расцветок и притязаний на красоту и перешли к приглушенным цветам и строгому покрою?Причиной «Великого мужского отречения», в ходе которого мужчины оставили женщинам «привилегию быть единственной обладательницей красоты», считается политика — революции во Франции (1789-1799) и Америке (1775-1783) изменили социальные тенденции. На смену подчеркнутому классовому неравенству пришли идеи «свободы, равенства, братства». Яркая одежда вызывала ненужные ассоциации с потерявшими голову французскими аристократами. С другой стороны, завершение промышленной революции требовало простой и функциональной одежды.Так или иначе, но мужчины оставили яркую моду женщинам, а Джордж Браммел кардинально поменял мужской костюм, заложив его каноны на пару столетий вперед.Икона стиляБудь во времена Браммелла интернет и соцсети, он, безусловно, стал бы блогером-миллионником. Его «модные луки» собирали бы десятки тысяч лайков, а остроумные шутки моментально становились бы мемами. Впрочем, примерно так оно происходило в Лондоне 200 лет назад и без соцсетей.Браммелл стал законодателем мод. Показному богатству в одежде Джордж противопоставил изысканную скромность. Он носит однотонные фраки и жилеты, отказывается от чулок и бриджей в пользу панталон — прообраза брюк, из украшений оставляет только часовую цепочку. Как и французские революционеры, Браммелл отказывается от париков, и носит прическу с коком и баками. В отличие от современников, которые заглушали запах немытого тела «кельнской водой», Браммелл не пользуется духами, но посвящает каждое утро долгому ритуалу — принимает ванну и чистит зубы изобретенным им составом зубного порошка.Он ведет себя как богач-аристократ. Однажды уличный нищий попросит у Браммелла полпенни. «Хм, я слышал от такой монете, но никогда ее не видел», — скажет Браммелл и даст нищему шиллинг.Но подражание аристократии требует значительных финансов. «Полагаю, при некоторой экономии можно уложиться в 800 фунтов» — небрежно ответит Красавчик поклоннику на вопрос, во сколько обойдется такой же гардероб. Для сравнения — хороший ремесленник в те времена зарабатывал 1 фунт в неделю. Наследства не хватало, и Красавчик использует свою репутацию в качестве основы для долговой пирамиды. Любая финансовая система держится на доверии к ней — и Браммеллу оставалось только поддерживать это доверие. Жизнь Браммелла напоминает сюжет рассказа М. Твена «Банковский билет в 1000000 фунтов стерлингов» — у героя нет ни гроша за душой, но все воспринимают его как эксцентричного миллионера, охотно дают в долг, и не требуют возврата. Однажды кредитор напомнил Браммеллу про деньги, на что тот ответил, что долг давно уплачен — «Сидя у окна клуба, я кивнул вам и сказал: ""Как поживаете, Джимми?“».Ходившие о нем легенды только укрепляли популярность — говорили, что он чистит сапоги шампанским, а утром принимает молочную ванну. Это молоко, якобы, потом отправляли на продажу, и многие лондонцы перестали покупать молоко. По слухам, над его перчаткой трудилось несколько портных, каждый из которых шил отдельные детали. «Фишкой» Браммелла стал шейный платок, на завязывание которого он, бывало, тратил несколько часов. Его платки имели вид красивой небрежности, которой никому, кроме него не удавалось достичь. Однажды к Браммеллу, который завершал утренний туалет, заглянул друг, и увидел смятые шейные платки на полу. Слуга, указав на них, произнес: «Это наши неудачи».Модный приговорВыходя в свет, каждый был озабочен одним — что скажет о нем Браммелл? Ирония человека, который не имел никаких титулов, не щадила никого. В какой-то момент он обратил дерзость шуток против своего покровителя — принца Уэльского и близкой ему дамы. Принц обиделся, но был готов помириться. В июле 1813 года Браммелл с друзьями устроил бал, послав приглашение принцу. По традиции, организаторы бала встречали гостей у входа. Войдя, принц поздоровался с одним из друзей Джорджа, но сделал вид, что не заметил Браммелла. И тогда Красавчик бросил вслед фразу, вошедшую в историю: «Кто этот толстяк, ваш приятель?».Позже, когда принц станет королем Георгом IV, журнал «Панч» разместит карикатуру — эскиз памятника Браммеллу, на которой он спрашивает: «Кто этот толстяк?», показывая на статую короля. Слух о ссоре с принцем быстро распространился. Занимать деньги стало труднее, а тут еще добавились и карточные долги. На первых порах Красавчику везло, он верил, что ему помогает талисман — шестипенсовик с дыркой, который он нашел на улице, и носил на цепочке.Но карточная удача отвернулась от него, а последней каплей стала потеря талисмана. «Не иначе, его подобрал Ротшильд» — нашел в себе силы пошутить Браммелл. Проблемы нарастали как снежный ком. Через три года после шутки над принцем долги Красавчика превысили 600 000 фунтов (примерно $80 млн на сегодня).Как и любая финансовая пирамида, выстроенная Браммеллом система должна была однажды рухнуть. Развязка наступила 16 мая 1816 года. В этот день он в отчаянии напишет другу с просьбой одолжить 200 фунтов: «Мои деньги в трехпроцентных бумагах, а банки закрыты. Я верну вам долг завтра». Друг ответит запиской: «Дорогой Джордж, мои деньги тоже в трехпроцентных бумагах». Той же ночью Браммелл пересек Ла-Манш и более никогда не возвращался в Англию.Его ждали долгие годы бедности и прозябания. С принцем они так и не помирились. В 1830 году, перед смертью, Георг IV уступил просьбам друзей Браммелла, назначив его британским консулом во французском городе Кане. Казалось, наступила новая пора в его жизни. Но место консула упразднили, другую должность Браммелл не получил, а кредиторы добились для него тюремного заключения на три месяца.Меж тем, его преследовали головные боли, депрессия, временные параличи — виной тому, как полагают исследователи, был сифилис.Денди, о котором Байрон говорил: «В мире есть только три великих человека — Наполеон, Браммелл и я», умер в психиатрической клинике Bon Saveur 30 марта 1840 года. В последние дни жизни он устраивал воображаемые приемы, беседовал с невидимыми гостями, блистая остроумием, а под конец вечера заливался слезами.В этой истории нет никакой морали — разве что финансовое правило о рисках, которые влечет высокая долговая нагрузка, и от которых не спасут ни высокое положение в свете, ни изящные манеры и остроумие.А как же Джермин-стрит, спросит внимательный читатель. Причем тут эта улица? Лондонская Джермин-стрит веками славилась портными, которые шили для самых важных особ. Джермин-стрит и по сей день известна как улица магазинов мужской моды, и там же стоит памятник великому денди, который однажды сформулировал универсальный закон: «Увидев хорошо одетого человека, люди не должны говорить: “Какой у него хороший костюм!”, пусть скажут: “Какой джентльмен!”»."
СберМаркет,,,"Дурная слава или Сердце, для которого не было ничего невозможного",2024-07-21T09:00:22.000Z,"Жизнь этого человека могла бы стать основой для романа в духе «Монте-Кристо наоборот» — с головокружительной карьерой финансиста и дипломата в начале и несправедливым обвинением, заключением и побегом в конце. Имя нашего героя мало что скажет широкой публике в России, но читатель, безусловно, знаком с ним по роману Булгакова:«— Первые! — воскликнул Коровьев, — господин Жак с супругой. Рекомендую вам, королева, один из интереснейших мужчин! Убежденный фальшивомонетчик, государственный изменник, но очень недурной алхимик. Прославился тем, — шепнул на ухо Маргарите Коровьев, — что отравил королевскую любовницу».Кто же вы, господин Жак, и за что удостоились дурной славы и литературного бессмертия? Впрочем, хватит предисловий — перенесемся в средневековую Францию.Где родился, там и пригодился«Есть много способов сделать карьеру, но самый верный из них - родиться в нужной семье», — в нашем рассказе этот рецепт успеха от Дональда Трампа следует дополнить словами «…и в нужном городе». Рождение в городе Бурже сыграет ключевую роль в стремительном взлете нашего героя.На рубеже XIV-XV веков Бурж пользовался славой крупного торгового центра и «столицы алхимии». В 1420-х годах, в разгар междоусобиц Столетней войны, здесь найдет убежище юный король Франции Карл VII. Для получившего презрительное прозвище «буржский король» дофина Бурж станет домом на долгие годы — Карл вернется в Париж только в 1437 году. Здесь родится его сын — будущий король Людовик XI. И здесь, в Бурже, он подружится со своим ровесником Жаком Кёром — сыном преуспевающего торговца мехами.Удачный брак с внучкой главы Монетного двора открыл Кёру доступ в близкое окружение дофина. Опираясь на деньги семьи и поддержку короля Франции, Кёр воплощает дерзкий план — отобрать часть бизнеса у венецианцев и генуэзцев, которые контролируют торговлю с Ближним Востоком. Кёр строит флот, базой которого становится порт города Монпелье. Его суда везут в Сирию, Ливан и Египет ткани, медь и другие товары, возвращаясь с шелком, пряностями, драгоценными камнями и винами. Торговый гений Кёра делает Монпелье главным торговым городом юга Франции — он уступит этот титул Марселю только в конце XV века.Скульптура во дворце Жака Кёра, г. Бурже. Считается, что это изображение Жака. Источник: wikimedia.orgБогатство Кёра достигает фантастических размеров. Современники не верят, что человек может столько заработать торговлей, и ищут более правдоподобное объяснение. И вновь свою роль играет место рождения Кёра. Бурж — «столица алхимиков», и богатство Жака связано с магией, уверена молва. Согласно слухам, отец Жака едва сводил концы с концами, пока не познакомился со знаменитым алхимиком Раймондом Луллием. Якобы Луллий открыл купцу-неудачнику тайну философского камня, а тот передал Жаку секрет превращения свинца в золото. Правда, Луллий умер в 1316 году, почти за век до рождения Жака Кёра — но тем хуже для фактов!Итак, репутацию мага и алхимика Кёр приобрел. До подозрения в фальшивомонетничестве оставалось совсем немного…Икар из БуржаСтолетняя война, меж тем, катится к концу. Жанна д'Арк разгромила англичан, дофин коронуется под именем Карла VII и в 1437 году занимает Париж. Разоренной стране нужен толковый финансист, и Карл вспоминает о друге юности из Бурже. Жак Кёр богат, прекрасно разбирается в финансах — назначив его главой Монетного двора, Карл не ошибся в выборе.За время Столетней войны страна оказалась наводнена английскими и французскими монетами разных периодов. Став хозяином Монетного двора, Кёр унифицирует выпуск монет, выводя из обращения лишнюю денежную массу — его примеру позже последует глава Монетного двора Англии сэр Исаак Ньютон. В 1438 году он становится аржантье — королевским казначеем, и еще через три года, наконец, получает дворянство. Обыгрывая свою фамилию — Кёр по-французски «сердце» — девизом он выбирает фразу «Для доблестного сердца нет ничего невозможного». И кажется, действительно, для Кёра возможно все.Балкон дворца Кёра в городе Бурже. Выбрав родовыми символами ракушку и сердце, Жак Кёр закодировал в них свое имя. Ракушка — символ святого Якова (Жак), а фамилия Кёр переводится как «сердце». Источник: wikimedia.orgПользуясь полным доверием короля, Кёр представляет Францию как дипломат. Он заключает мирный договор между султаном Египта и мальтийскими рыцарями Родоса, добивается привилегий для французских послов в Леванте, заложив тем самым основы многовекового влияния Франции на Ближнем Востоке. Именно Кёру Европа обязана бескровным завершением почти полувекового Папского раскола — он уговорил антипапу Феликса V сложить сан и помириться с папой Николаем V. Этот дипломатический успех потом сыграет важную роль в жизни Кёра.В Бурже Кёр строит резиденцию, которая, по свидетельству современников, превосходила королевские дворцы. Как причудливо тасуется колода — через два века этот дворец купит другой знаменитый финансист Франции — Жан-Батист Кольбер, и тоже попадет под конец жизни в опалу… Впрочем, о Кольбере мы расскажем в следующих выпусках.Достигнув невероятного могущества и богатства, Кёр не забыл и о своих родственниках. Его сын стал архиепископом Буржа, брат — епископом Люсона, сестра — женой королевского секретаря, а дочь вышла замуж за сына виконта Буржа.Не слишком ли много для сына торговца мехами? Подобно Икару, Кёр взлетел слишком высоко. В должниках у него ходила вся торговая и политическая элита, которая только и ждала повода избавиться от своего кредитора. И такой повод нашелся.Кто убил Аньес Сорель?Несмотря на интриги и зависть, положение Кёра представлялось незыблемым. Жак дружил не только с королем, но и с его фавориткой — Аньес Сорель. Недоброжелатели шептались, что Жака и Аньес связывают более близкие отношения, чем просто дружба. Иначе зачем Сорель сделала Кёра в завещании своим душеприказчиком? Неспроста это, ох, неспроста…Аньес Сорель и Жака Кёра сближала общая черта — они оба имели большое влияние на короля, что не могло не вызывать зависть придворных. Первой в истории официально признанной любовнице французского короля, Аньес подражали, и ее же ненавидели и презирали.Аньес диктовала моду при дворе, и ее наряды с глубоким декольте шокировали духовенство. Негодование святых отцов дошло до предела, когда они увидели так называемый «Меленский диптих». Художник Жан Фуке по заказу Этьена Шевалье (позже сменившего Кёра на посту королевского казначея) изобразил любовницу короля и мать его детей, рожденных вне брака в виде Девы Марии.Этот диптих стоит того, чтобы задержаться на нем чуть подробнее.Правая створка диптиха. Источник: wikimedia.orgЛевая створка диптиха.Источник: wikimedia.orgСорель ввела в моду платья, обнажающие одну грудь — именно эта особенность одежды Аньес отражена на картине. Кстати, мода на обнаженную грудь в Европе продержалась вплоть до XVIII века! Ангелы на картине — дети короля, а три синих ангела — дети Аньес от Карла. На левой части диптиха изображен сам Этьен со своим покровителем, св. Стефаном (по-французски - Этьеном). Святой на картине — посредник, который обращается к Деве Марии с просьбой Этьена Шевалье. Младенец на правой части диптиха указывает пальчиком на Этьена — это значит, что просьба его услышана.Зимой 1450 года Карл отправился в поход в Нормандию. Беременная четвертым ребенком Аньес поехала за ним в Жюмьеж, чтобы поддержать короля. Внезапно она заболела. Родив дочь, Аньес скончалась — ей было всего 28 лет.Что стало причиной смерти Аньес — загадка до сих пор. По самой распространенной версии, фаворитка умерла от дизентерии. Современные исследователи полагают, что причиной смерти могла стать ртуть — носившая прозвище «Дама красоты», Аньес активно использовала косметику, в состав которой в то время входил этот токсичный металл.Не исключено, что к смерти фаворитки мог быть причастен сын Карла — будущий король Людовик XI, который ревновал ее влияние на отца.Как бы то ни было — смерть Аньес могла быть выгодна очень многим, но только не Жаку Кёру. Но его должница придворная дама Жанна де Вандом обвинила Жака в смерти несчастной АньесЭх, человече, что с тобою…Суд над Кёром, долгий и неправый, проходил без свидетелей защиты. Помимо отравления, Кёру вменили «порчу» монет, хотя уменьшение веса денег не могло происходить без ведома короля. В список обвинений вошли занятия магией, государственная измена, отказ в помощи бежавшему из рабства христианину, искавшего спасения на его галере у берегов Ближнего Востока, похищение людей…Смертную казнь Жаку заменили покаянием и штрафом, собственность конфисковали. Но через несколько лет с помощью верных друзей он бежит из тюрьмы, и через Тараскон, Марсель, Ниццу и Пизу в 1455 году добирается до Рима. Помня о заслугах дипломата, прекратившего «Великий раскол» церкви, папа Николай V радушно принимает Кёра. Незадолго до этого пал Константинополь, и Кёру предлагают возглавить поход против турок. В качестве капитана флотилии Кёр отправился на греческие острова, но заболел и умер на о. Хиосе 25 ноября 1456 года. Сложись все иначе, возможно, за Кёром вместо репутации фальшивомонетчика и отравителя закрепилась бы слава защитника веры — ведь для доблестного сердца нет ничего невозможного.Великий современник Жака Кёра поэт Франсуа Вийон воспринял историю выдающегося финансиста в философском ключе. В 1461 году он всего одной строфой в поэме «Завещание» подарил его имени бессмертие: «Когда терзаюсь нищетою/ Мне сердце тихо говорит:/«Эх, человече, что с тобою,/Ну, ты не Кёр и не набит/Экю, с того и постный вид?/ Но лучше, брат, ходить в хламиде,/Чем быть сеньором, что лежит/ В гробнице пышной в лучшем виде».Достоин ли был Кёр дурной славы при жизни — сейчас сказать сложно. Но его история, полная невероятных приключений, и крупных свершений — денежную и налоговую реформы, восстановление разрушенной Столетней войной экономики Франции, дипломатические успехи, создание первых в Европе регулярных воинских частей, организацию торговли с Ближним Востоком — безусловно, заслуживает бессмертия, пусть и литературного."
СберМаркет,,,Видео с прозрачностью на Jetpack Compose – запросто,2024-07-11T22:19:40.000Z,"Статья рассчитана на читателя продвинутого уровня, уже знакомого с Jetpack Compose и Android-разработкой в целом.Привет! Меня зовут Владимир, и я мобильный разработчик в компании Финам. В своей практике мы активно используем Android Jetpack Compose, который зарекомендовал себя с лучшей стороны.В статье я хочу показать простой способ решения известной в Android-разработке проблемы – проигрывания видео-файла с полноценной прозрачностью. В Compose для этого пока нет готовых компонентов,  поэтому разработчику приходится придумывать разные хитрости.Какая может быть польза от этого решения? Ответ очевиден – любая сложная анимация в приложении с минимальным размером. Например, мультик на картинке для привлечения внимания занимает всего 370 КБ памяти при размере кадра 480х270.Откуда вообще взялась эта проблема? Дело в том, что не всекодекив Android поддерживают альфа-канал в кадре (потенциальные кандидаты – H.265, VP8, VP9). Производителей много, но никто не гарантирует, что файл проиграется штатными средствами как положено. Чаще всего поддержки прозрачности просто нет совсем! А в мобильной разработке, особенно на Android, очень важно получить стабильный и предсказуемый продукт на максимальном охвате клиентских устройств.В Интернете уже есть несколько статей на эту тему, и даже есть готовый работающий код. Я нашел два основных информационных источника, заслуживающих внимания:разидва. Оба описывают почти один и тот же способ. Но первый – как это сделать в xml-разметке, второй – адаптирует первый способ на Compose.В основе всех способов (в том числе того, который предлагается в этой статье) лежит общий принцип восстановления прозрачности видеокадра по маске. Это означает, что видео-файл, уже включенный в ресурсы приложения, должен быть подготовлен специальным образом. Для этого сначала основной видеопоток разделяется на два параллельных – на цветовой (RGB) и альфа-маску. А затем оба потока в подготовленном файле «склеиваются» в один, где каждый занимает половину кадра.Примерно так выглядит подготовленный видео-файл при проигрывании обычным плеером.Подготовить любой видео-файл для упаковки в ресурсы приложения можно с помощью всем известной утилитыffmpeg:ffmpeg -i input_file.mov -vf ""split [a], pad=iw*2:ih [b], [a] alphaextract, [b] overlay=w"" -c:v libx264 -s 960x270 output_file.mp4Как уже описано в упомянутых выше источниках, далее для отрисовки анимированного изображения в общую верстку экрана добавляется полотно для рисования с контекстом OpenGL (GLSurfaceViewилиTextureView). А также экземпляр видеоплеера, которому передается ссылка на ресурс подготовленного видео-файла для проигрывания. При этом в процесс рендеринга изображения видео-потока встроен специальный пиксельный шейдер, склеивающий две половинки кадра в одну – в формате RGBA (цвет с прозрачностью). Таким образом, картинка обретает прозрачность на этапе манипуляций с изображением в контексте OpenGL, с чем он хорошо справляется на большинстве Android-устройств.Второй упомянутый способ для Compose по сути делает тоже самое, что и первый. Но вместо стандартногоMediaPlayerпредлагается использоватьExoPlayer, обернутый TextureView в Compose-совместимый компонентAndroidView(от Compose в этом случае – только interop-обертка для View).Меньше посредников, больше контроляЯ предлагаю сделать с заранее подготовленным видео-файлом примерно то же самое, но упростить процесс до двух минимально необходимых звеньев: видео-кодека и непосредственно самого Compose в чистом виде без оберток.Для начала напишем свой удобный компонент для извлечения сырых данных из видео-файла для последующего декодирования. Внешний интерфейс нашего компонента будет таким:interface VideoDataSource {
    fun getMediaFormat(): MediaFormat
    fun getNextSampleData(): ByteBuffer
}Для реализации компонента воспользуемся стандартным классом Android для извлечения данных из медиа-контейнеров —MediaExtractor. Один экземпляр класса имплементации будет отвечать за чтение одного файла. Для этого добавим простую фабрику:object VideoDataSourceFactory {

    fun getVideoDataSource(context: Context, uri: Uri): VideoDataSource {
        return VideoDataSourceImpl(context = context, uri = uri)
    }
}Методы нашего компонента:getMediaFormat(): получить структуру MediaFormat с описанием характеристик открытого файла – она нам понадобится для настройки кодека;getNextSampleData(): прочитать очередную порцию сырых данных видео-потока (для последующей передачи кодеку).Код класса нашего компонента:Hidden textinternal class VideoDataSourceImpl(context: Context, uri: Uri) : VideoDataSource {

    private val mediaExtractor = MediaExtractor().apply {
        setDataSource(context, uri, null)
        setVideoTrack()
    }

    private var mediaFormat: MediaFormat? = null

    private var initialSampleTime: Long = 0L

    private val dataBuffer = ByteBuffer
        .allocate(SAMPLE_DATA_BUFFER_SIZE)
        .apply { limit(0) }

    override fun getMediaFormat(): MediaFormat {
        return mediaFormat!!
    }

    override fun getNextSampleData(): ByteBuffer {
        if (!dataBuffer.hasRemaining()) {
            mediaExtractor.readSampleData(dataBuffer, 0)
            if (!mediaExtractor.advance()) {
                mediaExtractor.seekTo(initialSampleTime, MediaExtractor.SEEK_TO_CLOSEST_SYNC)
            }
        }
        return dataBuffer
    }

    private fun MediaExtractor.setVideoTrack() {
        val availableMimeTypes =
        	(0 until trackCount).mapNotNull { getTrackFormat(it).getString(MediaFormat.KEY_MIME) }

        val videoTrackIndex = availableMimeTypes
            .indexOfFirst { it.startsWith(""video/"") }
            .takeIf { it >= 0 }

        this.selectTrack(requireNotNull(videoTrackIndex))

        mediaFormat = this.getTrackFormat(videoTrackIndex)
        initialSampleTime = this.sampleTime
    }
}

private const val SAMPLE_DATA_BUFFER_SIZE = 100_000Компонент в целях демонстрации бесконечно «зацикливает» чтение данных простым условием:if (!mediaExtractor.advance()) {
            mediaExtractor.seekTo(initialSampleTime, MediaExtractor.SEEK_TO_CLOSEST_SYNC)
}Далее нам необходим компонент для декодирования сырых данных видео-потока, интерфейс которого будет иметь всего один метод:interface VideoFramesDecoder {
    fun getOutputFramesFlow(inputSampleDataCallback: () -> ByteBuffer): Flow<Bitmap>
}Единственный метод компонента будет возвращать Flow с декодированными изображениями (кадрами) в виде классаBitmap, готовыми для отрисовки. Для реализации компонента воспользуемся стандартным классом Android для декодирования видео-потока —MediaCodec.Создавать экземпляр класса компонента будем так же через фабрику:object VideoFramesDecoderFactory {

    fun getVideoFramesDecoder(mediaFormat: MediaFormat): VideoFramesDecoder {
        return VideoFramesDecoderImpl(mediaFormat = mediaFormat)
    }
}Код класса нашего компонента:Hidden textinternal class VideoFramesDecoderImpl(private val mediaFormat: MediaFormat) : VideoFramesDecoder {

    private val mimeType = mediaFormat.getString(MediaFormat.KEY_MIME)!!
    private val frameRate = mediaFormat.getInteger(MediaFormat.KEY_FRAME_RATE)

    private val nowMs: Long
        get() = System.currentTimeMillis()

    private val random = Random(nowMs)

    override fun getOutputFramesFlow(inputSampleDataCallback: () -> ByteBuffer): Flow<Bitmap> {
        return channelFlow {
            val threadName = ""${this.javaClass.name}_HandlerThread_${random.nextLong()}""
            val handlerThread = HandlerThread(threadName).apply { start() }
            val handler = Handler(handlerThread.looper)

            val decoder = MediaCodec.createDecoderByType(mimeType)

            val frameIntervalMs = (1_000f / frameRate).toLong()
            var nextFrameTimestamp = nowMs

            val callback = object : MediaCodec.Callback() {

                override fun onInputBufferAvailable(codec: MediaCodec, index: Int) {
                    runCatching {
                        val sampleDataBuffer = inputSampleDataCallback()
                        val bytesCopied = sampleDataBuffer.remaining()
                        codec.getInputBuffer(index)?.put(sampleDataBuffer)
                        codec.queueInputBuffer(index, 0, bytesCopied, 0, 0)
                    }
                }

                override fun onOutputBufferAvailable(codec: MediaCodec, index: Int, info: MediaCodec.BufferInfo) {
                    runCatching {
                        codec.getOutputImage(index)?.let { frame ->
                            val bitmap = frame.toBitmap()
                            val diff = (nextFrameTimestamp - nowMs).coerceAtLeast(0L)
                            runBlocking { delay(diff) }
                            trySend(bitmap)
                            nextFrameTimestamp = nowMs + frameIntervalMs
                        }
                        codec.releaseOutputBuffer(index, false)
                    }
                }

                override fun onError(codec: MediaCodec, e: MediaCodec.CodecException) = Unit

                override fun onOutputFormatChanged(codec: MediaCodec, format: MediaFormat) = Unit
            }

        	decoder.apply {
            	    setCallback(callback, handler)
            	    configure(mediaFormat, null, null, 0)
            	    start()
        	}

            awaitClose {
         	     decoder.apply {
                	stop()
                	release()
            	     }
            }
        }.conflate()
    }
}В методе getOutputFramesFlow() класс создает и возвращаетChannelFlow, удобный для работы с callback-вызовами, в нашем случае с MediaCodec.Callback().Через обратные вызовы onInputBufferAvailable() и onOutputBufferAvailable() кодек сообщает о готовности входного и выходного буфера соответственно.Если готов очередной входной буфер, то отдаем ему порцию прочитанных сырых данных, возвращаемых функцией inputSampleDataCallback. А по готовности выходного буфера – читаем массив байтов изображения и отдаем по подписке всем потребителям данных нашего Flow.Перед отправкой изображения подписчикам производим задержку, равную межкадровому интервалу (в миллисекундах это 1000/FrameRate). Задержка сделана по-простому, через не-suspend блокировку потока (runBlocking). Для тестовой среды этого вполне достаточно: один отдельно выделенный поток в период ожидания не будет потреблять ресурс CPU и оказывать влияние на результат измерений.Затем сводим все компоненты вместе в один несложный Compose-виджет:@Composable
fun VideoAnimationWidget(
    @RawRes resourceId: Int,
    modifier: Modifier = Modifier
) {
    val context = LocalContext.current
    var lastFrame by remember { mutableStateOf<Bitmap?>(null) }

    LaunchedEffect(resourceId) {
        withContext(Dispatchers.IO) {
            val videoDataSource = VideoDataSourceFactory.getVideoDataSource(
                context = context,
                uri = context.getUri(resourceId = resourceId)
            )
            val videoFramesDecoder = VideoFramesDecoderFactory.getVideoFramesDecoder(
                mediaFormat = videoDataSource.getMediaFormat()
            )

            videoFramesDecoder
                .getOutputFramesFlow(inputSampleDataCallback = { videoDataSource.getNextSampleData() })
                .collectLatest { lastFrame = it }
        }
    }

    Canvas(modifier = modifier) {
        lastFrame?.let { frame ->
            drawImage(
                image = frame.asImageBitmap(),
                topLeft = Offset(
                    x = (size.width - frame.width) / 2,
                    y = (size.height - frame.height) / 2
                ),
                blendMode = BlendMode.SrcOver
            )
        }
    }
}В LaunchedEffect создаем источник данных и подписываемся на Flow, отдающий текущий кадр для отрисовки. Высвобождение ресурсов и закрытие файла происходит автоматически внутри компонента-декодера (по отписке от Flow), поэтому внутри виджета ничего для этого специально не делаем. В Canvas просто рисуем последний текущий кадр.Всё! Минимальный набор в Compose для видео с прозрачностью готов.Но, правда, есть еще некоторые детали, на которые, пожалуй, стоить обратить внимание. Было бы нечестно осветить только сильные стороны такого решения, не затронув слабые.Стандартный кодек Android в callback-функции возвращает изображение в форматеYUV_420_888(классImage). И для отрисовки на Canvas его еще надо как-то преобразовать в понятные всем RGBA-пиксели. А заодно восстановить прозрачность каждого пикселя (мы же подготовили наш файл заранее, разделив цветовую и альфа составляющие на две половинки кадра).Для этой статьи мной был взят и адаптирован один из готовых примеров преобразования. Алгоритм функции, кроме непосредственно преобразования, на каждой итерации получения цвета одного пикселя вычисляет его прозрачность, оптимизируя весь процесс в один проход.И эти вычисления, к слову, будут выполняться на CPU, а не графическом процессоре устройства. Да, это цена, которую надо заплатить за гибкость… Но об этом далее.Код извлечения конечного изображения в формате RGBA сразу с умножением на альфа-маску:Hidden textprivate fun Image.getBitmapWithAlpha(buffers: Buffers): ByteArray {
        val yBuffer = this.planes[0].buffer
        yBuffer.get(buffers.yBytes, 0, yBuffer.remaining())

        val uBuffer = this.planes[1].buffer
        uBuffer.get(buffers.uBytes, 0, uBuffer.remaining())

        val vBuffer = this.planes[2].buffer
        vBuffer.get(buffers.vBytes, 0, vBuffer.remaining())

        val yRowStride = this.planes[0].rowStride
        val yPixelStride = this.planes[0].pixelStride

        val uvRowStride = this.planes[1].rowStride
        val uvPixelStride = this.planes[1].pixelStride

        val halfWidth = this.width / 2

        for (y in 0 until this.height) {
            for (x in 0 until halfWidth) {

                val yIndex = y * yRowStride + x * yPixelStride
                val yValue = (buffers.yBytes[yIndex].toInt() and 0xff) - 16

                val uvIndex = (y / 2) * uvRowStride + (x / 2) * uvPixelStride
                val uValue = (buffers.uBytes[uvIndex].toInt() and 0xff) - 128
                val vValue = (buffers.vBytes[uvIndex].toInt() and 0xff) - 128

                val r = 1.164f * yValue + 1.596f * vValue
                val g = 1.164f * yValue - 0.392f * uValue - 0.813f * vValue
                val b = 1.164f * yValue + 2.017f * uValue

                val yAlphaIndex = yIndex + halfWidth * yPixelStride
                val yAlphaValue = (buffers.yBytes[yAlphaIndex].toInt() and 0xff) - 16

                val uvAlphaIndex = uvIndex + this.width * uvPixelStride
                val vAlphaValue = (buffers.vBytes[uvAlphaIndex].toInt() and 0xff) - 128

                val alpha = 1.164f * yAlphaValue + 1.596f * vAlphaValue

                val pixelIndex = x * 4 + y * 4 * halfWidth

                buffers.bitmapBytes[pixelIndex + 0] = (r * alpha / 255f).toInt().coerceIn(0, 255).toByte()
                buffers.bitmapBytes[pixelIndex + 1] = (g * alpha / 255f).toInt().coerceIn(0, 255).toByte()
                buffers.bitmapBytes[pixelIndex + 2] = (b * alpha / 255f).toInt().coerceIn(0, 255).toByte()
                buffers.bitmapBytes[pixelIndex + 3] = alpha.toInt().coerceIn(0, 255).toByte()
            }
        }

        return buffers.bitmapBytes
    }ПроизводительностьТеперь оценим применимость этого способа, сравнив его производительность с рендерингом в OpenGL.Для замеров скорости работы и потребления ресурсов я не стал дополнять код супер-модными бенчмарками, прогревая сборщик мусора и кэши всех видов. Вместо этого я выбрал самый простой подход – на одном и том же эмуляторе был запущен рендеринг одного видео-файла двумя разными способами. А стандартными инструментами профилирования записаны результаты загрузки центрального процессора (CPU) и графической подсистемы (GPU) в виде красивых графиков.Параметры эмулятора (Android API 34):Параметры ПК (ноутбук), на котором проводились эксперименты:Intel Core i5-12500H, RAM 40 ГБ, GeForce RTX 3050 4 ГБПервый замер (CPU):CPU загрузка: рисование в ComposeCPU загрузка: OpenGL с шейдеромВторой замер (GPU):GPU рендеринг: рисование в ComposeGPU рендеринг: OpenGL с шейдеромКак видим, чудес не бывает. Ключевые особенности каждого способа заметно отражаются на производительности.Загрузка CPU выше у чистого Compose-рисования, так как основные вычисления происходят в функции преобразования каждого кадра (из формата YUV_420_888 в формат RGBA). В рендеринге OpenGL это делает плеер (кодек), тесно связанный с контекстом OpenGL, и GPU-шейдеры. Это снимает всю вычислительную нагрузку с CPU.На GPU-диаграмме видим ту же картину: время на подготовку кадра в OpenGL уходит заметно больше (красная область). Compose почти не тратит ресурс GPU (только на свой внутренний механизм рисования). Отличие в оранжевых областях (сплошное поле против редких баров) я списываю на особенности работы обоих подсистем. Эта область для Compose выглядит точно так же, даже если запустить простейшую векторную анимацию.Вместо выводовЦель статьи – показать Jetpack Compose с еще одной хорошей стороны, но ни в коем случае не мотивировать использовать его абсолютно везде. Каждому инструменту – свой случай.Рендеринг с помощью OpenGL (GLSurfaceView, TextureView), по моему мнению, предназначен для видео-анимации с поверхностью отображения в единственном числе (идеально для видеоплеера и игрового приложения). С увеличением числа полотен рендеринга нагрузка на GPU (да и CPU тоже) кратно возрастает. У меня даже получилось «уронить» эмулятор высокой нагрузкой (уже на 20 одновременно запущенных анимациях OpenGL). При этом аварийное завершение процессов произошло не в приложении, а именно в самом виртуальном устройстве.Способ же, предложенный в статье, может быть уместен в случаях, когда шаблонная анимация нужна во множественном числе в один момент времени. Например, когда нужны живые метки на карте. В этом случае будет достаточно только одного компонента с кодеком, отдающим через Flow кадры отрисовки всем подписчикам. При этом нагрузка на ресурсы устройства не будет возрастать с ростом числа виджетов на экране.Исходный код для самостоятельного тестированиятут, там же есть готоваяrelease-сборкадля быстрого запуска на Android-устройстве."
СберМаркет,,,Джин из бутылки или загадка Хогарта,2024-06-30T09:00:45.000Z,"В феврале 1751 года художник Уильям Хогарт шокирует Лондон гравюрой «Переулок Джина» (Gin Lane). Пронзительные образы отчаяния и безумия на этой гравюре будоражат зрителя и почти три века спустя. Но что «читали» в гравюре современники Хогарта, почему он поставил высокое искусство на службу антиалкогольной пропаганде, при чем тут экономика Англии и пираты из «Острова сокровищ»?Пора по пабамЧуть ниже мы подробно разберем гравюру Хогарта, а пока отправимся в английский паб конца 1680-х годов и полюбопытствуем, что пьют англичане. В конце XVII века трон «короля напитков» в Англии прочно занимает французский бренди. Ром непопулярен, виски пьют только шотландцы да ирландцы, а джин вообще считается лекарством — «можжевеловую воду» врачи прописывают как мочегонное. Популярность коньяка может оспорить только пиво — но эти соперники находятся в разных категориях. Все меняется в 1689 году, когда английский престол занимает голландец Вильгельм Оранский. Французский король-католик Людовик XIV не признает легитимность короля-протестанта Вильгельма. Англия вступает в затяжной конфликт с Францией. Стремясь побольнее ударить французскую экономику, Вильгельм вводит пошлины на импорт коньяка.Создано с помощью Искусственного ИнтеллектаИ вот тогда настает звездный час напитка с родины голландского принца — джина.  В 1690 году парламент снизил налоги для винокуров — теперь перегонять зерновой спирт мог кто угодно.  С одной стороны, собственное производство заменило потребность в импорте коньяка, с другой — акциз на спиртные напитки помог финансировать войну с Францией. Элита аплодировала ловкому ходу правительства — и Франции насолили, и английским фермерам помогли, повысив спрос на зерно. Джин становится модным напитком, своего рода маркером «свой-чужой». Пьешь джин? Значит, ты за короля и протестантскую веру!В 1694 году конфликт Англии и Франции выходит на новый виток — начинается война за пфальцское наследство. Казна истощается. Правительство основывает Банк Англии, вводится понятие госдолга, а еще увеличивается налог на пиво.  Теперь джин и пиво практически сравниваются в цене. Те, кто раньше предпочитали только пиво, впервые обращают внимание на «крепкую можжевеловую воду» — и армия поклонников джина растет.За пенни — пьян, за два — мертвецки пьянПозиции джина усиливаются, когда Вильгельма сменяет на троне королева Анна. Тучная, страдающая подагрой, королева заслужила в народе прозвище «Бренди Нэн» — за любовь к бренди, который она разбавляла холодным чаем. При Анне в 1710 году возвели собор святого Павла в Лондоне, и в честь окончания строительства перед западным фасадом собора установили статую королевы. По иронии судьбы, напротив статуи располагалась лавка торговца джином. Злые языки нашли повод лишний раз посмеяться над пристрастием королевы к крепким напиткам, сочинив песенку «Brandy Nan, Brandy Nan, You’re left in the lurch, Your face to the gin shop, your back to the church» (Бренди Нэн, Бренди Нэн, ты оставлена в беде, лицом к джиновой лавке, спиной к собору).Помимо судьбоносных для страны решений — например, объединения Англии и Шотландии, в правление Анны принят закон, ставший камушком, который сдвинет лавину. Анна отменила лицензирование производства джина — так в окрестностях Лондона появились сотни винокурен, гнавших «можжевеловую воду» без какого-либо контроля за качеством.Дальше — больше. В 1720 году количество новоявленных джиноделов возросло в геометрической прогрессии с принятием закона, который освобождал винокуров от постойной повинности — и тогда гнать джин начинают еще и трактирщики, чтобы не размещать солдат на постой.Даниэль Дефо в 1726 году по заказу винокуренных заводов пишет «Краткий обзор винокуренной торговли», в котором восторгается качеством джина и с гордостью отмечает успех импортозамещения бренди джином. Всего через пару лет, видя нарастающее падение нации в алкогольный угар, автор «Робинзона Крузо» раскается в этой поддержке.Джин стал любимым напитком бедняков. «За пенни — пьян, за два — мертвецки пьян, солома, чтобы упасть — бесплатно» — зазывали клиентов торговцы.  Джин помогал согреться, заглушить голод. И убивал — десятками человек в день.  В напиток и без того низкого качества подмешивали скипидар и даже серную кислоту, люди нередко слепли и сходили с ума — но остановиться не могли, джин быстро вызывал привыкание.Первый звоночек, что выигрыш в экономической тактике оборачивается стратегической катастрофой, прозвенел в 1723 году — из-за джиномании в течение десяти лет смертность в Лондоне будет превышать рождаемость, а детская смертность достигнет чудовищных 75%. К 1730 году джином торговали около 7000 заведений, не считая подпольных, на каждого жителя Англии, включая младенцев, приходилось 10 литров джина в год.  Спохватившись, в 1729 году парламент выпускает закон о джине — до 1751 года выйдет еще семь законов, каждый из которых закрывал лазейки в предыдущем. Налог с продаж вырос в 60 раз, ответом стал рост нелегального производства и контрабанды.Скажи «мяу»В 1738 году выходит пятый по счету закон, который фактически объявляет производство джина нелегальным. К этому времени правительство ввело поощрение «стукачей», которые доносили о продавцах и производителях джина. Доносчиков стали отлавливать на улицах, избивать и даже убивать. Гнать пойло меньше не стали — винокурни и продажи просто ушли в тень. Несмотря на суровость закона, в нем все же оставались лазейки, чем и воспользовался ирландский авантюрист, шпион и драматург отставной капитан Дадли Брэдстрит.  По закону, для доноса требовалось знать имя человека, который арендует недвижимость, используемую для незаконной продажи джина. Без выполнения этого условия власти не могли войти в дом. Капитан Дадли попросил друга снять дом в лондонском Сити, въехал в этот дом с запасом джина и еды, и забаррикадировал входы и выходы. У двери он приколотил вывеску с изображением кота. В пасти кота проделали прорезь, а под лапой кота вывели трубу. Алчущий джина лондонец, подойдя к двери, произносил пароль: «Кис-кис!». Если Дадли за дверью отвечал: «Мяу!», то покупатель бросал два пенни в прорезь, и подставлял под трубу кружку, в которую лился джин.Выдумка оказалась очень удачна — Дадли целый месяц безнаказанно торговал джином, заработав 22 фунта. Возле его дома ежедневно собирались толпы, пока подобный трюк не освоили конкуренты капитана.Потому что пьет пират джинВ песенке из советского мультфильма по книге Стивенсона удачно срифмованы слова «Джим» и «джин» — все наверняка помнят, что у пирата нет шансов против Джима, «потому что пьет пират джин». Случайно или нет, но авторы песенки оказались исторически точны — действие романа «Остров сокровищ» разворачивается в самый разгар джиномании (примерно в 1745-1746 гг.), и пираты в то время, скорее всего, действительно предпочитали джин. По теории писателя В. Точинова, изложенной в книге «Остров без сокровищ», трактир «Адмирал Бенбоу» — перевалочная база контрабандистов джина. В этом есть своя логика — «Адмирал Бенбоу» расположен в полумиле от деревни, возле безлюдной дороги, на мысе. Ни один ресторатор или отельер не одобрит подобной локации. Но рядом с трактиром бухта Киттова Дыра, в которую может зайти только судно с небольшой осадкой, и эта бухта не просматривается из деревни. И если принять версию В. Точинова, то семья Хокинсов не ошиблась с местом для бизнеса…Оставляя за скобками споры о теории Точинова, которую одни считают сплошной конспирологией, а другие — интересным литературным расследованием, вернемся к гравюре Хогарта.Переулок ДжинаДавайте рассмотрим гравюру в деталях. Действие гравюры происходит в трущобах лондонского прихода Сент-Джайлс, которые под иглой мастера превращаются в ад. Место действия выбрано неслучайно — по этим улицам везли осужденных на виселицу, и на этом участке смертник имел право на «чашу св. Джайла» — наполненный джином кубок.Источник: royalacademy.org.ukВ гравюре заметно влияние Брейгеля и Босха — чего стоит хотя бы безумец с кузнечными мехами на голове, размахивающий пикой с насаженным младенцем. В левой части плотник и кухарка закладывают ростовщику пилу и кастрюли, чтобы раздобыть деньги на джин. Свисающая эмблема ростовщика словно заменяет крест на виднеющейся вдали церкви св. Георгия. Мальчик за парапетом отнимает кость у собаки, рядом с ним уснувшая стоя женщина. Присмотритесь — возле нее улитка, символ лени и греха. Висельник с верхнего этажа словно наблюдает за апокалиптическим безумием — дракой калек и тем, как пьяная мать поит младенца джином. Центральная часть композиции — женщина, в алкогольном угаре выпустившая младенца, который вот-вот погибнет. Возможно, эта часть гравюры — отсылка к скандальному делу Джудит Дюфур. В 1735 году Джудит забрала двухлетнюю дочь из работного дома и задушила ребенка платком.  Бросив тело в канаву, она продала одежду дочери, выданную в работном доме, а деньги потратила на джин. Джудит судили и повесили, и эта жуткая история положила начало волне осуждения джиномании.У женщины сифилитические язвы на ногах, она нюхает табак — символ проституции. Рядом с ней — полумертвец, бывший солдат, он более не нужен стране. В его корзине — памфлет против джина, который тоже никому не нужен на этой улице, рядом с ним черная собака — символ печали. На заднем плане рушащийся дом — знак упадка.Переулок ПиваИсточник: wikipedia.orgВ пару к «Переулку Джина» Хогарт выпустил «Пивную улицу», на которой счастливые люди наслаждаются национальным напитком — пивом. Они не бездельники — просто присели отдохнуть. На улице праздник — день рождения короля Георга II, о чем говорит флаг на церкви св. Мартина на заднем плане. Кстати, дом художника находился рядом с этой церковью. Хотелось бы обратить внимание читателя на фигуру маляра в центре гравюры. Маляр, рисующий рекламу джина — единственный человек на картине, одетый в обноски. Некоторые исследователи полагают, что в образе маляра Хогарт сатирически изобразил швейцарского художника Жан-Этьена Лиотара, автора знаменитой «Шоколадницы». По другой версии, бедный маляр — это сам Хогарт, об этом говорит палитра, которую художник сделал своим фирменным знаком на автопортретах.Уильям ХогартИсточник: wikipedia.orgГод 1751 стал началом конца джиномании. Хотя страна по-прежнему нуждалась в средствах, у властей хватило политической воли отказаться от доходов, которые приносил джин — здоровье нации оказалось дороже любых денег. Парламент выпустил восьмой акт о джине, в поддержку которого Хогарт и создал эти гравюры. Закон вернул лицензирование и ввел высокие сборы с торговцев.  Англичане переключились на пиво, а еще в моду вошел чай, импорт которого поощрялся государством.Безумие, длившееся более полувека, унесшее жизни тысяч людей, постепенно сошло на нет. Сейчас джин — это просто напиток, о страшной истории которого напоминают разве что гравюры Хогарта.Впрочем, оборотную сторону ослабления государством контроля над алкогольной индустрией мы знаем не понаслышке — антиалкогольная кампания времен перестройки и отмена монополии государства на производство алкоголя в начале 90-х годов прошлого века оставили не самые приятные воспоминания. Но это уже тема для другой статьи…"
СберМаркет,,,Понимание бизнес-сущности системы при тестировании,2024-06-23T08:46:12.000Z,"ВведениеДобрый день! Меня зовут Анастасия, я QA-инженер команды бэкофиса в «Финаме». С 2022 года занимаюсь тестированием бэкофисных и торговых систем финансовых компаний. До перехода в QA работала в эксплуатации и поддержке торгово-клиринговой системы СПБ Биржи. Моя сильная сторона — глубокое понимание бизнесовой части тестируемого продукта, о важности чего мы и поговорим в этой статье.ИдеяЦель этой статьи — подчеркнуть важность понимания бизнесовой сущности системы, которую вы тестируете. Я приведу реальные примеры, которые помогут вам развить смекалку в тестировании различных систем. Статья будет особенно полезна QA-инженерам, работающим в брокерских, банковских компаниях и на биржах.СодержаниеЛичный опыт, вдохновивший на написание статьиЧто важнее в QA: понимание продукта или теория тестирования?Лайфхаки для изучения предметной областиЗаключениеИстория из моего опытаЗа несколько лет работы в IT я поняла, что погружение в задачу с точки зрения пользователя помогает выявить уязвимости, которые не описаны ни в одном учебнике по тестированию. Начну с примера из своей практики. Однажды я тестировала сервис расчета комиссий, где формула была проста:0,25%*объем сделки=сумма комиссииПроверяя кейсы, я заметила, что для сделки в 10 долларов UI отображал:Цена: 1 долларКол-во: 10 бумагОбъем сделки: 10 долларовКомиссия 0.025 долларовВас ничего не смущает?Проблема заключалась в том, что сумма комиссии (0,025 долларов) не округлялась до двух знаков после запятой, как это принято для фиатных валют.Обратите внимание не на сумму комиссии, а именно на ее значение: 0,025 доллара  — это 2,5 цента. Половинка цента — это сколько? :)Фиатные валюты, такие как доллар и рубль, имеют только 2 знака после запятой. Все расчеты округляются до двух знаков. Вот и нашли багу на уровне ТЗ: не продуман механизм округления комиссии до двух знаков после запятой. Поэтому с чистой совестью возвращаем аналитику на доработку ТЗ и ждем новую версию сервиса. Подобные случаи подчеркивают важность понимания бизнеса и умения смотреть за рамки ТЗ.Что важнее в QA: понимание продукта или теория тестирования?Ответить на этот вопрос можно кратко: для QA важны оба аспекта. Но часто специалисты склоняются к одной из крайностей.Когда я переходила в QA, у меня не было теоретической базы тестирования, но был опыт использования системы, которую я собиралась тестировать. Мои интуитивные подходы улучшились после прочтения книги Святослава Куликова «Тестирование программного обеспечения — базовый курс». На собеседованиях меня больше спрашивали про брокерскую деятельность, чем про тестирование. Работодатели заинтересованы в практиках, которые понимают продукт.Чтобы стать востребованным QA-специалистом и продвигаться по карьерной лестнице, важно развиваться как в области теории тестирования, так и в понимании сферы деятельности компании.Лайфхаки для изучения предметной областиGoogle It.Если вы тестируете какой-то новый сервис и видите незнакомые бизнесовые определения, то не воспринимайте их как абстрактные объекты. Гуглите их, спрашивайте у ИИ. В общем, используйте все возможные инструменты, чтобы понять, с чем вы работаете.Спрашивайте у коллег.В этом нет ничего зазорного. Лучше какое-то время постоянно спрашивать, чем долгое время молчать и не понимать происходящего. По возможности будьте самым дотошным коллегой. Спрашивайте не только то, что тестируете, но и смежный функционал и его смысловую составляющую.Пример из жизни: как-то работали с сервисом расчета накопленного купонного дохода, поняла, что коллеге очень тяжело дается эта задача. Я ему на простом примере объяснила механизм расчета НКД. Задача была решена уже вечером: человек понял, как это работает, и ему стало легче справляться с этой задачей.Пройдите курсы или изучите другие материалы.Сейчас в интернете много качественной информации, которая поможет вам развиваться в вашей сфере.В этом пункте главное — конкретика. То есть вы должны понимать, какие точно знания хотите получить. Я, например, ставлю на полгода цели с конкретным минимальным списком той информации, которую бы хотела усвоить для изучения своей профессии.Интернет-ресурсы.Подпишитесь на 2-3 тематических канала и регулярно их читайте. Не перегружайте себя информацией.Записывайте термины. Фиксируйте новые термины прямо в тест-кейсы или в глоссарий проекта. Например, сегодня вы узнали, что такое KYC. Запишите это.6.Изучите базовый профессиональный английский в вашей сфере.Это вам поможет лучше понимать написанный код, а также делать названия объектов в своих автотестах более осмысленными.Пример из жизни: я тестировала зачисление и списание денег на счет клиента по новому тарифу. У трестируемого клиента баланс в конце дня — -1500. На начало дня было 0. Выгружаю исходные данные. Вижу таблицу:IdOperation codeValue1Money:output1002money:input7003money:input10004Money:output100Что не так? С английского:Output — это вывод денег.Input — это ввод денег.То есть у клиента должен быть баланс +1500. Ошибка была еще на уровне ТЗ, а разработчик тоже не заметил. Поэтому let’s study English :)ИтогГлавный вывод этой статьи — развивайтесь как в технических навыках, так и в понимании продукта. Это значительно облегчит вашу работу и сделает процесс тестирования более осмысленным и продуктивным.Волшебного чек-листа тестирования не существует, поэтому только эрудированность в области продукта и технические навыки сделают вас профессионалом."
СберМаркет,,,По дороге из желтого кирпича — что скрывается за сказкой о волшебной стране Оз?,2024-06-16T09:00:26.000Z,"«Удивительный волшебник из страны Оз» написан исключительно для удовольствия детей» — уверял Лаймен Фрэнк Баум в предисловии к первому изданию 1900 года. Спустя полвека после написания книги экономисты и историки открыли в ней политические и экономические аллегории. Но закладывал ли Баум в сказку тайные смыслы, и если да — то какие? А если нет — зачем он подчеркивал, что «Волшебник» всего лишь детская книга?Фрэнк Баум, источник WikipediaПрежде, чем отправиться за ответами в страну Оз, следует сказать несколько слов о ее авторе. Жизнь писателя и журналиста Фрэнка Баума пришлась на так называемый «Позолоченный век» — годы бурного роста экономики США после Гражданской войны. Он родился в 1856 году на северо-востоке США. В 1888 году его отец разорился, семья отправилась искать счастья на Средний Запад, в Южную Дакоту. Именно там Баум получил журналистский опыт, в одиночку работая в газете «Субботний пионер Дакоты». В ней он однажды напишет шутку про фермера, который из-за отсутствия корма надевает скоту зеленые очки и кормит опилками. Но и в Дакоте семье не повезло, и Баумы перебрались в Чикаго. Здесь Фрэнк и придумает сказку, которая принесет ему славу и богатство.В 1964 году научные круги США взрывает статья школьного учителя Генри М. Литтлфилда в академическом журнале American Quarterly. Исследователь заявил, что «Волшебник» — ни что иное, как «притча о популизме».  Споры только подогрели интерес к этой теме, которая к 1990-м годам оформилась в теорию.Но закончим вступительную часть — добро пожаловать в путешествие по следам Дороти и Тото!Ураган популизмаИтак, знакомьтесь — Дороти из Канзаса, у которой нет отца и матери, она живет с дядей Генри и тетей Эм. Девочка Дороти и есть весь американский народ, честный, простой и чуточку наивный. Безжизненная степь, серые краски, дядя и тетя девочки, которые работают с утра до ночи и никогда не улыбаются — это описание катастрофы 1870-х годов, когда засухи, суровые зимы и саранча превратили прерии Канзаса в пустоши. Для понимания этой катастрофы умножьте на два боль романа Стейнбека «Гроздья гнева» о разоренных Великой Депрессией фермерах.Именно Канзас стал одним из центров популистского движения — влиятельной силы в Штатах конца XIX в. Среди прочего популисты обещали избирателям биметаллический стандарт — равное хождение золота и серебра. Остановимся чуть подробнее на этой важной теме.После Гражданской войны Штаты ввели золотой стандарт. Из золота чеканились монеты достоинством в один, два с половиной, три, пять, десять и двадцать долларов. Из серебра чеканилась только мелочь — 50, 25 и 10 центов, при этом единовременный платеж серебром не мог превышать пяти долларов. Более того, в 1875 году запретили хождение бумажных долларов, что породило партию «гринбекеров», выступающую за возврат зеленых купюр. Фермеры считали, что их беды связаны с нехваткой дешевых серебряных денег. Введите в оборот серебро наравне с золотом, рассуждали они, тогда инфляция поднимет цены на продукты, а мы рассчитаемся с долгами. Одним из лидеров популистской партии, обещавшим фермерам неограниченную чеканку серебра, был Уильям Дженнигс Брайан — запомним это имя.Ураган, унесший Дороти — это тот самый ураган доверия к популистской партии фермеров Канзаса. Метафора урагана для современников Баума была понятна. Само популистское движение — пусть недолгое, но яркое, в газетах часто сравнивали с политическим ураганом. «Канзасским циклоном» называли ярую сторонницу популистов, вдохновенного оратора Мэри Элизабет Лиз. В своих речах она заявляла, чтокрупный бизнесвладеет страной, иУолл-стритсделал простых людей рабами, а монополистов — хозяевами. Репортеры писали, что Лиз призывала фермеров Канзаса: «Выращивайте меньше кукурузы и больше ада». Мэри этого не говорила, но цитата ей понравилась. Именно Мэри Лиз ряд исследователей творчества Баума считают прототипом Дороти. Любопытно, что в театральной версии «Волшебника» Баум дал Дороти фамилию Гейл (англ. Gale — ураган).Вот почему Дороти идет в серебряных башмачках по дороге из желтого кирпича — это намек на обещание популистами паритета золота и серебра. Дорога из желтого кирпича ведет в Изумрудный город — то есть Вашингтон. Название страны Оз — это буквенное обозначение унции (oz), что тоже говорит о золоте и серебре. Кличка песика Тото образована от слова «teetotaler» — так называли сторонников движения трезвости, которые горячо поддерживали лидера популистов и абсолютного трезвенника Брайана. Таким образом, Тото — это образ поддержки популистской партии.Четыре волшебницыИтак, перенесенный ураганом, домик Дороти убивает Злую Ведьму Востока. Под злой старухой имеется в виду Уолл-стрит и банкиры Восточного побережья, угнетающие Манчкинов (Жевунов) — фермеров Среднего Запада. По мнению популистов, банкиры и промышленники Востока организовали заговор с целью превратить в рабов «маленьких людей», и вовлекли в этот заговор политическую элиту. Кто, как не президент Кливленд отменил закон о покупке серебра, и кто как не президент Мак-Кинли не выполнил свои обещания уступок сторонникам серебра и ввел золотой стандарт?Дороти становится обладательницей серебряных башмачков убитой злой ведьмы — их дарит девочке добрая Волшебница Севера. Добрая фея — не кто иная, как население Верхнего Среднего Запада, горячо поддерживавшего популистов. Но волшебница не так сильна, как ее злые соперницы. Она целует Дороти в лоб — это знак электоральной поддержки. Жевуны (простые фермеры) рады избавлению от злой ведьмы, они теперь понимают силу серебряных туфелек (то есть, серебра).По велению Волшебника, Дороти с друзьями отправляется на битву со злой Ведьмой Запада — аллегорией засух и банкиров, ввергнувших фермеров Среднего Запада в беду. Как и ведьма Востока, эта волшебница поработила людей, захватив власть в Стране Винки (Мигунов). Она посылает против Дороти сорок волков, сорок ворон, и чёрных пчёл. Каждого противника Дороти ведьма вызывает, подув в серебряный свисток — вот и еще один символ захваченного злыми силами металла. К финалу битвы ведьма с помощью Золотой Шапки вызывает резерв — Крылатых Обезьян. Волшебство Золотой Шапки ведьма до того использует, чтобы поработить робких Мигунов.  Вот и еще одна аллегория, как золото творит несправедливость. Обезьяны не могут убить Дороти — ее охраняет поцелуй доброй волшебницы Севера. Но ведьма стремится завладеть туфельками Дороти — а для чего? Да чтобы усилить свою волшебную силу серебром!Девочка в серебряных туфельках побеждает ведьму и надевает Золотую Шапку — золото переходит от банкиров во власть народа, биметаллизм торжествует, его сила теперь очевидна. Закрепляя эту аллегорию, Баум дает читателю еще одну деталь — Железный Дровосек получает от Мигунов новый топор с рукояткой из золота и лезвием, сияющим словно серебро.Кроме того, злая Ведьма Запада — это еще и прозрачный намек на президента Уильяма Мак-Кинли. При этом президенте в 1898 году началась война с Испанией, по итогам которой испанцы уступили США Кубу, Пуэрто-Рико и Филиппины. Таким образом, Мигуны из Желтой страны — это филиппинцы, которые так и не получили независимость.Но есть и четвертая волшебница — Глинда, правительница Юга. Она правит Страной Кводлингов (Болтунов) (англ. Quadlings). Дорога в эту страну полна опасностей, а сами Болтуны не бывают в Изумрудном городе (Вашингтоне), и не любят чужаков. Что ж, и здесь аналогия прозрачна — с 1860-х годов ни один южанин не становился президентом, а северян на Юге, по понятным причинам, после войны недолюбливали. И вместе с тем, партия популистов имела сильную поддержку на Юге.Кто есть ктоА кто же такие спутники Дороти, и кто Волшебник? Страшила — это фермеры, у которых не хватило мозгов избежать долгового рабства. Но по ходу действия книги оказывается, что Страшила вовсе не глуп — то есть, простой народ способен понять экономический смысл введения биметаллического стандарта. Вместе с тем, Страшила — это и аллегория популистов, которых противники высмеивали, как «тупых простаков». Во время путешествия через лес, Страшила падает на желтые кирпичи — здесь видна аллюзия на заявления популистов, что золотой стандарт губит фермеров.Железный Дровосек — рабочие, которых превратила в бессердечную машину волшебница Востока, то есть банкиры и промышленники. Сама идея, что тяжелый труд дегуманизирует, превращая рабочих в машины, весьма характерна для того времени. Дровосек ржавеет, он не может работать — это символ тяжелой рецессии и безработицы, постигших США во время кризиса 1890 года.А вот Лев и Волшебник — это две ипостаси лидера популистов Брайана, о котором мы говорили чуть выше. Фамилия Брайан (Bryan) рифмуется с английским словам «лев» (Lion) и похоже на слово«ложь» (Lying), и на карикатурах политик нередко изображался в образе льва. Противники называли его «трусливым» за выступление против войны с Испанией в 1898 году (той самой, по итогам которой Филиппины остались без независимости). На деле выступление против войны требовало немалой смелости. В книге Лев, не зная о своей храбрости, сражается и побеждает чудовищного Паука — символ корпораций, захвативших экономику. Лев убивает Паука, благодарные жители леса клянутся ему в верности и провозглашают своим правителем. Так бы оно и было, победи в президентской гонке Брайан, который выступал против трестов.На этой карикатуре Уильям Дженнигс Брайан изображен в образе льва, на которого лает демократическая пресса. На медали надпись — «Free silver», лозунг популистов, выступающих за неограниченную чеканку серебра.Волшебник-обманщик, который меняет обличье — образ политика времен, когда республиканца от демократа отделяла очень тонкая грань. «Я не оказываю услуг без вознаграждения», говорит Волшебник — очень характерная черта политика-лоббиста. И в то же время — Волшебник родом из Омахи, штат Небраска, родных мест Брайана. Именно на съезде в Омахе популисты решили требовать от властей неограниченной чеканки серебра.Но Брайан проиграл президентские выборы Мак-Кинли, а идея неограниченной чеканки серебра так и не была реализована. В 1900 году пришедший на второй срок Мак-Кинли законодательно закрепил золотой стандарт.В конце истории Дороти переносится в Канзас, но ее серебряные туфельки потеряны в пустыне навсегда…Сказка или намек?Но задумывал ли Баум книгу, посвященную неудавшейся избирательной кампании Брайана? Или просто написал чудесную сказку, в которой через полвека раскопали тайные смыслы? Вопрос открыт до сих пор.Среди тех, кто выступал за прочтение «Волшебника» как аллегории, были историк американской культуры и литературы Рассел Най и знаменитый математик Мартин Гарднер, выпустившие критический анализ творчества Баума. Впрочем, аргументы как «за», так и «против» не убеждают ни одну из сторон. Одни уверяют, что Баум писал в защиту популизма, другие возражают, что да, «Волшебник» — это притча, но притча сатирическая, в которой популизм высмеивается. Третьи отрицают какие-либо намеки на политику, и утверждают, что все совпадения случайны, а детская сказка — всего лишь сказка.Чью сторону принять — пусть каждый решает сам. Но в любом случае, пусть даже вы, мистер Баум, и не задумывали литературно-экономическую загадку, спасибо вам за чудесную историю нашего детства.Если вы думали, что это конец, то нет, в истории данного произведения еще много ""белых пятян"". Мы обязательно напишем продолжение, если вам понравился этот материал."
СберМаркет,,,Фантастические таланты: Эпизод II – Атака внедрения,2024-06-10T16:45:56.000Z,"Предыдущие статьи серии:→Фантастические таланты: Эпизод I – Призрачный наймЧасть 1: Перед рубежом новых вызововВоскресенье. Два разных человека, два разных взгляда на предстоящий день:Специалист:«Завтра начинается новый этап в моей карьере. Как будет выглядеть мой первый день в новом проекте? Это  будет быстрое погружение или дадут время раскачаться ? Какие задачи меня ждут на испытательном сроке?»Руководитель:«Завтра в мою команду приходит новый сотрудник. Что ему поручить? Возьму-ка часть простых задач из текущего пула и дам новичку, пусть показывает себя».Привет!Cегодня я хочу поделиться своим опытом по онбордингу новых сотрудников в IT-команды. Чтобы глубже понять предложенные решения, давайте сначала окунемся в проблемы и боль — это необходимо, чтобы понять контекст.Несколько лет назад я руководил проектом X в компании Y. В нашей команде было два бекендера, два фронтендера и один тестировщик. Бизнес решил значительно увеличить объем функционала продукта. В таких ситуациях обычно функционал продумывается, из него выделяется MVP-версия, которая затем декомпозируется на задачи для технической оценки и планирования ресурсов. В нашем случае мы столкнулись с тремя крупными фичами, каждая из которых требовала примерно по три человеко-месяца работы (бекенд и фронтенд поровну).​​Варианты решения проблемы:Заимствование ресурсовВ больших компаниях можно попытаться заимствовать ресурсы из других команд или спецназ-групп, если такие имеются.Поиск на рынкеЭто часто утопичный вариант, так как найти нужное число квалифицированных специалистов за очень короткий срок бывает сложно.К тому же команде нужно время, чтобы сработаться.АутстаффингВ случае крупных аутстафф-компаний, где уже сформированы команды, это вполне может быть эффективным решением.Мы выбрали сочетание аутстаффинга и расширения штата.Часть 2: Аутстаффинг: реальность и вызовыДля полноты картины немного погрузимся в аутстаффинг.Аутстаффинг в IT — это когда компания предоставляет своих разработчиков в аренду для  участия в различных проектах. Этот подход часто выбирают, когда нужны специалисты на короткий срок, например, для стартапов или доработки продуктов.Основная особенность таких проектов — относительно низкая техническая сложность и небольшие нагрузки. Прокачать знания вглубь  в таких проектах довольно сложно.Отсюда следует несколько важных нюансов:Широкие, но неглубокие знанияМногие специалисты в аутстаффе обладают широким спектром знаний, но не углубляются в детали. Они часто переключаются с проекта на проект, а это не способствует глубокому погружению в конкретные технологии или продукты.Отсутствие долгосрочного интересаДля многих разработчиков в аутстаффе ваш проект — это лишь один из многих. Сегодня они работают с вами, завтра могут перейти к другому заказчику. Это создает определенные трудности в построении долгосрочных отношений и вовлеченности в проект.Несмотря на общую тенденцию, в аутстаффе можно встретить высокомотивированных специалистов с глубокими знаниями и идеями. И я таких знаю. Но такие случаи довольно редки.Если у вас высокие требования к команде и нет проверенной компании-аутстаффа, подготовьтесь к тому, что вам придется перебирать людей/команды/компании.Часть 3: Ломай меня полностью!Боль — хороший мотиватор к изменениям с последующей трансформацией в кайф.Итак, наша команда расширяется. Как интегрировать новичков в уникальный мир нашего проекта и компании? Классический подход: «Вот твой рабочий комп, вот репозиторий, вот задачи в Jira. Дерзай!» Будто мы говорим: «Добро пожаловать в джунгли, найди свой путь!»Когда я впервые столкнулся с этим, мой подход был прост, — встреча, знакомство с командой, доступы, краткий экскурс по проекту. Но вот загвоздка: многие большие проекты, и мой в том числе – это не прогулка по парку, а, скорее, поход через Гималаи. Технически сложные, с множеством интеграций и нюансов. Иногда сотрудник, работая над одним набором фич, даже не подозревает о существовании других. Как только он сталкивается с новым, начинается либо изучение, либо... Ну, вы знаете, изобретение велосипеда.И вот мы берем первую команду аутстаффа. Последовательно подключаются Петя, Вася, Коля... И вот, когда кажется, что все налажено, появляются новые Петя, Вася и Коля. И ты снова и снова рассказываешь одно и то же, отвечаешь на одни и те же вопросы.А если в команду вливается 2–5 человек одновременно, начинаешь чувствовать себя как в «Дне сурка»: кто что знает? кто что слышал? кто еще ждет доступов?!В один прекрасный момент ты задумываешься: «А точно ли я занимаюсь правильным делом? А я точно эффективно работаю? Может быть, меня заменить видео-роликом?»И тут приходит озарение: а почему бы и нет? Создаем краткую вводную инструкцию, снимаем часть нагрузки и делаем процесс более систематизированным. Звучит как план.Итак, проблемы, с которыми мы столкнулись:«Вечный понедельник»: каждую неделю одно и то же повторение о компании и продукте. Ответы на одни и те же вопросы.Сложности с запоминанием: кто что знает, кто кому что рассказал.Непрозрачность заявок при подключении новых людей: каждый раз как в первый.Долгий процесс онбординга, особенно на сложных проектах.Все страдают: лид, коллеги, новичок. Вопросы летят, как из пулемета.Часть 4: Путеводитель мне запили!Теперь давайте займемся созданием нашего собственного «Справочника по выживанию в джунглях проекта» или, если говорить официально, wiki-страницы под кодовым названием «Добавление разработчика».Страница будет состоять из трех блоков:ВступительныйДля руководителяДля разработчикаДавайте разберем каждый из них.Вступительный блок выглядит так:Теперь детальнее.Первое, что надо сделать, — правильно позиционировать эту страницу в глазах нашего нового коллеги. Здесь важно показать, что это не просто еще одна страница документации, а личный роадмап новичка, ключ к ответам на большинство его вопросов.Второе— ограничить сроки.Чтение не должно затягиваться вечно. Начните с оценки времени, которое, по вашему мнению, потребуется мидлу/джуну/сеньору для освоения материала, и установите этот срок.Нет смысла специально уменьшать сроки и делать их слабо достижимыми. Наша цель — не загнать коллегу, а помочь ему комфортно внедриться в проект и команду. Нет смысла завышать время, так как по закону Паркинсона - Работа занимает все отведенное на нее время.Третье- подача информации.Подача информации – это искусство, и здесь важно найти золотую середину. Мы не ставим сроки как непреложный ультиматум, а предлагаем их как дружеское напоминание. Вместо «Ты должен за три дня» мы говорим: «В среднем, на его прохождение требуется» или даже «Обычно коллеги укладываются в три дня». Это не только делает срок кажущимся более выполнимым, но и добавляет элемент здорового соперничества: «Если они смогли, почему бы и мне не попробовать?»Такой подход помогает сфокусировать внимание на роадмапе, а не на бесконечном изучении всей вики компании. «Есть срок, есть задача – действуем!»Также подсвечиваем, что первая задача – это не просто пункт в списке, а ваша отправная точка. Особенно это важно для тех, кто предпочитает действие словам. Вы знаете этих ребят: вместо того, чтобы читать инструкции, они сразу же погружаются в работу, создают новые ветки в Git и берут задачи из Jira, исследуя все на ходу.Для них мы особо подчеркиваем:это первая задачамаксимально самостоятельнопройти полностьюЧетвертое— границы!В больших компаниях легко потеряться среди множества разделов вики и документации. Наша задача – установить четкие рамки, чтобы новички знали, на  чем сосредоточиться. Мы хотим помочь новичку, а не перегрузить его информацией.Укажите, какие разделы/спейсы/каталоги можно смотреть. Или, может быть, ограничить статьи для изучения коллегой тегом?Ипоследнее, но не менее важное — актуализация документации.Новички – это не только ученики, но и учителя. Их свежий взгляд помогает выявить устаревшие, неправильные или недостающие моменты в документации. Поэтому мы приучаем их с первых дней не только читать, но и активно участвовать в процессе обновления и улучшения информации. Это не только помогает им понять систему, но и вносит свежий взгляд в нашу документацию.Блок руководителяВыход нового сотрудника в компанию — это как запуск ракеты. Множество систем должны сработать безупречно и вовремя. В идеальном мире добавление нового разработчика в команду происходит нажатием одной волшебной кнопки, которая запускает целую цепочку автоматизированных действий: создает аккаунты во всех необходимых системах, настраивает доступы, информирует команду... Как в космическом центре управления полетами!Но этот космический центр вначале надо построить, что может быть непростой задачей. И пока мы его строим, часть работы выполняется в ручном режиме или через заявки в другие отделы.В моем случае это выглядит примерно так:Это далеко не полный список, но вы наверняка уже дополнили его особенностями своей компании или проекта.Этот блок на странице роадмапа свернут, потому что основной читатель страницы — новый сотрудник, которому не нужно знать все закулисные тайны с первых минут. Однако он должен иметь возможность заглянуть туда и увидеть, сколько работы уже было проделано руководителем. Это как заглянуть за занавес перед большим спектаклем: важно знать, что все готово к вашему выходу на сцену.Для меня как руководителя несложно время от времени раскрывать этот блок, чтобы убедиться, что все идет по плану. Это как заглядывать в книгу рецептов во время приготовления сложного блюда: всегда полезно свериться с инструкцией!Блок разработчика выглядит так:Давайте тоже рассмотрим детальнее.Чек-лист как навигатор, который поможет не заблудиться в IT-джунглях. Каждый пункт — это шаг на пути к освоению проекта. Здесь нет ловушек, только четкие указания!Каждый пункт чек-листа сформулирован как конкретная задача со ссылкой на ресурс, где можно получить необходимые сведения.Разделение знаний на блоки помогает визуально упростить восприятие информации. Вместо того, чтобы смотреть на одну гигантскую колонку из сотни пунктов, вы видите структурированный план, разбитый на логические разделы. Это как разделение большой задачи на маленькие шаги — кажется гораздо более выполнимым и менее стрессовым.Часть 5: Магия видео-инструкцийЧек листы у нас есть, но каждый пункт требует ответов, а, значит, нужна эффективная документация. И тут на сцену выходят видео-инструкции.Видео-инструкции — это не просто удобство, а ключ к эффективному обучению. Многие не осознают его силу, пока не попробуют. А потом? Отказаться невозможно.Не верите? Давайте разберемся!Примеры в действии:«Флоу задач в Jira»: вы включаете запись, открываете доску Jira и в течение 5–10 минут демонстрируете весь процесс жизни задачи. От правильного создания, через разработку и тестирование, к успешной приемке. Это не просто инструкция,  а путешествие по вашему рабочему процессу.«Структура проекта в репозитории»: Запускаете IDE, начинаете запись и ведете зрителя по лабиринтам вашего кода, объясняя, где что находится и как все работает.Видео-инструкции: почему они работают?Лучше один раз увидеть, чем 100 раз услышатьВидео передает контекст и нюансы, которые трудно описать словами, — многое мы упускаем в виду кажущейся простоты и понятности. Новичок видит реальные процессы и интерфейсы, что ускоряет его понимание.Простой пример. Вы можете текстом написать: «Зайди на сервер Х и выполни команду Y». Казалось бы, понятно и просто.Но на видео мы сможем показать и пользователя, под которым выполняется команда, и директорию, даже оболочку, от которой тоже многое может зависеть,  и результат выполнения, который может отличаться от результата новичка.Эффективность времениЗаписать видео на 5–10 минут гораздо быстрее, чем писать тексты с такой же информационной емкостью.Плюс, это навык, который развивается и улучшается со временем. Через какое-то время видео можно будет записывать почти без подготовки и с первого дубля.Легкость восприятияВидео легче воспринимается, чем аналогичный текст. Особенно если ваша речь четкая и информативная.Ускоренный просмотрВ отличие от живых встреч, видео можно смотреть в ускоренном режиме, перематывать, пересматривать.Дополнительные советы по видеоСоздайте отдельную страницу с видео-инструкциями для удобства доступа. У меня она называется «Видео-инструкции для новичка».Чтобы видео попадало в текстовый поиск, указывайте под роликом теги и краткое описание с ключевыми словами. Вуаля! Теперь его легко найти.Искусство создания видео-материаловИскусство разметки видео-материалаПредставьте себя режиссером своего обучающего шоу. В основе видео —  сценарий, каждый кадр должен быть продуман. Начните с простого: бумага и ручка. Запишите кратко основные моменты, которые хотите рассказать. Со временем вы научитесь строить такие сценарии в уме, словно профессиональный режиссер, который видит фильм еще до съемки.Принцип «одна тема — одно видео»Помните о золотом правиле: не перегружайте зрителя. Лучше создать серию коротких, но емких видео, чем одно длинное, где зритель потеряется. Каждое видео — это отдельная глава книги, полная информации, но легкая для восприятия.Дикция и чистота речиИзбавьтесь от слов-паразитов, тренируйте четкость и уверенность. Записывайте себя, просматривайте материал сразу после записи. Обычно это помогает найти изъяны, — пересмотрите ролик, например, через 1-2 часа. Анализируйте и улучшайте. Это не только повысит качество ваших видео, но и сделает вас звездой любых презентаций и встреч.Тренировка, тренировка и еще раз тренировка!Без паники, это просто видеоНе бойтесь ошибок. Видео всегда можно отредактировать или перезаписать. Это ваша тренировочная площадка, где вы можете усовершенствовать навыки.Технические тонкости создания видео: звук, изображение и форматПомните, что звук в ваших видео — это не просто фон, а основа вашего сообщения. Если звук шипити в нем присутствует фоновый шум, то даже самое захватывающее содержание потеряет свою ценность. Ваши зрители должны слышать вас четко и без помех, как если бы вы говорили с ними лично. Инвестиции в хороший микрофон —  инвестиции в качество вашего общения.В мире, где экраны варьируются от 5K до Full HD, важно помнить о том, что не вся ваша аудитория будет смотреть видео в высоком разрешении. Снимая в 5K, вы рискуете создать контент, который будет трудно воспроизводить на менее мощных устройствах или из-за сжатия сильно потеряет в четкости. Снимайте в Full HD, чтобы обеспечить оптимальное качество и доступность вашего видео для всех зрителей, независимо от их оборудования.В эпоху высокоскоростного интернета легко забыть о том, что не каждый может быстро загрузить большой файл. Особенно если таких файлов много. Сжатие видео — это не только сокращение времени загрузки, но и уважение, экономия времени  вашей аудитории. Уменьшая размер файла, вы делаете свой контент доступнее и удобнее для просмотра, что, в свою очередь, увеличивает шансы на то, что ваше сообщение увидят и услышат.Часть 6: А что же дальше?Это был подход базового онбординга.Каждый раз, когда на работу выходит новый разработчик, вы копируете эту страницу саму в себя и называете «Добавление разработчика — Иван Иванов».И вот у вас есть раздел «Добавление разработчика», где внутри раздела — такие же страницы, только персонализированные, под каждого разработчика.Этот подход решает множество задач:Устранение рутины:теперь коллеги погружаются в работу самостоятельно, без необходимости вашего постоянного вмешательства.Глубокое и широкое погружение:вы обеспечиваете коллегу всей необходимой информацией для эффективного старта работы.Снижение нагрузки на команду:команда больше не тратит время на ответы на одни и те же вопросы.Четкость для руководителя:вы всегда знаете, что уже сделано и что еще предстоит сделать для каждого нового сотрудника.Понятность для сотрудника:каждый новичок точно знает, что ему нужно изучить.Прозрачность процесса:весь процесс онбординга становится прозрачным и понятным для всех участников.Но и это еще не все.Следующий шаг — углубление и расширение этого процесса, чтобы сделать онбординг еще более эффективным и целенаправленным.Часть 7: Улучшаемся. Продвижение и развитие в онбордингеПосле применения онбординга на более чем 30 новых сотрудниках, я выявил несколько ключевых моментов для улучшения:Честное чтениеНекоторые разработчики могут лениться или считать, что они уже знают все, если видят знакомые термины вроде Symfony, Composer, Gin, Git. Однако это недооценка важности глубокого понимания конкретного материала в контексте вашего проекта.Онбординг на 3 дняХорошо, но работа продолжается гораздо дольше. Задача каждого руководителя — развивать своих сотрудников постоянно, чтобы они становились лучше.Забывание информацииЭто естественный процесс, и его нужно учитывать.А потому делаем доработки.Разбиваем чек-лист на этапыПример:первые Х дней в проекте - что нужно знать, чтобы приступать к задаче эффективно.первые Х недель - что надо еще узнать о проекте, чтоб знания выросли в глубину.первые Х месяцев - какие скилы (хард/софт) и технологические знания надо прокачать.Таким образом, страница онбординга превращается в личный план развития сотрудника, который можно планировать на год вперед.Если вы пойдете по этому пути, то еще немного рекомендаций:сделайте явную и заметную отсечку испытательного срока. Чтобы сотрудник, заходя  в проект, понимал, где та линия успешности, когда он прошел входной экзамен в вашу команду и стал ее полноценной частью;сделайте разбивку на уровни, чтобы обучение и закрытие чекбоксов отражались на  грейде/уровне/записи в трудовой/ачивке в профиле. Должно быть позитивное закрепление, что обучение — это не просто так, а повышение ценности сотрудника в глазах компании.Экзамены.Каждый этап должен сопровождаться экспресс-экзаменом. К примеру, личный созвон с сотрудником минут на 30, где в свободной форме вы общаетесь по пройденному этапу. Это мотивирует ваших коллег не хитрить и не пропускать важные моменты.ПериодичностьВнедрение новой системы онбординга в вашей компании — это не только шанс для новых сотрудников быстро влиться в коллектив, но и уникальная возможность для «перезагрузки» уже работающего персонала. Представьте, что это не просто обучение, а, скорее, переосмысление и обновление знаний, которые могли устареть или потерять актуальность.Когда вы решите внедрить новую практику, начните с тех, кто уже давно с вами. Это не только позволит им освежить свои знания, но и даст вам ценную обратную связь о том, насколько эффективна новая система. Это как взгляд изнутри на то, что вы строите, и возможность улучшить его до того, как новички начнут свой путь.Важно понимать, что компания — живой организм, который постоянно развивается и меняется. То, что казалось важным год или два назад, сегодня может потерять актуальность. Поэтому регулярное обновление знаний даже у опытных сотрудников — это не просто хорошая практика, а необходимость.Подходите к этому процессу как к экспресс-режиму обучения. Большинство ваших старых сотрудников уже знакомы с основами, поэтому они могут быстро просмотреть материалы, освежить в памяти то, что забыли, и узнать новое. Это не только поможет им оставаться на одной волне с современными трендами и изменениями в компании, но и сделает их более гибкими и адаптивными к нововведениям.Таким образом, ваша система онбординга становится не просто инструментом для новичков, но и мощным каналом постоянного обучения и развития для всей команды. Это создает культуру непрерывного обучения, где каждый сотрудник, независимо от его опыта и стажа в компании, постоянно растет и развивается вместе с вашим бизнесом.Часть 8: Несколько ценных советов для эффективного онбордингаСоздание безопасного канала общенияПредставьте себя на месте нового разработчика. Вы изучили все материалы, посмотрели все видео, но у вас возникли вопросы. Кому и где вы можете их задать, не боясь показаться глупым? В нашем случае я создаю специальные чаты для каждой команды, там нет менеджеров. Это место, где каждый может свободно высказываться, даже если вопрос кажется простым или очевидным. Такой формат коммуникации укрепляет дух команды и поддерживает открытый диалог.Командность или курированиеЗдесь многое зависит от специфики вашего продукта и команды. В некоторых случаях эффективен подход с назначением куратора для новичка, который ведет его и помогает ему. В других — лучше работает командный подход, где все помогают друг другу и обеспечивает поддержку. Выбор подхода должен базироваться на культуре вашей команды и специфике работы.Информирование команды о новых сотрудникахПеред тем, как новый сотрудник присоединится к команде, важно проинформировать всех о его приходе. Это помогает избежать недопонимания и стресса, а также способствует лучшей интеграции новичка в коллектив. Каждый должен знать, кто присоединяется к команде, каковы его роль и область ответственности.Автоматизация процессовАвтоматизация рутинных задач — ключ к эффективности. В нашем случае мы разработали внутреннюю платформу, которая позволяет частично автоматизировать процесс добавления новых сотрудников. С помощью небольшой формы мы можем завести разработчика в наше пространство, указать его специфику (бэкенд/фронтенд),  вес его голоса на код-ревью. Далее автоматика уже поставит нужные задачи в правильные отделы и даже оповестит команду о его скором присоединении. Это несколько упрощает процесс и позволяет сосредоточиться на более важных аспектах работы.А как внедрение происходит у вас?"
СберМаркет,,,Как устроены выборы в США и какие компании выиграют от победы Трампа или Байдена в 2024 году?,2024-06-09T09:00:18.000Z,"США вступили в год выборов. Влияет ли идущая вовсю предвыборная кампания на рынок в целом и отдельные компании в частности? На первый взгляд, какого-то очевидного влияния не прослеживается. Но это только на первый взгляд.Бенефициары CHIP ActВ начале года была принята программа переноса в США критически значимых производств полупроводников (CHIP) из третьих стран, куда оно перемещалось последние четверть века. В рамках этой программы правительство выделяет гранты, софинансирующие строительство новых или расширение существующих фабрик. Кэпхит программы — $50 млрд.20 марта самый крупный грант этой программы на $8,5 млрд на существенное расширение (предполагающее увеличение выпуска втрое) получила компания Intel, а также еще и кредит на $11 млрд.8 апреля второй по величине грант в рамках этой программы в $6,6 млрд получила Тайваньская TSMC на строительство новой фабрики.Что объединяет эти два проекта? Оба они расположены в Аризоне.Таким образом, всего за 3 недели штат получил суммарно инвестиций на $60 млрд. Почему именно Аризона стала главным бенефициаром программы CHIP?Ответ лежит в политической плоскости. Выборы Президента США — очень особенные выборы. Система выборщиков и принцип winner takes it all (все выборщики штата достаются победителю вне зависимости от того, выиграл он 51/49 или 90/10) порождают ситуацию, когда кандидатам нет никакого смысла бороться за штаты, где есть чье-либо крупное преимущество. Борьба идет исключительно за штаты с близкими цифрами поддержки кандидатов, так называемые «колеблющиеся штаты» (swing states).Какие штаты реально влияют на выборы?Таким образом, реально президента США выбирают только жители не более чем 10 крупных «колеблющихся штатов», а жители Нью-Йорка, Калифорнии, Техаса и в целом 80% американских штатов де-факто никого не выбирают.Не существует какого-то закрытого окончательного списка «колеблющихся» штатов. Если мы вспомним очень близкие выборы 2000 года (Буш против Гора), то их исход решался во Флориде, которая тогда была колеблющимся штатом. Теперь Флорида, а также Огайо, Айова перешли из разряда «колеблющихся» в разряд уверенно республиканских штатов. Орегон, Нью-Мексико и Колорадо перешли из разряда «колеблющихся» в разряд уверенно демократических штатов. Однако список колеблющихся штатов пополнили Мичиган из числа ранее уверенно демократических штатов и Аризона, Джорджия, Северная Каролина и Вирджиния.Электоральная карта США на 2024 год выглядит следующим образом:Светло-красным и светло-голубым обозначены штаты, где преимущество Трампа либо Байдена соответственно составляет менее 5%, то есть, те самые «колеблющиеся штаты». И по выборщикам расклад такой, что если Трамп выигрывает в Аризоне, Джорджии и Северной Каролине, где по опросам он опережает Байдена примерно на 4%, то у него будет 268 выборщиков. Для победы нужно 270, но в некоторых штатах отдельные выборщики могут нарушить волю избирателей, и штраф за это составляет всего $1000. Если прибавить к этим трем еще и Неваду (+3,5% по опросам у Трампа), то это уже 274 выборщика и полноценная победа Трампа.Таким образом, чтобы выиграть выборы, Байдену необходимо «вырвать» у Трампа один из трёх штатов, где сейчас он отстаёт на 4% — Северную Каролину, Джорджию или Аризону. И из этого расклада становится понятно, почему именно Аризона получила столь значительные инвестиции.Впрочем, Байдену нужно выиграть и в других колеблющихся штатах, в том числе и Висконсине с Мичиганом, где пока по опросам он проигрывает, а также в Филадельфии, где оба кандидата идут практически вровень.Исходя из этого можно предположить, какие штаты так же, как и Аризона, могут до выборов получить солидные инвестиции и какие компании (базирующиеся в этих штатах) могут от этого выиграть.Какие штаты и компании могут получить инвестицииМичиганСтолица американского автопрома, который сейчас переживает не лучшие времена. General Motors и Ford могут получить дотации, но, скорее всего, не на строительство новых заводов, производственные мощности у них и так избыточны, а на перепрофилирование части мощностей на производство электромобилей и гибридов, что будет в духе экологической повестки Байдена.ВисконсинШтат известен прежде всего своим сельскохозяйственным сектором (производство молока). Тут базируется в том числе и производитель грузовиков и строительной техники Oskosh, который может получить льготные кредиты или налоговые послабления.ПенсильванияЭкономика этого штата довольно большая и широко диверсифицированная, здесь есть сталелитейные производители, машиностроительные компании, один из крупнейших банков США Wells Fargo. Вряд ли стоит ожидать расширения металлургических производств, это слишком противоречит экологической доктрине Байдена, более вероятно выделение средств на их перепрофилирование, что будет нейтрально для самих компаний.В Вирджинии и Миннесоте преимущество Байдена достаточно велико (около 4%), так что вряд ли эти штаты окажутся серьёзными бенефициарами.Северная КаролинаШтат также известен своим сельскохозяйственным сектором (производство табака). Такжев штате располагаются штаб-квартира Bank of America. Штат имеет хорошие природные условия для размещения электростанций на возобновляемых источниках энергии (солнечные и ветровые), уже 12,5% электроэнергии штата производится на этих электростанциях. Исходя из экологической доктрины Байдена, скорее всего, дотироваться будет именно этот сектор (связанные компании: First Solar, Solar Edge, Enphase Energy, etc.).ДжорджияСтолица штата — Атланта — имеет крупнейший международный авиаузел и в США, и в мире в целом. Так что штат выиграл бы от увеличения траффика авиакомпаний, однако это плохо сочетается с экологическими программами. Другое возможное направление, дружественное экологии в отличие от авиалиний — субсидии на расширение производства электромобилей, в штате расположены заводы Kia и Rivian."
СберМаркет,,,Фантастические таланты: Эпизод I – Призрачный наем,2024-06-03T15:48:05.000Z,"Собеседование на позицию разработчика программного обеспечения — это уникальное столкновение двух миров: мира интервьюера и мира кандидата. На первый взгляд может показаться, что они разделяют общий язык — технологии и программирование, однако их взгляды и ожидания могут сильно различаться. Это не просто вопрос проверки технических навыков, это исследование того, насколько хорошо миры двух участников могут синхронизироваться и создать новое, совместное пространство для развития и достижения целей. В этом столкновении интересов, предпочтений и приоритетов закладывается основа для будущих профессиональных отношений.Привет! Меня зовут Дмитрий. У меня за плечами сотни собеседований в поисках талантов, и я хочу поделиться своим богатым опытом и подходами к проведению собеседований, внедрению в проекты и развитию разработчиков. В этой статье мы обсудим собеседования.Статья будет полезна не только руководителям и лидам проектов, но и разработчикам, предоставляя возможность увидеть процесс глазами интервьюера и понять, что можно улучшить в своих собеседованиях.Последнюю сотню собеседований я провел удаленно, поэтому акцент будет несколько смещен на онлайн‑собеседования.Поехали!Этикет и первое впечатление в общенииТы ему про код, а он в телефон втыкает.Первое впечатление в общении — играет ключевую роль. Если кандидат вызвал у вас интерес, важно, чтобы и вы произвели на него положительное впечатление. Это взаимный процесс, и начинать его следует с первых секунд общения.Независимо от вашего мнения о кандидате, важно помнить о профессиональном этикете. Ведь вы представляете компанию, и ваше поведение будет ассоциироваться с ней в глазах кандидата.Вот несколько ключевых моментов:Визуальный контактОбщение с реальным человеком всегда более продуктивно и приятно. Поэтому стоит включать камеру во время собеседований. Общаться с черным квадратиком — не самое интересное занятие.Позитивное настроениеДружелюбное и отзывчивое общение создает комфортную атмосферу для обеих сторон. Никто не любит общаться с кем‑то, кто выглядит раздраженным или отвлеченным.Внимание к собеседникуУважайте время кандидата. Не отвлекайтесь на другие дела, особенно когда кандидат отвечает на ваши вопросы.Первый визуальный контакт имеет огромное значение. Поэтому при подготовке к интервью я рекомендую настроиться на позитив, быть внимательным и готовым к конструктивному диалогу. И, конечно же, стоит работать над своей речью, избегая лишних пауз и слов‑паразитов.Лично я перед подключением к собеседованию — выдыхаю, в голове — все рабочее оставляю в стороне, меня ждет общение с новым человеком, возможно даже очень крутым и интересным.И помните — возможно вам придется продавать свою вакансию кандидату, и если к концу собеседования (высоко оценив кандидата) вы начнете в позитив — может быть поздно.Дополнительнов последние несколько лет видеовстречи стали неотъемлемой частью разработки, поэтому важно уделить внимание техническому оснащению. Рекомендуется обзавестись качественной веб‑камерой и микрофоном.Нет необходимости создавать профессиональную видеостудию, но крайне важно, чтобы во время встречи было обеспечено четкое изображение, качественный звук, без посторонних шумов и щелчков, а связь стабильной и без перебоев. Это существенно повышает качество коммуникации.Начало общенияЧасто на начальном этапе интервью HR или я просим кандидата представиться и рассказать о своем опыте. Это помогает установить контакт и создать рабочую атмосферу. Однако важно уметь корректно переключить внимание, если кандидат углубляется в детали.Один из вариантов — прервать с целью доуточнить что‑то, что упомянул собеседник и что подразумевает краткий ответ, и после ответа уже повести кандидата в своем направлении. Фасилитируйте интервью.К примеру — кандидат долго рассказывает о своем опыте работы с задачами в jira/gitlab/symfony/etc:—вы«извини, прерву тебя — ты упомянул что использовал jira/gitlab/symfony, а какая версия?»—кандидатне помню / не знаю / версия ххх—выок, понял, давай тогда перейдем к теме ХМоя цель на интервью — создать дружелюбную и открытую атмосферу.Я предпочитаю общение вместо формального допроса.Чтобы сделать общение более неформальным, я часто предлагаю перейти на «ты», спрашивая: «Не против если перейдем на ты?» иногда добавляю «для упрощения общения». Реакция кандидата на такое предложение позволяет понять его степень комфорта и открытости.В конце концов, если я решу пригласить человека в команду, мне важно, чтобы нам было комфортно работать вместе.Помните, что для большинства людей собеседование — это стрессовая ситуация и выход из зоны комфорта, просто интенсивность этого стресса у каждого разная.Технические вопросыМногие, включая меня на моих первых собеседованиях (когда я был юн и зелен), допускают ошибку, пытаясь «поставить в тупик» кандидата или «переиграть/завалить» его, акцентируя внимание на узкоспециализированных вопросах. Однако настоящая цель — помочь кандидату демонстрировать свои сильные стороны.Каждый из нас имеет свой уникальный набор знаний, опыта и компетенций.Поэтому «переиграть» друг друга (исключая начинающих специалистов) может быть несложно. Главное — определить готовность кандидата к развитию, его стремление к знаниям и активность в рабочем процессе.Каждый имеет уникальный опыт и навыкиМоя цель — понять общий уровень кандидата: какие знания и опыт он приобрел за время своей карьеры. Насколько ему интересно развиваться? Есть ли у него жажда новых знаний?Даже если кандидат не знаком с конкретными инструментами X и Y, но у него богатый опыт и глубокие базовые знания — то это не проблема. Ведь многие специалисты способны быстро и эффективно обучаться.Так что же спрашивать?Тут я начну со своего негативного опыта успешных кандидатов:ТеоретикиКандидат отлично себя проявляет в теоретических вопросах, на зубок расскажет вам про SOLID, про паттерны разработки, про их виды. А потом оказывается, что когда он приступает к решению задач и ему в голову загружается модуль коддинга, то модуль теории выгружается и вот тебе спаггети‑код, публичные свойства и так далее.Мой вариант решения:Важно не знание теории, а умение ее применять и привычка ее применять. А как ты пишешь новые свойства — приватными/публичными/защищенными? А почему так? А как ты понимаешь, когда надо завести новый класс или писать в старом, вот прям в текущем твоем проекте — какие критерии?Сильные харды, слабые софтыКандидат отлично себя показывает по знаниям, по решению задач. А потом оказывается, что просить помощи он не умеет, как и в командную работу не умеет, и в любой проблеме будет копать самостоятельно до тех пор, пока ты не начнешь интересоваться, почему задача Х начинает становиться дорогой. Или оказывается, что человек совсем не готов к критике по своему коду и агрессивно ее воспринимает. Или не умеет вести переписки.Да, в некоторых случаях это можно прокачать, но готовы ли обе стороны к этому? Бывают сложные ситуации, когда ваши взгляды с кандидатом на тему развития могут расходиться, и вы потратите много сил, времени и нервов, а результата либо не будет, либо будет не сопоставимый.Мой вариант решения:Если есть сомнения — то я начинаю задавать вопросы из рабочей жизни:Ты считаешь что твой код правильный, но на ревью тебе поставили замечания и ты с ними не согласен — как ты будешь это решать? И потом дополнительные вопросы, в зависимости от ответов — Доказывать? А как? Как долго? Привлекать арбитра? А кого? А как потом не допускать таких кейсов?Тебе пришла задача, но ты ее не понимаешь полностью — что ты будешь делать? И потом дополнительные вопросы, в зависимости от ответов — Отложишь? А вдруг она важная? Спросишь? А у кого и как? А если заказчик в отпуске? А если заказчик игнорит?Умею только кодитьКандидат отлично все знает по языку разработки. А потом оказывается, что рабочие задачи не ограничиваются языком разработки. Оказывается (сейчас про бек в большей степени, но фронты не далеко ушли) надо знать SQL, уметь в Docker, уметь в Linux, чтобы банально выдать права на директорию, проверить свободное место на диске, погрепать логи своего продукта, понимать очереди и событийные модели. Если кандидат этого не умеет — он замучает команду и лида этими простыми вопросами.Мой вариант решения:Разработчик должен иметь широкое видение, естественно в рамках своего джуно‑мидло‑сеньор уровня.Структура моего собеседованияВременные рамкиУ меня выделено 1 час на интервью. Из них:10 минут — вступление, знакомство и переход к техническим вопросам.До 40 минут в среднем — обсуждение технических аспектов.Оставшиеся 10 минут — рассказ о вакансии и ответы на вопросы кандидата.Почему именно так?Когда у вас много собеседований, каждое из которых занимает более 1 часа, это становится утомительно. Поэтому я развиваю навык быстрого определения потенциала кандидата. Если я понимаю, что кандидат не соответствует ожиданиям, я не растягиваю собеседование и сразу корректно завершаю техническую часть.Моя стратегия собеседованияТемы для опросаУ меня есть определенные темы, которые я обсуждаю с кандидатами. Например, по бекенду (go/php/python) это язык разработки, подходы в разработке, SQL, базовый Linux, понимание Docker (от уровня — слышал, до пишу Dockerfile), понимание как работают очереди, событийные модели, как дебажить и какие инструменты есть/знает.Структура вопросовЯ начин аю с базовых вопросов и постепенно усложняю их, чтобы понять глубину знаний кандидата. Например по SQL — вопрос по джойнам → вопрос на группировку и работу с ней (Group BY + Having) → а как делать пейджинг и чем плох limit/offset → а какие варианты пейджинга есть еще → что такое индекс и как готовить → составные индексы → что такое селективность индекса → etc.Подход к ответамЕсли кандидат задумывается или я понимаю, что он не готов ответить — то на мой взгляд — не стоит включать режим «ну ты помучайся, мы подождем». Я или предлагаю небольшую подсказку, или предоставляю варианты выхода из ситуации: «если в моменте не помнишь — то расскажи хотя бы примерно, супер точный ответ не нужен» или «если с этим не работал — ок, не страшно, можем пропустить». И подчеркиваю, что «не страшно», чтобы кандидат не стрессовал сильнее и не завалил все остальные вопросы.Заключительная часть:Если кандидат произвел на меня впечатление, я предлагаю ему обсудить (именно обсудить, не писать код) конкретные задачи, чтобы понять его подход к решению проблем и глубину мышления.Пример задач:Нам надо написать систему деплоя, понятное дело, что в реальности мы возьмем jenkins/gitlab/etc, но давай попробуем порассуждать, как сделать это на php/go/etc. Вот прям от репозитория, до релиза на 100 серверов без даунтайма.А давай напишем свою очередь? Понятно дело, что в жизни мы возьмем pgq, rabbit, kafka, но давай попробуем написать свою систему? Что для этого нам надо? БД? Окей, а как работать с ней? Окей, а если у нас 20 вычитывателей и надо чтобы не было дублирования обработки данных?А давай напишем свою систему управления фоновыми скриптами? Вот чтобы отказоустойчиво, серверо‑распределенно и так далее.Выделяю 5–10 минут, чтобы обсудить, как бы кандидат решал эти задачи. Тут сразу большой спектр тем проверяется.Полемика и обучение во время интервью: Как реагировать?Когда вы задаёте вопрос на тему Х и понимаете, что ответ кандидата не соответствует действительности, каков ваш следующий шаг?Спорить с кандидатом?Это может вызвать оборонительную или негативную реакцию. Нужно ли вам это?Обучать на месте?Стоит ли, тратя своё время и усилия, и не всегда получая адекватный ответ в стрессовой ситуации?Я пришёл к выводу, что обучение во время интервью не всегда оправдано, потому что:Интервью — равноправное общениеИнтервью — это диалог между двумя профессионалами. Один предлагает свои навыки и время, другой — деньги и задачи. Это не отношения учитель‑ученик. Исключением является собеседование стажеров и начинающих специалистов, где наоборот, может быть интересно и полезно наблюдать за реакцией кандидата на обучение.Разные реакции на обучениеНе все кандидаты адекватно воспринимают попытки обучения. Некоторые могут реагировать негативно из‑за стресса, другие могут начать спорить, третьи могут воспринимать это лично.Неожиданная глубина ответаИногда ответ может быть настолько сложным/глубоким, что вы можете почувствовать себя не столь уверенно, по мере погружения.СтрессЕсли кандидат ошибается, акцентирование внимания на этом может только усилить его стресс.Тем не менее, иногда я делаю исключения. Если я чувствую, что мои комментарии будут полезны для кандидата или если я хочу увидеть его реакцию на критику, я могу высказать своё мнение.На что еще смотретьМанера общенияВялоеткущее собеседованиеПосмотреть на манеру общения, особое внимание на (кажется что это мелочи, но они очень важные):Слова-паразитыЗа 60 минут собеседования — они может быть и не помешают, но когда это будешь слышать каждый день в работе — это будет напрягать. Пример «нууууу типа это», «как бы это сказать».Четкость мыслиЕсли там небылинные (прям фантастические) рассказы про приключения или постоянный смех, поток шуток и подобные странности — это тревожный звоночек и возможно не стоит связываться.Легкость общенияИногда клещами не вытянешь информацию — хотите ли вы каждый день заниматься вытягиванием информации?ЭнтузиазмВидно ли вам, что у кандидата глаза горят? Или, наоборот, он выгорел и устал от своей специализации и придет к вам в команду делиться выгоранием?С этим человеком ведь предстоит общаться вам очень долгое время и много!Соответствие знаний и обучаемостьУбедитесь, что знания кандидата соответствуют вашим текущим задачам. Например, разработчик может знать все о сложных аспектах баз данных, но не понимать основ, которые вы используете каждый день.Вам, скорее всего, нужен разработчик, который может решать ваши реальные задачи, а не только олимпиадные. Однако если кандидат обучаем, то он может быстро освоить необходимые навыки. Главное — понять, почему он до сих пор этого не сделал.Конец собеседованияЕсли НЕ подходитТут я веду стандартно — «У меня вопросы кончились, давай кратко расскажу тебе о вакансии, и отвечу на твои вопросы». Я кратко рассказываю о вакансии и кратко отвечаю на вопросы.Суть — уважение. Человек потратил свое время, отвечал на мои вопросы, надо ответить взаимностью. Уважение — важная штука, она должна быть постоянно с нами.Если подходитТут все интереснее — кандидат мне себя продал. Теперь моя задача — продать мою вакансию кандидату.Как продать?Для себя я вывел такие пункты:Персональное впечатление:Важе н ваш контакт и как вы выглядите в глазах кандидата. Потому позитив, хорошая речь, внешний вид — важные моменты в продаже тоже.Четкость информацииНадо уметь четко рассказать о проекте и задачах. Чтобы не экать, не мекать, не вспоминать мучительно долго — какие задачи у вас там были, а какие можно говорить, какие нет, а как их преподнести человеку без контекста? Тут важно сразу сделать себе заготовку — прописать то, что вы расскажете о проекте и компании.Полный обзор и широтаНе просто «нуууу мыы это — на пехапе кодим, вот да. Будешь с нами?»А, к примеру (для php бекенда) — у нас php почти самый свежий, redis, postgresql, pgq, фреймворк symfony, брокер kafka, следуем стандартам psr, работаем с api как по rest, так и по grpc….Рассказать по инфраструктурные сервисы — у нас полный пакет atlassian — jira, как таск‑трекер, bitbucket для кода и кодревью, confluence для базы знаний, есть teamcity, sentry, docker, sonarqube, vault…..Рассказать про процесс разработки — у нас скрам/канбан, есть ретроспективы командные/личные, есть кодревью, проверка кодстайла……Рассказать о команде — у нас столько то разработчиков в команде, есть QA/дизайнеры/аналитики/овнеры/эксплуатация/дба/админы. Показать, что человек не будет оркестром, а есть большая команда профи.Отметить, что компания довольно большая, и у нас много команд разработки на разных языках.Рассказать пару примеров крутых задач, которые у вас стоят или были.Подача информацииВажно максимально красиво подать вакансию, но так, чтобы это было правдой. Лучше не акцентировать внимание на негативных аспектах, таких как бюрократия, поскольку:минусы есть везде и у всехне минусами живут продажи вакансийну и мы же работаем над устранением минусов? Значит это временная недоработка 🙂ЛайфХакиТеперь поговорим о полезных приемах, которые сильно упрощают проведение собеседований.Помогалка вопросовИметь готовый список тем и вопросов для обсуждения на собеседовании — это ключ к эффективному собеседованию.Я провожу собеседования с кандидатами на позиции php/go/python/js/devops, иногда у меня бывает по 3–4 интервью в день, и они идут одно за другим по разным специализациям. В такие моменты настоящим спасением становятся заготовленные списки тем для обсуждения.В моем случае — это набор Google Doc файлов. Для разработчиков, для devops, для ml.Краткий пример файла для разработчиков (покажу как некоторые пункты выглядят в реальности у меня):PHPчто такое Exception, и чем он лучше или хуже, чем return false/null? Как устроена конструкция try, какие блоки там есть?интерфейсы/приватные данные — в текущем своем проекте, как ты понимаешь, когда надо новый интерфейс завести или пометить данные приватными?….GOгорутины — что это? чем отличаются от системных потоков? как работает? как идет обмен данных между процессами?….Python…..JS…..SQL…..Linux…..Docker…..Софт‑скилы:у тебя на доске задача с непонятной тебе постановкой — что будешь делать? А если заказчик/лид в отпуске или не отвечают? А если задача срочная?конфликт на код ревью (по обе стороны баррикад) — ты написал код, у коллег появились к нему замечания, что будешь делать? А если считаешь замечания не справедливыми? А если наоборот — ты написал замечания, а на них не отреагировали?пришли замечания от тестировщиков по задаче — что ты будешь делать? А если там замечания, которые к задаче не относятся?…..и т.д.В зависимости от опыта кандидата, от языка разработки - по каким то блокам идем, какие то пропускаем.Помогалка по вакансии/проектуТакже крайне полезно иметь под рукой информацию о текущем техническом стеке и задачах, которые вы можете предложить кандидату. Эти детали часто интересуют кандидатов и могут сыграть важную роль в процессе «продажи» вакансии.Вместо того чтобы вспоминать на лету и метаться в памяти, думая: «У нас есть Git, Jira, Redis... Что еще? А, да, Elastic и наш собственный S3, что же еще я забыл?....», гораздо проще быстро взглянуть на подготовленный список. Это позволяет дать четкий и уверенный ответ без лишних пауз и «эээ...».Четкость и уверенность в речи создают профессиональное впечатление и помогают установить доверительные отношения с кандидатом.Для себя я такой список вывел по разным вакансиям, держу его в GoogleDocs, чтобы он всегда был у меня под рукой с любого устройства.Если будет интересно — могу попробовать отдельной статьей показать и рассказать об этих помогалках.Помогалка в оценке кандидата или «скоринг»Когда идет много собеседований, то картинка по каждому кандидату может размываться, и уже сложно вспомнить детали собеседования, ответы кандидата и уровень его подготовки.И тут на помощь приходит «скоринг» — система оценки кандидата.Если подходить формально, то можно составить список заранее определенных критериев, в качестве шаблона. И по мере прохождения собеседования — отмечать пункты как пройденные, или проваленные. Желательно это делать незаметно для кандидата, чтобы не раздражать и не сбивать с толку.Можно использовать систему баллов, где за каждую тему/критерий/вопрос можно выдавать некоторое количество баллов. К примеру — минимально приемлемый ответ — 1 балл, а максимально крутой — 3 балла. Сумма баллов — будет оценкой кандидату.В своей практике я пришел к следующему варианту — либо распечатываю суть резюме кандидата на 1 странице, с 1 стороны, либо в бумажном блокноте на отдельном листе записываю фамилию и имя кандидата.Далее, по мере прохождения вопросов — оставляю краткие заметки на этих листках, типа:Иванов Иванphp -> Exception +, private ++, interface -,sql -> having ~, offset -, пейджинг - -активный, быстрые ответы, ….По сути — это краткие ссылки‑ключи на мой Docs файл с вопросами, и простыми оценками хороший ответ (+), супер отличный (++), или плохой (‑), а может быть «так себе, пойдет»(~).По итогу ряда собеседований у меня остается набор листков, с заметками по каждому кандидату. В случае распечатки — еще и с резюме, зачастую с фото, что упрощает восстановление визуальной памяти собеседов ания.Помогалка «фильтр».Помогалка «фильтр» — это предварительный небольшой тест, который оказывается особенно полезным, когда нужно быстро закрыть несколько вакансий, а времени на полноценное собеседование всех кандидатов не хватает.Процесс начинается, когда HR находит подходящих кандидатов. Далее, важно связаться с ними заранее, предпочтительно через телефонный звонок или веб‑созвон, поскольку текстовая коммуникация в этом случае часто оказывается менее эффективной. HR описывает вакансию, и если кандидат заинтересован, ему задают 5–10 простых вопросов. Ответы на эти вопросы позволяют отсеять явно не подходящих кандидатов. Эти вопросы могут касаться опыта работы с определёнными технологиями или базовых знаний. Например, вопросы могут быть следующими:1. что значит having в sql?Ищем в ответе “фильтрация результатов группировки”2. с какими no-sql БД есть опыт?Ищем в ответе “redis / mongo”3. какие методы есть в rest api?Ищем в ответе “GET POST DELETE PUT PATCH”4. как посчитать количество 404 ошибок в логе на сервере?Ищем в ответе “grep wc”, если слышим “awk cut пайп” - тоже не плохоЕсли 50–80% этих вопросов успешно закрыты — тогда ставим кандидату уже основное собеседование.Важно, чтобы тест был относительно простым (оценивать будет HR, который не является разработчиком) и быстрым для прохождения — длительностью на пару/несколько минут.Напоминаю, что наша цель — выявить подходящих кандидатов, дать им возможность проявить себя, а не отсеивать их на раннем этапе. Потому фильтр должен отсеивать прям совсем не подходящих кандидатов.Что я сознательно пропустилОнлайн-коддингЯ не сторонник данного подхода. В обычной жизни у нас своя любимая IDE, со своими настройками, плагинами. Мы можем использовать AI помощников, stackoverflow и прочее. В общем — настроенное окружение и изученный проект.В условиях онлайн‑редактора на собеседовании доступа ко всем этим ресурсам может не быть, что лишь усиливает стресс.Ситуация, когда можно спокойно заварить кофе, подумать и поискать информацию в интернете, заменяется необходимостью адаптироваться к новому окружению в ограниченное время, что заставляет кандидатов сильнее выходить из зоны комфорта.Тестовые задания «на дом»Для стажеров и начинающих специалистов такой подход может быть оправдан, поскольку позволяет оценить их мотивацию, способность искать решения и применять их на практике. Однако для более опытных специалистов это может быть нецелесообразно по нескольким причинам:Оценка их навыков требует задач повышенной сложности, не сравнимых с простым калькулятором. Более сложные задачи требуют значительного времени, которое не каждый кандидат готов потратить.Время выполнения заданий и круг лиц, принимающих участие в их решении, могут быть неопределенными. Нужны ли вам кандидаты, которые решают задачи в два-три раза медленнее, чем остальная команда?Возможность использования AI-инструментов, таких как ChatGPT, при решении заданий ставит под вопрос ценность такой оценки. Что именно мы тогда оцениваем?ИтогиСобеседование — это больше, чем проверка навыков; это важное взаимодействие, формирующее будущее компании и карьеры кандидата.Каждый кандидат — уникальная личность, и ваша задача — помочь раскрыть их потенциал.Подходите к каждому собеседованию как к возможности создать нечто большее, чем просто рабочие отношения. Это шанс найти талант, который станет частью вашей команды и вместе с вами будет двигать компанию вперед.Следуюшая статья из серии:Фантастические таланты: Эпизод II – Атака внедрения"
СберМаркет,,,Превращаем планшет в терминал самообслуживания: осваиваем режим Kiosk в KNOX сервисах,2024-09-28T08:27:57.000Z,"Что такое киоск? Это ограничение набора приложений, которые доступны пользователю. Иными словами — замена лаунчера по умолчанию. Когда доступно только одно приложение, нельзя открыть браузер, другие приложения и даже залезть в настройки. Вообще ничего не доступно. Это как защита от нецелевого использования, так и от дурака…Давайте сделаем киоск режим для нашего приложения на планшете. Через сервисKNOX Configureэто делается быстро и без программирования. Ниже расскажу об этом на примере.Это вам не ларёк, а киоск. Киоск звучит гордо. Киоск бывает не только у союзпечати. Киоск, который не снести. Что за зверь это киоск мод.Применение Киоск режима – это не только интерактивные стенды для:бронирования переговорных комнатформирования очередейнавигации по бизнес-центруприема оплат с помощью приложения POS терминаламеню в ресторане для заказа и оплаты счетаcheck-in или коммуникация с персоналом в номере гостиницыно и кейсы, в которыхвнимание сотрудников фокусируют на решаемой задаче:оформление заявокпроведение инвентаризацийcheck-list для обходчиков при проверке оборудованияэлектронная подписьпроведение опросови так далее. Может в комментариях предложите ещё варианты?Пример: бронирование переговорныхВ нашем примере мы повесили панели у входных дверей в каждую переговорную комнату. Таким образом, стало возможно просто подойти к конкретной комнате, и увидеть свободные слоты на сегодня и сразу сделать нужное бронирование, пройдя идентификацию по лицу или вручную.Информационный киоск на планшете для бронирования переговорныхПользователю доступна только функциональность нашего приложенияПочему вообще правильно делать киоск на планшете, а не на дорогих профессиональных панелях? Экран — всегда экран, поэтому использовать последние бывает, как сложно и не нужно, так и не очень разумно. Это всегда дело бюджета, вкуса, требований заказчика и обстоятельств.Только для бизнес-решений. Шаг первыйKNOX Configure — это B2B сервис по удалённой кастомизации устройств Samsung, пробную версию которого можно испытать в течение 30 дней после оформления trial лицензии наKnox Adminпортале. Начинаем! Регистрация должна получить подтверждение, потому что сервис предназначен для клиентов-компаний и организаций, а не для физических лиц. За этим следят.Получаем лицензию. Шаг второйВ нижнем левом углу главной страницы личного кабинета выбираем раздел лицензии, нажимаем Get License и генерируем Knox Configure Dynamic (Per seat) free trial лицензию. Setup вариант годится только для первоначального развёртывания любого приложения на устройствах и не подходит для реализации режима киоск. Setup годится только если вам нужно автоматически раскидать по всем новым устройствам ваше приложение, анимации загрузки и выключения, обои и т.д. Такую схему мы использовали, избавив учителей от проблем с инсталляцией решенияSamsung Class. Но когда нам нужен киоск, мы используем Knox Configure Dynamic. Его основное отличие состоит в том, что конфигурация устройства находится под постоянным контролем Knox Configure иможно over the air менять приложения, контент и режим функционирования планшета.Генерируем free trial лицензию для Knox Configure DynamicСоздаём профиль настроек устройства. Шаг третийЗатем переходим в раздел Profiles, жмём CREATE NEW PROFILE > Setup/Dynamic Profile и заполняем название профиля. Указываем, что профиль создаётся для смартфонов и планшетов, а на следующей шаге выбираем нужные намDynamic editionиProKiosk modeДобавляем QR код для подключения к WiFi и сервису. Для его чтения надо нарисовать крестик на самом первом экране перед запуском Setup WizardНа этапе конфигурации настроек и добавлении QR кода обязательно отметьте checkbox «Also allow QR code enrollment for devices not uploaded by a reseller». Хотя обычно продавец партии, если он официальный партнер компании, добавляет IMEI устройств покупателю в его кабинет на портале Knox сервисов и, назначив им нужный профиль,можно обойтись и без QR кода. При первой загрузке планшет скачивает настройки и приложение. Это удобнее ручного enrollment.Когда и как считывать QR код. Нарисовать пальцем крестик на Welcome screen, который отображается при первом включении планшета или после factory resetНа последней странице настроек профиля загружаем наше приложение, которое будет работать в качестве киоска, убираем всё лишнее: уведомления, статус бар, часы, системные иконки.Загрузка приложения и настройка ограничений функциональностиРешаем проблему батареи при постоянном питании. Шаг четвёртыйРаботая киоском, устройство постоянно подключено к питанию, поэтому выбираем ограничение заряда батареи до 85%. Постоянная зарядка не способствует её долговечности.В идеале батарею нужно вообще вытащить, но это возможно только на защищенных планшетах серии Samsung Tab Active со съемной батареей, как мы и поступили. Защищённость и антивандальность — дело хорошее в ситуациях, где могут происходить  неприятности.С защищенным планшетом можно делать не только такие вещи, но и вытаскивать батарею. Подключенный к сети он будет работать без неё.Запрещаем все обновления Android. Шаг пятыйПереходим к самому важному:выключаем обновления операционной системы, исключая кризисы несовместимостей промышленной эксплуатации со следующими версиями AndroidЕщё раз отмечу почему мы использовали для панелей бронирования переговорных защищенные планшеты Samsung Tab Active: есть возможность вытащить батарею и избежать проблем с ней из-за постоянного подключения к питанию на протяжении многих лет. Для тех, кто хочет использовать другие модели можно ограничить заряд до 85%, но лучше вынуть батарею.  Основное же, на мой взгляд, — это отключение обновлений операционной системы. Если что-то хорошо работает, не надо это трогать. Более того, надо исключить все возможности для этого.ВыводыКакова стратегия Samsung в B2B: оказалось, что многофункциональный KNOX SDK — это слишком сложно для рядового бизнеса, потому что программирование велосипедов стоит дорого. Проще создавать сервисы под потребности целевой аудитории по мере их выявления.Один из примеров таких сервисов — Knox Configure для коммерческой эксплуатации планшетов в качестве интерактивных киосков. Без лишней головой боли получаете всё необходимое:автоматическое развертывание приложения out‑of‑the‑boxцентрализованное обновление приложениянастройки избегания быстрой деградации батареинастройка запрета обновлений операционной системыОпять же напомню, что батарею проще извлечь физически, но для этого нужны планшеты B2B серии Samsung Tab Active, который я использовал для тестирования всего описанного выше. Упомянутое решение для переговорных комнат, фотография которого есть в начале статьи, мы создали в своём отделе и используем на протяжении трёх лет без сбоев и нареканий."
СберМаркет,,,Мобильный AI на рабочем месте. Ищем реальную ценность,2024-07-30T15:36:17.000Z,"Пенни: Как заколки могут привлечь мужчин?Говард: Добавим туда BluetoothШелдон: Гениально! Мужчины обожают Bluetooth!Пенни: Подождите, вы хотите сделать заколку с Bluetooth?!Шелдон: Пенни, все становится лучше, когда есть Bluetooth!Теория большого взрываО хайпе вокруг AIЕщё пятнадцать лет назад я думал о том, почему в смартфоне нет функции создания транскрипта аудиозаписи.  Диктофон превратился в приложение для смартфона, но по-прежнему требовал последующего прослушивания и ручного конспектирования аудио. А вот AI сделал транскрибирование доступным рядовому пользователю.И эта мысль навела меня на размышления об искусственном интеллекте вообще. Я не понимал причину шума вокруг AI все эти годы. Можно понять специалистов по Natural Language Processing или компьютерному зрению, у которых реально возросла эффективность алгоритмов благодаря глубинному обучению. Остальным-то что с этого?Становится ли всё лучше, когда естьbluetoothAI? Я считаю, что не становится! Как в известной пословице,  «сколько ни говори халва, во рту слаще не станет». Пока NVIDIA, как и положено во времена золотой лихорадки, зарабатывает на кирках и лопатах, аналитики Goldman Sachs ужевыражают скепсис, не переоценивает ли общественность перспективы влияния AI на мировую экономику. На данный момент нет никаких практических применений AI для массового использования, которые перевернули бы мир. А массовое значит мобильное, потому что смартфон является основным инструментом потребления IT-сервисов.Может стоит не рассуждать о революции, а посмотреть на ситуацию с практической точки зрения: какие из повседневных задач хотя бы частично удается автоматизировать при помощи AI? Например, та же расшифровка аудиозаметок. Не прошло и двадцати лет, как штатная программа звукозаписи в смартфоне, наконец-то закрыла эту потребность. Конечно, онлайн сервисы уже существовали, однако только сейчас на смартфонах (а это и есть мобильный AI) в приложении «Звукозапись» появилась функция расшифровки аудио on-device, без подключения к облаку, а значит без рисков, связанных с конфиденциальностью.Транскрибация и резюмирование экономят любому офисному работнику столько сил и времени, что теперь можно начинать говорить о повседневности AI в бизнесе и постепенно к ней привыкать. Совещания, обучение, интервью – лингвистическая модель Galaxy AI пригодится везде, где необходимо конспектирование и анализ аудиальной информации, с которой проще иметь дело в текстовом виде.AI-сервисы продолжат качественно развиваться, ведь обучение алгоритмов, как и людей, требует времени. Поэтому в этой статье я постараюсь не просто описать свой опыт использования этих сервисов, но и сформулировать их реальную ценность в том виде, в котором они доступны сейчас. И объяснить, для чего нам это вообще нужно.Бизнес-применение 1: Полный транскрипт аудиозаписейРаньше я не пользовался функцией аудиозаписи, потому что у меня никогда не хватало терпения и времени прослушивать заново весь разговор.  Меня всегда интересовала возможность автоматического транскрипта голоса, чтобы по ключевым словам, вспомнить, о чем шла речь, например, на прошедшем совещании или лекции. Уже практически потерял веру в то, что это будет возможно, но дождался.Приведу пример использования функции транскрибирования в приложении «Звукозапись» на новых флагманах Samsung с Galaxy AI. On-device. Бесплатно. Обработка аудиозаписи с помощью AI проводится прямо на устройстве, файлы не уплывают в неизвестные облака. Это и есть одно из применений NPU (Neural Processing Unit).Я записал совещание, проводившееся по телефону, которое длилось тридцать минут. Записывающий смартфон лежал рядом с другим телефоном, по которому велся разговор на громкой связи. После завершения записи и нажатия кнопки «Транскрибир.» аудио обрабатывалось три минуты. В результате на экране приложения появился аккуратно отформатированный текст, с которым удобно иметь дело.Расшифровка аудиозаписи в приложении Звукозапись на смартфоне Galaxy Flip 6Программа продолжает работать и в свёрнутом виде, и с выключенным экраном. А вот ответ на входящий звонок запись приостанавливает. Для использования функции аудиозаписи на совещании или во время мозгового штурма в просторной переговорной рекомендую предусмотреть микрофонную систему. Даже пара простых беспроводных микрофонов решает многое, ведь качество аудиозаписи напрямую влияет на качество распознавания.Сгенерированный на основе аудиозаписей текст пока далек от идеала и требует редактирования. Чисто субъективно английскую речь Galaxy AI сейчас распознаёт лучше, чем русскую, в тексте меньше путаницы с определением начала и конца предложений. Но оно и понятно: функция поддержки русского языка появилась только весной этого года, поэтому разработчикам есть над чем поработать. Это вопрос обучения и объема данных: с каждым обновлением качество расшифровки будет улучшаться. Кроме того, в русской речи встречается много англицизмов, словосочетаний на иностранных языках и иноязычных аббревиатур, которые сейчас не всегда хорошо понимаются движком.Расшифровка и сводка записи в приложении ЗвукозаписьРасшифровку дискуссии, в отличие от записи лекций или длинного монолога, который AI понимает гораздо лучше, непосвященному человеку разобрать пока сложно из-за наличия в тексте некорректных сочетаний слов и явных ошибок распознавания. А вот участник обсуждения легко вспомнит, о чем именно шла речь и восстановит содержание разговора. Выделенные в тексте слова или фрагменты можно прослушать в оригинальной аудиозаписи. Диалоги, состоящие из коротких высказываний распознаются хуже, чем на английском, но я жду улучшений по результатам дальнейшего обучения алгоритмов. Самое интересное, что сводка всегда выглядит осмысленнее отдельных элементов транскрибации несмотря на качество расшифровки. Расскажу об этой функции поподробнее.Бизнес-применение 2: Краткость — сестра талантаЯзыковые модели призваны создавать не только «вау-эффект» от кратковременной симуляции «живого» разговора с GPT чатом. В современном мире мы потребляем огромное количество информации, определить стоит ли нашего внимания новый объемный материал или нет поможет функция краткой сводки, как одна из реализаций потенциала Natural Language Processing. Вообще, резюмирование содержания — это то, что все ждут от AI.Сгенерировано в https://rudalle.ru/kandinsky31Поэтому Yandex внедрил в свой браузер функцию краткого пересказа видеороликов, так что можно просмотреть краткое содержание буквально за минуту. Samsung оснастил свой мобильный браузер функцией создания сводки, что особенно полезно для тех, кто имеет дело с текстами и их аналитикой. Эта функция доступна не только в браузере, но и при создании транскрипта аудиозаписей. Я сравнил Yandex пересказ и сводку от Galaxy AI от прослушиванияОбзора направлений развития 6G. Для поиска подходящего контента Яндекс – однозначный выбор, потому что сводка создается очень быстро, всего за несколько секунд. Если же смотреть YouTube-ролик, используя Galaxy AI, сводка будет ещё более подробной. Те, кто пользуются двумя телефонами, как я, могут вообще оставлять один из них у компьютера, воспроизводящего нужный контент, и заниматься другими делами, а потом просмотреть полученный транскрипт и сводку, отражающие содержание и структуру контента. Если вы когда-либо имели дело с онлайн-обучением, то знаете, что часто никаких возможностей законспектировать содержание просто нет. Все надо записывать и переводить в текст.Так выглядит краткий пересказ от Yandex и сводка Samsung. Работу со сложными техническими материалами можно упрощать при помощи AIА вот как выглядит сравнение стандартной и подробной сводки классического отрывка письма Татьяны Онегину из романа «Евгений Онегин» на примере AI-анализа при создании сводки в браузере Samsung Internet:Слева представлена краткая сводка, а справа подробная. По-видимому, с резюмированием художественного текста AI справляется куда лучше. Ваши версии, почему?Функция запускается нажатием кнопки в нижней панели мобильного браузера Samsung Internet. Я открыл первый том романа «Война и мира» на lib.ru и попробовал сделать сводку. В результате мне была показана сводка лишь первой главы со сценой салона Анны Павловны Шерер. Никаких сообщений об ошибках браузер не выдал. Сервис просто проигнорировал основной многокилобайтный контент произведения. Зато любая статья на Хабре обрабатывается без проблем. Не нашёл официальной информации по ограничениям. Конечно, любой сервис имеет свои пределы разумного использования, и разработчики что-то противопоставили попыткам «повесить» облачный сервис запросами на разбор грандиозных по объему текстов. Как я понял, ориентировочно, это одна глава «Войны и мира».Сразу отмечу, что функция создания сводки не заменяет нашу способность к обобщениям и выводам. Если транскрибация восстанавливает детали субъективной и объективной картины происходившего, то чтение сводки помогает нам с оформлением собственных выводов и обобщений.Функция создания сводки использует облачный сервис, в отличие от транскрибирования, выполняемого на телефоне. Некоторых пользователй это может тревожить. Тут я задумался, что, вообще, может помешать повсеместному применению перечисленных выше функций?Барьеры для бизнес-применений AIГлавная сложность, как мне кажется, – это готовность собеседников к ведению переговоров «под запись». Законодательство требует охраны персональных данных, а  этика – согласия участников встречи на запись. Мы не склонны быть неформальными в общении, зная, что слова «пишут пером», поэтому в некоторых случаях овчинка использования записи для резюмирования выделки не стоит. Особенно в ситуация, когда доверие важнее учёта. Запись фиксирует оговорки, замешательства, реакции или промежуточные мнения, про которые не все участники переговоров хотели бы знать, что их «не вырубишь топором». Однако ситуация меняется, когда встречи менее эксклюзивны, более формальны, публичны или поставлены на поток, например, собеседования соискателей на работу, записи презентаций или докладов, глубинные интервью и опросы в маркетинговых исследованиях.Бизнес-применение 3: грамотность как самопрезентацияЯ всегда ленюсь исправлять «очепятки» в мессенджерах и не использую T9, не говоря уже о тире, запятых и двоеточиях. Это вечный конфликт между «меня и так поймут» и внутренним желанием всё исправить. С появлением AI-корректора я постепенно привыкаю к его использованию, и это касается не только орфографии и пунктуации. Ниже приведу пример того, как корректор исправляет текст, предлагая качественные альтернативные варианты даже для самых безграмотных формулировок.Теперь функция проверки орфографии и стилизации есть в каждом мессенджере на смартфоне Samsung с Galaxy AIТо же касается стилистики и способа выражения своих мыслейЭльдар Муртазинпишет, что по данным его источника, который он не приводит, женщины используют эту функцию чаще мужчин. Логично, кто обычно больше следит за своим стилем? Гендерные стереотипы проявляются и в статистике использования AI функций, ставших доступными массовому пользователю.В чем реальная ценность корректора стиля и грамотности? Это возможность не гуглить ответ на вопрос «как правильно», а нажатием кнопки увидеть и сделать так, как должно быть. А там глядишь постепенно уйдут в прошлое из оборота все эти «вообщем», «через чур», «ихний» и прочие «тся/ться».Бизнес-применение 4: генерация текстовЭта функция доступна на базе клавиатуры Samsung. Нажимаем на кнопку слева вверху и затем выбираем «Генератор текстов». Стилистика генерируемых сообщений может варьироваться в зависимости от контекста, который вы задаете: стандартная, электронная почта, социальные сети, комментировать, а также от стиля: вежливый, повседневный или профессиональный. Samsung предлагает двенадцать вариантов сказать по-разному об одном и том же.Сгенерированные Galaxy AI варианты. Какие-то фразы могут выглядеть неуместными, но ничто не мешает их заменить или убрать совсем. Редактировать проще, чем подбирать фразы самомуКаждый раз генерируется новая форма, предлагающая новые формулировки, поэтому если у кого-то возникает вопрос «как написать, ответить или предложить», — Galaxy AI лучший помощник, чтобы подобрать «рыбу» ответа, которую можно отредактировать в финальную версию. Теперь это одна из функций клавиатуры, что действительно удобно.Предлагаем встретиться оффлайн с Galaxy AI. Мечтаю, когда AI научится угадывать гендер собеседника и подставлять нужные местоименияНа мой взгляд, функция «генератор текста» прекрасный инструмент для избавления от привычки откладывать в долгий ящик желание написать. Иногда причина этого банальна. Не хочется писать много букв. А вот когда AI  готов подобрать варианты для написания и оформления текста, то это бесценно.Бизнес-применение 5: бытовое общение в командировках и на отдыхе / Функция «Переводчик»Невозможно пройти мимо Flip-версии переводчика, которая позволяет обмениваться простыми и понятными репликами. Эта программа  открывается через панель быстрого доступа. Flex экран активируется второй кнопкой из расположенных справа вверху открытого приложения.Собеседник видит перевод на своем экране и может нажать на кнопку записи для того, чтобы произнести ответную фразу для переводаТакже есть удобная специфика отображения и для нескладного варианта, доступного и на смартфонах с Galaxy AI без flex экранаДаже без сопровождающего носителя языка и переводчика можно уверенно чувствовать себя, решая простые вопросы с сотрудниками ресепшена, таксистами и случайными встречными, если вам потребуется помощь. Это гораздо удобнее, чем вертеть экран туда-сюда.Что касается синхронного перевода телефонных разговоров, то лично я жду реализацию функции live-транскрибации голоса собеседника в помощь уже владеющим иностранным языком на хорошем уровне, но испытывающим сложности с восприятием иностранной речи на слух. Произношение в разных странах и регионах сильно различается. Америка, Англия, Корея, Китай, Индия и иные не носители языка, но использующие английский в бизнес коммуникации. Было бы интересно общаться в наушниках или на громкой связи и видеть, как AI понимает сказанные собеседником слова вместе с их переводом. Это могло бы быть неплохой помощью тем, кто уже общается на других языках по бизнесу.Как всё может развиваться дальшеIDC (International Data Corporation)смотритна мобильный AI оптимистично, но снабдить смартфон нейронным процессором ещё не значит найти ему реальное применение в массах. Во-первых, дело это не быстрое. Например, с момента появления смартфонов до понимания реальной ценности, которую они внесли в нашу жизнь прошло не меньше десяти лет. Оказалось, что большой экран — это не столько средство для потребления развлекательного контента, сколько удобство для обмена текстовыми сообщениями. Мобильные мессенджеры потеснили даже социальные сети. Вряд ли с AI дело будет обстоять по-другому, то есть реальная и меняющая всё ценность станет очевидной не сразу, а лишь по мере развития практики применения.Возможно, когда анализ данных AI-помощником начнет представлять достаточную ценность, люди станут проще смотреть на его доступ к содержанию всей своей коммуникации через смартфон, включая голосовое общение, мессенджеры или запись всего происходящего вокруг. Тем более, если получится обеспечить как можно больше AI-функций возможностью выполнения на непосредственно устройстве. Вообще идея личного ассистента, повсюду сопровождающего пользователя, популярна в научной фантастике, однако пока не исчерпала себя, и мобильный AI – это очередной шаг в этом направлении.Сгенерировано в https://rudalle.ru/kandinsky31ВыводыЯ перечислил лишь те функции, которые нашли применение в моих рабочих буднях, упростив решение многих повседневных задач. Помимо названных выше сервисов, отдельно упомяну AIредактор, который позволяет дорисовывать картинки на основании «корявого» эскиза от руки. Возможно, я и ему найду практическое применение в повседневном использовании. Я дорисовывал улыбочки с такими непредсказуемыми и забавными результатами. Galaxy AI самостоятельно определяет эскиз чего я рисовал и подбирает место на фото.Оживление реальности с помощью эскизов и AIНаконец-то появились достойные решения, хотя кому-то все они могут показаться недостаточными. Немного написал о будущем, в котором, на мой взгляд, мы будем иметь дело с развитием идеи смартфона как личного ассистента в ежедневных делах. Мы увидим еще много новых решений и возможностей, включая улучшение качества алгоритмов, пока использование AI найдёт себе такой же массовый спрос, как и смартфоны, которые есть у каждого. Прогресс очевиден, но истинные  масштабы и значение мобильного AI мы сможем оценить лишь в течение десятилетий.Пользователи Хабра уже нашли золотую середину в разговорах об AI: это хороший инструмент и его надо воспринимать именно так. Искать его практическую ценность в своей жизни, а не витать в облаках. Интересно, какие идеи насчет настоящему и будущего мобильного AI выскажут читатели в комментариях.Медведев Павел, Samsung"
СберМаркет,,,Сложно ли пронести гаджет в школу,2024-08-30T16:00:34.000Z,"— Рыцарь — это человек, он без страха и упрёкаПетров и ВасечкинЭто история разработки B2B решения для планшетов, чтобы увеличить их востребованность в сфере образования. Вначале мы совершили все возможные ошибки: считали себя самыми умными, путались в «болях» целевой аудитории вместе с фокус-группой. Всё нравилось менеджерам, разработчикам и, главное, начальству. Затем пилот решения дошёл до пользователей и пришло время отваги для осознания наших заблуждений.Всё началось с участившихся запросов на применение планшета в школе. Мы съездили в несколько школ с целью выяснить, какая «боль» вызывает этот интерес, а какая не даёт его реализовать. Рядом с нами были коллеги-эксперты изIT Школы Samsung, которые помогали нам в исследовании. Мы наивно полагали, что всё сразу сделаем в лучшем виде.К этому просто надо быть готовымУзким про­ли­вом мы плы­ли, и в серд­це тес­ни­лись сте­на­нья;Сцил­ла с это­го боку была, с дру­го­го Харибда…Гомер. Одиссея. (греческий эпос 8 в. до н. э.)Эта статья о конечном видении продукта Samsung Class, реализовавшего «что хотела целевая аудитория». А еще о функциональных ролях продакт-менеджмента и разработчиков, которым важно следовать для достижения такого результата.Как сосредоточиться на боли пользователя и этим не создавать ему новыхСосредоточиться на «боли» целевой аудиторииТолько Samsung оказался абсолютно открытым к диалогуи готовым слышать то, что нужно реально для того,чтобы продукт стал образовательнымКонстантин Эдуардович ТхостовДиректор лицея №369, Санкт-ПетербургКак изменился урок в школе за последние двадцать лет? Дети остались те же, разве что гаджеты разбаловали их. А еще теперь по закону проносить их в школу нельзя. При этом, как и раньше, учитель для реализации плана урока за короткое время решает две задачи:ВовлечениеУдержание вниманияУченики пришли с другого урока после перемены и мысленно ещё там или вообще в смартфоне, несмотря на формальный запрет. Когда они мыслями придутв себясюда?Чем занято внимание детей, только пришедших на урокБлогеры YouTube и TikTok разбаловали всех соревнованием за секунды внимания, и учителю теперь приходится конкурировать с ними.Физическое присутствие на уроке вовлеченность не гарантирует. Для решения этой задачи уже есть метод — разнообразие педагогических приемов, и мы решили, что добавить в него еще вариантов — это адекватная и благородная цель.К тому же, с древних времён осталась незакрытой тема «мне плохо видно с задней парты», потому что первых парт мало, а учеников много. Вовлечение и удержание внимания удаленных парт даётся объективно тяжелее, чем ближних.Проблема задних рядов в школеС потребностями аудитории определились и началось самое сложное: соблазны забить продукт трендовой функциональностью. Настало время героических решений.Сперва честно признались себе, что не будем делать ничего про дистанционное обучение: мы про планшеты в оффлайн классе.Затем избавились от навязчивого желания сделать что-то вроде очередного приложения для образования.Сложнее всего было выкинуть мысль встроить мессенджер, потому что они сейчас везде.Итак, мы определились с целью:адаптировать планшеты к использованию на оффлайн уроке. Это означает, чтопланшеты должны быть окном внимания к учителю. Привычным окном, поскольку дети привыкли к гаджетам. Вместо того, чтобы сетовать на эту привычку, мы используем её во имя педагогического разнообразия. Для этого мы решили обеспечить связь между планшетами учеников и учителя с помощью нашего приложения Samsung Class.Важно не только понимать боль ЦА, но и сосредоточиться на ней при создании решения.Чем проще, тем лучше!Разработчик, не увеличивай «боль» целевой аудиторииКак мы рассуждали дальше со стороны разработчиков: никаких модных облаков или дополнительных серверов. Мы решили, чтоиспользование и развертывание будут простымидля обыкновенного учителя. Его планшет будет управляющим для планшетов учеников с возможностями:запуск трансляции экрана на устройства учениковзапуск трансляции содержания интерактивной панели на устройства учениковдоступ к экранам учениковотправка необходимых для работы файловудаленный и единовременный запуск приложений и веб-страницотслеживание «бездействия» учениковуправление политиками доступа на всех планшетахПриложение Samsung Class превращает разрозненные устройства в систему планшетов учащихся с планшетом учителя в качестве управляющегоСанитарные нормы диктуют минимальный размер устройства в 10.5”, поэтому базовым вариантом мы выбрали модели Samsung Tab A9+/S9. Интерактивная панель Flip Pro с диагональю от 65 до 85 дюймов это опциональная часть решения. Часто в классах уже встречаются подобные панели других производителей, и они тоже могут быть задействованы для вывода экрана планшета учителя.В качестве сервера используется планшет учителя.Никакой дополнительной инфраструктуры кроме сети Wi-Fi не требуется. Автоматическая установка и настройка реализованы с помощью сервиса Knox Configure: достаточно создать список серийных номеров в аккаунте сервиса и настроить удаленную установку программы.Мы предусмотрели даже вариант отсутствия Wi-Fi. В этом случае планшет учителя используется в качестве точки доступа. Но для мобильных точек доступа есть ограничение до десяти устройств, поэтому это опция скорее для мини-групп на природе.Как это выглядит на деле: планшеты «живут» в ящике с зарядками в шкафу, пронумерованы и закреплены за учениками согласно порядковому номеру в журнале.Весь урок на планшете не проходит, но использовать их на уроке имеет смысл. Например, в начале урока учитель проводит быстрый опрос, который позволяет в течение пяти минутвовлечь всехпришедших в урок, заодно проверив уровень усвоения знаний.Для решения задачи включения ученика в урок мы создали дополнительную опцию решения – проведение быстрых опросов. Альтернативными вариантами могут быть quiz-задания на уже используемых образовательных платформах или короткие ролики. Это на выбор учителя.Настройки системы тестированияПреподаватель имеет доступ к управлению происходящим на экране любого из учеников.  А также может отправлять им и на интерактивную панель изображение своего экрана. Таким образом, оно транслируется на устройства учеников, чтобы задним партам всегда «было видно». При необходимости преподаватель может снова подключать планшеты,единовременно запуская у всех приложения или открывая нужные ссылки.Когда мы вышли на оперативный простор, начав тестировать решение в реальных аудиториях, то стали выявляться аспекты, которые потребовали от разработчиков дополнительной работы. Приведу несколько примеров:Когда в классе много учеников, каждый раз подключать планшет через QR-код неудобно. Поэтому проще сделать шаблоны с названиями урока, аудитории, готовым списком учеников и выбранной папкой для материалов.Ученикам при использовании гаджетов захочется их «сломать». Тогда убираем таскбар и верхнюю панель доступа, так что запуском приложений на устройствах управляет исключительно учитель, а учащемуся доступны лишь папка материалов и ранее запущенные приложения.Иногда ученики авторизуются не под своими именами. Чтобы бороться с некорректной авторизацией, мы реализовали алгоритм, который автоматически привязывает ученика к планшету, используя соответствие номера планшета, устанавливаемого в приложении, с номером ученика в списке класса.И так далее.Удачно ли оказалось лавирование между Сциллой и ХарибдойСамым сложным было доносить идею о том, что Samsung Class это не образовательное приложение, вроде МЭШ, Uchi.ru или EdPad, а «предустановленный» B2B софт для образования, создающий из планшетов единый инструмент. Это не образовательная платформа и не контент, а техническая часть образовательной экосистемы.Samsung Notes на планшете учителя связывается с помощью Smart View с интерактивной панельюЭкран учителя доступен не только на интерактивной панели, но и на планшетах учениковПо отзыву преподавателя информатики лицея №369 Санкт-ПетербургаСкотникова Вадима Борисовичаосновные ценности решения:Безопасность— этопросмотр экранов учеников с устройства учителя. Контроль за происходящим на устройстве – это часть порядка в классе.Скорость— это управление и одновременный запуск приложений (или ссылок) на всех планшетах, что позволяет избежать траты времени: «а теперь запустите приложение…»Это одни из «болей», которые мы закрывали, чтобы гаджеты не усложняли ход урока.До Samsung Class планшеты уже нашли применение в образовательном процессе 369 лицея и решение оказалось полезным дополнением к немуВ лицее нашёл свое применение и обычный софт вроде Samsung Notes: во взаимодействии со школьной инфраструктурой, включающей облачное хранилище, куда выгружали скриншоты или pdf для себя или для последующей проверки выполненных заданий учителем.Не могу не напомнить о своей предыдущей статье пронедавно появившиеся функции AI: в Samsung Notes тоже можно делать аудиозапись и транскрибацию лекций.Вообще говоря, целевая аудитория — это не только общеобразовательные школы, но и высшие учебные заведения. Например, небольшие учебные группы или кабинеты по изучению языков. Также Samsung Class могут использовать бизнес-центры, предоставляющие в аренду аудитории для лекций и тренингов, чтобы сделать контент на интерактивной панели видимым каждому в зале.ВыводыСоздание продукта подобно маневрам между Сциллой и Харибдой из приключений Одиссея. Менеджеры по продукту и маркетинг должны концентрироваться на потребностях целевой аудитории, используя бритву Оккама для пресечения фантазий о ненужной функциональности. В свою очередь, разработчики должны быть готовы к доработкам и избегать создания новых «болей» пользователю, подбирая варианты технических решений.Если участники IT проекта этого не понимают, то выгорают либо менеджеры по продукту, либо разработчики, в результате принося пользователю больше проблем, чем пользы.Этим пониманием функциональных ролей в команде мы пользуемся, чтобы помогать B2B заказчикам в решении их запросов или искать решения для целых отраслей.Продакты должны услышать боли пользователя, а разработчики не добавить новыхМы поняли основные потребности = боли целевой аудитории, поняв и мотивацию – расширить разнообразие педагогических приемов для вовлечения учеников и удержания их внимания в ходе урока. Дали учителю планшет с Samsung Class, помогающим решать эту задачу,ради повышения эффективности обучения.JTBD (Jobs To Be Done) фреймворк для решения потребностей ЦАУдачность лавирования между Сциллой и Харибдой определяется отзывами и впечатлениями реальных пользователей. Такая вот Одиссея создания для любого продукта.Павел МедведевSamsung"
СберМаркет,,,"Если набрать 100 баллов за ЕГЭ не получается, но цель — поступить в вуз и учиться на разработчика: проектные олимпиады",2024-09-14T12:13:47.000Z,"Привет, Хабр! Поговорим о том, как увеличить шансы на поступление в вуз на IT-специальность. Если у абитуриента 100 баллов по профильному ЕГЭ или он победитель олимпиады, которую желанный вуз учитывает, то практически наверняка можно считать, что поступление на бюджет обеспечено. Однако, не всем подходит система проверки знаний, на которой построен ЕГЭ, или школьные олимпиады по программированию, которые по сути являются соревнованиями по спортивному программированию. Я хочу рассказать про другой путь для более широкого круга увлеченных программированием старшеклассников — поступление через победы в проектных олимпиадах, например, НТО, «Юниор» или Московская предпрофессиональная олимпиада.Команда готовится презентовать решение комиссии на Предпрофессиональной олимпиадеЧто такое проектная олимпиадаКак следует из названия, ключевым соревновательным достижением является проект. Здесь можно заняться тем, что действительно интересует школьника: от создания роботов до разработки приложений. В России такого рода олимпиады это НТО, «Юниор», Предпрофессиональная олимпиада и другие.В чем преимущества таких олимпиад? Мы поспрашивали выпускников IT Школы Samsung, которые в разные годы становились их призерами.В олимпиадах по спортивному программированию решаются теоретические задачи и ценится скорость написания кода в ущерб его качеству (что в профессиональной практике не всегда приветствуется). Проектные олимпиады учат не просто теории, а тому, какприменять свои знания на практике. Здесь требуется деятельность, приближенная к индустриальной реальности: формализовать ТЗ, выбирать ЦА, проявлять креативность, разрабатывать не только код, но и интерфейс, проектную документацию, представлять свои идеи и защищать их перед экспертами.На олимпиаде «Юниор» мне понравился практический подход к представленным проектам. Участников, и меня в том числе, заставили задуматься о том, как раскрутить свой проект и привлечь клиентов. Даже глядя на приложения других ребят, я почерпнула для себя много идей для будущей работы и раскрутки собственного стартапа.Анжелика Демидова, студентка МИФИ, призер олимпиады «Юниор»Работа в команде.Это отличный шанс понять, каково это — вместе с друзьями или новыми знакомыми решать задачи, придумывать идеи и воплощать их в жизнь. В будущем умение работать в команде будет тебе очень полезно, так что начинать лучше уже сейчас.В первый раз я нашла команду, придя на консультацию по кейсу. Среди своих одноклассников и знакомых я была единственной, кто участвовал в олимпиаде, поэтому поиск коллег «на месте» стал для меня единственным вариантом. Во второй раз я уже шла на олимпиаду со своим знакомым, с которым мы вместе увлекались IT.Анастасия Таразевич, студентка РТУ МИРЭА, призер Предпрофессиональной олимпиадыВозможность прокачаться. Нет лучше способа что-то освоить, чем решать конкретные задачи. Здесь точно не хватит школьной программы, а значит, придется самостоятельно учиться новому. И даже если не займешь призовое место, сам процесс подготовки и участия уже сделает тебя умнее и опытнее.Для меня был сложнее предметный этап олимпиады, поскольку на тот момент у меня уже был довольно большой проектный опыт. В целом, я всегда отдавал предпочтение прикладному программированию, а прикладное и олимпиадное программирование — это два совершенно разных мира.Федор Седов, студент РТУ МИРЭА, призер Предпрофессиональной олимпиадыВстречаешь единомышленников. Олимпиады собирают увлеченных ребят со всей страны, а иногда и из других стран мира. Это значит, что ты сможешь найти новых друзей, которые разделяют твои интересы и с которыми можно будет обсуждать идеи и проекты. Кто знает, может, именно здесь ты найдёшь тех, с кем в будущем запустишь свой стартап или научное исследование!Один из членов моей команды позже стал моим одногруппником, и мы поддерживаем общение. За время участия в Предпрофессиональной олимпиаде мы очень сработались, поэтому в дальнейшем делали различные проекты вместе (в том числе и коммерческие).Анастасия Таразевич, студентка РТУ МИРЭА, призер Предпрофессиональной олимпиадыПобедители получаютпреимущества при поступлении в вуз(уточняйте на сайте приемной комиссии выбранного вуза про каждую олимпиаду). Но даже если не победишь, участие в таких мероприятиях может стать отличным пунктом в твоём портфолио. Университеты и работодатели ценят людей, которые готовы работать над собой и проявляют инициативу.Олимпиада помогла мне сохранить колоссальное количество нервов, так как она дала мне возможность поступить по БВИ, пока мои знакомые еще только думали над вопросом: «А куда поступать?»Анжелика Демидова, студентка МИФИ, призер олимпиады «Юниор»Из всех ВУЗов я выбрал МИФИ, так как в нём много очень хороших направлений в этой области (ещё, как приятный бонус, победа дала мне БВИ). Конкретно я пошёл на «Экстремальное программирование», так как я думал (и эти ожидания подтвердились), тут самое сильное программирование из всех направлений.Георгий Анохин, студент МИФИ, призер олимпиады «Юниор»Георгий Анохин показывает свое приложение «Моя семья» ректору НИЯУ МИФИЯ подавал заявку на летние курсы по мобильной разработке от одной крупной IT-компании. Успешно прошел все отборочные этапы, а потом понял, что этот курс только для студентов, и требуется справка из вуза. В итоге, благодаря тому, что я поступал по БВИ (куда  зачисляют ранее других вузов) — я успел получить подтверждение того, что я студент. Мне удалось принять участие в этих курсах, они дали большой вклад в мое профессиональное развитие, и в итоге я стал полноценным мобильным разработчиком.Федор Седов, студент РТУ МИРЭА, призер Предпрофессиональной олимпиадыИ, наконец, это простовесело! Это классный новый опыт. Не каждый день выпадает возможность поработать над реальными проектами и пообщаться с такими же увлеченными разработкой ребятами.Я пытался в своём мобильном приложении реализовать получение и отображение push-уведомлений. Конечно, полез в интернет читать официальную документацию и гайды. Вроде бы всё делал как надо, но оно не работало. Я работал над этой задачей около 3х недель и безрезультатно. В итоге я решил вернуться в самое начало (к официальной документации, которую я перечитал десятки раз) — оказалось, там был блок «введение», который я всё время пропускал и не читал. А в нём было решение моей проблемы — всего-то нужно было в одном месте флажок поставить. На этом 3х-недельные мучения закончились. С тех пор я всегда читаю документацию полностью =)Георгий Анохин, студент МИФИ, призер олимпиады «Юниор»На хакатоне нужно было сделать довольно сложную задачу с отображением графиков прогнозов погоды. В случае с Android-разработкой это очень нетривиальная задача. Мы вложили в это много усилий: много часов делали базу данных, импорт данных, отрисовку графиков. Ничего не получалось, не стыковалось. И буквально в последние минуты у нас получилось все соединить и оно заработало! Мы были сами удивлены и рады.Федор Седов, студент РТУ МИРЭА, призер Предпрофессиональной олимпиадыФедор, кстати, не только разработчик, но и музыкант. О своем музыкальном приложении Wavenote он написал отдельнуюстатью.Формат проектных олимпиадВсе это замечательно, скажете вы, а все же хотелось бы конкретики. Давайте расскажу про три проектные олимпиады – НТО, «Юниор» и Московскую предпрофессиональную олимпиаду, а самое важное, как к ним подготовиться. Все эти олимпиады входят вПеречень олимпиад школьников.Национальная технологическая олимпиада (НТО)Командные инженерные соревнования для школьников и студентов. НТО включаетмного проектов и направлений. Важно понимать, что многие профили НТО являются олимпиадами 2-ого уровня, а, следовательно, предоставляют льготы при поступлении в вузы, включая БВИ.В данной статье поговорим о проекте НТО «Создание виртуальных миров», состоящем из треков:Технологии виртуальной реальностиТехнологии дополненной реальностиРазработка компьютерных игрРазработка мобильных приложенийС 16 сентября начинается первый этап НТО по профилю «Разработка мобильных приложений», так что если вы заинтересовались, самое время регистрироваться. Как и в любом профиле НТО, здесь есть разделение ролей: Android-разработчик, UI/UX-дизайнер, Backend-разработчик, тимлид, так что дело найдется каждому.Профиль в этом году проводится первый раз (и это единственный новый профиль этого года!), поэтому уровень ему пока не присвоен. Но вузы-партнеры могут включить его в перечень мероприятий, за которые даются баллы индивидуальных достижений. К примеру, НИЯУ МИФИ позволит победителям сразу выйтив  в финал конкурса «Код МИФИста».Участвовать в НТО можно и в регионах. Финал профиля «Дополненная реальность» проводится на базе Иркутского государственного университета.ЮниорОлимпиада с исследовательской компонентой. Её организует и проводит НИЯУ МИФИ. Состоит из защиты научного проекта и выполнения заданий по предметам. Например, в секции «Разработка мобильных приложений под Android» нужно продемонстрировать комиссии разработанное Android-приложение и выполнить задание по математике и информатике. Проводится для школьников 9-11 классов. В 2023-2024 учебном году конкурс «Юниор» являлся олимпиадой 3 уровня и проводился по двум направлениям:«Инженерные науки» (физика, информационные технологии, математика, робототехника,разработка мобильных приложений под Android, технологии биомедицины)«Естественные науки» (химия, биология)Прием заявок на участие обычно идет до февраляна сайте олимпиады.Школьники на награждении, среди них – Анжелика Демидова, о которой мы писали вышеПредпрофессиональная олимпиадаМосковская предпрофессиональная олимпиада школьников в 2023-2024 учебном году была 3 уровня. Ее победители получаютпреференциипри поступлении на отдельные специальности в ведущие московские вузы — РТУ МИРЭА, НИЯУ МИФИ, НИУ ВШЭ, МФТИ, МИСиС, НИУ МИЭТ и другие.В олимпиаде три сектора: исследовательский, продуктовый, технологический. Сектора, в свою очередь, подразделяются на профили, некоторые из них звучат необычно и не встречаются в школе, например, «Арктика» или «Электронные системы». Предметный тур проходит уже в ноябре-феврале. Точный список предметов в этом туре зависит от выбранного сектора.  Узнать об этом больше можно насайтеолимпиады.Так проходит Предпрофессиональная олимпиада в технопарке «Альтаир»ПодготовкаПодготовка к проектным олимпиадам ­— процесс непростой. Если с предметным этапом все в целом понятно, то как подготовиться к проектной части, известно далеко не всем.Рассмотрим конкретноенаправление «Разработка мобильных приложений», а именно, шаг за шагом разберем путь, по которому нужно пройти, чтобы научиться разрабатывать клиент-серверные программные продукты и стать победителем и в НТО, и в олимпиаде «Юниор», и в Предпрофессиональной олимпиаде.Шаг 1. Поступаем в IT Школу SamsungIT Школа Samsung – социально-образовательная программа компании в рамках глобального проекта Samsung Innovation Campus. Программа стартовала в 2014 году и на протяжении 10 лет предоставляет школьникам обучение по направлению «Мобильная разработка на Java» совершенно бесплатно.Курс «Мобильная разработка на Java» рассчитан на 1 год и включает 5 модулей:Основы языка JavaВведение в объектно-ориентированное программированиеОсновы программирования Android-приложенийАлгоритмы и структуры данныхОсновы разработки серверной части мобильных приложенийЧтобы поступить в IT Школу Samsung, необходимо:Подать заявкуна сайте(приемная кампания идет до 19 сентября!)Выполнить вступительный онлайн-тестПройти отбор на площадкеПолучить подтверждение о зачисленииОбучение доступно в офлайн-формате во многих городах (смотритеполный переченьплощадок). Если не нашли свой регион в списке, то отчаиваться не стоит. Уже два года в IT Школе Samsung есть и онлайн-группы, причем речь не о предзаписанных видео. Обучение в онлайне здесь – это по сути те же уроки в офлайн-классе с учителем перенесенные в онлайн. В IT Школе сохранены все плюсы офлайн-занятий, где вы общаетесь с опытным наставником. Он не просто рассказывает тему, но и отвечает на интересующие вопросы, более подробно освещает непонятные места и держит внимание группы на протяжении всего занятия.Возникает мысль: «Поступаю в IT Школу Samsung, хорошо учусь и все, – и я готов побеждать в НТО, Юниоре и Предпрофе». Отчасти мысль правдивая, но рассмотренное обучение продолжается в течение всего учебного года и как раз основные модули, необходимые для написания качественного мобильного приложения, выпадают на момент проведения заключительного этапа желаемых олимпиад. А это значит, что после окончания IT Школы, вы сможете уверенно выступить на олимпиаде лишь на следующий учебный год. И такой вариант не устроит многих, особенно одиннадцатиклассников. Но грустить не стоит, переходим к шагу 2!Шаг 2. Участвуем в Samsung Android BootcampВ рамках IT Школы ежегодно проводится Samsung Android Bootcamp. Поучаствовать в нем приглашаются все учащиеся школы. Буткемп является одним из ключевых событий учебного года и состоит он из следующих этапов:Размещение резюмеКаждый участник должен составить и разместить собственное резюме, где      указывается не только опыт с точки зрения разработки, но и желаемая роль в      команде (Backend-разработчик, Frontend-разработчик, UX/UI-дизайнер, Тимлид).Формирование командВ рамках этого этапа ребята с ролью «Тимлид» набирают команды, перебирая все резюме. Да, да тут все как в реальной взрослой жизни!ИнтенсивыИменно в этом этапе появляется возможность за короткий срок получить концентрированный набор знаний, который поможет разработать отличное мобильное приложения до момента окончания IT Школы Samsung. Действующие разработчики ведущих IT-компаний и преподаватели IT Школы проводят с командами вебинары, на которых рассматривается все необходимое для создание клиент-серверного мобильного приложения под Android.ХакатонФинал Samsung Android Bootcamp — хакатон, где командам предстоит за парудней разработать собственное мобильное приложение. Здесь можно получить      практический опыт командной разработки, который далее пригодится в НТО. Кроме опыта, после участия в буткемпе у вас также останется отличный набор заготовок и материалов, которые можно использовать в обсуждаемых олимпиадах.Начиная с 24/25 учебного года Samsung Android Bootcamp будет проходить в середине января. То есть даты выбраны таким образом, что сразу после завершения буткемпа молодые разработчики смогут успешно выступить в финальных этапах вышеперечисленных проектных олимпиад.Шаг 3. Участвуем и побеждаем в НТО, «Юниор» и Предпрофессиональной олимпиаде!На этом шаге все просто!Используя знания, опыт и заготовки проектов, полученные на шаге 1 и шаге 2, разрабатываем отличное мобильное приложение и побеждаем в соответствующем треке проектной олимпиады!ИтогРегистрируйся вНТО,«Юниоре»иПредпрофессиональной олимпиаде-> Проходи предметный тур -> Учись в IT Школе Samsung (до 19 сентября подача заявки!) -> Участвуй в Android Bootcamp -> Разрабатывай Android-приложение -> Выигрывай НТО, «Юниор» и Предпрофессиональную -> Поступай в лучшие вузы страны -> Становись крутым разработчиком ;)Если хотите узнать больше о вариантах дополнительных баллов для школьников, смотритевыступлениепредставителей олимпиад «Юниор» и Предпрофессиональной на финале конкурса IT Школы Samsung!Александр НепретимовУчитель IT Школы Samsung"
СберМаркет,,,Алгоритм сравнения отпечатков пальцев: комбинация классических алгоритмов,2024-09-11T19:10:16.000Z,"Про алгоритмы распознавания по отпечаткам пальцев человека написано много статей. Описание алгоритмов обработки и сравнения отпечатков пальцев включено во многие учебники по компьютерному зрению и обработке цифровых изображений. Целью этой заметки не является дать исчерпывающую информацию по алгоритмам распознавания отпечатков пальцев, а на примере решения задачи сравнения отпечатков пальцев показать, как можно использовать и комбинировать между собой классические алгоритмы Сomputer Science (обход графа и нахождение наибольшей общей подпоследовательности) для решения практической задачи.КДПВ сгенерировано с помощью ChatGPT.Постановка задачиНа мой взгляд, наилучшее и исчерпывающее введение в задачу распознавания отпечатков пальцев даётся в справочнике [1]. Там же рассматриваются различные подходы к описанию отпечатков (выделению характерных признаков) и их последующему сравнению. Наиболее распространённый и наиболее старый из существующих методов - описание отпечатков пальцев путём выделения на папиллярном узоре характерных особенностей – минюций (окончаний и ветвлений папиллярных линий).Рисунок 1. Характерные точки (минюции), выделяемые на изображениях отпечатков пальцев человека.Примером алгоритма выделения минюций на изображениях отпечатков пальцев является алгоритм MINDTCT, реализация которого включена в пакет программ NIST Biometric Image Software (NBIS)[3]. Исходный код NBIS доступен по ссылке [4].Применяя алгоритм выделения минюций к изображению отпечатка, мы получаем описание отпечатка в виде списка минюций. Каждая минюция описывается набором параметров, который включает  в себя:- координаты минюциипо отношению к системе координат, связанной с изображением отпечатка (центр координат - левый верхний угол изображения, ось ординат направлена вниз);- её направление, которое определяется значением угла касательной к папиллярной линии в точкепо отношению к горизонтальной оси;- тип минюции (окончание или ветвление);- другая информация, описывающая степень уверенности в корректном выделении минюции (коэффициент качества минюции), а также уникальные особенности минюции, которые могут быть использованы при сравнении.Рисунок 2. Параметры минюции: а) окончания, б) ветвления.Далее мы будем рассматривать описания минюций только их координатами и направлением.Задача алгоритма сравнения двух отпечатковибудет заключаться в вычислении коэффициента подобия, отражающего степень совпадения между двумя отпечатками пальцев.В случае описания отпечатка набором минюций нам необходимо посчитать число совпадающих минюций после компенсации сдвига и поворота отпечаткапо отношению к отпечаткус учётом локальных нелинейных деформаций (Рис. 3). Примеры отпечатков взяты из датасетов, размещённых на сайтах международного конкурса Fingerprint Verification Competition [2].Рисунок 3. Сравнение отпечатков на основе выделенных минюций.Здесь я сознательно опускаю строгое математическое описание задачи сравнения, стараясь передать суть простыми словами. Любителей строгих математических формулировок отсылаю к уже упомянутому справочнику [1], раздел 4.3.1.Приведённый выше рисунок хорошо иллюстрирует основные проблемы, возникающие при сравнении отпечатков с использованием минюций:- Список выделенных на отпечатке минюций в общем случае неупорядочен и имеет переменный размер: некоторые минюции могут отсутствовать из-за того, что часть поверхности пальца отсутствует на изображении отпечатка. А некоторые минюции могут быть не обнаружены алгоритмом выделения минюций. Возможна и обратная ситуация - алгоритм обнаружения создаёт несуществующие минюции.- Изображение отпечатка пальца обычно получают путём контакта трёхмерной поверхности пальца с плоской поверхностью сканера отпечатков. Поэтому помимо относительного сдвига и поворота между сравниваемыми отпечатками возникают различные нелинейные деформации папиллярного узора, которые в общем случае достаточно сложно промоделировать. Это приводит к тому, что даже если у нас будет достоверная информация о взаимном сдвиге и повороте отпечатков относительно друг друга, после компенсации сдвига и поворота минюции могут не совпасть по своим координатам и направлениям.Для решения последней из указанных проблем, вводят следующее правило: две минюцииибудут образовывать пару в том случае, если выполняются следующие условия:В последнем выражении выбирается минимум из двух величин:ииз-за цикличности значения разности углов (разность между угламиисоставляет всего лишь).Пороговые значенияизадают пространство допусков (tolerance box): чтобы минюции образовали пару, необходимо, чтобы после взаимного совмещения отпечатков они находились достаточно близко к друг другу, а их направления не слишком сильно различались. Пример образования пар минюций для двух сравниваемых отпечатков после компенсации взаимного сдвига и поворота представлен на рисунке ниже.Рисунок 4. Пример образования пар минюций при сравнении.Алгоритм сравнения отпечатков пальцевДля решения задачи сравнения отпечатков рассмотрим алгоритм, описанный в статье [5]. Алгоритм включает в себя следующие этапы:подготовка промежуточного представления отпечатков (representation), необходимого для применения алгоритма сравнения;сравнение промежуточных представлений отпечатков, которое включает в себясравнение локальных структур минюций (local structure matching), которые имеют параметры, инвариантные относительно глобальных преобразований. Что такое ""локальная структура минюций"", мы рассмотрим немного позднее;объединение результатов сравнения локальных структур минюций (consolidation), цель которого проверить правильность гипотезы о совпадении отпечатков на глобальном уровне.Такое построение алгоритма сравнения позволяет избежать необходимости предварительного совмещения точечных образов, что в случае некорректной оценки параметров глобальных преобразований может приводить к принятию ошибочного решения о схожести сравниваемых отпечатков.Теперь пройдёмся по каждому из описанных выше этапов.Подготовка промежуточного представления отпечаткаВходными данными для этого шага является список выделенных на отпечатке минюций:. Здесь.Для получения промежуточного представления отпечатка выполним следующие шаги:Шаг 1.Вычисляем координаты центра тяжести для списка минюций, полагая, что ""масса"" каждой из минюций равна 1:Если для каждой из минюций алгоритм выделения минюций вычисляет коэффициент качества, то этот коэффициент можно использовать в качестве значения ""массы"" минюции:Шаг 2.Отсортируем список минюций по возрастанию их взаимного расстояния от центра тяжести.Зачем проводить такую сортировку, будет понятно после рассмотрения последующих этапов алгоритма сравнения.Шаг 3.Для каждой минюции формируют локальную структуру, которую авторы алгоритма [5] назвали ""K-plet"". ""K-plet"" строится таким образом, чтобы быть инвариантным к возможному взаимному сдвигу и повороту, который может присутствовать на сравниваемых отпечатках.""K-plet"" состоит из центральной минюцииидругих минюций отпечатка. К тому, как выбрать этиминюций, мы вернёмся позднее.Каждая минюцияпредставляется тройкой чисел. Здесь- Евклидово расстояние между минюциямии;- угол, который образует отрезок, соединяющий минюциии;- направление минюцииотносительно направления центральной минюции.Звучит не очень понятно, но если посмотреть на рисунок, то всё встаёт на свои места.Рисунок 5. Параметры минюций, формирующих ""K-plet"".Ниже приведены формулы расчёта параметровдля минюциив случае, если центральная минюция:Здесь функцияопределяется аналогично соответствующей функции стандартной библиотеки языка С.Теперь рассмотрим подробнее процедуру формирования ""K-plet"" для очередной минюциив предположении, что для всех других минюциймы уже вычислили значения.Шаг 3.1.Разделим множество минюциина четыре непересекающиеся группы, соответствующие квадрантам, на основании значений:Рисунок 6. Разделение минюций по квадрантам.При этом в какой-то группе может и не быть минюций (как например в группе III).Шаг 3.2.Для каждой из сформированных на предыдущем шаге групп проводят сортировку минюций внутри группы в порядке неубывания значений.Шаг 3.3.Сформируем пустой список минюций, который будет описывать ""K-plet"".Совершаем последовательный циклический обход сформированных групп (I группа -> II группа -> III группа -> IV группа -> I группа -> ...).При каждом посещении какой-либо группы выбираем очередную минюцию и добавляем её в ""K-plet"". При этом одновременно удаляем выбранную минюцию из рассматриваемой группы и переходим к следующей группе.Если в какой-либо группе изначально отсутствуют минюции или группа оказывается пустой на каком-либо цикле обхода, то переходят к следующей группе.Циклический обход групп завершается, если из всех четырёх групп суммарно было выбраноминюций или если выбраны все минюции.Пример, иллюстрирующий шаг 3.3. для случаяпредставлен на рисунке ниже:Рисунок 7. Пример выбора минюций при формировании ""K-plet"".Указанный способ выбора минюций для построения ""K-plet"" позволяет сохранить высокую связность между различными областями отпечатка, в отличие от простого выбора наиболее близких по расстоянию от центральной минюций. Как мы увидим в дальнейшем, высокая связность является важным свойством, используемым алгоритмом сравнения.Итак, в результате мы получаем структуру данных, представляющую собой массив ""K-plet"" для каждой из минюций, выделенных на отпечатке.Визуализация промежуточных представлений отпечатков для нашего примера приведена на рисунке ниже.Рисунок 8. Визуализация промежуточных представлений отпечатков.Сравнение промежуточных представлений отпечатковРассматриваемый алгоритм сравнения основан на попарном сравнении локальных структур минюций (""K-plet"") с последующим переходом к сравнению локальных структур для соседних минюций. Звучит опять не очень понятно.Чтобы разобраться в устройстве алгоритма сравнения, обратимся ещё раз к рисунку выше, на котором изображены два промежуточных представления отпечатков одного и того же пальца. Перед нами - два графа (обозначим которыеисоответственно), вершины этих графов - минюции. Выберем две вершины (минюции), одна из которых принадлежит, а другая -. Будем считать, что выбранные вершины представляют одну и ту же минюцию.Чтобы убедиться, что два отпечатка принадлежат одному и тому же пальцу, нам будет необходимо обойти все оставшиеся вершины в двух графах и посчитать число вершин, которые представляют одну и ту же минюцию на двух отпечатках.Обозначим число таких вершин через. Тогда коэффициент подобиявычисляется по следующей формуле:где- количество минюций, выделенных на отпечатке, а- количество минюций, выделенных на отпечатке.Для обхода вершин графов мы можем воспользоваться либо алгоритмом обхода графа в ширину (breadth-first search, BFS), либо алгоритмом обхода графа в глубину (depth-first search, DFS) [6]. В данном конкретном случае нет абсолютно никакой разницы, какой алгоритм обхода (BFS или DFS) мы можем использовать. Авторы алгоритма [5] предлагают использовать алгоритм BFS. Мы же, чтобы было интереснее, будем использовать алгоритм DFS.Хотелось бы напомнить, что разница между BFS и DFS заключается в порядке обхода вершин. Порядок обхода задаётся структурой данных, которая служит для хранения и последующей выборки посещённых, но не обработанных вершин. Для BFS такая структура - очередь (First In, First Out - FIFO). Для DFS - стек (Last In, First Out - LIFO).Описание алгоритма DFS можно найти, например, в книгах [6,7] или в публикациях на Хабре (например, [8]).Здесь мы остановимся лишь на модификациях алгоритма DFS, необходимых для использования DFS в описываемом алгоритме сравнения отпечатков.Модификации заключаются в следующем:Мы будем обходить графыиодновременно.Вернёмся обратно к определению промежуточного представления отпечатка. Это массив, содержащий ""K-plet"" для каждой из минюций. В свою очередь ""K-plet"" представляет собой упорядоченный список смежных вершин. Фактически, сравниваемые графы задаются упорядоченными списками смежных вершин (adjucency-list representation). В этом случае алгоритм DFS при одновременном обходе двух графов для отпечатков одного и того же пальца будет одновременно посещать вершины, представляющие одну и ту же минюцию.Алгоритм DFS будет посещать только такие пары вершин, для которых мы убедились, что такие пары соответствуют одной и той же минюции на двух сравниваемых отпечатках.Тогда алгоритм сравнения будет выглядеть так:Входные данные:графыи, заданные упорядоченными списками смежных вершин;- номера вершин в графахисоответственно; эта пара вершин используется как начальная для последующего обхода оставшихся вершин в графахи. Полагаем, что выбранные вершины представляют одну и ту же минюцию.Выходные данные:- число пар вершин в графахи, которые представляют одну и ту же минюцию на двух отпечатках.Непосредственно алгоритм сравнения:Окрашиваем все вершиныив белый цвет;Кладём в стек начальную пару вершини окрашиваем эти вершины в серый цвет;Пока стек не пуст, извлекаем верхнюю пару вершин:4.1. Сравниваем два ""K-plet"" для вершини; результат сравнения - список пар смежных вершин, которые представляют одну и ту же минюцию (другими словами, образовали пары);4.2.    Из полученного списка пар смежных вершин кладём в стек только те пары, в которых вершины - белые; обозначим число таких пар; вершины в этих парах перекрашиваем в серый цвет;4.3.Красим вершиныив чёрный цвет;Получившееся число совпавших париспользуем для вычисления коэффициент подобия.Точность описанного выше алгоритма зависит от того, каким образом реализован шаг 4.1 - сравнение локальных структур минюций (двух ""K-plet"").Сравнение локальных структур минюцийИтак, ""K-plet"" - это упорядоченный список смежных вершин. Результат сравнения двух ""K-plet""- список пар смежных вершин, которые образовали пары (представляют одну и ту же минюцию). А значит, задачу сравнения двух ""K-plet"" можно представить как задачу о нахождении наибольшей общей подпоследовательности (Longest Common Subsequenbce - LCS)[7,9].Задача LCS состоит в том, чтобы найти общую подпоследовательность наибольшей длины для двух последовательностейи. Эта задача решается путём применения динамического программирования.В этом случае алгоритм решения задачи LCS будет состоять из двух последовательных этапов:Этап 1, который следуя [7] назовём ""LCS-Length"". Результат этого этапа - длина наибольшей общей подпоследовательности, а также данные, необходимые для построения наибольшей общей подпоследовательности;Этап 2(""Print-LCS""), в результате которого мы получим наибольшую общую подпоследовательность.Следуя [7], приведём псевдокод для ""LCS-Length"" и ""Print-LCS"".LCS-Length(X, Y)for i ← 0 to m
    do c[i,0] ← 0
for j ← 0 to n
    do c[0,j] ← 0
for i ← 1 to m
    do for j ← 1 to n
           do if IsEqual(X[i-1], Y[j-1])
                 then c[i,j] ← c[i-1,j-1] + MatchingWeight(X[i-1], Y[j-1])
                      b[i,j] ← ""↖""
                 else if c[i-1,j] ≥ c[i,j-1]
                         then c[i,j] ← c[i-1,j]
                              b[i,j] ← ""↑""
                         else c[i,j] ← c[i,j-1]
                              b[i,j] ← ""←""
return c,bЗдесь- двумерный массив для хранения стоимости формируемой наибольшей общей подпоследовательности,- двумерный массив, сохраняющий ""происхождение""; эта информация необходима для последующего построения наибольшей общей подпоследовательности:Print-LCS(b, X, Y)l ← {}
i ← m
j ← n
while i > 0 and j > 0
      do if b[i,j] = ""↖""
            then l ← l + {(X[i-1],Y[j-1])}
                 i ← i-1
                 j ← j-1
            elseif b[i,j] = ""↑""
                   then i ← i-1
            else j ← j-1
return lЗдесь- список пар смежных вершин, которые образовали пары.Если бы сравниваемые последовательностиипредставляли собой обычные строки, то функция ""IsEqual(x,y)"", которая вызывается на строке 7 функции ""LCS-Length"", превратилась бы в простую проверку на равенство. А функция ""MatchingWeight(x,y)"", вызываемая на строке 8, возвращала бы всегда 1. В этом случае код функции ""LCS-Length"" полностью совпадёт с кодом, приведённым в книге [7].В нашем случае мы сравниваем два ""K-plet"", где каждый ""символ"" - это тройка чисел (Евклидово расстояние от центральной минюции; угол, который образует прямая, проходящая через текущую минюцию и центральную минюцию; и направление текущей минюции относительно направления центральной минюции).Тогда функция ""IsEqual(x,y)"" превратиться в проверку следующих условий:Здесьи- две сравниваемые минюции. Пороговые значения,иподбираются на основе обучающей выборки и фактически задают пространство допусков (tolerance box), который упоминался в разделе, посвящённом простановке задачи сравнения отпечатков.Функция ""MatchingWeight(x,y)"" должна отражать степень совпадения сравниваемых минюцийи. Логичным решением будет вычислить вес, который должна возвращать функция ""MatchingWeight(x,y)"" так:где коэффициентыитакже подбираются эмпирически на основе обучающей выборки.Прежде чем двигаться дальше, нам необходимо ввести ещё одно изменение в алгоритм работы функции ""LCS-Length"". Необходимо, чтобы в наибольшую общую подпоследовательность не попадали пары минюций, которые не могут образовать пары.Выделим два таких условия:пары могут образовывать только вершины (минюции) белого цвета. Это условие становится понятным, если вернуться к описанию алгоритма сравнения в предыдущем разделе. На шаге 4.2 мы перекрашиваем все вершины, которые образовали пары в результате работы алгоритма сравнения двух ""K-plet"";пару не могут образовать вершины (минюции), для которых функция ""IsEqual"" возращает значение.В случае приведённых выше условий взаносим штрафное значение FALSE_WEIGHT, равное максимально возможному значению, взятому со знаком минус. В случае приведённой выше формуле для расчёта, FALSE_WEIGHT.Тогда модифицированная функция ""LCS-Length"" (назовём её ""LCS-Length-Mod"") будет иметь следующий вид:LCS-Length-Mod(X, Y)for i ← 0 to m
    do c[i,0] ← 0
for j ← 0 to n
    do c[0,j] ← 0
for i ← 1 to m
    do for j ← 1 to n
           do if ""X[i-1] белая"" and ""Y[i-1] белая"" and IsEqual(X[i-1], Y[j-1])
                 then c[i,j] ← c[i-1,j-1] + MatchingWeight(X[i-1], Y[j-1])
              else c[i,j] ← c[i-1,j-1] + FALSE_WEIGHT

              if c[i,j] > c[i-1,j] and c[i,j] > c[i,j-1]
                 then b[i,j] ← ""↖""
              elseif c[i-1,j] > c[i,j-1] and c[i-1,j] > c[i,j]
                     then c[i,j] ← c[i-1,j]
                          b[i,j] ← ""↑""
              else c[i,j] ← c[i,j-1]
                   b[i,j] ← ""←""
return c,bВернёмся обратно к описанию алгоритма сравнения в предыдущем разделе. На шаге 4.1 мы последовательно вызываем функции ""LCS-Length-Mod"" и ""Print-LCS"", чтобы получить список пар смежных вершин как результат сравнения двух ""K-plet"". А на шаге 4.2 нам необходимо пройти по полученному списку пар смежных вершин с тем, чтобы положить найденные пары в стек для последующей обработки, а также посчитать число таких пар для последующего определения коэффициента подобия для сравниваемых отпечатков.Чтобы исключить необходимость повторного прохода по списку пар смежных вершин на шаге 4.2 мы можем выполнить действия шага 4.2 в процессе построения списка пар смежных вершин в функции ""Print-LCS"". Модифицированная функция ""Print-LCS"" (назовём её ""Print-LCS-Mod""):Print-LCS-Mod(b, X, Y)q ← 0
i ← m
j ← n
while i > 0 and j > 0
      do if b[i,j] = ""↖""
            then if ""X[i-1] белая"" and ""Y[i-1] белая""
                    then ""помещаем в стек пару (X[i-1],Y[j-1])""
                         ""перекрашиваем вершины X[i-1] и Y[j-1] в серый цвет""
                         q ← q+1
                 i ← i-1
                 j ← j-1
         elseif b[i,j] = ""↑""
                then i ← i-1
         else j ← j-1
return qВ соответствии с выкладками, приведёнными в статье [5], cложность получившегося алгоритма равна, где- число минюций.Работа построенного алгоритма сравнения для нашего примера:Рисунок 9. Пример работы алгоритма сравнения.За кадром остался вопрос выбора начальной пары минюций, которая необходима для начала работы описанного выше алгоритма сравнения. Но поскольку нам изначально ничего не известно о том, какие вершины сравниваемых графов представляют одну и ту же минюцию (или, если рассматривать задачу шире, принадлежат ли сравниваемые отпечатки одному и тому же пальцу), то мы можем выбрать следующие стратегии:Будем рассматривать в качестве начальной пары вершин все возможные пары вершин сравниваемых графов. В этом случае описанный выше алгоритм сравнения будет запускатьсяраз, а в качестве результирующего коэффициента подобия будет выбран максимальный из получившихся коэффициентов подобия. Таким образом, сложность работы такого алгоритма составит.Одним из условий успешного сравнения отпечатков пальцев является достаточная область перекрытия: на сравниваемых отпечатках должна присутствовать одна и та же область пальца, достаточная для достоверного сравнения площади. Так как мы сравниваем отпeчатки на основе выделяемых на них минюций, то сформированное выше условие превращается в условие нахождения достаточного количества минюций на отпечатках. При этом для того, чтобы результату сравнения отпечатков можно было бы доверять, необходимо найти не менее 12 пар совпадающих минюций на сравниваемых отпечатках. Это правило носит название ""правила 12 точек"" (12-point guideline) [1].В случае, если отпечатки принадлежат одному и тому же пальцу и, в то же время, имеют общую область перекрытия, то сортировка списка минюций по возрастанию их взаимного расстояния от центра тяжести для сравниваемых отпечатков (шаги 1 и 2 алгоритма формирования промежуточного представления отпечатка) приведёт к тому, что в начале списка окажутся минюции, которые присутствуют на обоих отпечатках. Тогда мы можем взятьпервых минюций из двух списков и рассматривать их в качестве начальных пар (т.е. мы будем запускать алгоритм сравненияраз). В этом случае сложность работы такого алгоритма будет такой:.Что касается достигаемой точности распознавания для описанного алгоритма, любопытные читатели могут найти эту информацию непосредственно в статье [5].Практическая реализация алгоритма сравнения отпечатков пальцевТак как впредыдущей заметкеналичие псевдокода явилось причиной обвинения автора в том, что он реально ничего не реализовывал, то ниже приведены фрагменты кода, реализующего описанный алгоритм сравнения. Сразу хочу сказать, что код написан на языке C древней редакции и может быть написан оптимальнее со всех точек зрения.Сначала введём следующие структуры данных:#define MAX_RAY_AMOUNT    8

typedef struct _ray_element
{
    int32_t neighbour_minutiae;
    int32_t rel_dist;
    int32_t rel_angle;
    int32_t rel_theta;
} RAY_ELEMENT, *P_RAY_ELEMENT;

typedef struct _minutiae_element
{
    uint32_t ray_amount;
    RAY_ELEMENT rays[MAX_RAY_AMOUNT];
} MINUTIA_ELEMENT, *P_MINUTIA_ELEMENT;

typedef struct _fp_feature_vector
{
    uint32_t minutiae_amount;
    MINUTIA_ELEMENT minutiae_data[1];
} FP_FEATURE_VECTOR, *P_FP_FEATURE_VECTOR;Тогда мы можем выделить память под промежуточное представление отпечатка, на котором выделено minutiae_amount минюций:P_FP_FEATURE_VECTOR p_feature_vector =
      (P_FP_FEATURE_VECTOR)calloc(sizeof(FP_FEATURE_VECTOR) + 
                                  (minutiae_amount-1) * sizeof(MINUTIA_ELEMENT), 
                                  sizeof(uint8_t));Следующие массивы будут необходимы для работы алгоритма сравнения:#define MAX_STAR_MINUTIAE_AMOUNT    50
#define LCS_MAX_RAY_SIZE            (MAX_RAY_AMOUNT+1)
#define LCS_MAX_RAY_AMOUNT          (LCS_MAX_RAY_SIZE*LCS_MAX_RAY_SIZE)

int32_t LeftColorArray[MAX_STAR_MINUTIAE_AMOUNT];
int32_t RightColorArray[MAX_STAR_MINUTIAE_AMOUNT];
int32_t LeftStack[MAX_STAR_MINUTIAE_AMOUNT];
int32_t RightStack[MAX_STAR_MINUTIAE_AMOUNT];

int32_t CostArray[LCS_MAX_RAY_AMOUNT];
int32_t DirArray[LCS_MAX_RAY_AMOUNT];Здесь мы предполагаем, что максимальное число минюций, выделенных на отпечатке, не может превышать 50.Функция, реализующая алгоритм сравнения:// matching parameters
#define MIN_MINUTIAE_PAIR_THR 12

#define TRUE_WEIGHT       16
#define FALSE_WEIGHT     -16
   
#define STAR_DIST_THR    12
#define STAR_ALPHA_THR   20
#define STAR_THETA_THR   30

#define STAR_DIST_COEFF    2
#define STAR_ALPHA_COEFF   4
#define STAR_THETA_COEFF   6

// constants of DFS algorithm
#define WHITE_COLOR      0
#define GRAY_COLOR       1
#define BLACK_COLOR      2

#define STACK_EMPTY      (-1)

// for LCS algorithm
#define LEFT_TOP_DIR     0
#define LEFT_DIR         1
#define UP_DIR           2

// for search space dimension
#define SEARCH_SPACE_DIM    15

double PerformMatching(P_MATCHER_HANDLE pHandle, P_FP_FEATURE_VECTOR pLeftFV, P_FP_FEATURE_VECTOR pRightFV)
{
    int top_of_stack;
    int index1, index2;
    int index3, index4;
    int max_score, cur_score;
    int cur_template_root_minutiae,
        cur_input_root_minutiae;
    P_MINUTIA_ELEMENT pTemplateMinutiae = NULL;
    P_MINUTIA_ELEMENT pInputMinutiae    = NULL;

    // LCS algorithm variables
    int left_cost, up_cost, left_up_cost;
    int delta_dist;
    int delta_alpha;
    int delta_theta;

    // full search
    max_score = 0;

    memset(CostArray, 0, LCS_MAX_RAY_AMOUNT*sizeof(int32_t));

    for (index1 = 0; index1 < (pLeftFV->minutiae_amount < SEARCH_SPACE_DIM ? pLeftFV->minutiae_amount : SEARCH_SPACE_DIM); ++index1)
    {
        for (index2 = 0; index2 < (pRightFV->minutiae_amount < SEARCH_SPACE_DIM ? pRightFV->minutiae_amount : SEARCH_SPACE_DIM); ++index2)
        {
            // DFS algorithm
            memset((void*)LeftColorArray,  WHITE_COLOR, MAX_STAR_MINUTIAE_AMOUNT*sizeof(int32_t));
            memset((void*)RightColorArray, WHITE_COLOR, MAX_STAR_MINUTIAE_AMOUNT*sizeof(int32_t));

            LeftColorArray[index1]  = GRAY_COLOR;
            RightColorArray[index2] = GRAY_COLOR;

            top_of_stack = 0;

            LeftStack[top_of_stack] = index1;
            RightStack[top_of_stack]    = index2;

            cur_score = 1;

            while (STACK_EMPTY < top_of_stack)
            {
                cur_template_root_minutiae = LeftStack[top_of_stack];
                cur_input_root_minutiae    = RightStack[top_of_stack];
                --top_of_stack;

                // LCS algorithm
                pTemplateMinutiae = &pLeftFV->minutiae_data[cur_template_root_minutiae];
                pInputMinutiae    = &pRightFV->minutiae_data[cur_input_root_minutiae];

                // LCS-LENGTH
                for (index3 = 1; index3 < pTemplateMinutiae ->ray_amount + 1; ++index3)
                {
                    for (index4 = 1; index4 < pInputMinutiae ->ray_amount + 1; ++index4)
                    {
                        up_cost   = CostArray[LCS_MAX_RAY_SIZE*(index3 - 1) + index4];
                        left_cost = CostArray[LCS_MAX_RAY_SIZE*index3 + index4 - 1];

                        if ( LeftColorArray[pTemplateMinutiae->rays[index3 - 1].neighbour_minutiae] != WHITE_COLOR ||
                             RightColorArray[pInputMinutiae->rays[index4 - 1].neighbour_minutiae]   != WHITE_COLOR )
                            left_up_cost = CostArray[LCS_MAX_RAY_SIZE*(index3 - 1) + index4 - 1] + FALSE_WEIGHT;
                        else
                        {
                            delta_dist  = abs(pTemplateMinutiae->rays[index3 - 1].rel_dist   - pInputMinutiae->rays[index4 - 1].rel_dist);
                            delta_alpha =  abs(pTemplateMinutiae->rays[index3 - 1].rel_angle - pInputMinutiae->rays[index4 - 1].rel_angle);
                            delta_alpha = delta_alpha > 180 ? 360 - delta_alpha : delta_alpha;
                            delta_theta = abs(pTemplateMinutiae->rays[index3 - 1].rel_theta  - pInputMinutiae->rays[index4 - 1].rel_theta );
                            delta_theta = delta_theta > 180 ? 360 - delta_theta : delta_theta;

                            if ( delta_dist <= STAR_DIST_THR && delta_alpha <= STAR_ALPHA_THR && delta_theta <= STAR_THETA_THR )
                                left_up_cost = CostArray[LCS_MAX_RAY_SIZE*(index3 - 1) + index4 - 1]   +
                                               TRUE_WEIGHT - delta_dist/STAR_DIST_COEFF - delta_alpha/STAR_ALPHA_COEFF - delta_theta/STAR_THETA_COEFF;
                            else
                                left_up_cost = CostArray[LCS_MAX_RAY_SIZE*(index3 - 1) + index4 - 1] + FALSE_WEIGHT;
                        }

                        if ( left_up_cost > up_cost && left_up_cost > left_cost )
                        {
                            CostArray[LCS_MAX_RAY_SIZE*index3 + index4] = left_up_cost;
                            DirArray[LCS_MAX_RAY_SIZE*index3 + index4]  = LEFT_TOP_DIR;
                        }
                        else
                        {
                            if ( up_cost > left_cost && up_cost > left_up_cost )
                            {
                                CostArray[LCS_MAX_RAY_SIZE*index3 + index4] = up_cost;
                                DirArray[LCS_MAX_RAY_SIZE*index3 + index4]  = UP_DIR;
                            }
                            else
                            {
                                CostArray[LCS_MAX_RAY_SIZE*index3 + index4] = left_cost;
                                DirArray[LCS_MAX_RAY_SIZE*index3 + index4]  = LEFT_DIR;
                            }
                         }
                     }
                 }
 
                 // PRINT-LCS
                 --index3;
                 --index4;

                 while ( index3 > 0 && index4 > 0)
                 {
                     if ( LEFT_TOP_DIR == DirArray[LCS_MAX_RAY_SIZE*index3 + index4] )
                     {
                         // DFS algorithm part II
                         if (WHITE_COLOR == LeftColorArray[pTemplateMinutiae->rays[index3 - 1].neighbour_minutiae] &&
                             WHITE_COLOR == RightColorArray[pInputMinutiae->rays[index4 - 1].neighbour_minutiae] )
                         {
                             ++top_of_stack;
                             LeftStack[top_of_stack] = pTemplateMinutiae->rays[index3 - 1].neighbour_minutiae;
                             RightStack[top_of_stack]    = pInputMinutiae->rays[index4 - 1].neighbour_minutiae;
                             ++cur_score;
                         }

                         LeftColorArray[pTemplateMinutiae ->rays[index3 - 1].neighbour_minutiae] = GRAY_COLOR;
                         RightColorArray[pInputMinutiae ->rays[index4 - 1].neighbour_minutiae]   = GRAY_COLOR;

                         --index3;
                         --index4;
                         continue;
                     }

                     if ( UP_DIR == DirArray[LCS_MAX_RAY_SIZE*index3 + index4] )
                     {
                         --index3;
                         continue;
                     }

                     if ( LEFT_DIR == DirArray[LCS_MAX_RAY_SIZE*index3 + index4] )
                     {
                         --index4;
                         continue;
                     }
                 }

                 // DFS algorithm part III
                 LeftColorArray[cur_template_root_minutiae] = BLACK_COLOR;
                 RightColorArray[cur_input_root_minutiae]   = BLACK_COLOR;
            }
            // end of DFS algorithm

            if ( cur_score > max_score )
                max_score = cur_score;
        }
     }

     // calculate matching score
     if ( max_score > MIN_MINUTIAE_PAIR_THR )
         return (double)(max_score*max_score)/(pLeftFV->minutiae_amount*pRightFV->minutiae_amount);
     else
         return 0.0;
}Литература1. D. Maio, D. Maltoni, A. K. Jain, and J. Feng. Handbook of Fingerprint Recognition. Springer Verlag, 2020.2. Fingerprint Verification Competition.http://bias.csr.unibo.it/fvc2000/download.asp;http://bias.csr.unibo.it/fvc2002/download.asp3. K. Ko, W. J. Salamon. NIST Biometric Image Software (NBIS).https://www.nist.gov/services-resources/software/nist-biometric-image-software-nbis- 2015.4.https://nigos.nist.gov/nist/nbis/nbis_v5_0_0.zip5. S. Chikkerur, A.N. Cartwright, and V. Govindaraju. K-plet and Coupled BFS: A Graph Based Fingerprint Representation and Matching Algorithm. In: Advances in Biometrics. ICB 2006. Lecture Notes in Computer Science, vol 3832. Springer, Berlin, Heidelberg.https://doi.org/10.1007/11608288_42- 2015.6. Скиена, С.С. Алгоритмы. Руководство по разработке. - 3-е изд.: Пер. с англ. - СПб.: БХВ-Петербург, 2022.7. Кормен, Т., Лейзерсон, Ч., Ривест, Р. Алгоритмы: построение и анализ. М.: МЦНМО, 2000.8. Реализуем алгоритм поиска в глубину. Перевод Ксении Мосеенковой (kmoseenk). Блог компании OTUS.https://habr.com/ru/companies/otus/articles/660725/- 2022.9. Нахождение максимальной общей подпоследовательности. Автор: hamst.https://habr.com/ru/articles/142825/- 2012."
СберМаркет,,,COFFEE MEET-UP: Знакомства и кофе в одной чашке,2024-09-03T07:45:05.000Z,"Всем привет! Меня зовут Алиса и в этом году я закончила 10 класс и IT Школу (Samsung Innovation Campus), где в рамках обучения я разработала свое первое крупное Android приложение.  Сегодня я расскажу вам о моем проекте Coffee meet-up от задумки до реализации и тестирования.В будущем я планирую развивать приложение дальше, поэтому мне важно услышать мнение аудитории Хабра. Надеюсь, что вы расскажете мне о ваших впечатлениях от идеи приложения в комментариях, а, может, даже станете одним из будущих пользователей.ВведениеИдея создания приложения Coffee meet up пришла мне осенью прошлого года, когда я только переехала в Москву. Как и у многих, кто осваивается на новом месте, возник вопрос, как построить новый круг знакомств и найти друзей. Оказалось, что это непростая задача. Постоянно ускоряющийся темп жизни практически не оставляет нам свободного времени, поэтому даже в мегаполисах, наполненных людьми, встретить подходящего человека очень сложно. И, как многим, первое, что пришло мне в голову,  - это использовать приложение для знакомств. И я углубилась в их изучение.ПроблематикаВ настоящий момент  количество онлайн-знакомств постоянно растет. Согласно некоторым исследованиям, более50% подростков заводят друзей в интернете. Сервисы для знакомств выполняют важную роль посредника между виртуальным и живым общением, поэтому выпускать и развивать такие продукты актуально и важно.Как  известно, современный рынок приложений для знакомств вращается вокруг поиска романтического партнера. Как правило подобные приложения строятся по классической анкетной “tinder-формуле”:свайп - метч - свиданиеВ 2022-ом году каждое из топ-3 популярных приложений для знакомств в России имело именно такую структуру.Топ 3 приложения для знакомств в России в 2022, ресурс:https://livetyping.com/Тем не менее у этого подхода есть несколько существенных минусов:Свайпы и Скроллы - это долгоОгромное количество “потенциальных партнеров” на экране телефона создает иллюзию, что подходящий человек найдется очень быстро. Однако часто желанный “метч” происходит только через несколько дней после начала пользования. Более того, вероятность метча возрастает с количеством просмотренных анкет, что вынуждает пользователя регулярно тратить время на монотонные операции.Опыт мужчин и женщин в Tinder, ресурс: https://duro.dataНа этом дело не заканчивается: после взаимного лайка нужно назначить свидание, а это тоже время. Еще есть риск, что пользователь не останется удовлетворенным встречей, и столь большая трата усилий будет вызывать фрустрацию.Топ 5 причин удаления дейтинговых приложений ресурс: https://www.prnewswire.com/Малая безопасностьПожалуй, это настолько наболевшая тема для пользователей подобных приложений, что вместо того, чтобы описывать проблему, можно просто вспомнить сериалTinder Swindlerили предложить читателям открыть поисковик и самим убедиться в наличии многочисленных “скелетов в шкафу”  рынка дейтинговых приложений.Многих потенциальных пользователей отталкивает именно этот пункт, поэтому важно предлагать новые безопасные концепции приложений или дорабатывать старые.Не даем шансов тем, кто их заслуживаетКогда нам становится доступен на первый взгляд бесконечный объем красивых и интересных людей, наши стандарты зачастую резко повышаются, в результате чего мы нередко пролистываем тех, кто на самом деле может быть “тем самым”.“Умные алгоритмы”, которыми кишат подобные приложения, иногда только ухудшают ситуацию, частично решая за нас, кого выбрать.Как решать эти проблемы?Способы решенияНу начнем с пункта про время. Можно банально ограничить самих пользователей. Так, например, поступаетBreakfast. Приложение работает так:“1. Если вы готовы позавтракать в ближайшее время — жмёте кнопку на главном экране.2. Умный алгоритм подбирает потенциального собеседника в течение нескольких часов. Основатели называют это шанс (ченс, от англ. chance).Дальше у двух людей есть 24 часа, чтобы договориться о встрече, иначе ченс исчезнет. В сутки можно получить только один ченс на завтрак.3. Встречаетесь с человеком и классно проводите время за завтраком. Или нет. Если беседа не сложилась — это тоже нормально. Нужно просто дождаться другого ченса и не сильно заморачиваться по этому поводу.”Breakfast, ресурс: https://thebreakfast.app/Подобные “мгновенные встречи” решают не только вопрос о времени, но и о безопасности, так как привязаны к публичным местам. Тем не менее, все равно придется успеть самим договориться и быстро выбрать место встречи.Другое приложение, которое может сэкономить ваше время,  -  этоSkyLove. Здесь нету “свайпов”, вместо этого вы создаете мероприятие и назначаете место и время встречи. Уже после этого будут приходить запросы от людей, которые хотят составить вам компанию.Skylove, ресурс: https://skylove.su/Для инициатора встречи это значительно ускоряет процесс отбора, а имплементация карты помогает посетителю выбрать ближайшее и наиболее удобное событие. Тем не менее, здесь как будто бы можно ожидать какой-то “подставы” от организатора встречи, поэтому за критерий безопасности SkyLove от меня полный балл не получил.Концепцию “событий” также использует, например, приложениеFitil. Оно позиционирует себя как сервис для серьезных знакомств на основе психоаналитики (в начале вам придется пройти тест из 111 вопросов), где вам подбирают потенциального партнера, основываясь на совместимости. Однако, просто так написать ему вы не сможете, придется покупать билет на одно из мероприятий, которые организует фирма (типы мероприятий варьируются: от походов до мастер-классов и т.д.).Fitil, ресурс: https://fitil.club/Безопасно? Очень даже, но оооочень долго.В итоге, ни одно приложение мне не пришлось по душе. Выход оставался один: самой создать достойную альтернативу, которая смогла бы соответствовать всем моим требованиям.Итак, что нам нужно? Публичное место, где мы с высокой вероятностью окажемся минут, скажем, на 15, чтобы провести часть своего досуга. Чтобы это могло быть?! Думаю, многие скажут “кофейня”. Так подумала и я. Это было отправной точкой в создании Coffee meet-up.Концепция приложенияЗадумка была такова:В начале пользователь создает аккаунт, после чего ему становится доступна карта с ближайшими кофейнями, он выбирает подходящую и прокладывает маршрут. Когда он доходит до кофейни, то становится видимым людям поблизости. После этого другие пользователи могут отправлять ему запрос на встречу, и он сможет решить, с кем сегодня он хотел бы разделить чашку кофе.Изначально планировалось, что видны будут все кофейни, но возникли проблемы с базой данных и движком для карты, и было решено, что пользователю будут доступны кофейни какой-либо одной фирмы. Так как тестировать приложение я планировала в Москве, мой выбор пал на одну сеть известных кофеен, в столице вы можете увидеть их буквально на каждом углу. Это сделало идею еще более продаваемой и внедряемой, ведь вы можете заменить эту сеть на любую другую (а, может, даже и на вашу) фирму.Я хотела абстрагироваться от идеи приложений сугубо для поиска романтических отношений, в Coffee meet-up “знакомства” - это скорее про поиск интересных собеседников и друзей, а не только потенциальных партнеров.Кроме того, раз уж я решила сделать приложение для знакомств в кофейнях, я никак не могла не добавить опцию “коворкинг”. Поиск партнеров для совместной работы может дать буст и помощь в работе и учебе, а помимо вашего черного кофе, подбадривать вас перед дедлайном сможет и ваш новый знакомый.Также хотелось, чтобы люди знакомились без предубеждений и ожиданий. Поэтому было решено, что фильтрации по алгоритму подбора не будет, можно только настроить фильтры на возраст (от и до), тип встречи (коворкинг/знакомства) и пол (м/ж).Работа над проектомИ вот, после проработки концепции приложения, пришло время его реализовывать.Над проектом я работала с октября по май и, в целом, добилась желаемого результата за 8 месяцев.Здесь стоит отметить, что это был не просто мой первый опыт написания приложения под Android, я впервые бралась за создание крупного проекта. До этого у меня был опыт работы с Python, но он ограничивался решением задач и телеграм-ботами, ни о каких больших программах речи не шло. Осенью я поступила в IT Школу Samsung в РУДН (в Москве, на ул. Орджоникидзе) и именно благодаря учебе и практике там реализация моей задумки стала возможна.На момент начала работы мои знания Java и Android были почти что на уровне “Hello world”, поэтому я разделила проект на несколько частей, имплементируя вначале более простые компоненты, затем более сложные.Работу я начала с подключения Firebase, реализации профиля, регистрации и входа. Итого получилось 5 Активностей.Структура профиля в Coffee meet-upДалее пришло время реализации чата, куда же без него в приложении для знакомств? Специальных библиотек для реализации чата я не использовала, только firebase для получения и обмена данными, поэтому с этим пунктом пришлось повозиться. Тут были реализованы две активности RecentConversationsActivity (“Недавние диалоги”) и сама ChatActivityРаботы чата в Coffee meet-upИ вот пришло время делать самое сложное: подключать карту и программировать логику встреч. Для этого я выбрала движок OSMdroid, может, не самый лучший, но зато бесплатный и надежный. На реализацию ушли 4 Активности и несколько фрагментов.Процесс встреч я освещу подробнее в отдельном пункте позже, так как это главная “фича” Coffee meet-up.Много внимания я уделила UI. Хотелось, чтобы он был понятным, соответствующим тематике кофеен, а также, чтобы он выделялся. Для этого я регулярно консультировалась со своими знакомыми и друзьями. Так, в частности, совместными усилиями был создан маскот приложения.Маскот приложенияПро то, что существуют такие волшебные вещи как Clean Architecture и Navigation Graph я узнала в мае, когда решила самостоятельно подробнее изучать Android разработку на Kotlin, поэтому  Coffee meet-up хорошей архитектурой похвастаться не может. Кроме того есть memory leaks и некоторые deprecated методы. Но, несмотря на все это, Coffee meet-up прекрасно справляется со своей задачей быть “прототипом” для реализации идеи приложения.ВстречиА теперь вишенка на торте! Как проходят встречи в Coffee meet-up и что делает их безопасными, быстрыми и интересными?Как уже было сказано, есть два сценария развития событий: первый для инициатора встречи, второй - для посетителя.Инициатор ВстречиДопустим, перед походом на работу или в учебное заведение вы решили найти кофейню около дома, а заодно разделить свою первую чашечку кофе с новым другом. Ваш план действий таков:Встреча с точки зрения инициатора1. Открываете приложение и переходите в карту2. Выбираете опцию “Свободные кофейни”3. Теперь просто выбираете свободную кофейню4. Прокладываете маршрут5. Доходите до кофейни и активируетесь6. Выбираете понравившегося пользователя и переходите в чат7. Если вы решили встретиться, то скидываете локацию8. А теперь ждите (можете даже заказать посетителю кофе)ПосетительЧто же делает в это время ваш будущий “сокофейник”?Его алгоритм таков:Встреча с точки зрения посетителя1. Открывает приложение и переходит в карту2. Выбираете опцию “Активные точки”3. Из доступных людей выбирает вас4. Отправляет запрос5. Получает от вас сообщение и вступает в переписку6. Получает от вас локацию и переходит в “Маршрут”7. Идет к вам и ждет не дождется встречи (и кофе, конечно же)Взаимодействие участников встречиОтмечу, что посетитель  не знает, в какой именно кофейне вы находитесь (это безопасность), зато может быть уверен, что вы точно не дальше, чем в  1 км от него (а это скорость).Каждый из пользователей может отменить встречу, если заподозрит что-то неладное. Также и активация, и встреча отменяется, если инициатор встречи покинул кофейню.Бета-тестированиеВесной после успешной проверки работы на эмуляторах, я начала подключать своих друзей для тестирования, всего было 8 участников, включая меня. Вначале мы заранее договаривались, кто и в какой кофейне будет встречаться, и проверяли работу приложения. Затем мы выбирали район и тестировали приложение Coffee meet-up, не договариваясь заранее о месте встречи. Последний этап был организовать встречу незнакомым друг с другом людям.Благодаря тестированию мне удалось выявить и устранить некоторые баги, а также сделать интерфейс более понятным для пользователей. Кроме того, мы просто хорошо провели время, а также получили много приятных и смешных воспоминаний.В итоге, нам удалось организовать несколько успешных встреч, а участники третьего этапа даже продолжили общаться после тестирования.В июне мы провели повторное тестирование с двумя новыми участниками, которые заранее не знали о том, как работает приложение. Ребятам удалось разобраться в общем функционале, и благодаря помощи других тестировщиков они смогли сами организовать встречу.Сохранившаяся переписка бета-тестеровВсе бета тестеры остались довольны работой приложения и теперь ждут дальнейшего развития проекта.РезультатыПриложение Coffee meet-up продолжает развиваться и уже делает первые успехи.Я защитила этот проект как свой выпускной в IT Школе Samsung и получила 10 из возможных 10 баллов, стала победителем первого и второго этапов конкурса “IT Школа выбирает сильнейших!” и заняла второе место в номинации“Социальные приложения” в финале конкурса.Я верю, что это не конец, а только начало. В будущем я планирую сделать полноценный сервис, доступный не только на Android, сотрудничать с реальными кофейнями и, конечно же, помочь людям найти новых друзей, чтобы разделить с ними  вкусный кофе.Только зарегистрированные пользователи могут участвовать в опросе.Войдите, пожалуйста.А вы бы пользовались приложением Coffee meet-up?50%Да, я бы пользовался.1421.43%Затрудняюсь ответить.628.57%Нет, мне не интересно это приложение.8Проголосовали 28 пользователей.   Воздержались 6 пользователей.Только зарегистрированные пользователи могут участвовать в опросе.Войдите, пожалуйста.Если да, то вы бы хотели с помощью него найти21.05%Партнеров для коворкинга (людей для совместной работы/учебы в кофейне)463.16%Новых друзей1215.79%Потенциальных романтических партнеров3Проголосовали 19 пользователей.   Воздержались 12 пользователей."
СберМаркет,,,Сложно ли пронести гаджет в школу,2024-08-30T16:00:34.000Z,"— Рыцарь — это человек, он без страха и упрёкаПетров и ВасечкинЭто история разработки B2B решения для планшетов, чтобы увеличить их востребованность в сфере образования. Вначале мы совершили все возможные ошибки: считали себя самыми умными, путались в «болях» целевой аудитории вместе с фокус-группой. Всё нравилось менеджерам, разработчикам и, главное, начальству. Затем пилот решения дошёл до пользователей и пришло время отваги для осознания наших заблуждений.Всё началось с участившихся запросов на применение планшета в школе. Мы съездили в несколько школ с целью выяснить, какая «боль» вызывает этот интерес, а какая не даёт его реализовать. Рядом с нами были коллеги-эксперты изIT Школы Samsung, которые помогали нам в исследовании. Мы наивно полагали, что всё сразу сделаем в лучшем виде.К этому просто надо быть готовымУзким про­ли­вом мы плы­ли, и в серд­це тес­ни­лись сте­на­нья;Сцил­ла с это­го боку была, с дру­го­го Харибда…Гомер. Одиссея. (греческий эпос 8 в. до н. э.)Эта статья о конечном видении продукта Samsung Class, реализовавшего «что хотела целевая аудитория». А еще о функциональных ролях продакт-менеджмента и разработчиков, которым важно следовать для достижения такого результата.Как сосредоточиться на боли пользователя и этим не создавать ему новыхСосредоточиться на «боли» целевой аудиторииТолько Samsung оказался абсолютно открытым к диалогуи готовым слышать то, что нужно реально для того,чтобы продукт стал образовательнымКонстантин Эдуардович ТхостовДиректор лицея №369, Санкт-ПетербургКак изменился урок в школе за последние двадцать лет? Дети остались те же, разве что гаджеты разбаловали их. А еще теперь по закону проносить их в школу нельзя. При этом, как и раньше, учитель для реализации плана урока за короткое время решает две задачи:ВовлечениеУдержание вниманияУченики пришли с другого урока после перемены и мысленно ещё там или вообще в смартфоне, несмотря на формальный запрет. Когда они мыслями придутв себясюда?Чем занято внимание детей, только пришедших на урокБлогеры YouTube и TikTok разбаловали всех соревнованием за секунды внимания, и учителю теперь приходится конкурировать с ними.Физическое присутствие на уроке вовлеченность не гарантирует. Для решения этой задачи уже есть метод — разнообразие педагогических приемов, и мы решили, что добавить в него еще вариантов — это адекватная и благородная цель.К тому же, с древних времён осталась незакрытой тема «мне плохо видно с задней парты», потому что первых парт мало, а учеников много. Вовлечение и удержание внимания удаленных парт даётся объективно тяжелее, чем ближних.Проблема задних рядов в школеС потребностями аудитории определились и началось самое сложное: соблазны забить продукт трендовой функциональностью. Настало время героических решений.Сперва честно признались себе, что не будем делать ничего про дистанционное обучение: мы про планшеты в оффлайн классе.Затем избавились от навязчивого желания сделать что-то вроде очередного приложения для образования.Сложнее всего было выкинуть мысль встроить мессенджер, потому что они сейчас везде.Итак, мы определились с целью:адаптировать планшеты к использованию на оффлайн уроке. Это означает, чтопланшеты должны быть окном внимания к учителю. Привычным окном, поскольку дети привыкли к гаджетам. Вместо того, чтобы сетовать на эту привычку, мы используем её во имя педагогического разнообразия. Для этого мы решили обеспечить связь между планшетами учеников и учителя с помощью нашего приложения Samsung Class.Важно не только понимать боль ЦА, но и сосредоточиться на ней при создании решения.Чем проще, тем лучше!Разработчик, не увеличивай «боль» целевой аудиторииКак мы рассуждали дальше со стороны разработчиков: никаких модных облаков или дополнительных серверов. Мы решили, чтоиспользование и развертывание будут простымидля обыкновенного учителя. Его планшет будет управляющим для планшетов учеников с возможностями:запуск трансляции экрана на устройства учениковзапуск трансляции содержания интерактивной панели на устройства учениковдоступ к экранам учениковотправка необходимых для работы файловудаленный и единовременный запуск приложений и веб-страницотслеживание «бездействия» учениковуправление политиками доступа на всех планшетахПриложение Samsung Class превращает разрозненные устройства в систему планшетов учащихся с планшетом учителя в качестве управляющегоСанитарные нормы диктуют минимальный размер устройства в 10.5”, поэтому базовым вариантом мы выбрали модели Samsung Tab A9+/S9. Интерактивная панель Flip Pro с диагональю от 65 до 85 дюймов это опциональная часть решения. Часто в классах уже встречаются подобные панели других производителей, и они тоже могут быть задействованы для вывода экрана планшета учителя.В качестве сервера используется планшет учителя.Никакой дополнительной инфраструктуры кроме сети Wi-Fi не требуется. Автоматическая установка и настройка реализованы с помощью сервиса Knox Configure: достаточно создать список серийных номеров в аккаунте сервиса и настроить удаленную установку программы.Мы предусмотрели даже вариант отсутствия Wi-Fi. В этом случае планшет учителя используется в качестве точки доступа. Но для мобильных точек доступа есть ограничение до десяти устройств, поэтому это опция скорее для мини-групп на природе.Как это выглядит на деле: планшеты «живут» в ящике с зарядками в шкафу, пронумерованы и закреплены за учениками согласно порядковому номеру в журнале.Весь урок на планшете не проходит, но использовать их на уроке имеет смысл. Например, в начале урока учитель проводит быстрый опрос, который позволяет в течение пяти минутвовлечь всехпришедших в урок, заодно проверив уровень усвоения знаний.Для решения задачи включения ученика в урок мы создали дополнительную опцию решения – проведение быстрых опросов. Альтернативными вариантами могут быть quiz-задания на уже используемых образовательных платформах или короткие ролики. Это на выбор учителя.Настройки системы тестированияПреподаватель имеет доступ к управлению происходящим на экране любого из учеников.  А также может отправлять им и на интерактивную панель изображение своего экрана. Таким образом, оно транслируется на устройства учеников, чтобы задним партам всегда «было видно». При необходимости преподаватель может снова подключать планшеты,единовременно запуская у всех приложения или открывая нужные ссылки.Когда мы вышли на оперативный простор, начав тестировать решение в реальных аудиториях, то стали выявляться аспекты, которые потребовали от разработчиков дополнительной работы. Приведу несколько примеров:Когда в классе много учеников, каждый раз подключать планшет через QR-код неудобно. Поэтому проще сделать шаблоны с названиями урока, аудитории, готовым списком учеников и выбранной папкой для материалов.Ученикам при использовании гаджетов захочется их «сломать». Тогда убираем таскбар и верхнюю панель доступа, так что запуском приложений на устройствах управляет исключительно учитель, а учащемуся доступны лишь папка материалов и ранее запущенные приложения.Иногда ученики авторизуются не под своими именами. Чтобы бороться с некорректной авторизацией, мы реализовали алгоритм, который автоматически привязывает ученика к планшету, используя соответствие номера планшета, устанавливаемого в приложении, с номером ученика в списке класса.И так далее.Удачно ли оказалось лавирование между Сциллой и ХарибдойСамым сложным было доносить идею о том, что Samsung Class это не образовательное приложение, вроде МЭШ, Uchi.ru или EdPad, а «предустановленный» B2B софт для образования, создающий из планшетов единый инструмент. Это не образовательная платформа и не контент, а техническая часть образовательной экосистемы.Samsung Notes на планшете учителя связывается с помощью Smart View с интерактивной панельюЭкран учителя доступен не только на интерактивной панели, но и на планшетах учениковПо отзыву преподавателя информатики лицея №369 Санкт-ПетербургаСкотникова Вадима Борисовичаосновные ценности решения:Безопасность— этопросмотр экранов учеников с устройства учителя. Контроль за происходящим на устройстве – это часть порядка в классе.Скорость— это управление и одновременный запуск приложений (или ссылок) на всех планшетах, что позволяет избежать траты времени: «а теперь запустите приложение…»Это одни из «болей», которые мы закрывали, чтобы гаджеты не усложняли ход урока.До Samsung Class планшеты уже нашли применение в образовательном процессе 369 лицея и решение оказалось полезным дополнением к немуВ лицее нашёл свое применение и обычный софт вроде Samsung Notes: во взаимодействии со школьной инфраструктурой, включающей облачное хранилище, куда выгружали скриншоты или pdf для себя или для последующей проверки выполненных заданий учителем.Не могу не напомнить о своей предыдущей статье пронедавно появившиеся функции AI: в Samsung Notes тоже можно делать аудиозапись и транскрибацию лекций.Вообще говоря, целевая аудитория — это не только общеобразовательные школы, но и высшие учебные заведения. Например, небольшие учебные группы или кабинеты по изучению языков. Также Samsung Class могут использовать бизнес-центры, предоставляющие в аренду аудитории для лекций и тренингов, чтобы сделать контент на интерактивной панели видимым каждому в зале.ВыводыСоздание продукта подобно маневрам между Сциллой и Харибдой из приключений Одиссея. Менеджеры по продукту и маркетинг должны концентрироваться на потребностях целевой аудитории, используя бритву Оккама для пресечения фантазий о ненужной функциональности. В свою очередь, разработчики должны быть готовы к доработкам и избегать создания новых «болей» пользователю, подбирая варианты технических решений.Если участники IT проекта этого не понимают, то выгорают либо менеджеры по продукту, либо разработчики, в результате принося пользователю больше проблем, чем пользы.Этим пониманием функциональных ролей в команде мы пользуемся, чтобы помогать B2B заказчикам в решении их запросов или искать решения для целых отраслей.Продакты должны услышать боли пользователя, а разработчики не добавить новыхМы поняли основные потребности = боли целевой аудитории, поняв и мотивацию – расширить разнообразие педагогических приемов для вовлечения учеников и удержания их внимания в ходе урока. Дали учителю планшет с Samsung Class, помогающим решать эту задачу,ради повышения эффективности обучения.JTBD (Jobs To Be Done) фреймворк для решения потребностей ЦАУдачность лавирования между Сциллой и Харибдой определяется отзывами и впечатлениями реальных пользователей. Такая вот Одиссея создания для любого продукта.Павел МедведевSamsung"
СберМаркет,,,Мобильный AI на рабочем месте. Ищем реальную ценность,2024-07-30T15:36:17.000Z,"Пенни: Как заколки могут привлечь мужчин?Говард: Добавим туда BluetoothШелдон: Гениально! Мужчины обожают Bluetooth!Пенни: Подождите, вы хотите сделать заколку с Bluetooth?!Шелдон: Пенни, все становится лучше, когда есть Bluetooth!Теория большого взрываО хайпе вокруг AIЕщё пятнадцать лет назад я думал о том, почему в смартфоне нет функции создания транскрипта аудиозаписи.  Диктофон превратился в приложение для смартфона, но по-прежнему требовал последующего прослушивания и ручного конспектирования аудио. А вот AI сделал транскрибирование доступным рядовому пользователю.И эта мысль навела меня на размышления об искусственном интеллекте вообще. Я не понимал причину шума вокруг AI все эти годы. Можно понять специалистов по Natural Language Processing или компьютерному зрению, у которых реально возросла эффективность алгоритмов благодаря глубинному обучению. Остальным-то что с этого?Становится ли всё лучше, когда естьbluetoothAI? Я считаю, что не становится! Как в известной пословице,  «сколько ни говори халва, во рту слаще не станет». Пока NVIDIA, как и положено во времена золотой лихорадки, зарабатывает на кирках и лопатах, аналитики Goldman Sachs ужевыражают скепсис, не переоценивает ли общественность перспективы влияния AI на мировую экономику. На данный момент нет никаких практических применений AI для массового использования, которые перевернули бы мир. А массовое значит мобильное, потому что смартфон является основным инструментом потребления IT-сервисов.Может стоит не рассуждать о революции, а посмотреть на ситуацию с практической точки зрения: какие из повседневных задач хотя бы частично удается автоматизировать при помощи AI? Например, та же расшифровка аудиозаметок. Не прошло и двадцати лет, как штатная программа звукозаписи в смартфоне, наконец-то закрыла эту потребность. Конечно, онлайн сервисы уже существовали, однако только сейчас на смартфонах (а это и есть мобильный AI) в приложении «Звукозапись» появилась функция расшифровки аудио on-device, без подключения к облаку, а значит без рисков, связанных с конфиденциальностью.Транскрибация и резюмирование экономят любому офисному работнику столько сил и времени, что теперь можно начинать говорить о повседневности AI в бизнесе и постепенно к ней привыкать. Совещания, обучение, интервью – лингвистическая модель Galaxy AI пригодится везде, где необходимо конспектирование и анализ аудиальной информации, с которой проще иметь дело в текстовом виде.AI-сервисы продолжат качественно развиваться, ведь обучение алгоритмов, как и людей, требует времени. Поэтому в этой статье я постараюсь не просто описать свой опыт использования этих сервисов, но и сформулировать их реальную ценность в том виде, в котором они доступны сейчас. И объяснить, для чего нам это вообще нужно.Бизнес-применение 1: Полный транскрипт аудиозаписейРаньше я не пользовался функцией аудиозаписи, потому что у меня никогда не хватало терпения и времени прослушивать заново весь разговор.  Меня всегда интересовала возможность автоматического транскрипта голоса, чтобы по ключевым словам, вспомнить, о чем шла речь, например, на прошедшем совещании или лекции. Уже практически потерял веру в то, что это будет возможно, но дождался.Приведу пример использования функции транскрибирования в приложении «Звукозапись» на новых флагманах Samsung с Galaxy AI. On-device. Бесплатно. Обработка аудиозаписи с помощью AI проводится прямо на устройстве, файлы не уплывают в неизвестные облака. Это и есть одно из применений NPU (Neural Processing Unit).Я записал совещание, проводившееся по телефону, которое длилось тридцать минут. Записывающий смартфон лежал рядом с другим телефоном, по которому велся разговор на громкой связи. После завершения записи и нажатия кнопки «Транскрибир.» аудио обрабатывалось три минуты. В результате на экране приложения появился аккуратно отформатированный текст, с которым удобно иметь дело.Расшифровка аудиозаписи в приложении Звукозапись на смартфоне Galaxy Flip 6Программа продолжает работать и в свёрнутом виде, и с выключенным экраном. А вот ответ на входящий звонок запись приостанавливает. Для использования функции аудиозаписи на совещании или во время мозгового штурма в просторной переговорной рекомендую предусмотреть микрофонную систему. Даже пара простых беспроводных микрофонов решает многое, ведь качество аудиозаписи напрямую влияет на качество распознавания.Сгенерированный на основе аудиозаписей текст пока далек от идеала и требует редактирования. Чисто субъективно английскую речь Galaxy AI сейчас распознаёт лучше, чем русскую, в тексте меньше путаницы с определением начала и конца предложений. Но оно и понятно: функция поддержки русского языка появилась только весной этого года, поэтому разработчикам есть над чем поработать. Это вопрос обучения и объема данных: с каждым обновлением качество расшифровки будет улучшаться. Кроме того, в русской речи встречается много англицизмов, словосочетаний на иностранных языках и иноязычных аббревиатур, которые сейчас не всегда хорошо понимаются движком.Расшифровка и сводка записи в приложении ЗвукозаписьРасшифровку дискуссии, в отличие от записи лекций или длинного монолога, который AI понимает гораздо лучше, непосвященному человеку разобрать пока сложно из-за наличия в тексте некорректных сочетаний слов и явных ошибок распознавания. А вот участник обсуждения легко вспомнит, о чем именно шла речь и восстановит содержание разговора. Выделенные в тексте слова или фрагменты можно прослушать в оригинальной аудиозаписи. Диалоги, состоящие из коротких высказываний распознаются хуже, чем на английском, но я жду улучшений по результатам дальнейшего обучения алгоритмов. Самое интересное, что сводка всегда выглядит осмысленнее отдельных элементов транскрибации несмотря на качество расшифровки. Расскажу об этой функции поподробнее.Бизнес-применение 2: Краткость — сестра талантаЯзыковые модели призваны создавать не только «вау-эффект» от кратковременной симуляции «живого» разговора с GPT чатом. В современном мире мы потребляем огромное количество информации, определить стоит ли нашего внимания новый объемный материал или нет поможет функция краткой сводки, как одна из реализаций потенциала Natural Language Processing. Вообще, резюмирование содержания — это то, что все ждут от AI.Сгенерировано в https://rudalle.ru/kandinsky31Поэтому Yandex внедрил в свой браузер функцию краткого пересказа видеороликов, так что можно просмотреть краткое содержание буквально за минуту. Samsung оснастил свой мобильный браузер функцией создания сводки, что особенно полезно для тех, кто имеет дело с текстами и их аналитикой. Эта функция доступна не только в браузере, но и при создании транскрипта аудиозаписей. Я сравнил Yandex пересказ и сводку от Galaxy AI от прослушиванияОбзора направлений развития 6G. Для поиска подходящего контента Яндекс – однозначный выбор, потому что сводка создается очень быстро, всего за несколько секунд. Если же смотреть YouTube-ролик, используя Galaxy AI, сводка будет ещё более подробной. Те, кто пользуются двумя телефонами, как я, могут вообще оставлять один из них у компьютера, воспроизводящего нужный контент, и заниматься другими делами, а потом просмотреть полученный транскрипт и сводку, отражающие содержание и структуру контента. Если вы когда-либо имели дело с онлайн-обучением, то знаете, что часто никаких возможностей законспектировать содержание просто нет. Все надо записывать и переводить в текст.Так выглядит краткий пересказ от Yandex и сводка Samsung. Работу со сложными техническими материалами можно упрощать при помощи AIА вот как выглядит сравнение стандартной и подробной сводки классического отрывка письма Татьяны Онегину из романа «Евгений Онегин» на примере AI-анализа при создании сводки в браузере Samsung Internet:Слева представлена краткая сводка, а справа подробная. По-видимому, с резюмированием художественного текста AI справляется куда лучше. Ваши версии, почему?Функция запускается нажатием кнопки в нижней панели мобильного браузера Samsung Internet. Я открыл первый том романа «Война и мира» на lib.ru и попробовал сделать сводку. В результате мне была показана сводка лишь первой главы со сценой салона Анны Павловны Шерер. Никаких сообщений об ошибках браузер не выдал. Сервис просто проигнорировал основной многокилобайтный контент произведения. Зато любая статья на Хабре обрабатывается без проблем. Не нашёл официальной информации по ограничениям. Конечно, любой сервис имеет свои пределы разумного использования, и разработчики что-то противопоставили попыткам «повесить» облачный сервис запросами на разбор грандиозных по объему текстов. Как я понял, ориентировочно, это одна глава «Войны и мира».Сразу отмечу, что функция создания сводки не заменяет нашу способность к обобщениям и выводам. Если транскрибация восстанавливает детали субъективной и объективной картины происходившего, то чтение сводки помогает нам с оформлением собственных выводов и обобщений.Функция создания сводки использует облачный сервис, в отличие от транскрибирования, выполняемого на телефоне. Некоторых пользователй это может тревожить. Тут я задумался, что, вообще, может помешать повсеместному применению перечисленных выше функций?Барьеры для бизнес-применений AIГлавная сложность, как мне кажется, – это готовность собеседников к ведению переговоров «под запись». Законодательство требует охраны персональных данных, а  этика – согласия участников встречи на запись. Мы не склонны быть неформальными в общении, зная, что слова «пишут пером», поэтому в некоторых случаях овчинка использования записи для резюмирования выделки не стоит. Особенно в ситуация, когда доверие важнее учёта. Запись фиксирует оговорки, замешательства, реакции или промежуточные мнения, про которые не все участники переговоров хотели бы знать, что их «не вырубишь топором». Однако ситуация меняется, когда встречи менее эксклюзивны, более формальны, публичны или поставлены на поток, например, собеседования соискателей на работу, записи презентаций или докладов, глубинные интервью и опросы в маркетинговых исследованиях.Бизнес-применение 3: грамотность как самопрезентацияЯ всегда ленюсь исправлять «очепятки» в мессенджерах и не использую T9, не говоря уже о тире, запятых и двоеточиях. Это вечный конфликт между «меня и так поймут» и внутренним желанием всё исправить. С появлением AI-корректора я постепенно привыкаю к его использованию, и это касается не только орфографии и пунктуации. Ниже приведу пример того, как корректор исправляет текст, предлагая качественные альтернативные варианты даже для самых безграмотных формулировок.Теперь функция проверки орфографии и стилизации есть в каждом мессенджере на смартфоне Samsung с Galaxy AIТо же касается стилистики и способа выражения своих мыслейЭльдар Муртазинпишет, что по данным его источника, который он не приводит, женщины используют эту функцию чаще мужчин. Логично, кто обычно больше следит за своим стилем? Гендерные стереотипы проявляются и в статистике использования AI функций, ставших доступными массовому пользователю.В чем реальная ценность корректора стиля и грамотности? Это возможность не гуглить ответ на вопрос «как правильно», а нажатием кнопки увидеть и сделать так, как должно быть. А там глядишь постепенно уйдут в прошлое из оборота все эти «вообщем», «через чур», «ихний» и прочие «тся/ться».Бизнес-применение 4: генерация текстовЭта функция доступна на базе клавиатуры Samsung. Нажимаем на кнопку слева вверху и затем выбираем «Генератор текстов». Стилистика генерируемых сообщений может варьироваться в зависимости от контекста, который вы задаете: стандартная, электронная почта, социальные сети, комментировать, а также от стиля: вежливый, повседневный или профессиональный. Samsung предлагает двенадцать вариантов сказать по-разному об одном и том же.Сгенерированные Galaxy AI варианты. Какие-то фразы могут выглядеть неуместными, но ничто не мешает их заменить или убрать совсем. Редактировать проще, чем подбирать фразы самомуКаждый раз генерируется новая форма, предлагающая новые формулировки, поэтому если у кого-то возникает вопрос «как написать, ответить или предложить», — Galaxy AI лучший помощник, чтобы подобрать «рыбу» ответа, которую можно отредактировать в финальную версию. Теперь это одна из функций клавиатуры, что действительно удобно.Предлагаем встретиться оффлайн с Galaxy AI. Мечтаю, когда AI научится угадывать гендер собеседника и подставлять нужные местоименияНа мой взгляд, функция «генератор текста» прекрасный инструмент для избавления от привычки откладывать в долгий ящик желание написать. Иногда причина этого банальна. Не хочется писать много букв. А вот когда AI  готов подобрать варианты для написания и оформления текста, то это бесценно.Бизнес-применение 5: бытовое общение в командировках и на отдыхе / Функция «Переводчик»Невозможно пройти мимо Flip-версии переводчика, которая позволяет обмениваться простыми и понятными репликами. Эта программа  открывается через панель быстрого доступа. Flex экран активируется второй кнопкой из расположенных справа вверху открытого приложения.Собеседник видит перевод на своем экране и может нажать на кнопку записи для того, чтобы произнести ответную фразу для переводаТакже есть удобная специфика отображения и для нескладного варианта, доступного и на смартфонах с Galaxy AI без flex экранаДаже без сопровождающего носителя языка и переводчика можно уверенно чувствовать себя, решая простые вопросы с сотрудниками ресепшена, таксистами и случайными встречными, если вам потребуется помощь. Это гораздо удобнее, чем вертеть экран туда-сюда.Что касается синхронного перевода телефонных разговоров, то лично я жду реализацию функции live-транскрибации голоса собеседника в помощь уже владеющим иностранным языком на хорошем уровне, но испытывающим сложности с восприятием иностранной речи на слух. Произношение в разных странах и регионах сильно различается. Америка, Англия, Корея, Китай, Индия и иные не носители языка, но использующие английский в бизнес коммуникации. Было бы интересно общаться в наушниках или на громкой связи и видеть, как AI понимает сказанные собеседником слова вместе с их переводом. Это могло бы быть неплохой помощью тем, кто уже общается на других языках по бизнесу.Как всё может развиваться дальшеIDC (International Data Corporation)смотритна мобильный AI оптимистично, но снабдить смартфон нейронным процессором ещё не значит найти ему реальное применение в массах. Во-первых, дело это не быстрое. Например, с момента появления смартфонов до понимания реальной ценности, которую они внесли в нашу жизнь прошло не меньше десяти лет. Оказалось, что большой экран — это не столько средство для потребления развлекательного контента, сколько удобство для обмена текстовыми сообщениями. Мобильные мессенджеры потеснили даже социальные сети. Вряд ли с AI дело будет обстоять по-другому, то есть реальная и меняющая всё ценность станет очевидной не сразу, а лишь по мере развития практики применения.Возможно, когда анализ данных AI-помощником начнет представлять достаточную ценность, люди станут проще смотреть на его доступ к содержанию всей своей коммуникации через смартфон, включая голосовое общение, мессенджеры или запись всего происходящего вокруг. Тем более, если получится обеспечить как можно больше AI-функций возможностью выполнения на непосредственно устройстве. Вообще идея личного ассистента, повсюду сопровождающего пользователя, популярна в научной фантастике, однако пока не исчерпала себя, и мобильный AI – это очередной шаг в этом направлении.Сгенерировано в https://rudalle.ru/kandinsky31ВыводыЯ перечислил лишь те функции, которые нашли применение в моих рабочих буднях, упростив решение многих повседневных задач. Помимо названных выше сервисов, отдельно упомяну AIредактор, который позволяет дорисовывать картинки на основании «корявого» эскиза от руки. Возможно, я и ему найду практическое применение в повседневном использовании. Я дорисовывал улыбочки с такими непредсказуемыми и забавными результатами. Galaxy AI самостоятельно определяет эскиз чего я рисовал и подбирает место на фото.Оживление реальности с помощью эскизов и AIНаконец-то появились достойные решения, хотя кому-то все они могут показаться недостаточными. Немного написал о будущем, в котором, на мой взгляд, мы будем иметь дело с развитием идеи смартфона как личного ассистента в ежедневных делах. Мы увидим еще много новых решений и возможностей, включая улучшение качества алгоритмов, пока использование AI найдёт себе такой же массовый спрос, как и смартфоны, которые есть у каждого. Прогресс очевиден, но истинные  масштабы и значение мобильного AI мы сможем оценить лишь в течение десятилетий.Пользователи Хабра уже нашли золотую середину в разговорах об AI: это хороший инструмент и его надо воспринимать именно так. Искать его практическую ценность в своей жизни, а не витать в облаках. Интересно, какие идеи насчет настоящему и будущего мобильного AI выскажут читатели в комментариях.Медведев Павел, Samsung"
СберМаркет,,,Обработка изображений и видео на смартфонах: handcrafted-алгоритмы против глубокого обучения,2023-12-27T13:17:24.000Z,"Зеркалка — хорошая штука, но смартфон однозначно компактнее и удобнее. Да, мобильная оптика все еще далека до уровня зеркалок, однако получить красивые фотографии человек хочет здесь и сейчас. Как быть? Взамен харда, подключается софт, алгоритмы!Именно софт смартфона помогает снять фото в HDR или сделать видео менее смазанным. Он состоит из «классических» вычислительных алгоритмов и нейросетей. Вычислительные алгоритмы требуют много ручной работы и глубокого понимания решаемой задачи, зато хорошо работают при ограниченных вычислительных ресурсах. Нейросети прожорливы, но сильно упрощают жизнь разработчику и потенциально позволяют достичь большего. Объединение двух этих подходов даёт замечательные результаты!Несколько примеров улучшения изображенийВычислительные алгоритмы и нейросети: в чём отличия?Все вы знаете, что современные парадигмы обработки цифровых, визуальных, звуковых данных — это методы машинного, либо глубокого обучения. Разница между ними не очень большая, но тем не менее, есть.Deep Learning «забирает» у человека функцию извлечения признаков. Источник картинки: https://levity.ai/blog/difference-machine-learning-deep-learningМашинное обучение (Machine Learning, ML)— это некий процесс, в котором человек участвует на этапе формирования правил по извлечению признаков. То есть человек, используя свою интуицию, эвристику, экспертное представление об объектах анализа предполагает, какие из признаков (для изображения: контуры, цвет, размер связанных объектов, частотные характеристики и так далее) могут быть использованы для задач, выполняемых машиной, к примеру, для классификации (разбиения данных на группы, например, для e-mail — на спам и нормальные письма). Таким образом, человек берёт на себя функцию, связанную с тем, что стоит считать важным для данной задачи, какие признаки данных следует далее передавать алгоритму для принятия решения. В этом случае значительную роль играет личный опыт и понимание специфики задачи. Такой подход оправдан до сих пор для случаев, когда отсутствуют тренировочные данные достаточного качества и количества для обучения нейронных сетей.Глубокое обучение(Deep Learning, DL) —в этом случае человек делегирует ответственность по извлечению признаков самой нейронной сети. Нейронная сеть формирует внутреннее представление признаков в соответствии с тем, какая целевая задача перед ней стоит, и таким образом сама определяет, что считать важным в тренировочных данных. Обычно, целевая задача формулируется в виде математических критериев, численным образом формализующих разницу между предсказанными и истинными значениями. Эту разницу называютфункцией потерь. Нейронная сеть пытается решить поставленную задачу наиболее оптимальным образом за счёт минимизации разницы между прогнозируемыми и истинными результатами в ходе тренировки, сводя эту разницу, «потери», к минимуму. Роль человека в данном случае касается выбора предпочтительной архитектуры нейронной сети (модели), и что не менее важно, формулировки критериев обучения с помощью функции потерь.Кроме выбора характеристик нейронной сети и способа тренировки, ещё одним немаловажным аспектом в глубоком обучении является представительность и качество тренировочных данных, поскольку именно из этих данных нейронная сеть далее черпает представление о том, как должен выглядеть результат. Соответственно, если в тренировочных данных отсутствуют примеры, которые потом будут встречаться на практике, то такая несбалансированность может привести к неверному «поведению» нейронной сети: она будет иметь склонность сводить результаты только к тому, что «видела» в тренировочных данных.Сейчас большинство инженеров, когда приступают к новой задаче, решают её методами машинного или глубокого обучения. Глубокое обучение в некотором смысле даже более простой вариант, поскольку система устроена как черный ящик, которому мы даем входные данные, определяем задачу с помощью математических критериев и контролируем результат. Поэтому часто забывается, что предшествовало эпохе машинного и глубокого обучения.Раньше всё приходилось делать рукамиМожно сказать, это была ранняя эра разработки алгоритмов, так называемых handcrafted («крафтовых») алгоритмов. Немногие сейчас углубляются в эту область, ведь современный инженер — это, прежде всего, специалист в машинном/глубоком обучении. Handcrafted — это немного пренебрежительное название алгоритмов, в которых человек сам определяет все шаги обработки данных (далее мы будем называть их вычислительными алгоритмами). Например, для задачи классификации человек самостоятельно устанавливает последовательность процесса обработки, способы вычисления признаков и принятия решения, при этом опирается на свое понимание проблемы и наблюдения. В итоге, исследователь создаёт модель этой проблемы и реализует её решение в виде алгоритма действий.Такой подход обладает рядом недостатков (но есть и ряд преимуществ, о которых я расскажу далее): он очень сильно зависит от того, насколько человек погружен в проблематику, насколько он может достоверно сделать модель, учесть все нюансы. В задаче улучшения качества изображения инженер составляет математическую модель, описывающую деградацию (ухудшение качества) изображения: то, как оно изменяется под воздействием некоторых мешающих факторов (добавление шума, геометрических искажений или размытия), потом решает обратную задачу и реализует её. Такой подход существовал 10-15 лет назад и в ряде задач им пользуются до сих пор.В компьютерном зрении 2012-й год стал «водоразделом»: нейросетевые подходы стали преобладать над «традиционными» вычислительными методами. Источник изображения: https://link.springer.com/article/10.1007/s11263-019-01247-4До 2012 года, когда появилась всем известная сеть для классификацииAlexNet, доминировали традиционные методы — машинное обучение и вычислительные алгоритмы. После того, как был опубликован AlexNet, внимание разработчиков сконцентрировалось на нейросетевых подходах. Если мы правильно представляем данные и правильно формируем критерии обучения, то потенциал нейросетевых подходов гораздо выше, чем разработка алгоритмов, которые человек создаёт, пользуясь своей интуицией и инженерной эвристикой.Нейросети: бочка мёдаОптимальное решение вырабатываетсясамостоятельносистемой в процессе тренировки. Оно будет оптимально в рамках установленных критериев и данных, на которых обучалась сеть.Параллельно с эволюцией нейросетевых решений и развитием методов оптимизации появляется множествофреймворков, сред разработки, которые позволяют разработчику достаточно быстро погрузиться в предмет и использовать сложные математические функции, вызывая их буквально парой строк кода на Python.Более того, идет очень плотная интеграция этих фреймворков с «железом», поскольку нейросетевые решения очень требовательны к вычислительным ресурсам. Разработчик получает прослойку в виде фреймворков, которая его отодвигает от необходимости понимания того, как вычисления будут распараллеливаться на графических картах, какие операции наиболее эффективно поддерживают нейросетевые вычисления и так далее. Это очень удобно. Фреймворки часто публикуются по модели Open Source, участники сообщества делают свой вклад и это способствует более стремительному развитию всей этой области. Я уже упоминал, что разработчики мобильных устройств сейчас серьёзно озадачены тем, чтобы обеспечить аппаратную поддержку для нейросетевых решений, поскольку понимают их преимущество. Таким образом, помимо графических ускорителей в смартфонах появляются дополнительные вычислительные модули — нейропроцессоры, которые нацелены именно на особенности вычислений нейронных сетей. Появляются инфраструктурные решения — коммерческие продукты, которые упрощают разработчику жизнь и уменьшают время, необходимое для получения первого результата.Какие же есть сложности в глубоком обучении? Кратко пробежимся по ним.Нейросети: ложки дёгтя1. Хорошие датасеты на вес золотаПоскольку разработчики постепенно отдаляются от старых приёмов разработки, то роль моделирования изменения данных перекладывается на обучающий датасет. С его помощью мы представляем нейронной сети примеры входных данных и того, как должен выглядеть результат, эталоны. При этом качество датасета играет большую роль: он должен быть достаточно сбалансирован, иметь представительность по всем возможным вариантам результатов.Для задач улучшения качества изображений обучающие датасеты обычно представляют собой пары фото, например, исходное изображение с камеры смартфона, а другое, эталонное — с цифровой DSLR-камеры (цифровая зеркальная камера). В этом случае нейросеть имеет эталон (ground truth), к которому пытается приблизиться, изменяя входное изображение.Сбор хороших данных — это сложный и дорогостоящий процесс. Существуют крупные компании, специализирующиеся на подготовке тренировочных данных, такие как Яндекс.Толока, Amazon Mechanical Turk, Dbrain, но такие сервисы в основном сконцентрированы на задачах, которые хорошо описываются и масштабируются, например, разметка изображений для сегментации изображений или распознавания текста. Привлекать их для создания эффективных и узкоспециализированных датасетов, как, например, для задачи повышения разрешения изображений (super resolution), — сложно, потому что здесь необходимо использовать оборудование и иметь понимание процесса, происходящего в камере.Любопытно, но даже в таких отлаженных процессах как разметка изображений, можно неожиданно натолкнуться на сложности, например, для Amazon Mechanical Turk существовал такой термин как «мексиканский след» в данных. Просто так получалось, что на такую низкооплачиваемую и рутинную работу с готовностью вовлекались краудсорсинговые работники из Мексики, для которых она выглядела как привлекательная подработка. И, в итоге для задачи разметки изображений на две категории: привлекательные или менее привлекательные, мы получаем набор изображений, которые жители Мексики в среднем считают красивыми и интересными. Но, большой вопрос, насколько это может быть справедливо для жителей остальных стран, представители которых не заинтересовались заработком около 2$ в час для участия в разметке данных.Машинное обучение в м/ф Futurama. Источник: https://habr.com/ru/articles/452392/2. Ограничения обобщающей способности нейросетейКаждая нейросеть имеет свой предел, называемый «обобщающей способностью». Нейросеть пока не может, подобно человеческому мозгу, работать со всей вариативностью реального мира.В задачах классификации или распознавания образов есть очень известные проблемы. Например, сеть не может сфокусироваться на отдельном предмете, когда у нее множество мелких объектов. Она не может понять, какой из этих объектов наиболее важен, или принять решение о том, к чему он относится. Для такой сети нужны подсказки, по крайней мере, выделение областей интереса.Источник: https://www.freeimages.com/premium/cartoon-toy-icon-415366Вот другой пример, на полу лежит бесформенный носок. Как роботу-пылесосу понять, что это именно носок и с ним нужно выполнить определенную последовательность действий? Для робота сложно воспринимать его как конкретный предмет, потому что форма слишком неопределенна и изменчива. Даже если мы зададимся целью обучить систему компьютерного зрения робота разным возможным вариантам того, как носок может выглядеть, у робота будет недостаточно визуальных признаков, чтобы классифицировать такой предмет правильно, или наоборот он начнёт множество других предметов считать носками.Восстания машин не будет из-за раскиданных по комнате носков. Источник:https://shansonline.ru/index.php/novosti/item/3040-v-khakasii-ukrali-million-rublej-khranivshijsya-v-noske-pod-krovatyuТакже проблемы возникают с протяженными предметами, такими как кабели, удлинители. Это тоже очень сильно сбивает с толку искусственный интеллект, поскольку ему сложно восстановить общую связность предмета из набора наблюдаемых изменчивых фрагментовИсточник:https://novate.ru/blogs/240717/42311/Вот еще одна широко известная иллюстрация узкой специализации нейронных сетей, связанной с их ограниченной обобщающей способностью. Нейронная сеть учится видеть мир только в тех обучающих картинках, которые она видела. Если мы ее обучали на собачках, то она на фотографиях кексиков будет постоянно встречать собак, которые очень похожи в рамках её накопленных признаков, и даже говорить, что она с высокой достоверностью видит чихуахуа в фото кекса. Таким образом, сеть даже не сможет подсказать пользователю, что в её решении что-то не так.Классический пример обобщающих возможностей нейросетей: чихуахуа распознаётся на фото кексов с изюмом …. сеть “видит” только то, на чём она училасьВ мультфильме «Митчеллы против машин» (2021) есть эпизод, где семья побеждает восстание машин, показывая взбунтовавшимся роботам мопса, и роботы выходят из строя, не в силах понять, что перед ними — буханка хлеба или мопс.Можно добавить еще более современный пример. В последние пару лет большой импульс в области искусственного интеллекта получили диффузионные генеративные сети, которые удивительным образом генерируют фотореалистичные фотографии по текстовому описанию того, что хочется увидеть на изображении. Но если попробовать эксперимент и попросить нейросеть нарисовать, например, что она считает эталоном настоящей красоты, то обнаружим, что интерпретация такого широкого термина у генеративных сетей находится в некоторой локальной области внутреннего представления, из которого сеть не может выбраться, и каждый раз генерирует очень похожие примеры.Настоящая красота «Real beauty» для fusionbrain.ai – это исключительно голубоглазая девушка с каштановыми волосамиНастоящая красота «Real beauty» в воображении Shedevrum.ai более разнообразна, но в среднем также есть свой локальный экстремум – африканская девушка с большим количеством цветов в волосах3. Эффект «черного ящика»Следующая сложность: нейросеть представляет собой «черный ящик». Мы подаем на вход данные, управляем её поведением с помощью критериев оптимизации, смотрим на результаты, валидируем их, но, тем не менее, мы не защищены от множества неприятностей. Эффект «черного ящика» очень усложняет разработчику понимание того, что же пошло не так, почему результаты отличаются от ожидаемых.Вас же тоже раздражает, когда не знаешь, что именно пошло не так? Источник: https://www.freepik.com/premium-vector/robot-with-speech-bubble-login-form_10620044.htmАлгоритмы, которые мы называем вычислительными, позволяют разработчику следить за каждым этапом обработки. Это гораздо ближе к тому, чтобы получить контролируемые результаты.В случае нейронных сетей контролируемость результата и влияние на него превращается в проблему. Самый банальный пример, это ошибки в обучающих данных. Когда разработчик замечает неверное поведение нейронной сети, и тренировочных данных относительно немного, гипотетически их можно просмотреть и постараться найти ошибки, хотя даже для небольшого количества данных это нетривиальная задача. Самое элементарные ошибки: перепутали эталон с входом, допустили ошибки в наименованиях файлов или в процессе подготовки данных. Но, если датасет увеличивается до миллионов примеров и мы имеем дело с терабайтами изображений, то уже физически невозможно вручную проверить эти данные. Поэтому приходится вводить критерии и метрики для датасетов и результатов работы нейронной сети, которые могли бы сигнализировать, что что-то пошло не так, и помочь определить источник ошибки.Для задач классификации метрики относительно просты и легче интерпретируются, изучая результаты, можно понять где ошиблись. Но, в задачах улучшения изображений, когда мы работаем с изменением очень мелких деталей, сложно подобрать объективные метрики. Обычно используются усреднённые генерализованные метрики, которые вычисляются по всему изображению, и если такой правильно подобранной метрикой можно поймать момент, например, что трава вместо зеленой стала синей, то если вдруг нейронная сеть стала производить ступенчатый эффект на границах объектов, такой эффект можно обнаружить только «глазами», внимательно изучая результаты обработки, при этом может остаться неясным источник проблемы в обучающих данных. Для контроля результата сети иногда даже создаются специальные нейросети, которые пытаются повторять работу когнитивного аппарата человека и в соответствии с ним возвращать оценку того, как бы изображение воспринималось человеком, не появился ли в результате обработки какой-нибудь неприятный для человеческого глаза элемент (артефакт).Кроме контроля ошибок в датасетах, другой немаловажный аспект это сложность интерпретации поведения, решений нейронной сети. Существуют исследования, направленные на то, каким образом можно описать внутреннее представление сети,  чтобы облегчить понимание происходящих процессов внутри сети в ходе обучения и их корректировку.Есть подход с визуализацией признаков, которые выработала нейронная сеть, чтобы понять, какие из них она посчитала наиболее важными. Другой подход состоит в том, чтобы избегать сложные глубокие сети и заменять их на комбинацию простых, легко интерпретируемых моделей, по крайней мере на начальном этапе работы с задачей. Это важно для разработчиков нейронных сетей чтобы понять, как сеть видит мир и если что-то идет не так — как поменять архитектуру сети, усложнить функцию потерь.И здесь мы возвращаемся к той самой интуиции инженера, которая была актуальна во времена вычислительных алгоритмов — традиционных методов, от которой нам нейронные сети вроде как позволяли уйти, чтобы более объективно выполнять нахождение оптимального решения.4. Аппаратная зависимостьБольшинство разработчиков этот момент не очень волнует, поскольку есть фреймворки на Python, но если мы хотим, чтобы нейросеть выполнялась на конкретном устройстве, например на смартфоне, то мы получаем такую неприятную особенность, как аппаратная зависимость.Больше слоёв Богу слоёв! Источник: https://habr.com/ru/articles/455353/Первое. Нейронный сопроцессор на смартфоне или компилятор может не поддерживать какие-либо операции, слои или поддержать недостаточно оптимально. Обычно, когда говорят про вычислительное железо, то различают устройства общего назначения, такие как центральный, графический процессор (CPU, GPU), так и специализированные, например нейронный процессор (NPU).CPU хорошо подходят для большинства задач, но малоэффективны для выполнения нейронных сетей.GPU выше по эффективности, поскольку поддерживают многопоточность, матричные вычисления, но уже имеют ограничения в том, что на них можно считать.NPU самые быстрые для нейронных сетей, поскольку базовые операции «зашиты» в железо, и вычисления требуют минимальные энергозатраты, но обеспечивают наиболее высокое быстродействие. Но, пользоваться NPU сложнее всего, поскольку требуется учитывать то, что он умеет, а что нет. Соответственно, разработчику приходится перекраивать модель нейронной сети под вид, поддерживаемый NPU, проводить соответствующую оптимизацию, обеспечивать нужную битность, некоторые критически важные вычисления, которые нельзя выполнить на NPU, – отдельно реализовывать на GPU.Разработчику приходится тратить свое время и силы на подбор архитектуры, дружественной к вычислительным ресурсам конкретного устройства. А потом выходит новая модель смартфона с обновленным железом, и приходится проходить путь заново. Справедливости ради стоит сказать, что индустрия стремительно развивается и приходится всё меньше сил тратить на поддержку или обновление модели нейронной сети для эффективного выполнения, но тем не менее проблема остаётся актуальной.Второе. Фреймворки отстают от железа. Разработчики графических процессоров и нейропроцессоров обычно делают дополнения к существующим крупным фреймворкам, но это процесс небыстрый. Появление нового железа и его полная поддержка занимают некоторое время, и в этот период могут возникать ошибки, требующие отладки.Третье. Тренировочные данные также часто бывают аппаратнозависимыми. Нейронная сеть обеспечит более корректный результат, если будет обучаться на данных с «целевого» устройства. Например, если снова обращаться к теме улучшения изображения, то характеристики камеры, архитектура сенсора, особенности оптики,  количество и размер пикселей имеют существенное значение. Датасеты со старых камер не годятся, нужно снимать данные заново. При этом есть жесткие временные ограничения по срокам выхода очередной модели смартфона на рынок. Конечно, можно пробовать использовать данные с других устройств, за счёт моделирования оптических эффектов, интерполяции стараться приблизиться к специфике целевой камеры, но от точности такого моделирования зависит эффективность обработки и зачастую по трудоёмкости такой подход не уступает самому сбору тренировочных данных. В статьях иногда встречаются упрощенные варианты моделирования датасетов, например, для задачи повышения разрешения используют хорошие фотографии высокого разрешения (эталон), уменьшают размер (входные данные) и на этих данных тренируют нейронную сеть. Но здесь есть риск, что нейронная сеть просто выучит обратную функцию для операции понижения разрешения, и результат на реальных данных не будет соответствовать ожиданиям.Теперь посмотрим пример: как нейросети и вычислительные алгоритмы используются в задачах обработки фото и видео на смартфоне.Особенности задач улучшения изображений и видеоФотография проходит длинный путь, начиная c накопленных зарядов на фотоматрице сенсора камеры и заканчивая изображением, которое пользователь видит на экране смартфона. Преобразованием сырых данных с сенсора камеры, основными этапами обработки изображения и его улучшением занимается сигнальный процессор (image signal processor — ISP), который является неотъемлемой частью любой цифровой камеры. Этот процессор — «железное» воплощение вычислительных алгоритмов. Сигнальные процессоры строятся на тех самых вычислительных hand-crafted алгоритмах, куда заложена модель изменения изображения: контраст, яркость, баланс белого, четкость границ, шумоподавление и так далее. С одной стороны, это обеспечивает максимальное быстродействие и саму возможность реализации алгоритмов в чипе. С другой стороны, алгоритмы в ISP достаточно консервативные, использование памяти для хранения промежуточных вычислений, рекурсивные приёмы обходятся слишком дорого для таких устройств, поэтому туда закладываются не совсем новые подходы, да и сам процесс разработки ISP занимает несколько лет.Наиболее типичная задача для камеры смартфона  - это задачашумоподавления. Мобильные камеры обычно имеют маленькие сенсоры, что делает их очень подверженными воздействию шумов, грубо говоря, маленький пиксель сенсора собирает меньше фотонов при низкой освещённости фотографируемой сцены, и шумы сенсора становятся сопоставимы с энергией собранного света. Гонка за размером сенсора как раз и связана с тем, чтобы захватывать больше света и обеспечить меньший уровень шума. При недостаточном размере сенсора задача шумоподавления решается алгоритмически, но в некоторых пределах.Алгоритмы спасли оленя от шакалов. (Слева фрагмент с шумом, справа результат алгоритмического шумоподавления) Источник фото: https://www.fujirumors.com/topaz-denoise-ai-v3-2-improved-raw-color-processing-and-performance/Другая задача связана сограниченностью динамического диапазонакамеры. Например, при съемке заката у моря камера не способна передать весь динамический диапазон, который мы видим своими глазами. Обычно это ограничение также решается алгоритмически: накапливается некоторое количество кадров, полученных с разной экспозицией, и далее они объединяются в одну фотографию с более насыщенными яркими цветами, становятся видны детали в тенях и ярких областях сцены. Возможно, вам известна аббревиатура HDR — она как раз и означает расширенный динамический диапазон (high dynamic range).Алгоритмы сделают это фото теплее и романтичнее. Источник: https://www.electronicwings.com/users/SujanaAbirami/projects/2074/dark-image-enhancement-using-image-processingСледующая картинка — это уже пример более высокоуровневой задачи. Например, когда человек фотографирует через стекло, то отражения и блики на фотографии потом сильно бросаются в глаза. Фотографу или дизайнеру хочется их отретушировать, но для этого нужно обладать соответствующими навыками и программами, поэтому обычные пользователи рассчитывают на автоматические алгоритмы.Блики тоже удаётся убрать. Источник: https://www.fiverr.com/yanewore/improve-the-quality-of-your-photo-and-change-the-formatИтак, подведем небольшой итог. Улучшение изображения включает в себя разнообразные задачи, как низкоуровневые (шумоподавление, HDR), так и высокоуровневые (ретуширование, коррекция артефактов — например, открыть глаза у человека или добавить улыбку, убрать мешающий объект и т. д.).Вычислительная сложность и вызовы мобильной фотографииКакая же главная проблема при работе над улучшением изображений? Увеличивается вычислительная сложность задачи, особенно по сравнению с задачами классификации, распознавания изображений.Вычислительные затраты на нейросети, понижающие размерность (как классификатор слева), существенно ниже. Источник:http://www.add3d.ru/?page_id=21517,https://www.researchgate.net/figure/UNet-structure-for-localizing-the-clouds-and-shadows-coarsely_fig2_342399093Например, при классификации изображений нейросеть архитектуры VGG-16 получала на вход изображение, а на выход давала класс (размерность слоёв нейронной сети постепенно уменьшается и требуется сравнительно немного вычислительных ресурсов). В случае же сегментации изображений требуется модель нейронной сети формата image-to-image, когда размерность выхода сети сопоставима с размерностью входа (например, широко известные нейросети архитектуры U-Net), в этом случае на порядки возрастает вычислительная сложность задачи.Размер изображений напрямую влияет на вычислительную сложность. Для сохранения деталей нужно работать попиксельно, с полным разрешением, в то время как производители, стремясь улучшить качество изображения, постоянно увеличивают количество мегапикселей мобильных камер.«Гонка мегапикселей»: разрешение мобильных камер растет стремительно. Это создает дополнительные вызовы для обработки изображений: даже просто сохранить в памяти 200 Мп изображение — нетривиальная задача.Удобство для пользователя: пользователи не готовы ждать обработку фото больше 3-4 секунд с момента съемки. Сложную обработку можно производить в фоновом режиме, но к моменту, когда пользователь перейдёт в галерею, его должно ждать готовое фото полного разрешения, т.е. применение облачных вычислителей для подготовки фотографии ограничено.Видео обычно обрабатывается в реальном времени: предполагается, что пользователь снимает видео, и алгоритм тут же пытается уменьшить уровень шума или добавить HDR-эффект. Поэтому требования к обработке видео растут, особенно с внедрением HD, Full HD и даже 4K разрешений на мобильных устройствах. Важно обеспечить высокую скорость обработки видео, в рамках 30-60 кадров в секунду, что требует всего десятки миллисекунд на один кадр.Как уже упоминалось выше, объективные численные критерии качества для обработки изображений существуют, но они не охватывают все нюансы, поэтому наряду с метриками по-прежнему используется трудоёмкая экспертная оценка результатов человеком на отсутствие видимых артефактов, поскольку человеческий глаз очень чувствителен к тому, что происходит не так на изображении, даже если удельный вес искажений (артефактов) очень мал (например, если замылились границы или появились цветовые искажения).Специфика работы со смартфоном требует осторожного распределения нагрузки на вычислительных ресурсах (например, учета работы приложений в фоновом режиме) и энергопотребления, выражающегося, прежде всего, в нагреве устройства. Все компании, производящие смартфоны, стараются оптимизировать и контролировать вычисления, чтобы ограничить расход батареи и неприятный нагрев смартфона.Дополнительно, алгоритмам необходимо конкурировать с хардварными сигнальными процессорами ISP, в том смысле, что это устройство уже присутствует в смартфоне, оно выполняет свою работу, пусть даже и не всегда наилучшим образом. Таким образом, разрабатываемое решение (например, нейросеть) должно либо превосходить по качеству, либо предоставлять преимущества пользователю там, где обычный ISP не справляется, например, давая возможность качественной ночной съемки.Важно учитывать изменчивость аппаратного обеспечения (не только количества мегапикселей в камерах, но и того, как устроены сенсоры), так как каждое новое поколение устройств вносит свои особенности, что также требует адаптации алгоритмов.Кто побеждает в итоге: машинное обучение или handcrafted алгоритмы?Это вопрос из раздела «кого любишь больше: маму или папу?»Источник: https://www.livemaster.by/topic/135625-tsena-i-tsennostМожно сказать, что машинное обучение перевешивает, так как этот подход обладает огромным потенциалом, тем более, появляются мощные процессоры с поддержкой ИИ. Машинное обучение избавляет разработчика от необходимости вдаваться в детали и позволяет собирать данные и задавать критерии, в то время как ручная настройка требует от человека разработки собственных алгоритмов на основе его понимания задачи.Но в ряде случаев вычислительные алгоритмы оказываются вне конкуренции:Когда у нас нет тренировочных данных, например, когда по каким-то причинам нет камеры, которая будет снимать фото для датасета (а мы помним, что алгоритмы машинного обучения очень зависимы от оборудования). Потребуется использовать вычислительные алгоритмы, чтобы поддерживать будущие особенности «железа»Человек, конструируя алгоритм вручную, получает контроль за вычислительной сложностью и промежуточными вычислениями. Инженер, понимая, какие подпроцессы работают медленнее всего (например, умножение и деление вычислительно гораздо «дороже», чем сложение) может это проецировать на то, как он видит будущий алгоритм. Соответственно, такой алгоритм получится более оптимальным.В ряде случаев (например, для аппроксимации эллипса или круга) использование нейронных сетей — это как забивание гвоздей микроскопом. Нейросеть будет использовать миллионы параметров и большой объём ресурсов. Человек, обладающий инженерной интуицией, может для этого использовать всего несколько параметров и легко это решить.Побеждают гибридные алгоритмы!Сочетание вычислительных и нейросетевых алгоритмов — один из способов решения проблем, о которых я написал выше. В задачах обработки видео стараются отдать нейросетевым алгоритмам небольшую часть, когда они работают с данными, которые уже агрегируют какую-то визуальную информацию.Рассмотрим несколько примеров.Пример: увеличение количества кадровНиже пример алгоритма для увеличения количества кадров (frame rate conversion, FRC), он используется как дополнение для высокоскоростных сенсоров камеры, обеспечивающих съёмку до 960 кадров в секунду.Гибридный алгоритм увеличения количества кадров (frame rate conversion, FRC) обеспечивает на порядок больше кадров в секунду даже на смартфоне!В этом алгоритме нейросети отвечают только за маленький блок, детектирующий эффект окклюзии, когда из-за движения в кадре один из объектов заслоняет другой, и вычислительный алгоритм с высокой вероятностью запутается и сгенерирует промежуточные кадры с ошибками. Здесь нейронная сеть справляется гораздо лучше мозга инженера. Для неё проще определить признаки окклюзии. Поскольку в данном случае нейросеть оперирует не с пикселями, а с векторами движения, ей проще уложиться в требуемые временные рамки и выделенные вычислительные ресурсы. Анализируя сложность движения на видео, алгоритм предсказывает момент появления артефактов и включает меры для предотвращения этого.Благодаря современным алгоритмам шакалы не доберутся до этого щеночкаПример: как получить датасет с данными глубиныНиже пример end-to-end задачи (вычисляемой полностью на нейросетевых алгоритмах), чтобы рассказать о способе получения данных для датасета без затраты больших ресурсов. Хорошие датасеты — это нефть IT-индустрии. Если команда имеет хорошие представительные тренировочные датасеты или инструменты для их сбора, она может «торговать» ими не хуже, чем алгоритмическими решениями.Существует задача предсказания глубины по всего одному изображению: камера видит одну картинку и пытается понять, на каком расстоянии находятся объекты. У сети нет вспомогательной информации, и поэтому она по аналогии с человеческим восприятием учится понимать относительные пропорции «знакомых» объектов, и за счёт понимания их взаиморасположения строить карту глубины. Карта глубины может использоваться как для навигации роботов, так и для фотоэффектов, например имитации эффекта размытия объектов вне фокуса, который возникает для профессиональных камер с хорошей оптикой.Это не кадры из фильма ужасов, это фото с картами глубины (depth)Идея подхода вроде простая, как раз для нейронной сети, но возникает проблема, где искать тренировочные данные и в каком представлении? Самый простой вариант: используем оборудование для измерения глубины (например, с помощью ToF камеры или лидара) и совмещаем с карту глубины с фотографией от обычной камеры. Это трудоемко и затратно, требуется специальное оборудование, выравнивание фотографии и карты глубины и время для съёмки.Но можно использовать более простой и интересный способ, который уже себя оправдал: показывать нейронной сети коммерческие стереофильмы высокого качества. Этого будет достаточно, чтобы нейросеть за счёт наблюдения непохожестей в изображениях стереопары сформировала внутреннее представление, используемое далее для оценки глубины изображений. Такие фильмы доступны, и за редким исключением, когда резко меняется сцена или появляются субтитры, применимы без проблем.Знаете, я своего рода тоже нейросеть!Но даже несмотря на доступность данных, возникали сложности: в части датасетов были перепутаны левое и правое изображения, и нужно было искать ошибки в данных вручную. Таким образом, даже при использовании нейросетей, инженерный подход иногда остается необходимым для анализа и фильтрации входных данных.Пример: как получить датасет со смазанными изображениямиЕщё один пример того, как можно достаточно «дешево» получить обучающие данные. Да, всё равно приходится собирать датасет, но значительно с меньшими затратами времени и сил.Инженеры Samsung решали задачу компенсации смаза и для сбора данных использовали высокочастотную камеру, которая снимала со скоростью 400 кадров в секунду. Таким образом, получали 400 четких изображений сцены — эталон (ground truth). Затем имитировали смазанное изображение с помощью специального алгоритма, который использовал несколько последовательных кадров и объединял их, моделируя дрожание рук у пользователя и длительное открытие апертуры камеры. Это позволило создать пары из смазанных и четких изображений, что сильно упростило обучение и позволило быстро достичь первых результатов.Алгоритмы сделали изображения справа гораздо более чёткимиЗаключениеГлавное: при решении задачи вам следует иметь в виду разные способы получения обучающих данных, иметь широкий кругозор и понимать, что не всегда нейросетевые решения исключительно удобны и эффективны. Если ваше решение ориентировано на реальный мир, эти датасеты должны быть очень высокого качества.На данный момент, нейросетевые решения доминируют в задачах классификации, сегментации, например, распознавания лиц, отслеживания взгляда и других задачах. Но для задач улучшения качества изображений и видео более эффективно себя пока показывают гибридные алгоритмы, которые используют нейросетевые подходы только на той стадии, где они наиболее эффективны. Но, с ростом производительности и по мере развития специализированных нейропроцессоров, нейронные сети постепенно будут теснить алгоритмические подходы.Важно подходить к задаче гибко и искать оптимальное сочетание методов для ее решения, учитывая специфику и требования конкретной задачи.Илья Курилинканд. техн. наук, эксперт SamsungСтатья основана налекции, прочитанной в рамках финала конкурса IT Школы Samsung.А если вас заинтересовало, как освоить методы глубокого обучения и применять их в своей практике, то приглашаем вас пройти бесплатный онлайн-курс«Нейронные сети и компьютерное зрение»от Samsung! Курс включает 20 часов видеолекций, тесты и упражнения, а итоговое задание — самостоятельная задача на Kaggle.https://stepik.org/course/50352"
СберМаркет,,,Как возникают и развиваются стандарты систем связи: рассматриваем на примере 6G,2023-12-24T13:48:31.000Z,"Все слышали о поколениях мобильной связи, и сокращения 4G, 5G, а может быть даже и 6G уже у всех на слуху. Но чем определяется смена технологических поколений, кто решает, что новое поколение уже наступило? Как так вышло, что вроде бы 5G массово еще не используется, а уже начинают говорить про 6G? И кто создает стандарты, по которым потом живет мир?В этой статье я рассказываю о трендах шестого поколения мобильной связи (6G) исходя из моего опыта разработки подобного рода систем, а также участия в создании современных стандартов связи в комитете 3GPP — фактически единственной организации в мире, которая разрабатывает спецификации для систем связи различных поколений.План:Введение: текущие планы разработки технологии 6G и планы ее внедрения в коммерческие системыТребования для перспективных систем шестого поколения, модели использования и сравнение с технологиями предыдущих поколенийТехнические улучшения и инновации для 6GПроцесс стандартизации в комитете 3GPPК моменту, когда вы дочитаете эту статью, вы будете знать, зачем нужна эта красивая антенная решетка (на фото — прототип модуля от Samsung для 6G)План разработки 6GПрежде всего, отвечу на самый популярный вопрос: а существует ли сейчас 6G, и если нет, то когда появится? Вот предварительный план разработки технологии мобильной связи 6G комитетом 3GPP:2023 — работа по стандарту 6G официально еще не началась, 3GPP продолжает эволюцию систем связи пятого поколения, и в конце этого года планируется завершение очередного релиза этой системы2024 — предварительное обсуждение основных требований и ключевых показателей эффективности систем связи 6G2025 — изучение возможных технических решений, которые могут быть внедрены в спецификацию комитетом 3GPP2027 — фактическая нормативная работа по спецификации систем связи 6G2028 — первый релиз систем шестого поколения, то есть примерно 5 лет от текущего моментаПредварительная дорожная карта для 6GЕсли брать за точку отсчёта 2017 год (первый релиз системы связи 5G) и дату выхода первого релиза 6G (2028), то этот интервал примерно соответствует десятилетнему интервалу появления нового поколения системы связи. Сейчас мы находимся в самом начале пути разработки технологии связи шестого поколения. Несмотря на это, многие компании уже выдвигают свое видение функциональности систем и планы развития этой технологии на будущее.Кроме этого, на картинке отражен предварительный план работы организации ITU (МСЭ) по системам связи шестого поколения. Работа этой организации тоже очень важна для успешного внедрения системы по всему миру. ITU была создана в рамках ООН, одна из ее ключевых задач — разработка рекомендаций по глобальному выделению спектра для систем связи различных поколений, а также разработка требований к ним и методологии их проверки. Фактически, ITU принимает решение, может ли спецификация называться технологией определённого поколения — 4G, 5G или 6G.Требования к связи 6GДавайте поговорим про основные требования, которым должна будет удовлетворять технология мобильной связи шестого поколения. Поскольку эти требования еще не определены комитетом ITU, то постараемся их предсказать на примере требований, выработанных для систем мобильной связи предыдущих поколений.Например, мобильные системы связи третьего поколения (или 3G) изначально проектировались в рамках концепции IMT2000, разработанной ITU, и оптимизировались под приложения для передачи голоса с возможностью передачи данных со скоростями несколько мегабит в секунду.Артефакт из далеких времен - иллюстрация концепции IMT-2000 (для 3G), взятая напрямую с сайта ITU:https://www.itu.int/itunews/issue/2000/09/the_dawn.htmlТиповые устройства для 3G. Кто помнит такие?Далее, системы связи 4G уже были построены на основе концепции IMT-Advanced, к ним предъявлялись более высокие требования на скорость передачи данных в Интернете — порядка нескольких сотен мегабит в секунду. На этапе разработки технологии 4G практически все приложения, которые мы сейчас широко используем вместе с мобильным интернетом, еще не были разработаны. Тем не менее, разработка в рамках концепции IMT-Advanced позволила спроектировать очень успешную систему связи LTE-Advanced (которая является системой связи четвертого поколения), получившую широкое распространение по всему миру для передачи данных в сотовых системах связи.Это изображение концепции IMT-Advanced (для 4G) получило название The Van Diagram, из-за сходства с силуэтом автомобиля. Источник:https://www.itu.int/itunews/manager/display.asp?lang=en&year=2008&issue=10&ipage=39&ext=htmlТиповые устройства для 4G. Уже ближе к нам.Примерно через 10 лет в индустрии в рамках уже концепции IMT-2020 стартовала разработка нового стандарта мобильной связи пятого поколения, и в 2017 был выпущен первый релиз спецификации 5G New Radio.Концепция IMT-2020, для 5GВ концепции IMT-2020 помимо еще более высоких требований на скорость передачи данных до нескольких Гб /c (для широкополосного интернета или т.н. eMBB) появились новые требования для поддержки передачи данных между устройствами Интернета вещей. Связь с устройствами существенно отличается от привычной нам связи для приложений смартфонов:●     Для простых устройств Интернета вещей (или т.н. Massive IoT) появляется требование по количеству соединений. Согласно IMT-2020, это требование составляет 1 миллион устройств на 1 кв.км●     Для промышленных устройств или для критически важных приложений в концепции IMT-2020 появляется дополнительное требование на поддержку ультранадёжной связи с низкой задержкой (т.н. URLLC). Суть этих требований в необходимости обеспечения очень высокой надежности передачи данных на физическом уровне с вероятностью ошибки порядка 0.001% и задержек порядка 1 мс.Разнообразие устройств для 5GЕсли мы постараемся экстраполировать эти требования для концепции IMT-2030, разрабатываемой для систем связи шестого поколения в ITU, то стоит ожидать, что они будут более экстремальными или более высокими. Например, требования на пиковую скорость передачи для широкополосного интернета, или требования на количество соединений для IoT-устройств. Возможно, появятся дополнительные требования или их комбинации для поддержки мобильного соединения для новых видов устройств и приложений. Резюмируя, можно качественно сформулировать новые требования для 6G как возможность поддержки передачи данных для всех устройств и в различных условиях.Примеры использования 6GПоговорим о моделях использования систем связи 6G:Новые системы будут использоваться для существующих приложений, например, для передачи широкополосного Интернета, но будут работать эффективнее, и, как следствие, удовлетворять новым более высоким требованиямПередачу голографических изображений можно рассматривать как эволюцию передачи обычного видео. Из-за бОльшего объема информации, необходимого для представления таких изображений, требования на скорость передачи данных существенно возрастутПриложения тактильного Интернета можно рассматривать как эволюцию передачи видео и голосаПоддержка сенсоров, которые работают как устройства Интернета вещей, но с более высокими требованиями на энергоэффективность в сравнении с IoT-устройствами, которые поддерживаются системами связи 5GНовые модели применения — использование цифровых двойников (digital twins) сетей, которые за счет сбора данных и использования нейронных сетей позволят операторам связи оптимизировать работу сети в реальном времениЕсли посмотреть на всю картину и области применения, то системы связи 6G должны поддержать переход различных сфер индустрии на цифровой формат. Как результат, спектр требований от компаний и их количественные значения существенно возрастут:Для передачи данных стоит ожидать требования на пиковые значенияскоростейдо нескольких Гб/сДля практических систем будут важны не только пиковые, но иреально достижимыезначения скорости передачиПоддержказоны покрытияв несколько сот километров для спутниковой связиЭнергоэффективностьважна с точки зрения пользовательского опыта использования широкополосного интернета на смартфонахДля индустриального интернета и критически важных приложений требования нанадежность и задержкутакже возрастутИ как я говорил ранее, скорее всего появятся новые требования или их комбинация для новых приложенийКлючевые особенности 6GТак что же такое 6G c технической точки зрения? Я нашел интересную картинку, которую опубликовали сотрудники корейского оператора SK Telecom.На ней показаны ключевые слова, которые компании упоминают в своих документах с обзором технологии 6G. Как видно, рассматривается очень богатый спектр возможных технических решений:Поддержка нейронных сетейНовые диапазоны частотСпутниковая связьНовые типы устройств и передачи информацииПоддержка новой функциональности с точной локализацией предметов и их зондированиеСпектрыПервое и ключевое направление развития систем связи — использование различных спектров.Вот выражение для пропускной способности канала, полученное Шенноном.Пропускная способность — это верхняя граница достижимой скорости передачи. Как видно из этого выражения, ее увеличение возможно путем повышения отношения сигнал-шум, задаваемое переменной SNR (signal-to-noise ratio). Но на практике это не всегда возможно для всех пользователей из-за ограничений на уровень передаваемой мощности, а также условий распространения сигнала. Другой, более эффективный подход — увеличение полосы передачи сигнала, задаваемое параметром B (bandwidth), который входит в выражение линейным коэффициентом.Таким образом, эволюция возможностей технологий (как минимум с точки зрения скорости передачи) и появление новых поколений связи в первую очередь связаны с возможностью эффективного использования нового спектра, недоступного в предыдущем поколении. В качестве примера приведены приблизительные диапазоны частот, которые использовались различными поколениями систем связи.Системы связи 4G в основном проектировались для см-диапазона частот от 1 до 2.5 ГГц. В этом диапазоне доступны каналы с полосой передачи порядка нескольких десятков МГц, но этот спектр, как правило, фрагментирован и ограничен.Для обеспечения более высоких скоростей передачи в системах 5G была внедрена поддержка более высоких диапазонов частот. Так, в см-диапазоне верхняя граница сместилась до 3.5 ГГц, где стали доступны каналы с шириной полосы порядка 100 МГц, а также была добавлена поддержка мм-диапазона от 30 до 60ГГц с полосой частот порядка 1ГГц.Чтобы удовлетворить новым требованиям концепции IMT-2030, в системах связи 6G планируется поддержка дополнительного спектра частот 7-13 ГГЦ (upper mid band), терагерцового или суб-терагерцового диапазона (sub-Thz).Какие же задачи будут стоять?В первую очередь, 6G будет развернута на существующих частотах, которые используются системами связи предыдущих поколений. Для оператора систем связи очень важно эффективно поддержать работу сразу двух или более технологий в одном диапазоне (например, 6G и 5G). Это нетривиальная задача: внедрение новых устройств, поддерживающих новый стандарт связи, осуществляется крайне неравномерно. В рамках одного города и даже в одном месте пропорция использования устройств с различной поддержкой технологии может изменяться в течение суток.В рамках 5G были разработаны интересные технические решения плавного перехода от одной технологии к другой без существенного снижения эффективности работы каждой из них — так называемое динамическое разделение спектра (dynamic spectrum sharing). В контексте 6G такая задача плавного перехода будет также очень актуальна.Для миллиметрового диапазона основной задачей будет являться повышение эффективности работы системы связи. К сожалению, в системах связи 5G опыт применения этого диапазона был не совсем однозначным.Наверное, наиболее интересным диапазоном с точки зрения его использования в системах сотовой связи является частота несущей от 7 до 13 ГГц. Этот диапазон теоретически можно использовать с помощью уже существующей инфраструктуры сети без добавления или с минимальным добавлением новых локаций базовых станций (об этом будет далее). В нем есть дополнительные участки непрерывного спектра с шириной полосы в несколько сотен мегагерц. Эти участки можно использовать для существенного увеличения пропускной способности системы связи. Причем в этом диапазоне в зависимости от региона могут быть развернуты и другие радиотехнические системы (включая системы передачи данных) — необходимо решать задачу совместимости.Суб-терагерцовый диапазон тоже очень интересен с точки зрения доступности участков непрерывного спектра шириной в несколько ГГц. Однако из-за высоких потерь в линии связи применение этого диапазона будет возможно только для локальных моделей использования в условиях распространения с прямой видимостью.Проблема уменьшения зоны покрытияПоговорим немного о сложностях использования новых диапазонов высоких частот. Вот формула Фрисса, задающая мощность принимаемого сигнала в свободном пространстве в зависимости от параметров системы:Здесь стоит обратить внимание, что уровень принимаемого сигнала падает с увеличением расстояния между передатчиком и приемникомd, и самое важное, уровень сигнала также падает с уменьшением длины волныλ, то есть ростом несущей частоты.Получается, что использование диапазона с более короткой длиной волны делает спектр более доступным. В тоже время, малая длина волны существенно ограничивает зону покрытия сети даже в открытом пространстве. С другой стороны, с точки зрения операторов очень важно использование существующей инфраструктуры (вышек и зданий), на которых уже развернута сеть.Итак, поставлена задача: использование нового высокочастотного спектра с той же зоной покрытия, что и существующие системы связи. Какие же существуют способы ее решения?Антенная решетка (XMIMO — Extreme MIMO)Рассмотрим пример использования нескольких антенных элементов для передачи сигнала (антенную решетку). В радиофизике хорошо известно, что использование нескольких антенн приводит к формированию диаграммы направленности (beamforming). При этом происходит концентрация излучаемой мощности в определенном направлении и рост коэффициента усиления (antenna gain) антенной решетки. С ростом количества антенн в антенной решетке ширина диаграммы направленности уменьшается, и растет коэффициент усиления антенны.Технология использования нескольких антенн на передатчике и приемнике в литературе получила название MIMO и позволяет компенсировать потери, связанные с увеличением несущей частоты в новых диапазонах, про которые мы говорили ранее. MIMO позволяет гибко управлять диаграммой направленности в зависимости от требуемого направления на пользователя и создавать несколько пространственных каналов на одного или нескольких пользователей.Данная технология была успешно реализована в системах связи пятого поколения для диапазона частот 3.5ГГц и получила название Massive MIMO. Она позволяет обеспечить надежную передачу данных в данном диапазоне частот.Пример антенной решётки, содержащей 256 антенных элементов с двумя поляризациями:Для диапазона частот 13ГГц для 6G предполагается использовать антенну примерно такого же физического размера, но из-за более короткой длины в том же корпусе можно разместить гораздо большее  количество антенных элементов (например, около 3000 элементов, как показано на рисунке). Как следствие, коэффициент усиления антенной решетки увеличивается, тем самым компенсируя потери при распространении сигнала в данном диапазоне.Такая технология получила название Extreme MIMO (XMIMO) из-за экстремально большого количества используемых антенн, и считается одной из самых перспективных для систем связи шестого поколения для поддержки диапазонов частот 7-13ГГц.XMIMO дает и другие преимущества:За счет использования узкого луча можно передавать (или принимать) сигналы к бОльшему числу устройств (например, до 64) в том же частотном канале, при этом не создавать большие помехи между этими сигналамиМожно формировать бОльшее количество пространственных лучей одному устройству (до 16) и передавать параллельно несколько сигналов, тем самым повышая скорость передачи на устройство в несколько разУзкий луч позволяет снизить помехи на устройства, обслуживаемые другими базовыми станциямиАнтенная решетка становится похожа на настоящий радар, что позволяет решать ряд дополнительных интересных задач (например, задачу зондирования), про которые мы поговорим чуть позжеЗона покрытияЕще одна проблема реальных сетей — зона покрытия. Вот стандартная топология сети, в которой зона покрытия определяется расположением базовых станций.Развертывание базовых станций с точки зрения оператора определяется наличием относительно большого числа абонентов. В городах зона покрытия достаточная хорошая, однако за городом развертывание базовых станций становится проблематичным из-за требуемой инфраструктуры и капитальных затрат оператора. Как результат, типичный сценарий (особенно для таких больших стран, как Россия) — отсутствие мобильного Интернета за городом в малонаселенных областях.Возможное решение проблемы — спутниковая система связи, где спутниковый Интернет обеспечивает базовую связь с большой зоной покрытия, а там, где необходима высокоскоростная передача, дополнительно используют наземные базовые станции. Фактически, наземные и спутниковые системы дополняют друг друга. Технология спутниковой связи уже успешно применяется в некоторых странах. Пример — StarLink от компании SpaceX.Но есть ряд направлений, где использование 6G для спутниковой связи будет иметь преимущество относительно существующих решений:Использование одного оборудования и одного вида устройств будет очень выгодно абонентам и операторам с точки зрения стоимости услуг мобильной связи. Напомню, что StarLink от компании SpaceX требует определенного оборудованияБесшовное покрытие сети будет позволять переключаться от спутниковой связи к наземной в зависимости от условий распространения сигналаБолее эффективное использование спектра между наземными и спутниковыми системамиЕсть, конечно, и ряд технических проблем, связанных с поддержкой спутниковой связи:Для геостационарных спутников, которые располагаются на высокой орбите относительно поверхности Земли, уровень принимаемого сигнала будет очень маленьким и необходимы технические решения для надежной связи. На физическом уровне будут очень высокие задержки из-за времени распространения сигнала от устройств до спутникаДля низкоорбитальных спутников основной проблемой является компенсация Допплеровского смещения, связанного с высокой скоростью перемещения спутника относительно поверхности ЗемлиТем не менее, все эти проблемы решаются, и применение спутниковой связи становится очень привлекательным сценарием использования 6G.Эволюция систем позиционированияТеперь поговорим про нестандартное применение мобильных систем связи, не связанное с передачей данных. А именно, про эволюцию систем позиционирования, то есть определения местоположения пользователя.Всем хорошо известны спутниковые системы позиционирования, такие как GPS и GLONASS. Данные системы показали свою эффективность, когда устройство находится вне помещения. Мобильные системы сотовой связи тоже используются для определения местоположения, особенно когда устройство находится внутри помещения. Местоположение пользователя в этом случае определяется с помощью сигналов, передаваемых и принимаемых базовой станцией. При этом возможно определение местоположения пользователя в трехмерном пространстве, то есть определения этажа в здании, на котором он находится.Такие методы позиционирования уже внедрены в системы связи различных поколений, и мы ими уже активно пользуемся. Как эволюцию развития технологии позиционирования, в системах связи 6G предлагается ввести методы зондирования объектов (sensing). С помощью этой технологии можно узнавать другие параметры объектов, а не только их координаты. Например, классифицировать тип объекта, его свойства, движение и расстояние до него. По сути, инфраструктура сети может быть использована для решения дополнительных радарных задач.Переиспользование сетевой инфраструктуры для зондирования объектовПри этом объекты, для которых будет проводиться зондирование, могут быть и пассивными, то есть не иметь устройства приема и передачи. Это открывает дополнительные возможности для операторов с помощью существующей инфраструктуры решать новые задачи путем объединения возможности передачи информации, позиционирования и зондирования.Примеры использования техники зондированияВот несколько примеров, которые можно встретить в литературе. Первый пример — определение локальных погодных условий с помощью базовых станций. Всем известно, что распространение сигнала зависит от свойств среды. Таким образом, изменение параметров среды из-за погодных условий можно классифицировать по измерениям принимаемого сигнала.Далее, с помощью технологии sensing можно проводить локализацию объектов для беспилотных автомобилей. Например, определения типа препятствий, расстояния до препятствий и т.д. По сути, использование радиосигнала позволяет получить дополнительные измерения для управления автомобилем и в комбинации с видеоизображением заменить более дорогостоящие лидары.Далее, с помощью зондирования на производстве можно обнаруживать различные объекты, определять расстояние до них, использовать эту информацию для оптимизации и обеспечения безопасности.С помощью системы зондирования возможно удаленное измерение биологических параметров объектов, таких как частота сердцебиения и дыхания.Интересно использование технологии зондирования для распознавания жестов и выполнения определенных функций на устройстве без использования камеры. Такое приложение наиболее выгодно для небольших предметов, таких как наручные часы.С помощью технологии зондирования можно определять изменения в условиях распространения сигналов и детектировать события внутри помещения, такие как падение человека.Процедуру зондирования можно также использовать для оптимизации работы сети, где параметры передачи могут быть изменены в зависимости от наличия препятствия.ЭнергоэффективностьВ некоторых странах энергопотребление является одной из основных частей операционных затрат оператора, и для повышения привлекательности технологии 6G системы должны реализовывать алгоритмы экономии энергии. Например, частичное отключение радиоблоков базовых станций на определенных частотах в зависимости от загрузки сети, положения пользователей или предсказания их траекторий.Повышение энергоэффективности актуально и для мобильных устройств. Так, для устройств Интернета вещей в системах связи 6G будет интересна поддержка соединения для так называемых zero-energy или zero-power устройств. Это простейшие сенсоры, которые будут работать без замены батарейки и получать энергию от внешних источников: радиоизлучения телевизионных сигналов, механических вибраций, изменения температуры и тд. Здесь перед инженерами стоит много задач по проектированию системы со сверхнизким энергопотреблением. Многие функции, выполняемые устройствами в существующих системах связи, такие как мобильность, шифрование и обработка сигнала, требуют существенных энергозатрат.Для привычных нам смартфонов задача повышения энергоэффективности тоже важна. Для многих пользователей скорость передачи в существующих системах уже достаточна, и низкое энергопотребление устройства важнее для улучшения опыта использования мобильного Интернета. Для решения этой задачи, например, можно рассматривать динамическое отключение некоторых блоков приемного устройства в зависимости от паттерна принимаемого трафика.Полный дуплексЕще одно интересное направление развития технологии 6G — поддержка передачи данных с полным дуплексом.Для начала рассмотрим разделение передачи в нисходящем (Downlink, DL) и восходящем (Uplink, UL) канале с помощью частотного разделения (FDD), которое применяется в существующих системах. В таких системах передача сигнала в DL и UL происходит одновременно, но с частотным разнесением.Частотное разделение — Frequency Division Duplex (FDD). Восходящий и нисходящий канал используют разные частоты.Другой пример, более привычный для нас — временное разделение, когда нисходящая и восходящая линии связи работают в одном частотном канале, но разделяют время на передачу сигнала в различных направлениях. Как результат, скорость передачи в таких системах сокращается в зависимости от выделяемого времени на соответствующую передачу.Временное разделение — Time Division Duplex (TDD). Восходящий и нисходящий канал используют одну и ту же частоту в разное время.Для систем связи 6G рассматривается третий, дополнительный и более эффективный вариант — передачи с полным дуплексом, когда передача по восходящей и нисходящей линии связи осуществляется в одном диапазоне и без разделения во времени. Например, базовая станция может передавать сигнал одному пользователю в нисходящей линии связи и одновременно принимать сигнал от другого пользователя в восходящей линии связи.Если приводить аналогию для нашего общения, это эквивалентно возможности общаться с собеседником, одновременно говоря и слушая его.Полный дуплекс — Full Duplex (XDD). Восходящий и/или нисходящий канал используют одну и ту же частоту в одно и то же время.Здесь основная проблема в высоком уровне помех, которые будет создавать базовая станция себе при передаче сигнала.Существует несколько решений:пространственное разнесение приемных и передающих антенннаправленная передача сигнала с помощью алгоритмов beamformingиспользование алгоритмов активной компенсации помех, которые применяются в современных наушниках для подавления внешнего шумаКак устроена стандартизация в 3GPPТеперь мы плавно переходим к обзору стандартизации в 3GPP. И в первую очередь хотелось бы проиллюстрировать, как изменялись компании-участники со временем. Когда разрабатывалась технология LTE, основными участниками разработки были телекоммуникационные компании и операторы. Поскольку мобильная связь в основном использовалась для обычных приложений мобильного Интернета.Трансформация экосистемы 3GPPОднако картина существенно изменилась, когда комитет 3GPP начал разработку систем связи пятого поколения. Поскольку системы связи 5G могут применяться и для других приложений, круг компаний, принимающих участие в разработке стандарта существенно вырос. Появились компании из другого бизнеса: производители автомобилей, авиастроение, автоматизация индустрии, и т.д. Для систем 6G, скорее всего, мы также будем наблюдать этот тренд из-за более широкого круга задач, решаемых данной системой.Так что же такое 3GPP? Это глобальная организация, в которую входит множество компаний-участников. Основная задача — разработка спецификации всей системы связи от устройства до базовых компонент сети. На высоком уровне в 3GPP входят три группы по спецификациям, в каждой из них несколько рабочих групп. Наиболее популярна RAN1, которая отвечает за разработку физического уровня.Структура 3GPPРазработка групп ведется на основе технических документов (contribution), которые компании готовят для обсуждения. Все решения принимаются на основе консенсуса и принимаются практически без формального голосования на основе результатов дискуссии.Samsung — одна из компаний-лидеров, которая активно вносит вклад в развитие технологий. Несколько сотрудников компании занимают позиции председателей рабочих групп. Например, на момент публикации этой статьи председатель группы RAN1 — сотрудник компании Samsung.Почему же стандартизация так популярна?Возможность определять направления развития технологии согласно приоритетам компанииВозможность внедрять наиболее предпочтительные решения и тем самым защищать бизнес компанииВозможность повысить прибыльность бизнеса за счет лицензирования. Многие компании в 3GPP предлагают запатентованные решения, которые будут использоваться во всех устройствах, реализующих данный стандарт.Статья основана на материалелекции, прочитанной в финале Межвузовского конкурса выпускных проектов Samsung Innovation Campus.Об автореДавыдов АлексейКандидат физ.-мат. наук. Окончил радиофизический факультет Нижегородского государственного университета им. Лобачевского по специальности «Информационные системы». С 2003 года занимался разработкой и анализом широкополосных систем связи: WiFi, WiMAX, LTE и 5G NR. В последние 10 лет Алексей был делегатом в комитете стандартизации 3GPP RAN1 и принимал непосредственное участие в создании 5G NR. С 2017 по 2022 год был редактором спецификации 3GPP физического уровня стандарта мобильной связи 5G. В настоящее время Алексей является экспертом компании Samsung. Алексей — соавтор более 20 научных работ и 300 патентов."
СберМаркет,,,История разработки приложения для складных смартфонов Samsung. Часть 2 — гайдлайны,2023-12-21T11:05:44.000Z,"Эта статья - продолжение истории про фрилансера Мишу и его знакомство со складными устройствами от авторов - выпускников «IT Школы Samsung». В предыдущей части главный геройосваивалинструментарий Remote Test Lab.В далеком 2019 году Samsung выпустила Galaxy Fold - инновационный складной смартфон корейской компании. Прошло уже почти 5 лет, а Миша до сих пор ни разу не сталкивался с адаптацией приложений под подобный тип устройств. В первую же неделю выполнениязнаменитого фриланс заказау него возникли проблемы:Приложение некорректно работало при складывании/раскладывании смартфонаНаш герой не знал, что конкретно подразумевается под “адаптацией”Если первая проблема решается довольно просто, то вот вторая выглядит неординарно. В связи с этим, Миша решил написать заказчику, на что получил ответ:ЗаказчикПонял Миша, что сначала стоит начать сисправления очевидных багов.Исправление баговПересоздание ActivityСуть бага: введенные пользователем данные сбрасывались при складывании / раскладывании смартфона - типичная проблема, с которой сталкивался почти каждый начинающий Android-разработчик. Если человек при написании кода из SMS, менял конфигурацию устройства, то приложение Activity возвращало свое состояние до начального.Видимо, приложение делалось в спешке, поэтому для быстрого решения этой проблемы были заблокированы все возможные пути пересоздания Activity. Однако нельзя чисто физически заблокировать смену конфигурации складного смартфона по объективным причинам. Поэтому приложение и ломается на Fold’е.РешениеБаг Миша исправил с помощью SavedInstanceState.@Override
protected void onSaveInstanceState(@NonNull Bundle outState) {
   outState.putString(""currentState"", currentState.toString());
   outState.putParcelable(""user"", user);
   super.onSaveInstanceState(outState);
}currentState - enum со всеми возможными состояниями активитиuser - parcelable с данными о пользователеВозможное решениеВторой способ сохранить данные перед пересозданием Activity — сложить их во ViewModel. Можно было бы класть все состояния в нее. Но есть одно «но»: изначально в проекте не подразумевалась ViewModel для описываемого Activity, а создавать ее только ради сохранения двух переменных — жесткий boilerplate. Гораздо проще все положить в Bundle в методе onSaveInstanceState.Проблемы разметкиСуть бага: интерфейс приложения плохо адаптировался под экран необычного размера.Этот баг, как и предыдущий, является еще более распространенным. Поэтому во всех Layout’ах был выполнен переезд на Constraints.Ну вот, баги исправлены, а теперь стоит озаботиться непосредственно адаптацией под изменяющийся экран Fold-a.Для упрощения этого процесса Samsung опубликовалгайдлайны. Основные из них мы сейчас рассмотрим.Погружение в гайдлайныСамое главное - отзывчивая версткаУ устройств серии Fold два экрана: внешний и внутренний. Пользователи часто складывают и открывают свои устройства, поэтому нужно, чтобы интерфейс подстраивался под оба экрана. Правильную реализацию такого поведения можно будет переиспользовать на устройствах с большими экранами, например, на планшетах.Все окей, если приложение корректно:Отображается на экранах с разным соотношением сторон и размерностьюВедет себя в горизонтальной ориентацииЗаполняет весь экран своим контентомТакже можно почитать:Responsive Layouts - Android DevelopersResponsive UI - Material DesignНе тратьте место впустуюПользователи часто недовольны, когда приложение просто растягивается на большом экране. Они используют устройства с большими экранами для просмотра большого количества контента одновременно, и точно не для того, чтобы видеть большие вьюхи, растянутые между констреинтами, как в примере ниже.Неудачный вариант построения интерфейсаИнтерфейс адаптирован под большой размер экранаКак решать эту проблему?Показывайте больше информации одновременноБольшой экран можно разбить пополам, отделив, например, одну из частей для показа предыдущего узла графа навигации. Такой подход позволяет получить доступ к содержимому без полноэкранного перехода.Также можно почитать:Sliding Pane Layout - Android DevelopersActivity Embedding - Android DevelopersДля некоторых приложений будет эффективнее использовать не двух-, а трехкомпонентный макет. Попробуйте представить ваше приложение в обоих вариантах и выберите лучший.Можно двигаться дальше, если приложение:Отображает упрощенный контент на свернутом экранеИспользует двух/трехкомпонентный макет для максимально эффективного использования свободного места на экранеРаскрывает предыдущие экраны навигации на развёрнутом экранеПозволяйте делать простые вещи, не уходя с экранаДля простой формы, отображающей небольшое количество контента, стоит использовать всплывающее окно малого размера. Показ поверх предыдущего экрана помогает пользователям выполнить свою задачу и не забыть, над чем они работали.Используйте плавающие окна для использования нескольких функций одновременноЕсли ваше приложение имеет функциональность, которую было бы полезно держать на виду, но которая не требует частого взаимодействия, попробуйте ее вынести в плавающий виджет. В некоторых случаях будет полезно реализовать его отображение поверх других приложений.Уменьшите расстояние перемещения пальцевНа больших экранах стоит отслеживать места взаимодействия пользователя с интерфейсом. Знание последней точки нажатия позволит открыть диалоговое окно рядом с ней, экономя пользовательское время.Примеры классно спроектированного интерфейсаМессенджерыВ приложениях подобного типа можно использовать преимущества двухпанельного режима, настраиваемой разделительной панели и всплывающего окна.Приложения - камерыЭти приложения в полной мере используют преимущества больших экранов при съемке.Приложения для просмотра контентаВ таких приложениях можно реализовать много фич, например:Адаптивный макет, разработанный для разных форм-факторовСплит-бар настраивается в многокомпонентном представленииИспользуются всплывающие окнаПоддерживается режим Flex для просмотра видеоИ это еще не всеНа самом деле, мы разобрали лишь основную часть правил и особенностей адаптации мобильных приложений к складным экранам. Если после прочтения этого списка у вас осталось желание копать глубже, то ниже приведен список дополнительных статей, которые отлично дополняют уже изученный материал.App continuity и мультизадачностьЭксклюзив складываемых устройств - режим FlexОб авторахИгорь ЕфимовВсем привет! Меня зовут Игорь, я ученик Томского Государственного университета. В 2021 году я выпустился из «IT Школы Samsung», получив Гран-при в ежегодном Конкурсе по разработке мобильных приложений «IT Школа выбирает сильнейших!» в рамках проекта Samsung Innovation Campus, и уже второй год я работаю в ней преподавателем. Параллельно больше года на фрилансе я разрабатываю нативные приложения под обе мобильные платформы: Android и iOS.Антон ВоробьевА меня зовут Антон, из IT-школы я выпустился ровно через год после Игоря, в 2022 году. Так же, как и Игорь, победил во всероссийском конкурсе ‘'IT-школа выбирает сильнейших’',взяв Гран-при. Сейчас я учусь в Высшей Школе Экономики и активно развиваюсь в мобильной разработке, чтобы устроиться на свою первую в жизни работу."
СберМаркет,,,Ещё раз про алгоритм сжатия Хаффмана,2023-11-02T14:47:27.000Z,"К написанию этой заметки меня сподвигло почти полное отсутствие информации на русском языке относительно эффективной реализации алгоритма оптимального префиксного кодирования алфавита с минимальной избыточностью, известного по имени своего создателя как алгоритм Хаффмана. Этот алгоритм в том или ином виде используется во многих стандартах и программах сжатия разнообразных данных.Канонический алгоритм ХаффманаХорошее описание алгоритма Хаффмана можно найти в книгах [1,2]. Поэтому я почти дословно процитирую информацию из раздела, посвящённого описанию алгоритма Хаффмана в книге [1].Алгоритм относится к группе “жадных” (greedy) алгоритмов и использует только частоту появления одинаковых символов во входном блоке данных. Ставит в соответствие символам входного потока, которые встречаются чаще, цепочку битов меньшей длины. И напротив, встречающимся редко - цепочку большей длины. Для сбора статистики требует двух проходов по входному блоку (также существуют однопроходные адаптивные варианты алгоритма).Характеристики алгоритма Хаффмана [1]:Степени сжатия: 8, 1.5, 1 (лучшая, средняя, худшая степени)Симметричность по времени: 2:1 (за счёт того, что требует двух проходов по массиву сжимаемых данных)Характерные особенности: один из немногих алгоритмов, который не увеличивает размера исходных данных в худшем случае (если не считать необходимости хранить таблицу перекодировки вместе с файлом).Основные этапы алгоритма сжатия с помощью кодов ХаффманаСбор статистической информации для последующего построения таблиц кодов переменной длиныПостроение кодов переменной длины на основании собранной статистической информацииКодирование (сжатие) данных с использованием построенных кодовОписанный выше алгоритм сжатия требует хранения и передачи вместе c кодированными данными дополнительной информации, которая позволяет однозначно восстановить таблицу соответствия кодируемых символов и кодирующих битовых цепочек.Следует отметить, что в некоторых случаях можно использовать постоянную таблицу (или набор таблиц), которые заранее известны как при кодировании, так и при декодировании. Или же строить таблицу адаптивно в процессе сжатия и восстановления. В этих случаях хранение и передача дополнительной информации не требуется, а также отпадает необходимость в предварительном сборе статистической информации (этап 1).В дальнейшем мы будем рассматривать только двухпроходную схему с явной передачей дополнительной информации о таблице соответствия кодируемых символов и кодирующих битовых цепочек.Первый проход по данным: сбор статистической информацииБудем считать, что входные символы - это байты. Тогда сбор статистической информации будет заключаться в подсчёте числа появлений одинаковых байтов во входном блоке данных.Построение кодов переменной длины на основании собранной статистической информацииАлгоритм Хаффмана основывается на создании двоичного дерева [3].Изначально все узлы считаются листьями (конечными узлами), которые представляют символ (в нашем случае байт) и число его появлений. Для построения двоичного дерева используется очередь с приоритетами, где узлу с наименьшей частотой присваивается наивысший приоритет.Алгоритм будет включать в себя следующие шаги:Шаг 1.Помещаем все листья с ненулевым числом появлений в очередь с приоритетами (упорядочиваем все листья в порядке убывания числа появления)Шаг 2.Пока в очереди больше одного узла, выполняем следующие действия:Шаг 2a.Удаляем из очереди два узла с наивысшим приоритетом (самыми низкими числами появлений)Шаг 2b.Создаём новый узел, для которого выбранные на шаге 2a узлы являются наследниками. При этом число появлений нового узла (его вес) полагается равным сумме появлений выбранных на шаге 2a узловШаг 2c.Добавляем узел, созданный на шаге 2b, в очередь с приоритетами.Единственный узел, который останется в очереди с приоритетами в конце работы алгоритма, будет корневым для построенного двоичного дерева.Чтобы восстановить кодирующие слова, нам необходимо пройти по рёбрам от корневого узла получившегося дерева до каждого листа. При этом каждому ребру присваивается бит “1”, если ребро находится слева, и “0” - если справа (или наоборот).Пример работы описанного выше алгоритма представлен на рисунке 1.Пример построения кодов ХаффманаРисунок 1. Пример работы алгоритма ХаффманаТаблица 1.Построенные коды Хаффмана для примера, приведённого на рисунке 1.Входной символЧисло появленийДлина битового кодаБитовый код""A""2010""B""17210""C""641111""D""341100""E""2511100""F""2511011""G""2511010""H""16111010""I""171110111""J""171110110Приведённое выше описание алгоритма создаёт впечатление, что реализация алгоритма построения кодов должна быть основана на указателях и требовать дополнительной памяти для хранения адресов внутренних узлов двоичного дерева. Пример такой реализации можно найти, например, в статье [3]. Для целей обучения такая реализация хороша, но и только. И становится очень печально, когда на полном серьёзе такую реализацию пытаются использовать в реальном ПО.Построение кодов с минимальной избыточностью, эффективно использующее оперативную памятьОсновные идеи алгоритмаПрежде чем перейти к описанию такого эффективного алгоритма построения кодов с минимальной избыточностью, нам придётся немного погрузится в теорию кодирования [4].До этого момента мы подразумевали, что результатом построения кодов является назначение каждому входному символу битового кода. Соответствие битовых кодов каждому входному символу задаёт схему кодирования.Чтобы кодирование было взаимно-однозначным, схема кодирования должна обладать свойством префикса – т.е. ни один используемый для кодирования определённого символа битовый код не должен быть префиксом битового кода, предназначенного для кодирования любого другого входного символа [1].Пусть– длина битового кода, предназначенного для кодирования-го символа из алфавита входных символов. Тогда необходимым условием того, что схема кодирования обладает свойством префикса, является выполнение следующего неравенства:Это неравенство в литературе называется неравенством Макмиллана-Крафта (Kraft inequality) [4,5].В то же время, чтобы схема кодирования обладала свойством минимальной избыточности, необходимо построить такие битовые коды, чтобы математическое ожидание длины битовых кодовбыло минимальным (- вероятности появления символов входного алфавита).Необходимо ещё раз отметить, что схема кодирования, задающая множество битовых кодов с длинами, для которых выполняется неравенство Макмиллана-Крафта, не гарантирует свойство префикса. Так, для схемы кодирования двухсимвольного алфавита, состоящей из битовых кодов “0” и “00”, неравенство Макмиллана-Крафта будет выполняться:. Но битовые коды очевидно не будут обладать свойством префикса. Тем не менее, если у нас есть информация о длинах кодов, для которых выполняется неравенство Макмиллана-Крафта, мы всегда можем построить соответствующие этим длинам битовые коды, которые будут обладать свойством префикса [4,5].К алгоритму построения битовых кодов по их длинам мы вернёмся позднее, а сейчас зафиксируем тот факт, что задача построения кодов с минимальной избыточностью сводится к нахождению длин таких кодов для символов входного алфавита на основе собранной статистической информации.Следуя статье [6], опишем алгоритм построения кодов с минимальной избыточностью, не требующий для своей работы дополнительной памяти (“in-place”).Прежде всего рассмотрим основные идеи алгоритма. В его основе лежат следующие два наблюдения.Первое наблюдение: мы можем хранить листья двоичного дерева отдельно от внутренних узлов, которые формируются в процессе слияния (шаги 2a и 2b) описанного выше алгоритма Хаффмана. Таким образом у нас получается две очереди с приоритетами. Более того, поскольку внутренние узлы формируются в отсортированном порядке, а листья также сортируются на шаге 1, то очереди с приоритетами превращаются в обычные списки.При этом вес любого узла дерева необходим только до того момента пока узел не будет обработан. Для любого заданного слияния необходимо хранить по крайней меревесов. В конце слияний у нас сохранится лишь один вес.Второе наблюдение: нам нет необходимости хранить указатели на родительские узлы (индексы родительских узлов) в двух списках. Если глубина каждого внутреннего узла известна, то мы можем восстановить глубину для каждого из листьев (так как можно предположить, что длины кодов не будут возрастать). Например, для дерева, изображённого на рисунке 2, с глубинами внутренних узлов (обозначенных оранжевым цветом)[3, 3, 2, 1, 0]глубины листьев (обозначенных зелёным цветом) будут равны[4, 4, 4, 4, 2, 1].Рисунок 2. Пример дерева с глубинамиРисунок 2. Пример дерева с глубинамиВ начале процесса слияния ни в одном из списков нет указателей на родительские узлы (индексов родительских узлов). А в конце процесса слияния у нас будетуказателей на родительские узлы (индексов родительских узлов). Но все эти указатели (индексы) будут принадлежать внутренним узлам.Общее число весов (хранящихся для узлов, которые не были ещё обработаны) и указателей на родительские узлы (индексов родительских узлов) (хранящихся для внутренних узлов, которые уже были обработаны) никогда не превысит. Т.е. указатели на родительские узлы (индексы родительских узлов) и веса могут храниться во входном массиве.Таким образом, входной массиводновременно является рабочим для алгоритма. В начале работы алгоритма массивсодержит числа (частоты) появлений символов входного алфавита, состоящего изсимволов, во входном потоке данных. Затем на каждом из этапов содержимое массива изменяется: элементы в массивена разных этапах алгоритма используются для хранения числа (частоты появлений) символов, весов внутренних узлов, индексов родительских узлов, глубин внутренних узлов, и наконец, глубины листьев (длины кодов для каждого символа).Основные этапы алгоритмаПерейдём к непосредственному описанию алгоритма “inplace” построения кодов с минимальной избыточностью.Этот алгоритм включает в себя следующие три основных этапа:Этап №1. Формирование внутренних узлов.Входные данные: массив, содержащий числа (частоты) появлений символов входного алфавита изсимволов.Выходные данные: массив, содержащий следующие данные:- указатели на родительские узлы (индексы родительских узлов);- вес корневого узла;- не используется.1   s ← 0
    2   r ← 0
    3   for t ← 0 to n-2
    4       do ⇨ выбираем первый узел-потомок
    5          if (s > n-1) or (r < t and A[r] < A[s])
    6          then ⇨ выбираем внутренний узел
    7               A[t] ← A[r]
    8               A[r] ← t+1
    9               r ← r+1
    10         else ⇨ выбираем лист
    11              A[t] ← A[s]
    12              s ← s+1
    13         ⇨ выбираем второй узел-потомок
    14         if (s > n-1) or (r < t and A[r] < A[s])
    15         then ⇨ выбираем внутренний узел
    16              A[t] ← A[t]+A[r]
    17              A[r] ← t+1
    18              r ← r+1
    19         else ⇨ выбираем лист
    20              A[t] ← A[t]+A[s]
    21              s ← s+1Важные замечания к показанному выше псевдокоду: - Операции “or” и “and” вычисляются условно (так, как это, например, происходит в языке программирования C/C++).Суть первого этапа в следующем: на каждой итерации цикла сравнивают следующий внутренний узел с наименьшим весом (если он существует) со следующим листом с наименьшим весом (если он существует) и выбирают наименьший по весу из двух (строки 5 - 12). Найденный наименьший вес назначается вновь создаваемому узлу дерева. Затем кдобавляется следующее наименьшее значение (строки 14 - 21). Если в результате на предыдущих шагах был выбран хотя бы один внутренний узел, то весзаменяется индексом родителя выбранного внутреннего узла.Пример работы описанного первого этапа алгоритма для случая входных данных, приведённых на рисунке 1, представлен на рисунке 3.Пример формирования внутренних узловРисунок 3. Пример работы первого этапа алгоритмаЗдесь светло-коричневым цветом выделены ячейки массива, содержащие число появлений каждого символа входного алфавита; светло-зелёным - ячейки массива, содержащие веса внутренних узлов до того момента, как они будут объединены; светло-жёлтым - указатели на родительские узлы (индексы родительских узлов) после операции объединения.Этап №2. Преобразование индексов родительских узлов в значения глубин каждого узла.Входные данные:- указатели на родительские узлы (индексы родительских узлов);Выходные данные:- значение глубины для каждого из внутренних узлов.1   A[n-2] ← 0
    2   for t ← n-2 downto 0
    3      do A[t] ← A[A[t]-1]+1Продолжая наш пример, на этапе №2 мы получим следующий результат:Пример преобразования индексов родительских узловСветло-фиолетовым выделены значения глубины для внутренних узлов.Этап №3. Преобразование значений глубин внутренних узлов в значения глубин листьев (длин кодов).Входные данные:- значение глубины для каждого из внутренних узлов.Выходные данные:- значения глубины для каждого из листьев (т.е. длины кодов для каждого символа).1   a ← 1
    2   u ← 0
    3   d ← 0
    4   t ← n-2
    5   x ← n-1
    6   while a > 0
    7   do ⇨ определяем количество внутренних узлов с глубиной, равной d
    8      while t ≥ 0 and A[t] = d
    9         do u ← u+1
    10           t ← t-1
    11     ⇨ назначаем листьями узлы, которые не являются внутренними
    12     while a > u
    13        do A[x] ← d
    14           x ← x-1
    15           a ← a-1
    16     ⇨ переходим к следующему значению глубины
    17     a ← 2∙u
    18     d ← d+1
    19     u ← 0глубин внутренних узлов преобразуются вглубин листьев (длин кодов для каждого символа). Для этого производится сканирование массивасправа налево с использованием указателядля внутренних узлов и указателяна элемент, в котором будет сохранено значение длины кода очередного листа дерева.Иллюстрация работы этапа №3 для нашего примера приведена на рисунке ниже:Пример преобразования значений глубин узлов в длины кодовРисунок 4. Пример работы третьего этапа алгоритмаЗдесь голубым цветом выделены получающиеся значения длин кодов.Подводя итог, описанный выше алгоритм состоит из трёх этапов, на каждом из которых осуществляется проход по массиву (один раз в прямом направлении, и два - в обратном).Алгоритм выполняется за времяи используетдополнительной памяти в случае, если входной массивчисла (частоты) появлений символов входного алфавита предварительно отсортирован.Ещё раз подчеркну, что алгоритм не требует явного построения дерева с использованием указателей (дополнительного динамического выделения памяти под формируемые узлы дерева), что на практике приводит к увеличению времени, необходимого для построения кодов переменной длины.Построение кодов переменной длиныТеперь, когда у нас есть длины кодов для каждого символа, построим для них битовые коды.Итак,- длина битового кода, предназначенного для кодирования-го символа из алфавита входных символов. Обозначим черезчисло символов входного алфавита, для которых длина кода равна:Здесь- получившаяся после применения описанного выше алгоритма максимальная длина кода.Далее, код с длинойдля-го символа из множества символов, для которых длина кода одинакова и равна, может быть получен с помощью следующей формулы:гдеВходные данные: массив, содержащий длины кодов для каждого символа входного алфавита изсимволов.Выходные данные: массив, где целое числосодержит битовое представление кода в-ых младших битах.1    ⇨ подсчитываем число символов с одинаковыми длинами кодов
    2   for i ← 0 to n-1
    3       do m[i] ← 0
    4   for i ← 0 to n-1
    5       do m[A[i]] ← m[A[i]] + 1
    6    ⇨ вычисляем значения base для каждой длины кода
    7   s ← 0
    7   for k ← L downto 1
    8       do Base[k] ← s >> (L - k)
    9          s ← s + (m[k] << (L - k))
    10   ⇨ вычисляем коды для каждого символа входного алфавита
    11  p ← 0
    12  for i ← 0 to n-1
    13     do if p ≠ A[i]
    14        then j ← 0
    15             p ← A[i]
    16        B[i] ← j + Base[A[i]]
    17        j ← j + 1Построим коды переменной длины для нашего примера.Таблица 2.Пример построения кодов переменной длины.Входной символДлина битового кодаbasejБитовый код""H""600000000""I""61000001""J""51000001""E""5100010""F""5200011""G""5300100""D""5400101""C""4300011""B""21001""A""1101Обратите внимание на вид получившихся кодов. Коды одинаковой длины - это возрастающая последовательность целых чисел. Такие коды носят название канонических [5,7] и позволяют проводить быстрое кодирование и декодирование.Кроме того, зная только длины кодов, можно достаточно быстро построить канонические коды при декодировании. Это позволяет компактно представить дополнительную информацию, которую необходимо передать декодеру, чтобы однозначно восстановить таблицу соответствия кодируемых символов и используемых для этого кодов переменной длины (таблицу перекодировки).Например, дополнительная информация может иметь такую структуру:Пример структуры данных, необходимый для хранения дополнительных данныхВ англоязычной литературе дополнительная информация носит название “prelude”, и компактное представление такой дополнительной информации - это отдельная интересная задача, в которую я не буду сейчас углубляться. Для интересующихся оставлю ссылку на статью [8], где подробно разбирается этот вопрос.Внимательный читатель может заметить, что построенные коды переменной длины, представленные в таблице 2, и коды Хаффмана из таблицы 1 не совпадают ни по длинам кодов, ни по их битовым представлениям (см. таблицу 3).Таблица 3.Сравнение результатов построения кодов с минимальной избыточностью.Входной символКоды ХаффманаКанонические кодыДлина битового кодаБитовый кодДлина битового кодаБитовый код“H”61110106000000“I”711101116000001“J”71110110500001“E”511100500010“F”511011500011“G”511010500100“D”41100500101“C”4111140011“B”210201“A”1011Применение алгоритма Хаффмана всегда будет создавать код с минимальной избыточностью, но не каждый возможный код с минимальной избыточностью может быть получен с помощью применения алгоритма Хаффмана. Поэтому заголовок этой заметки, строго говоря, некорректен.Для приведённых в таблице 3 канонических кодов и кодов Хаффмана легко проверить, что неравенство Макмиллана-Крафта выполняется (), а средняя длина кодаодинакова и равна.Кодирование/декодирование данных с помощью кодов переменной длиныТеперь остановимся на вопросе кодирования (сжатия) данных с использованием построенных канонических кодов.Для этого вернёмся к нашему примеру. Пусть длины битовых кодов для каждого символа хранятся в массиве CodeLen[]. Запишем битовое представление каждого из кодов в массив CodeWord[]. Каждый элемент массива CodeWord[] - беззнаковое целое число, где биты кода выровнены по правому краю (Таблица 4).Таблица 4.Представление кодов переменной длины для проведения кодирования.Входной символCodeLen[]Битовый кодCodeWord[]“H”60000000“I”60000011“J”5000011“E”5000102“F”5000113“G”5001004“D”5001015“C”400113“B”2011“A”111Тогда процедура кодирования данных D длинойс помощью построенных кодов переменной длины будет иметь следующий вид:1   for i ← 0 to n-1
    2       do ⇨ берём очередной символ из массива входных данных D
    3          s ← D[i]
    4          l ← CodeLen[s]
    5          c ← CodeWord[s]
    6          PutBits(c, l)В приведённой выше процедуре кодирования используется функция, которая записывает в выходной поток данныхмладших битов целого беззнакового числа.Для проведения декодирования преобразуем таблицу кодов следующим образом. Во-первых, сформируем массив L_code[], где каждый элемент - беззнаковое-битное целое число, которое сформировано путём выравнивания битового кода по левому-битному краю. Здесь- максимальная длина кода.Для нашего примера массив L_code[] представлен в таблице 5.Таблица 5.Представление кодов переменной длины при декодировании.Входной символCodeLen[]Битовый кодL_code[]“H”60000000“I”60000011“J”5000012“E”5000104“F”5000116“G”5001008“D”50010110“C”4001112“B”20116“A”1132Далее, из массива L_code[] выберем первые по порядку следования элементы, соответствующие различным длинам кодов и сформируем массив FirstCode[] (Таблица 6). В случае, если коды с некоторыми длинами отсутствуют (в нашем примере, коды с длиной, равной 3), то для них берётся следующее значение.Таблица 6.Вспомогательный массив для проведения декодирования.Длина битового кодаFirstCode[]6052412316216132064Значение FirstCode[0] необходимо нам для корректной работы алгоритма поиска длины кода при декодировании очередного закодированного символа. К этому алгоритму мы вернёмся чуть позже, а здесь лишь приведём формулу для вычисления FirstCode[0]:FirstCode[0] = FirstCode[] + (1 << ()),где- минимальная длина кода, “<<” - операция битового сдвига влево.Затем, сформируем структуру данных DecTable[][], которая для нашего примера будет иметь следующий вид:Пример структуры данных, необходимый для декодированияЗдесь offset - порядковые индексы символов, имеющих одинаковую длину кода.Процедура декодирования будет иметь следующий вид:1   for i ← 0 to n-1
    2       do ⇨ получаем из потока закодированных данных L бит
    3          Buffer ← ShowBits(L)
    4          ⇨ определяем длину кода для очередного символа s
    5          for l ← L to 1
    6              do if FirstCode[l] ≤ Buffer < FirstCode[l-1] 
    7                     then goto line 8
    8          ⇨ на основании найденной длины кода декодируем символ s
    9          offset ← (Buffer - FirstCode[l]) >> (L - l)
    10          s ← DecTable[l][offset]
    11         ⇨ удаляем l битов из потока закодированных данных
    12         RemoveBits(l)Приведённую выше процедуру декодирования можно ускорить, если линейный поиск длины кода (строки 5 - 7) заменить алгоритмом бинарного поиска:1   for i ← 0 to n-1
    2       do ⇨ получаем из потока закодированных данных L бит
    3          Buffer ← ShowBits(L)
    4          ⇨ определяем длину кода для очередного символа s
    5          left ← 0, right ← L + 1
    6          while (right - left) > 1
    7          do
    8              middle = (right + left) / 2
    9              if FirstCode[middle] ≤ Buffer
    10             then
    11                 right ← middle
    12             else
    13                 left ← middle
    14          ⇨ на основании найденной длины кода декодируем символ s
    15          offset ← (Buffer - FirstCode[right]) >> (L - right)
    16          s ← DecTable[right][offset]
    17         ⇨ удаляем right битов из потока закодированных данных
    18         RemoveBits(right)Если мы вернёмся к описанию процедуры кодирования, то можно заметить, что суммарное число битов для закодированных данных может быть некратно 8. Это значит, что при записи закодированных данных в память или файл на диске нам необходимо дополнить закодированные данные таким числом битов, чтобы получить целое число байтов (например, добавить нулевые биты, если это необходимо).Однако эти дополнительные биты должны быть проигнорированы при декодировании. В описанной выше процедуре декодирования предполагается, что мы передаём исходный размер данных (длину незакодированных данных) вместе с закодированной информацией.Следует заметить, что это - не единственный способ, необходимый для правильного декодирования данных. Другим вариантом является включение в алфавит входных символов еще одного специального символа (обозначим его- “EndOfData”), частота появления которого полагается равной 1. Тогда для символабудет построен префиксный код. Мы помещаемв конец кодируемых данных, а при декодировании появление кода дляостановит процедуру декодирования. У такого способа остановки декодирования есть один недостаток, о котором мы поговорим позднее.Вопросы практической реализацииТеперь предлагаю перейти в практическую плоскость и рассмотреть проблемы реализации описанных до этого алгоритмов для реальных вычислительных систем.При описании процедуры построения битовых кодов по найденным длинам, мы предполагали, что целые числа, которые служат для хранения битовых представлений кодов, имеют достаточный для этого размер в битах (битовую разрядность).Так как мы изначально договорились, что входные символы — это байты, то входной алфавит будет состоять из 256 символов. Символы такого алфавита могут иметь вероятности, что при построении префиксных кодов приведёт к появлению двух кодов с длиной 255 битов каждый. На практике для таких распределений применяют другие методы кодирования, например арифметическое кодирование [1].Тем не менее, в худшем случае нам необходимо предусмотреть возможность хранения битовых кодов большой длины.Далее, разрядность целых чисел задаётся разрядностью вычислительной системы (для педантов - разрядностью машинного слова вычислительной системы). Для ноутбука, на котором я писал этот текст, разрядность равна 64 битам. И для работы с битовыми кодами, длина которых превышает 64 бита, необходима реализация специальных алгоритмов, которые будут осуществлять базовые операции для таких кодов, что в большинстве случаев приводит к катастрофическому замедлению работы алгоритмов кодирования.Чтобы решить эту проблему, при реализации алгоритма кодирования мы можем до некоторой степени пожертвовать степенью сжатия в обмен на обеспечение высокой скорости кодирования/декодирования. Рассмотрим два основных подхода к решению этой проблемы.Применение алгоритма ограничения максимальной длины кода.Входными данными для этого алгоритма является массив, содержащий длины кодов для каждого символа входного алфавита изсимволов.1    ⇨ подсчитываем число символов с одинаковыми длинами кодов
    2   for i ← 0 to n-1
    3       do m[i] ← 0
    4   for i ← 0 to n-1
    5       do m[A[i]] ← m[A[i]] + 1
    6    ⇨ пересчитаем количество кодов с длинами, меньшими требуемой L_restrict
    7   for i ← L downto L_restrict+1
    8       do while m[i] > 0
    9          do
    10            j ← i - 1
    11            do
    12               j ← j - 1
    13            while m[j] ≤ 0
    14            m[i] ← m[i] - 2
    15            m[i-1] ← m[i-1] + 1
    16            m[j+1] ← m[j+1] + 2
    17            m[j] ← m[j] - 1
    18    ⇨ переназначим длины кодов символам входного алфавита
    19   n ← 0
    20   for i ← L_restrict downto 1
    21   do
    22       k ← m[i]
    23       while k > 0
    24       do
    25          A[n] ← i
    26          n ← n + 1
    27          k ← k - 1Приведённый выше алгоритм ограничения максимальной длины кода состоит из 3 этапов [9]. На первом этапе (строки 1 - 5) производится подсчёт числа кодов для каждой из длин, где- максимальная длина кода. Результат такого подсчёта записывается в массив(точно также как и в начале процедуры формирования битовых кодов по их длинам).Второй этап (строки 6 - 17) заключается в пересчёте количества кодов с длинами, меньшими требуемой величины, так, чтобы длины кодов для всех символов входного алфавита были меньше или равны.Так как два символа входного алфавита (два листа двоичного дерева) формируют одну вершину-родителя, то на каждой итерации внутреннего цикла (строки 8 - 17) мы берём пару символов с длиной кода. Для этих двух символов вершина-родитель будет иметь длину кода. Затем мы ищем символ с длиной кода, меньшей чем, и заменяем его выбранной вершиной-родителем. При этом заменяемый символ помещается на место вершины-родителя. Звучит очень запутанно, поэтому предлагаю рассмотреть работу этого этапа для нашего примера, полагая.Пример работы алгоритма ограничения максимальной длины кодаАлгоритм оперирует количеством символов, для которых длина кода одинакова. Поэтому нам оказывается неважной привязка каждого листа к определённому символу входного алфавита. Такая перепривязка производится на третьем этапе (строки 18 - 27).Корректная работа приведённого алгоритма гарантируется при следующем условии:где- размер входного алфавита.По сути указанное выше ограничение - это минимально возможная высота двоичного дерева с количеством листов, равным количеству символов входного алфавита. Для нашего примера такое дерево будет иметь следующий вид:Двоичное дерево мимниальной длины для алфавита из 10 символовДля такого дерева применить описанный алгоритм невозможно, так как на определённом шаге будут отсутствовать листья, которые можно было бы заменить вершиной-родителем для двух листов, имеющих длину кода 4.Применение escape-кодирования.Так как редко встречающимся символам входного алфавита назначаются более длинные коды по сравнению с часто встречающимися символами, то одной из возможностей ограничить максимальную длину получающихся кодов является кодирование группы редко встречающихся символов парой.Здесь- префиксный код псевдосимвола, имеющего частоту появления, равную сумме частот появления символов из группы редко встречающихся символов,- код фиксированной длины, позволяющий однозначно декодировать символ из этой группы. Такая схема кодирования в литературе носит название escape-кодирования (кодирования с использованием escape-псевдосимвола).Хотелось бы заметить, что вариант escape-кодирования применяется в случае, если размер входного алфавита оказывается достаточно большим, и построение кодов переменной длины для каждого символа будет гарантированно приводить к появлению кодов с большими длинами.Например, в стандарте JPEG требуется кодировать целые числа из диапазона, и каждый символ кодируется парой, где- префиксный код, кодирующий информацию о длине цепочки нулевых символов, предшествующих очередному ненулевому кодируемому символу, а также о количестве битов, необходимых для однозначного декодирования этого ненулевого символа.имеет длину, равную количеству битов, которое закодировано в. Более подробное описание такой схемы кодирования можно найти в [10]. Английский язык там достаточно простой, а автор излагает суть алгоритмов, используемых в стандарте JPEG, простыми словами.Вернёмся обратно к нашей проблеме ограничения максимальной длины кода. Проведём замену символов с длинами кодов, большими требуемой величины, парой символов.- код переменной длины длиной,- код фиксированной длины. Так как мы договорились что входные символы - это байты, то размер кодаможет быть равен 8 битам.Рассмотрим такую процедуру ограничения максимальной длины кода для нашего примера. Пусть, как и в предыдущем случае,. Тогда таблица кодирования будет выглядеть следующим образом:Таблица 7.Схема кодирования с использованием escape-символа.Входной символCodeLen[]CodeWord[]FixedCodeWord[]“H”40011111“I”40011110“J”40011101“E”40011100“F”40011011“G”40011010“D”40011001“C”40011000“B”201-“A”11-Исходя из числа символов, которые будут кодироваться с помощью escape-символа, длина кодабудет равна 3 битам, что позволяет однозначно декодировать символы входного алфавита.Процедура кодирования данных D длинойбудет иметь следующий вид:1   for i ← 0 to n-1
    2       do ⇨ берём очередной символ из массива входных данных D
    3          s ← D[i]
    4          if s ∈ группе символов, кодируемых с помощью escape-символа
    5          then ⇨ помещаем в выходной битовый поток код, соответствующий escape-символу
    6               l ← CodeLen[s]
    7               c ← CodeWord[s]
    8               PutBits(c, l)
    9               ⇨ помещаем в выходной битовый поток код фиксированной длины
    10              l ← FixedCodeLen
    11              c ← FixedCodeWord[s]
    12              PutBits(c, l)
    13         else ⇨ кодируем символ напрямую с помощью кода переменной длины
    14              l ← CodeLen[s]
    15              c ← CodeWord[s]
    16              PutBits(c, l)Здесь FixedCodeLen - наперёд заданная длина кода фиксированной длины.Процедура декодирования очевидно вытекает из процедуры кодирования. Как только из входного кодированного потока появляется код, соответствующий escape-символу, то производится считывание FixedCodeLen битов, которые помогают однозначно декодировать символ входного алфавита.Приведённая схема кодирования снижает степень сжатия данных в обмен на ограничение максимальной длины кода с целью ускорения кодирования/декодирования. Чтобы поднять степень сжатия и при этом не потерять в скорости кодирования/декодирования, можно использовать не один, а несколько escape-символов, а также осуществить предобработку входных данных перед их кодированием. Но здесь я бы не хотел углубляться в эту тему, так как этo требует, на мой взгляд, отдельной заметки, а то и целой серии статей.В заключение, необходимо заметить, что ограничение максимальной длины кода может помочь сделать декодирование достаточно быстрым.Быстрое декодированиеНапомним, что- максимальная длина кода. Тогда при декодировании мы можем построить одномерную таблицу соответствия, которая может ускорить процедуру декодирования.Для нашего примера такая одномерная таблица будет выглядеть так:Таблица 8.Пример одномерной таблицы соответствия для быстрого декодирования.Битовый L-битный кодФактическая длина кодаВходной символ0000006“H”0000016“I”0000105“J”0000115“J”0001005“E”0001015“E”0001105“F”0001115“F”0010005“G”0010015“G”0010105“D”0010115“D”0011004“C”0011014“C”0011104“C”0011114“C”0100002“B”0100012“B”………0111102“B”0111112“B”1000001“A”………1111111“A”Битовый-битный код задаёт адрес в одномерной таблице соответствия, по которому в процессе декодирования определяется фактическая длина кода и символ.В этом случае процедура декодирования имеет следующий вид:1   for i ← 0 to n-1
    2       do ⇨ получаем из потока закодированных данных L бит
    3          Buffer ← ShowBits(L)
    4          ⇨ определяем длину кода и декодируем очередной символ
    5          s ← FastDecSymbolTable[Buffer]
    6          l ← FastDecLenTable[Buffer]
    7          ⇨ удаляем l битов из потока закодированных данных
    8          RemoveBits(l)Здесь- это длина незакодированных данных, FastDecSymbolTable и FastDecLenTable— одномерные массивы, содержащие декодированный символ и соответствующую ему длину кода.Таким образом, вычислительная сложность декодирования уменьшится с(для декодирования, использующего бинарный поиск) до.Размер массивов FastDecSymbolTable и FastDecLenTable равен. Еслидостаточно большое, то указанные массивы будут огромного размера, и в худшем случае не поместятся в оперативной памяти вычислительной системы. А даже если и поместятся, то фактическая скорость декодирования будет невелика из-за постоянных обращений к оперативной памяти. Поэтому приведённая процедура быстрого декодирования имеет смысл, если массивы FastDecSymbolTable и FastDecLenTable будут помещаться в кеш (“cache”) вычислительной системы.В заключение хотелось бы отметить, что за рамками этой заметки остались такие важные вопросы, как: применимость кодирования данных с использованием кодов переменной длины, моделирование вероятностей при кодировании данных, а также общие вопросы теории энтропийного кодирования. Я сознательно старался не касаться этих тем, а сосредоточиться на алгоритме построения кодов переменной длины и его практической реализации.Хочу поблагодарить Ивана Соломатина за помощь и ценные советы в процессе подготовки этой публикации.Алексей Фартуковруководитель лаборатории SamsungЛитератураД. Ватолин, А. Ратушняк, М. Смирнов, В. Юкин. Методы сжатия данных. Устройство архиваторов, сжатие изображений и видео. Диалог-МИФИ - 2003.Томас Х. Кормен, Чарльз И. Лейзерсон, Рональд Л. Ривест, Клиффорд Штайн. Алгоритмы: построение и анализ, 3-е издание. Вильямс - 2013.Алгоритм сжатия Хаффмана. Перевод MaxRokatansky. Блог компании OTUS. https://habr.com/ru/company/otus/blog/497566/ - 2020.Яблонский С. В. Введение в дискретную математику. М.: Наука - 1986.A. Moffat, T.C. Bell and I. H. Witten. Lossless Compression for Text and Images. International Journal of High Speed Electronics and Systems. Vol. 08, No. 01, pp. 179-231 (https://doi.org/10.1142/S0129156497000068) - 1997.A. Moffat and J. Katajainen. In-Place Calculation of Minimum-Redundancy Codes. WADS ’95: Proceedings of the 4th International Workshop on Algorithms and Data Structures, pp. 393–402 - 1995.A. Moffat. Huffman Coding. ACM Computing Surveys. Vol. 52 Issue 4, pp. 1–35 ( https://doi.org/10.1145/3342555) - 2020.A. Turpin and A. Moffat. Housekeeping for prefix coding. IEEE Transactions on Communications, vol. 48, no. 4, pp. 622-628, April 2000, doi: 10.1109/26.843129.T.81 : Information technology - Digital compression and coding of continuous-tone still images - Requirements and guidelines (09/92) https://www.w3.org/Graphics/JPEG/itu-t81.pdfCristi Cuturicu. A note about the JPEG decoding algorithm. http://www.opennet.ru/docs/formats/jpeg.txt - 1999."
СберМаркет,,,Превращаем обычный электросчетчик в умный: продолжаем осваивать Samsung SmartThings,2023-10-21T09:00:04.000Z,"Вам не кажется, что вручную переписывать цифры с ЖК-экрана на электросчетчике каждый месяц - это немного странное занятие? Все слышали про умные электросчетчики, но не все готовы заплатить несколько тысяч (а счетчик с дистанционным снятием показаний стоит как минимум 15000 рублей). И не факт, что этот электросчетчик нормально встроится в вашу экосистему умного дома, скорее всего там будет свое собственное закрытое приложение.Как насчет старого доброго DIY? Давайте сделаем свой собственный умный электросчетчик с подключением к любой платформе, которую захотим. И делать мы это будем максимально мягкими методами, без вскрытия корпуса самого счетчика.Расскажем, как мы подключили электросчетчик «Меркурий» к ESP32 двумя разными способами (через встроенные интерфейсы RS-485 и оптопорт), соединили по Wi-Fi с платформой Samsung Smart Things и получили возможность записывать статистику энергопотребления в реальном времени чтобы оптимизировать свои траты на электроэнергию.Зачем нужен умный счетчикУмный электросчетчик - это, прежде всего, шаг вперед к экономии ваших средств. Ведь чтобы контролировать параметр, сначала нужно начать его измерять. Если вы смотрите на показания счетчика один раз в конце месяца, картина остается непрозрачной.Другое дело, если вы видите, в какие периоды дня для вас пользоваться электроэнергией выгоднее, и можете подстраиваться под них. Да и просто лишний раз выключить свет становится более интуитивно понятным (и ощутимым в деньгах, особенно если речь идет про лампочки накаливания) действием. Особенно, если вы платите за электричество по двум (день/ночь) или трем (ночь/полупиковая/пиковая зона) тарифам.Также вы можете последить за своей бытовой техникой и проводкой. Смогут ли ваши старые алюминиевые провода потянуть 16 А, которые возникнут, например, при включенной стиральной машине, посудомойке и чайнике?И любому человеку, близкому к технике, будет чуточку радостнее от того, что ресурсы распределяются рациональнее и экологичнее. В конце концов, делать свой умный дом просто интересно!Наше устройство Smart Energy MeterРаботать решили со счетчиками марки Меркурий, поскольку такие устройства являются одними из самых распространенных. Выбрали семейство однофазных многотарифных счетчиковМеркурий-206, обладающих несложной системой команд и интерфейсами для снятия показаний.Наш микроконтроллер с Wi-Fi-модулем делает из обычного электросчетчика умный. Он подключается к счетчику через проводной интерфейс RS-485 или оптопорт, и далее с помощью Wi-Fi (2.4 ГГц) передает показания силы тока, напряжения, мощности и потребленной энергии по тарифам на облачный сервер системы Samsung SmartThings. В приложении можно посмотреть графики и настроить уведомления.Структурная схема системы считывания показаний электросчетчикаГлавный элемент устройства — плата Lilygo TTGO T1 на базе микроконтроллера Espressif ESP32, обеспечивающая работу управляющей программы и связь через сеть Wi-Fi. Подключение к счетчику через проводной интерфейс RS-485 выполняется с помощью конвертера TTL-RS485, а подключение через оптопорт - с помощью доработанного ИК-датчика линии из любительской робототехники с превращением его в оптический UART, характерный для электросчетчиков. Схема устройства представлена ниже.Электрическая устройства для считывания показаний электросчетчикаВсе компоненты устройства, включая миниатюрный сетевой источник питания, помещены в стандартный корпус для монтажа на DIN-рейку. Если покупать компоненты устройства в розницу, его стоимость не превысит 2500 рублей.Компоненты устройства для считывания показаний электросчетчикаТак выглядит наш экспериментальный стенд (входной автомат, счетчик, Smart Energy Meter и розетка для нагрузки) при подключении счетчика через RS-485 и оптопорт. Выбор интерфейса выполняется с помощью переключателя на корпусе. Стенд оснащен выходным автоматом и розеткой для подключения нагрузки.Подключение считывателя показаний электросчетчика через порт RS-485Подключение считывателя показаний электросчетчика через оптопортИтог: система позволяет получать текущие значения тока, напряжения и потребляемой мощности, строить графики зависимости этих величин от времени и анализировать историю потребления энергии по тарифам для большей наглядности.Программное обеспечениеВ качестве платформы умного дома используется Samsung SmartThings. Данные отправляются на платформу через Wi-Fi, в «родном»мобильном приложенииотображаются показания счетчика и выдаются уведомления.Для написания программы Smart Energy Meter использовалиSDK for Direct Connected Devices for Cдля ESP32. Основой программы, использующей Samsung SmartThings SDK, послужил официальный примерswitch_example, описанный вдокументации, а также статьи на Хабре (проумный выключатель,умный чайники егопродолжение). Как и в этих статьях, наше устройство работает без хаба, таким образом используется прямое подключение (Directly connected) к облаку SmartThings Cloud. Так что для старта работы не нужно ничего, кроме самого устройства.Полный исходный код разработанной нами программы Smart Energy Meter свободно доступен врепозитории проектана GitLab.Выбор платформы не принципиален: при желании вы можете подключиться к той платформе, с которой вы умеете работать. SmartThings - один из возможных вариантов платформы, она бесплатная, открытая, обладает задокументированными API и примерами, поддерживает множество устройств от различных вендоров.Архитектура SmartThings. Мы использовали вариантDirectly Connected(слева посередине)Как создать прошивку умного счетчика - руководствоНиже поделимся инструкцией, для всех, кто хочет повторить проект.Перед написанием программы необходимо создать учетную запись Samsung Developer, в которой создается проект дляDirect-connected Device. В процессе конфигурирования проекта в среде Developer Workspace в качестве типа устройства (Device Type) выбираем измеритель мощности (Power Meter), как наиболее подходящий для решения нашей задачи.Далее добавляем следующиеComponents & Capabilities:Voltage Measurementдля отображения текущего значения напряжения сети;Power Meterдля отображения текущего значения мощности нагрузки на выходе счетчика;Energy Meterв количестве 4 штук для отображения значений потребления энергии по 4 тарифам,и настраиваем отображение значения мощности на виджете устройства в программе SmartThings.Затем получаем конфигурационные JSON-файлы, необходимые для написания программы и регистрации устройства в мобильном приложении SmartThings –device_info.jsonиonboarding_config.json.Комментарии к кодуРазберем подробнее код (вы можете посмотреть его врепозиториипроекта).Структурно программу из главногофайла main.cможно разделить на две части, реализованные в отдельных потоках:взаимодействие с SmartThings (app_main_task);циклический опрос счетчика (meter_task).Добавление указанных вышеCapabilitiesтребует включения в проект следующих файлов, обеспечивающих отображение напряжения, мощности и показаний по 4 тарифам:caps_voltageMeasurement.h;caps_voltageMeasurement.c;caps_powerMeter.h;caps_powerMeter.c;caps_energyMeter.h;caps_energyMeter.c.Рассмотрим каждый из потоков программы более подробно.В потокеapp_main_taskвыполняется стандартный набор действий для регистрации устройства и его работы в системе SmartThings. Инициализация необходимых намCapabilitiesдобавлена в функциюcapability_init.В потокеmeter_taskвыполняется опрос счетчика через UART независимо от реализации его физического уровня (RS-485 или оптопорт), выводы которого заданы в константахTXD_PINиRXD_PIN. Для взаимодействия со счетчиком через UART нужен его заводской серийный номер, указанный на наклейке и штрих-коде на передней стороне счетчика. В текущей реализации он задается в константеCOUNTER_SERIALна этапе компиляции, а в перспективе может быть задан пользователем на этапе первичного сопряжения (т.е., прохождения процедурыonboarding) в программе SmartThings в качестве PIN-кода.Для однофазных счетчиков Меркурий 200, 201, 203 (кроме 203.2TD), 206 доступна официальнаядокументацияпо протоколу обмена (системе команд). В нашей программе реализованы наиболее употребительные команды для таких счетчиков:0x27– чтение содержимого тарифных аккумуляторов активной энергии;0x28– чтение идентификационных данных счетчика;0x2f– чтение серийного номера;0x63– чтение значений U (напряжение), I (ток), P (мощность);0x66– чтение даты изготовления.При отправке данных в SmartThings используется только две команды –0x27(потребление по тарифам) и0x63(напряжение, ток и мощность). В перспективе этот список может быть расширен при добавлении в проект соответствующихCustom Capabilities.Наиболее кратко работу функций внутри потокаmeter_taskможно представить в виде следующей последовательности шагов:функцияprepare_commandподготавливает для счетчика с заданным серийным номером указанную команду вместе с расчетом последних двух байтов контрольной суммы;встроенная функцияuart_write_bytesотправляет команду в счетчик через UART;встроенная функцияuart_read_bytesпринимает ответ от счетчика через UART в массивbytesReceived;функцияparse_replyвыполняет проверку контрольной суммы массиваbytesReceivedи далее производит его парсинг для вывода соответствующей информации в терминал и отправки показаний счетчика в SmartThings и их показа в мобильном приложении;выдерживается пауза в 10 секунд, после чего весь процесс повторяется.Вспомогательными функциями для этого потока являются следующие:init_uart– инициализация UART;crc16MODBUS– расчет контрольной суммы CRC16 по схеме Modbus;bcd2dec– преобразование BCD в десятичный формат.Далее программа загружается в плату через USB-кабель стандартным способом с помощью командыpython build.py esp32 energy_meter flash, а ее состояние в любой момент времени может быть оценено путем подключения к терминалу платы –python build.py esp32 energy_meter monitor. При эксплуатации устройства у пользователя подключение USB-кабеля не требуется.Первое подключение устройства к приложению SmartThings может быть выполнено двумя способами: поиском устройств поблизости по Wi-Fi или сканированием QR-кода, доступного в папке проекта по путиoutput/esp32/iotcore_energy_meter_latest/*.png.Использование устройства Smart Energy MeterНаше устройство может работать со счетчиком электрической энергии через дваинтерфейсана выбор — RS-485 и оптопорт, в зависимости от способа подключения к электросчетчику.Схема подключения электросчетчика к Smart Energy Meter через интерфейс RS-485Схема подключения электросчетчика к Smart Energy Meter через оптопортПорядок действий пользователя при первом запуске:Найти свободное место на DIN-рейке для установки Smart Energy Meter.Отключить входной электрический автомат и подключить провода питания к Smart Energy Meter.Выбрать интерфейс подключения счетчика и выполнить подключение в соответствии с одним из рисунков – для RS-485 или для оптопорта.Включить питание входным автоматом.Убедиться, что на выходе счетчика есть сетевое напряжение и что красный светодиод на Smart Energy Meter загорелся.Запустить на смартфоне приложение Samsung SmartThings, выполнить поиск Smart Energy Meter и его первичное подключение к сети Wi-Fi.Наблюдать поступление показаний от счетчика в приложение SmartThings и, при желании, настроить пользовательские сценарии (об этом ниже).После подключения устройства кприложению Samsung SmartThingsпользователю становится доступным виджет устройства.Виджет устройства в приложении SmartThings на AndroidБез дополнительных настроек пользователь может видеть текущие значения показаний электросчетчика: напряжение, мощность нагрузки и потребление энергии по разным тарифам.Текущие значения показаний электросчетчика в приложении SmartThings на AndroidА при нажатии на соответствующие кнопки отображения истории показаний отображаются графики мощности и потребления энергии по тарифам (T1, T2, T3) в зависимости от времени (в течение дня, за день, за неделю, за месяц и т.п.).История показаний мощности нагрузки электросчетчика в приложении SmartThings на AndroidИстория показаний потребления энергии электросчетчика в приложении SmartThings на AndroidДополнительно пользователь может настроить оповещения (в терминологии приложения SmartThings называемые “сценариями”), связанные с электросчетчиком. На рисунке ниже продемонстрированы следующие оповещения:Ночной тариф– предупреждает пользователя о начале и окончании действия ночного тарифа на электрическую энергию, позволяет сэкономить и/или оптимизировать потребление электроэнергии, например, включив стиральную машину ночью;Дневной тариф– предупреждает пользователя о начале и окончании действия дневного тарифа на электрическую энергию, помогая предотвратить переплату за электроэнергию;Вне дома, потребление >500W– определяет отсутствие пользователя дома и в случае заданного превышения порога потребляемой мощности (например, 500 Вт), выдает предупреждение. Наконец-то можно перестать мучить себя тревогами на тему “а не оставил ли я включенным утюг”!При желании пользователь может создать и другие оповещения.Сценарии оповещений электросчетчика в приложении SmartThings на AndroidИтогВсе задуманное получилось осуществить: устройство Smart Energy Meter получает данные от электросчетчика и отправляет данные в облачную платформу Samsung SmartThings через Wi-Fi. Это позволяет отслеживать показания напряжения, мощности и потребления электрической энергии по нескольким тарифам в приложении на смартфоне или планшете. Подключение к счетчику реализовано двумя способами - через проводной (RS-485) и оптический интерфейс, являющимися традиционными для счетчиков электрической энергии.Любой желающий может повторить этот проект или реализовать свой аналогичный, пользуясь свободно распространяемымиисходниками на GitLabпод лицензией Apache License 2.0.Эта статья - продолжение серии обучающих материалов по работе с платформой Samsung SmartThings. Предыдущие статьи цикла:Встраиваем свое устройство «умного дома» в экосистему SmartThingsИнтеграция устройства в экосистему Samsung SmartThings на примере «умного чайника»Интеграция устройства в экосистему Samsung SmartThings на примере «умного чайника» Часть 2: Переход с ESP8266 на ESP32Об авторахУстройство было спроектировано студентомлаборатории Интернета Вещей НИУ «МЭИ»Каплинским Андреем Владимировичем и объявлено победителем специальной номинации «Выбор Samsung» в финале пятого ежегодного всероссийского межвузовского конкурса среди выпускников социально-образовательного проекта «Samsung Innovation Campus» в 2022 году (кстати, конкурс 2023 года можно посмотретьздесь, он состоится 26 октября).Научный руководитель - к.т.н. Стрелков Николай Олегович, руководитель очно-заочной программы магистратуры 11.04.01 Радиотехника (Киберфизические системы и интернет вещей) НИУ «МЭИ».В проектеSamsung Innovation Campusуже 31 вуз-партнер. Мы приглашаем вузы к сотрудничеству по следующим учебным трекам: «Искусственный интеллект», «Интернет вещей», «Мобильная разработка», «Большие данные»."
СберМаркет,,,ForBlitz Statistics: Как я в 10 классе сделал приложение на 10K+ установок,2023-09-14T11:14:02.000Z,"Привет! Я — один из администраторов в проекте ForBlitz, разработчик приложения ForBlitz Statistics и выпускник программы «IT Школа Samsung» в Санкт-Петербургском Дворце творчества юных. В июле 2023 года я стал победителем ежегодного конкурса по разработке мобильных приложений «IT Школа выбирает сильнейших!», заняв 2-е место в номинации «Программирование» с приложением ForBlitz Statistics, предназначенного для ведения соревновательной статистики по боям на танках в известной игре. Учась в 10 классе, я создал продукт, получивший 10K+ установок уже за первые полгода после выпуска, и теперь хочу поделиться краткой историей тернистого, но интересного пути разработки.Изучение Android-разработки с нуля, три версии дизайна, несколько полных рефакторингов, публикация в Google Play — вот, как это было…Промо-изображение приложенияНебольшая предысторияВ СНГ‑пространстве мало кто не слышал про «танки» (WoT/Мир Танков) — одну из популярнейших (по данным исследования Wargaming и Superdata от 2019 года эта игра стала самой прибыльной в категории F2P в России) игр среди взрослой аудитории. А вот менее известным является тот факт, что многие игроки возводят игровой процесс в статус киберспорта. А где киберспорт, там и стремление к как можно более высоким результатам, и, как результат, упор в «циферки» — статистику своих успехов и неудач. То же самое касается и мобильной версии «танков» — WoT/Tanks Blitz, пиковый дневной онлайн которых достигает 25 000.Игровой процессКак ни странно, в самих «танках» статистика почти не представлена: всего лишь для одного игрового режима, и то в минимальном виде. Зато компании‑разработчики (в 2022 году разработка и права на RU‑ и BY‑регионы под новым названием «Мир Танков» и «Танки Blitz» были переданы компании Lesta Studios, а остальные оставлены Wargaming) предоставляют доступ к Wargaming Public API — инструменту для получения подробнейших данных об игре и аккаунтах. В итоге большинству желающих приходится для отслеживания своих результатов всё же пользоваться сторонними ресурсами, в основном — сайтами.Почти с первых дней игры меня заинтересовали внутриигровые модификации: сначала я просто устанавливал чужие моды, затем начал публиковать уже свои, потом получил приглашение в команду небольшого сообщества, состоящего из таких же начинающих мододелов (так называют людей, создающих модификации), а оттуда уже получилось вступить в ForBlitz — самый большой и самый старый проект на эту тему, существующий с 2016 года. Прошло несколько месяцев, и создание модов наскучило, зато заинтересовало программирование. К этому времени проект получил собственный сайт и мобильное приложение, и рук сильно не хватало: всего два администратора‑программиста «тянули» на себе десятки тысяч пользователей. При этом всё ещё хотелось расширить проект за рамки обычных модификаций.На этом фоне и возникла общая идея создать быстрое, удобное и функциональное приложение для просмотра статистики. Я сам к моменту начала разработки играл несколько лет и, будучи игроком «выше среднего», часто ощущал нехватку такого инструмента, так что предложение попробовать себя в разработке принял на ура.Подробнее о аналогах и нишевостиГлавными игроками на этом рынке были сайты wblitz.net и kttc.ru. В силу особенностей API они имели сильные ограничения по скорости и по какой‑то непонятной причине также давали не очень много информации. Так или иначе их в целом вполне хватало, чтобы облегчить отслеживание статистики.Среди Android‑приложений был лишь один конкурент: informal WoT BLITZ Statistics. Вроде как «монополист», но… возможности почти не превышали таковые в самой игре.Бэкенд: наконец, разработкаСперва — разобраться с самим Wargaming Public API. К счастью, у этого инструмента от разработчиков есть даже полноценная документация! Данные идут в обычном JSON. На этом этапе оставалась лишь сериализация. Интересно, что при всей своей продуманности API имеет множество вариаций исключений, никак в документации не описанных. Позже пришлось некоторые случаи буквально «хардкодить» через if. Грустно, но терпимо.Как ни странно, самой сложной частью оказалась работа с сетью. Удивительно: самая популярная библиотека для создания HTTP‑запросов, Retrofit, не предусматривает, что интернета может и не быть! Решением оказалось встраивание проверки наличия сети при каждом intercept в связке с повторением запроса, если он всё же падает (к примеру, по timeout). Уже из чисто спортивного интереса добавлено и соответствующее диалоговое окно, создание которого потребовало отдельного service.Фронтенд: путь к современному UI при хорошем UXКак водится, сначала были красивые‑красивые (и то на первый взгляд) концепты. Картинки в качестве фона, кнопки всевозможных форм и размеров… В итоге это всё оказалось непонятным, неудобным и излишне вычурным уже на стадии реализации.Скрытый текст«Проба пера» в Figma. Красивые, но не очень функциональные макетыНа очереди были строго функциональные концепты. Лаконично и просто. Пришло осознание, что изобретать велосипед не обязательно: привыкшие к расположению элементов в стандартном клиенте и существующих аналогах люди захотят увидеть подобную структуру и в новом приложении. Только вот дизайн каждой своей деталью буквально молил о переработке.Скрытый текстТак выглядела первая версия дизайна приложения. Информативно, но не красивоИ, наконец, нечто среднее. Фон — не совсем монотонный, добавился градиент. Layout стали полупрозрачными чёрно‑белыми «карточками», надписи — разными по яркости, кнопки — с цветовыми акцентами. После недолгих раздумий были добавлены и анимации. Теперь на главном экране отображаются ключевые значения, остальное вынесено «под спойлер».Скрытый текстТак выглядит приложение сейчасРабота над архитектуройСначала все действия казалось логичным запихнуть в одну‑единственную Activity, но уже через несколько месяцев разработки добавление любой функции стало подобием поиска устойчивой кочки в болоте. Что делать? Скрепя душу, переписывать всё под MVC. Появились собственные View для разных целей, конфигурационные файлы, константы.Подготовка к выпуску на тестКонечно, хобби — это «для души», но и капелька рекламы не помешает. Так как аудитория планировалась преимущественно из СНГ, были выбраны решения от Яндекса. Также я добавил сервисы для отслеживания багов, вылетов, crash‑free и в целом аналитики: уже во время альфа‑теста сообщения по типу «вылетает в случайный момент» заставили ощутить потребность в таковых. Выбор остановился на Google Firebase и, в частности, Crashlytics. Уже перед самым выпуском я добавил контроль версии сборки: при каждом запуске приложение «сверяет» свою версию с сервером и при необходимости показывает уведомление о выходе обновления.Настоящее и будущее: перспективы развитияПосле выпуска сразу была проведена небольшая реклама: в сообществе проекта ForBlitz во «ВКонтакте» был выпущен информационный пост о релизе, а недавно приложение стало первым официальным расширением к приложению проекта.Скрытый текстСтраница ForBlitz Statistics в Google PlayСейчас приложение преодолело «моральную планку» в 10,000 установок в Google Play. Судя по отзывам, в основном пришли люди, ранее пользовавшиеся сайтами на схожую тематику: нативное приложение оказалось удобнее. Из других причин выделяли быстродействие и некоторые эксклюзивные виды статистики, которые привлекли не нашедших этого в конкурентах.На очереди — полная реализация всего API, включая требующие авторизации виды статистики, внедрение общей для всего бренда подписки за синхронизацию с облаком и общие доработки, информация о которых собирается до сих пор: приложение находится в стадии бета-теста. В совсем дальней перспективе — перевод разработки на Kotlin Multiplatform и релиз на iOS.ФидбекВесь процесс разработки сопровождался и сопровождается по сей день обширным сбором отзывов. Сначала это был альфа‑тест среди избранных тестеров. Находили баги, нюансы совместимости… Потом пришло время бета‑теста, в котором приложение находится и сейчас.Как и ожидалось, отзывы смешанные: на момент написания статьи средний рейтинг — 4,4/5. Почти все замечания связаны с несколькими моментами UX. К счастью, то, ради чего приложение создавалось, не подвело: его хвалят за быстродействие и информативность.Про обучение в «IT Школе Samsung»Здесь я хочу сказать о том, какую важную роль сыграло обучение в создании приложения. Можно долго говорить про самообразование, но… в реальности всё оказалось не так радужно.К началу 2022/2023 учебного года у меня уже были наработки по приложению: на первый взгляд всё даже работало. Но вот на «второй взгляд» открывалась темная сторона самостоятельного изучения такой глубокой темы без продуманности и системы. Приложение вылетало при отсутствии сети (извечная проблема при использовании Retrofit, самой популярной библиотекой для работы с API), зависало при создании длинных списков, не умело обрабатывать ошибки поиска.И тут, к счастью, началось моё обучение в «IT Школе Samsung». Наиважнейшими я склонен считать обратную связь от куратора‑преподавателя, Георгия Семёнова, и саму по себе учебную программу. За мучительным падением розовых очков последовало осознание: надо переделывать… абсолютно всё. Сразу были внесены правки UI, UX, а главная работа была проведена над кодом. Один класс превратился в десятки, появилось соответствие стандартам, даже банальный Git — и тот был новинкой! Прошло полгода, и код пришлось переписать практически полностью, по большей части с нуля. Большая часть правок была проведена благодаря моему преподавателю, за что я ему и сейчас очень благодарен: он неоднократно проводил полное код‑ревью и практически в любое время суток давал рекомендации по исправлению проблем, которые ему удавалось находить даже в решениях, казавшихся мне до того совершенными.Конкурс проектов «IT Школы Samsung»Обучение в «IT Школе Samsung» вместо экзаменов завершается созданием и защитой своих разработанных приложений — что‑то вроде дипломной работы в институте. Более того: с каждой группы площадок жюри выбирают лучшие проекты, которые в дальнейшем выходят в Финал ежегодного конкурса по разработке мобильных приложений «IT Школа выбирает сильнейших!», в этом году ставшего интернациональным — впервые в этом году к выпускникам «IT Школа Samsung» 2022/2023 учебного года присоединились ученики из Армении.Конечно, именно ForBlitz Statistics стал моим выпускным проектом. Бессонные ночи подготовки к конкурсу и… выход в его финал! А значит — что? Снова рефакторинг!Подготовка к финалу была похожа на трагикомедию: проведение финала практически идеально совпало с отключением Интернета у единственного оператора в глухой карельской деревне, в которой я находился, так что все на мероприятия приходилось ездить в деревню побольше и сидеть в придорожном кафе у заправки.Перед сдачей организаторами был проведен тренинг, на котором мы потренировали произношение, жестикуляцию, — в общем, умение «правильно» выступать.Как ни странно, сам финал для меня прошел почти без волнения. Наверное, потому, что на это не было времени: тысяча проверок оборудования, в полевых условиях становящегося особенно капризным, тысяча напутствий от преподавателя, однокурсников, близких людей… И, наконец, выступление.Выступал я вторым из всех финалистов, так что подготовиться к ответам жюри времени почти не было. А вопросы были: к разработке, к релизу и, конечно же, к техническим моментам. Несколько волнительных минут — и сдача завершена; финалисты продолжают свои выступления, а я «со всех ног» бегу на единственный в этих местах автобус.На следующий день узнаю результаты и… второе место!Скрытый текстЧасть призов за второе местоЭпилогС начала разработки прошло уже немало времени. Попутно почти с нуля став Android‑разработчиком, я превратил смутную идею в реальный, пусть и небольшой, коммерческий продукт. Познакомился с талантливыми людьми, которыми были мои одногруппники, работал под руководством прекрасного преподавателя, пообщался с невероятными жюри и организаторами конкурса.Теперь я искренне надеюсь, что, может быть, кому‑то эта статья даст понять: возраст и опыт не всегда являются определяющими факторами. Программирование даётся упорным — и пусть таковым и остаётся!Ярослав - выпускник бесплатной программы по подготовке будущих IT-специалистов «IT Школа Samsung», которая является частью глобальной инициативы Samsung Innovation Campus. Можно заниматься как очно, так и онлайн на десятках площадок в России, а с прошлого года также и в Армении. Хотите достичь уровня Junior-разработчика мобильных приложений на Java уже в школе? Скоро начинается учебный год, успейте подать заявку здесь:https://innovationcampus.ru/itschool/#SIC_Russia_2023 #enabling_people #SamsungInnovationCampus"
СберМаркет,,,История разработки приложения для складных смартфонов Samsung. Часть 1,2023-08-30T14:27:43.000Z,"26 июля прошёл очередной Samsung Galaxy Unpacked. В этот раз компания представила уже пятую версию складных смартфонов Fold. Ещё совсем недавно подобные устройства считались экзотическими, а сейчас это обычный гаджет, которым пользуются достаточно много людей.Появление подобных устройств привело к тому, что разработчики мобильных приложений должны дополнительно думать о том, как их приложения будут работать, а главное, смотреться на нестандартных экранах foldable гаджетов. Звучит просто, но, как всегда, путь будет тернист.Итак, знакомьтесь: Михаил.МихаилМиша - фрилансер. Он занимается мобильной разработкой под Android на протяжении нескольких лет. За это время он научился вслепую дебажить NDK-код, потратил сутки своей жизни только на gradle-сборку и двое на синхронизацию, потерял счет инвалидированным кэшам и перезапущенным Android Studio. В целом, работа ему очень нравится, он рад, что научился превращать раскрашенные кнопки в деньги.Однажды ему предложили работу, с которой он никогда не сталкивался:  проверка и корректировка мобильного приложения для складного устройства. Денег предлагали вагон и маленькую тележку, да и рейтинг у заказчика был хороший. Подозрительно несложная работа. Миша долго размышлял, стоит ли брать ему этот неординарный заказ, но, проснувшись в одно утро с бодрым настроением, наш герой твердо решил для себя, что сделает это. Через 10 минут Миша уже был добавлен в репозиторий с единственным сообщением от заказчика: “Удачи ?”.После запуска приложения на своем, привычном для всех, смартфоне герой не увидел в нем ничего сложного: пара экранов, небольшая навигация между ними. Он был очень рад и уже представлял, как будет на заработанные деньги лежать на пляже, попивая смузи.spoiler alert, spoiler alert!И Михаил радостно приступил к работе: пошел вStarbucksStars Coffee, заказал кофе, включил свой ноутбук, открыл проект. И тут задался несколькими вопросами:А как протестировать работу приложения не в стерильных условиях эмулятора, а на реальном устройстве?Как проверить его производительность на мобильных процессорах?Как изучить поведение приложения при запуске на версии Android с оболочкой?Как быть уверенным в правильной работе с реальной камерой или с живыми датчиками?На помощь к нему пришел Samsung RTL.Remote Test Lab (RTL) - это сервис от Samsung, который предоставляет пользователям возможность тестировать и пробовать свои приложения на устройствах Samsung удаленно, через интернет.Мише не придется приобретать на всю зарплату новый складной Galaxy. Все, что ему потребуется - компьютер с доступом в интернет. С помощью Remote Test Lab он сможет удаленно использовать реальные устройства, находящиеся в лаборатории Samsung, и сможет взаимодействовать с ними так же, как если бы они были у него в руках.Ему просто нужно будет выбрать любое устройство, расположенное на ферме, и подключиться к его демонстрации экрана.Один из примеров фермы устройствПеред началом работы Миша наткнулся настатью, которая рассказывала об RTL. Внимательно ее прочитав, он заметил, что с момента написания парк устройств вырос с 17 до 40 моделей, и что теперь не требуется скачивать отдельный клиент для подключения к устройству: все доступно в браузере.Наш герой работал над приложениемUmbrella- системой проката зонтиков. Идея проста: пользователь во время дождя может брать зонты в аренду. Идея похожа на прокат powerbank’ов / самокатов.Во время регистрации Миша заметил, что в RTL уже можно было арендовать новые Galaxy Z Flip и Fold 5, хотя с момента их анонса прошло меньше недели. Вместе с ними были доступны и устройства A, S, F, M, Note, Tab, Watch - серий и предыдущие foldable-поколения.Картинка, которую увидел МишаПосле регистрации герой сразу получил 10 внутренних ежедневных кредитов, которых хватило бы на два с половиной часа тестирования, но для начала он решил ограничиться получасом.АрендаВзяв в аренду новый Z Fold и установив на него последнюю сборку приложения, он приступил к тестированию.Процесс установки приложенияРешено было начать с моментов, связанных с изменением состояния экрана при повороте и складывании / раскладывании устройства. В первом случае проблем не возникло, приложение отображалось только в портретном режиме. Для второй проверки потребовалась соответствующая функция в RTL.Интерфейс Remote Test Lab транслирует экран реальных смартфонов Samsung, подключенных к фермеТаким образом был найден первый баг: введенные пользователем данные не сохранялись при раскладывании экрана. Видимо, разработчики знали об этом баге и просто запретили поворот экрана, не задумавшись о том, что в некоторых случаях его может быть недостаточно.Изменением состояния экрана Миша решил протестировать и другие моменты работы приложения при:Сетевом взаимодействии - получении данных с сервераОтображении статического контентаВторой баг был обнаружен с разметкой при использовании приложения на первом Galaxy Fold: layout ввода кода подтверждения отображался неверно на внешнем экране.Текст и кнопка “уехали”Кроме того, Миша обратил внимание, что иногда интерфейс был не совсем оптимален для текущего состояния устройства. Требуется добавить больше вариаций расположения виджетов на экране.Под конец тестирования Миша был очень доволен тем, что больше багов он не нашел, что смог протестировать приложение на большом количестве складных устройств совершенно бесплатно, и что дальше он может использовать Samsung Remote Test Lab для тестирования приложений на устройствах Samsung всех актуальных линеек.Наш герой уже начал представлять, как оставит всего один эмулятор для разработки, заменив остальные на RTL, как ему будет намного проще оценивать реальную производительность приложения на живых устройствах, как он забудет о словах: “Это баг эмулятора” и насколько реже он будет видеть сообщения об обновлении системных образов эмуляторов.На текущий момент парк смартфонов в RTL для России насчитывает250 устройств.В этот список входят следующие модели:И это далеко не весь списокТестировать приложение возможно даже на часахТакие удаленно управляемые устройства предоставляют настоящий пользовательский опыт. Приложение тестируется на реальном “железе”, поэтому баги, связанные с особенностями эмулятора, исключеныКаждое устройство подключено к точке доступа и имеет бесплатный доступ в интернетНа устройствах установлены разные версии Android, следовательно можно запускать приложение на разных версиях APIПарк постоянно обновляется. Можно тестировать приложение на самых новых продуктах компании SamsungТестировщик имеет доступ к debug-инструментам(ADB, logcat, file browser, automated tests и т.д)Samsung RTL- невероятно мощный и удобный инструмент в арсенале мобильного разработчика, позволяющий тестировать мобильные приложения в реальных условиях, на настоящих, физически существующих смартфонах.А что насчет Миши? Поехал ли он на пляж? Вкусный ли был смузи? Хочет ли Миша продолжать заниматься таким фрилансом? Миша начал работать над приложением для системы проката зонтиков, а вместе с этим изучает гайдлайны, которым следуют при разработке на foldable-устройства. С нетерпением ждем Мишу через две недели с рассказом о том, насколько интересен и необычен дизайн-мир складных устройств.АвторыИгорь ЕфимовВсем привет! Меня зовут Игорь, я ученик Томского Государственного университета. В 2021 году я выпустился из IT-Школы Samsung, получив Гран-при в выпускном конкурсе, и уже второй год я работаю в ней преподавателем. Параллельно больше года на фрилансе я разрабатываю нативные приложения под обе мобильные платформы: Android и iOS.Антон ВоробьевА меня зовут Антон, из IT-школы я выпустился ровно через год после Игоря, в 2022 году. Так же, как и Игорь, победил во всероссийском конкурсе ‘'IT-школа выбирает сильнейших’',взяв Гран-при. Сейчас я учусь в Высшей Школе Экономики и активно развиваюсь в мобильной разработке, чтобы устроиться на свою первую в жизни работу.Антон и Игорь - выпускники бесплатной школы программирования «IT Школа Samsung». Можно заниматься очно на одной из 67 площадок. Хотите достичь уровня Junior-разработчика мобильных приложений на Java уже в школе? Скоро начинается учебный год, успейте подать заявку здесь:https://innovationcampus.ru/itschool/#SIC_Russia_2023 #enabling_people #SamsungInnovationCampus"
СберМаркет,,,Смартфон в роли датчика Умного дома — обзор приложения Upcycle от Samsung,2023-07-28T10:41:54.000Z,"Всем привет!Продолжаем находить новые полезные применения для устаревшей, но горячо любимой техники. Это практико-ориентированный видеокурс «Galaxy Upcycling - новая жизнь старого смартфона», в котором рассказывается, как из вышедшего из употребления смартфона сделать полезное устройство. Видеолекции курса размещены наYouTube-канале «IT ШКОЛЫ Samsung».Мы уже посмотрели множество разных классных сценариев, это и микроскоп, и спектрометр, и физическая лаборатория, но все это были сторонние решения. Сейчас я проведу обзор официального приложения от Samsung, которое так и называется - Galaxy Upcycle, и примечательно тем, что оптимизирует использование батареи смартфона для задач Умного дома.Это приложение доступно в Galaxy Store и еще находится на ранней стадии разработки. Однако в нем уже предусмотрены два варианта, как использовать смартфон повторно: как датчик звука и как датчик освещенности. Я покажу оба сценария: мы сделаем из смартфона умную сигнализацию, а затем настроим адаптивное освещение в нашем Умном доме. Сегодняшний урок подойдет новичкам, я объясню самые базовые вещи, начиная с установки приложения, подключения вашей первой умной лампочки и так далее.Текст написан по мотивам видео:Устанавливаем приложениеПолучите ссылку из QR кода. По этой ссылке откроется магазин приложений Galaxy Store, а в нем - страница приложения Galaxy Upcycle:Также ссылка в текстовом виде:https://apps.samsung.com/appquery/appDetail.as?appId=com.samsung.android.smartthingshome.Внимание! Пока приложение доступно для ограниченного набора моделей смартфонов. Будем надеяться, что в будущем оно будет портировано везде, а пока его можно протестировать на следующих моделях: это линейки Galaxy S, Note и Z, начиная с 2018 года. Я всё делала на смартфоне Galaxy S9+.Итак, если вам повезло и у вас на руках поддерживаемая версия смартфона, то давайте же запустим приложение. Несмотря на то, что это бета-версия, с ней интересно ознакомиться, поскольку в ней реализованы некоторые важные возможности для концепции Upcycling.Когда я запускаю приложение, меня предупреждают, что заряд смартфона будет ограничен до 70%. Это оптимизация заряда аккумулятора. Важный момент, о чем написано в приложении в самом начале работы: если вы решите прекратить экспериментировать с приложением Upcycle, и удалите приложение, то режим оптимизации заряда тем не менее сохраняется до следующей перезагрузки смартфона. Учитывайте это, если пробуете приложение на своем основном смартфоне.В настоящий момент приложение находится в стадии разработки и регулярно обновляется, если вы столкнетесь с проблемами в его работе - сообщите об этом в комментариях.Делаем звуковую сигнализациюПервая область применения – детекция звуков. Например, я могу использовать смартфон как сигнализацию в квартире, когда буду в отъезде. Встроенная нейросеть может классифицировать звуки и различать стук в дверь, звон стекла, лай собаки, плач ребенка и так далее - всего восемь разных звуков на данный момент. Можно записывать звуки, а также говорить в режиме интеркома, например, чтобы голосом успокоить ребенка. Таким образом это еще и сценарий радионяни.Итак, на старом смартфоне открываем приложение Upcycle и там нажимаем “Добавить” напротив датчика звука. Теперь наш старый смартфон работает в режиме детекции звуков. По кнопке в приложении Upcycle можно отключить подсветку экрана, что удобно с точки зрения экономии заряда.Созданный нами виртуальный датчик звука можно увидеть в приложении Samsung SmartThings уже на основном смартфоне. Он отобразится там как один из датчиков вашего Умного дома.Проверим работу нашей сигнализации. Издаю любой звук, и в приложении SmartThings на смартфоне появляется индикация шума. Нажав на иконку этого сенсора, можно посмотреть более детальную информацию.Теперь проверим работу нейросети. Я постучу по столу, и нейросеть распознает это как стук в дверь. Создастся событие, и вы увидите его на таймлайне в SmartThings.Давайте проверим, как хорошо работает встроенная нейросеть и сможет ли она отличить лай собаки, стук, мяуканье кошки, звон стекла. Например, включим эти звуки на компьютере. Вы увидите, что они отобразились на таймлайне.Наконец, посмотрим, как работает запись голоса для последующего воспроизведения.Нажмем кнопку “Записать”, и начнется запись 10-секундного звукового клипа. Затем значок этого клипа появится на таймлайне, где его можно будет послушать.Теперь посмотрим, как смартфон работает в режиме интеркома. Нажатием на кнопку “Прослушивание” можно узнать, что происходит в том месте, где лежит ваш старый смартфон: будут воспроизводиться звуки с его микрофона. А чтобы поговорить, нажмите на кнопку “Разговор”: у вашего основного смартфона включится микрофон и вы сможете говорить с человеком на другой стороне. Например, успокоить ребенка или поговорить с гостем, постучавшим в дверь.Теперь ознакомимся с настройками приложения. Здесь есть две функции: создать уведомление и автоматическая запись звуков. «Создать уведомление» означает, что вам на смартфон придет всплывающее уведомление из приложения SmartThings, чтобы вы могли среагировать. Можно сделать уведомление обо всех звуках, или только об определенной категории из вышеупомянутых (звон стекла, стук в дверь…) А «Автоматическая запись звуков» будет создавать 10-секундные клипы при регистрации звука - опять же, любого или только из категории. Эти клипы будут появляться на таймлайне.Для примера, можно записать звук после стука в дверь, и тогда вы сможете услышать голос человека, который пришел в гости.Смартфон как датчик освещенностиВторое применение в приложении Upcycle - датчик освещенности. Как видите, мне постоянно показывается уровень текущей освещенности, вычисляемый благодаря сенсору, встроенному в корпус смартфона. Вообще этот сенсор используется, чтобы затемнять экран, когда смартфон поднесли к уху для разговора. Мы видим уровень освещенности в уровнях от 1 до 7, где 1 - это полная темнота, а 7 - очень яркий свет. Нажатием на знак вопроса можно получить более детальную информацию.Сейчас я хочу в деталях продемонстрировать сценарий со светом, потому что он в полной мере показывает интеграцию приложения Upcycle с платформой Умного дома Samsung SmartThings и взаимодействие разных устройств от разных производителей в рамках одной экосистемы.Подключаем умную лампочку к SmartThingsДля сценария адаптивного освещения нам понадобится умная лампочка. Я покажу самый простой вариант: когда лампочка подключается по WiFi напрямую к сети.Я купила умную лампочку, у которой в описании было WiFi и ключевые слова: Tuya Smart, Smart Life. Это то, что нужно. Существует стандарт Tuya, он достаточно популярен для задач недорогого “Умного дома”, и разные производители выпускают свои устройства с поддержкой этого стандарта. Лампочка стоила порядка 800 рублей. Если у вас что-то не заработает, то скорее всего производитель вашей лампочки не реализовал функционал в прошивке, поэтому попробуйте сменить лампочку на другую, от другого производителя.Вообще Samsung SmartThings это платформа, сильной стороной которой является поддержка множества различных стандартов. И сейчас мы в этом убедимся: лампочка, на коробке которой даже не стояло значка “Совместимо с Samsung SmartThings”, тем не менее без проблем добавляется, и с ней можно взаимодействовать. Все потому, что лампочка действует по интерфейсу Tuya Smart, а значит, к ней можно подключиться, используя функционал платформы SmartThings “Подключение через стороннее облако”.Для начала я запускаю “родное” для лампочки приложение Smart Life. В нем понадобится создать аккаунт и подтвердить его через почту. Важно, что для соединения с лампочкой требуется WiFi, и просто раздать сеть с того же самого смартфона не получится: если смартфон раздает сеть, то подключить через него лампочку невозможно, нужен либо другой смартфон, либо другая WiFi-сеть. Перевести лампочку в режим конфигурации можно, быстро включив и выключив ее несколько раз. После чего, открыв окно приложения, можно управлять лампочкой, включать на ней разные режимы и даже цветомузыку.Здесь сразу видна проблема современного “Умного дома”: множество производителей, у каждого свое собственное приложение, переключаться между ними неудобно. Если есть общая платформа, где объединены все умные устройства, то появляется принципиально новая возможность: можно создавать различные сценарии взаимодействия разной умной техники. Посмотрим, как это реализовано на платформе Samsung SmartThings.Добавляю лампочку в SmartThings, это очень просто. “Новое устройство”, дальше поиск по марке “Smart Life”, и там устанавливаю соединение с облаком Smart Life, после чего лампочка добавится в приложение SmartThings и ей можно будет управлять: включать, выключать, задавать цвет и уровень яркости.Создаем сценарий адаптивного освещенияТеперь создадим сценарий для автоматической регулировки освещения так, чтобы лампочка функционировала как торшер в комнате и реагировала на верхний свет.Этот сценарий раскладывается всего на два случая:если в комнате темно (верхний свет выключен), и лампочка при этом выключена, то включить лампочкуесли в комнате светло (верхний свет включен), и лампочка при этом включена, то выключить лампочкуДля начала замерьте нижние и верхние границы уровня освещенности, выключив и включив свет в комнате. Для этого используйте упомянутый сенсор освещенности. У меня получились 3 и 5. То есть условия будут такие: “не выше уровня 3” и “не ниже уровня 5”. Что это значит: освещенность меньше или равно 3 - это когда в комнате вообще ничего не светится. Освещенность больше или равно 5 - это когда горит и умная лампочка, и верхний свет. При значении 4 светится только умная лампочка, но не верхний свет.При создании сценария есть полезная кнопка “Проверить действие сценария”, она запускает конечное действие безотносительно того, выполняются ли сейчас условия, ее можно использовать для проверки - к примеру, работает ли лампочка.Наш сценарий будет состоять из двух правил:Правило для выключения лампочки, если окружающий свет уже достаточно яркий:Правило для включения лампочки, если окружающий свет слишком тусклый:Тщательно протестируйте этот сценарий. Чтобы он стал жизнеспособным, его стоит настроить по времени суток (нет смысла включать свет ночью, когда вы уже спите), и привязать к вашему присутствию дома (нет смысла управлять светом, если в квартире никого). Все это тоже можно сделать при помощи платформы Samsung SmartThings. Займемся этим.Поработаем с временем. Очевидно, что после 22:00 лампочка точно не должна гореть, ведь пора спать. И раньше 08:00 ее тоже нет смысла включать. Добавим еще одно условие в правило: оно должно срабатывать, только если сейчас период между 08:00 и 22:00. Интересно, что настройки времени всегда по умолчанию являются предварительным условием (об этом написано в окне создания сценария), то есть проверяются в первую очередь, и только после этого идет проверка остальных условий.И добавим еще одно условие: сценарий должен срабатывать, только если я есть дома.Итоговый сценарий будет выглядеть так:Сценарий звук + светВ завершение, попробуем сделать сценарий: стук в дверь - включить свет. Это очень логично, ведь так мы можем заранее включить себе свет в прихожей. Настроим срабатывание умной лампочки по событию звукового сенсора.Сценарий будет выглядеть так:А в жизни это будет выглядеть, как на гифке:ЗаключениеМы сделали общий обзор приложения Galaxy Upcycling и начали строить свою систему Умного дома.Без сомнения, тема “Умного дома” очень затягивающая, непростая, но мы уже сделали первый шаг! Теперь вы видите, что даже сенсор на пробу покупать необязательно, если его можно сделать из смартфона. Попробовав такой сценарий, вы сможете сделать прототип умного освещения у себя в квартире без особых денежных вложений. Пишите в комментариях, какие устройства “Умного дома” у вас уже есть, и как вы ими пользуетесь. Всем пока, до новых встреч!Другие статьи этого цикла:Несложные оптические трюки со смартфоном: голограмма и проекторСпектрометр из смартфона, картона и осколка DVD-диска: смотрим на спектры лампочек, фонариков, солнцаНовый год не за горами: делаем супергирлянду на базе ESP и WLED, управляем со смартфонаДелаем физическую лабораторию из смартфона своими рукамиLinux на смартфоне: делаем экран погодной станции, используя Termux и Node-REDLinux на смартфоне: считываем показания сенсоров и программируем «Куб времени» в Node-REDТатьяна ВолковаКуратор образовательных программ Samsung Innovation Campus#SIC_Russia_2023 #enabling_people #SamsungInnovationCampus"
СберМаркет,,,"Совершенство достигается к моменту краха. История о том, как мы создали лабораторный стимулятор",2023-07-21T05:00:01.000Z,"Привет,ХАБР!А вы сталкивались с проблемой устаревшего лабораторного оборудования в учебных заведениях?Помните, как настраивали прибор, когда основная часть потенциометра отвалилась? Или как нажимали на кнопку, от которой осталось только название?Мы, Лиза и Андрей, студенты 4 курса биотеха НИУ МЭИ радиотехнического факультета, создали прибор для лабораторных работ по физиологии, по сути своей – это генератор электрических импульсов, управляемый по Wi-Fi через смартфон.Поскольку прибор нишевый, проект отлично себя показал на многих научных конференциях, и получил три приза на конкурсе образовательного проекта Samsung Innovation Campus. Мы бы хотели рассказать историю создания этого прибора.Развитие электростимулятора биотканейУдивительно, но проблема устаревшего оборудования гораздо шире, чем кажется на первый взгляд.Нам это знакомо, как никому другому. Именно поэтому эта статья здесь!Встречали такое?Наша история. НачалоНа протяжении всего нашего обучения мы прошли множество различных лабораторных работ на разных кафедрах, и где-то оборудование было супер-супер, а где-то хотелось кричать «ПОМОГИТЕ»! И у нас возникал вопрос: а почему так? Неужели у нас в университетах не хватает специалистов? Или всё упирается только в деньги?Такие вопросы возникали у нас много раз. От этого казалось, что, может быть, наше жизненное предназначение - это ДЕЛАТЬ ЛАБОРАТОРНОЕ ОБОРУДОВАНИЕ?Наше учебное направление немного связано с биологией, физиологией и живыми организмами, поэтому у нас была дисциплина «Биофизические основы живых систем», которую преподавал профессор из РУДН.Как-то раз он пригласил нашу группу к себе в ВУЗ, чтобы показать некоторые опыты. И под конец мероприятия профессор попросил попробовать устранить неисправность в одном из электростимуляторов.Электростимуляторы? Зачем?Если обратиться к истокам: физиология, по природе своей, наука экспериментальная.  С давних времен эксперимент был основной формой для её исследования, он сочетался с использованием различного рода технических приспособлений и устройств. Даже простейший опыт, в котором, например, необходимо зарегистрировать сокращение мышцы или изменение кровяного давления, требует применения определенных инструментов: рычажка, манометра, приборов для графической регистрации и т.п. Таким образом, инструментальным методам в физиологическом эксперименте принадлежит ведущая роль.А теперь ответим на вопрос,зачем нужен электростимулятор?– Для того, чтобы сформировать правильное представление о работе мышечной и нервной ткани, проводят лабораторные работы, которые, как правило, проходят с использованием образцов нервно-мышечного препарата лягушки (можно использовать мышек или крыс) и специального прибора - электростимулятора.А следующая схема позволяет визуально пояснить вышесказанное.Что c отечественными производителями электростимуляторов? И как завязалась наша история в качестве разработчиков?В общем, в наших руках оказался стимулятор ЛЭС-1 кафедры нормальной физиологии РУДН, и нам нужно было каким-то образом его починить.Электростимулятор ЛЭС-1 с кафедры нормальной физиологии РУДН(1 – потенциометр, 2 – кнопки переключения масштаба амплитуды, 3 – кнопки переключения частоты, 4 – светодиоды для отображения заданной частоты, 5 – кнопка «однократного» режима, 6 – выходные клеммы, 7 – тумблер включения/выключения)На самом деле, стимулятор был вполне рабочим, просто его выходные данные были далеки от градуированных, то есть, тех, которых ожидает пользователь. На некоторых значениях наблюдалась относительная погрешность 90% и выше, при том, что для проведения лабораторных работ она должна быть ниже 10%.Тут мы поняли, что чинить тут нечего, и проще было бы, как нам тогда казалось, собрать новый!Так возникла большая-большая работа, в которой выяснилось, что большинство существующих электростимуляторов можно сосчитать по пальцам и местами они не очень-то и работоспособны. Схемы к ним утеряны, а мастера, привыкшие их чинить, давно ушли на пенсию. Как Вам такое?Ни схемы тебе, не методичек по функционалу, придумай сам! А мы такое можем, умеем и практикуем!Наш первый стимуляторПосмотрите сами: мы сделали стимулятор биотканей с механическим управлением. На рисунке представлена его структура. Впечатляет?Структура симулятора с механическим управлениемНас тоже. Особенно радует тот факт, что по габаритам он в 4 раз меньше ЛЭС-1, а его мы тоже приводили для сравнения.Данный электростимулятор мы разрабатывали почти год, продумывали режимы работы, думали над тем, какой должен быть корпус, какие элементы управления следует выбрать.На самом деле, разработка прибора – большое дело, ведь, как мы уже говорили: на «ЛЭС-1» не было никаких схем и описаний, приходилось «сочинять» самим.Созданный нами стимулятор реализует 4 режима работы:Режимы работы электростимулятораВ отличие от ЛЭС-1 и других подобных приборов (а их, напоминанием, не так много, и некоторые доживают свой век), созданный нами стимулятор обзавёлся режимом нарастания сигнала, также в нём можно задавать начальный и конечный уровни амплитуды. А режим постоянного уровня не увидишь в обычных стимуляторах. И это внедрённое нами новшество  порадовало всех.Как говорил британский историк, писатель Сирил Норкот Паркинсон:«Совершенство достигается только к моменту краха»С данным стимулятором в марте 2022 года мы выступалина конференции МНТК «Радиотехника, электротехника и электроника»и заняли2 место.Что было дальше?Дальше наша история закручивается вихрем жизненных событий:Мы начинаем учиться и интенсивно вбирать в себя всё-всё, что дают нам наши преподаватели в «IT Академии Samsung» на треке IoT (интернет вещей).Наш проект получает высокую оценку преподавателей кафедры нормальной физиологии РУДН и обратную связь: а что, если добавить смартфон в качестве задающего параметры устройства и передающего их по беспроводной связи в стимулятор?Да, такого со стимуляторами ещё точно никто не делал! «Мы будем первыми», - сказали мы тогда себе.Нетрудно догадаться, что мы реализовали этот проект, защитив его в качестве выпускного проекта по курсу «IT Академии Samsung», а также вышли с ним в финал ежегодного межвузовского конкурса проектов выпускников образовательных программ «IT Академии Samsung» в 2022 году, получив награду в трех номинациях! 3 место в общем рейтинге, приз «Выбор зрителей», а наш ментор Вячеслав Шумаев стал ментором года!!!Помимо этого, мы выступалина нашей вузовской конференции МНТКв 2023 году ина конференции «Агаджаняновские чтения» в РУДН.В итоге у нас получилось вот такое устройство:Сам электростимуляторИнтерфейс мобильного приложения для управления стимуляторомО конкурсе «IT Академии Samsung» и работе с менторомОтдельно хотелось бы рассказать о работе с нашим ментором в рамках проекта «IT Академия Samsung». До этого мы никогда не работали с ментором, мы благодарны @Slava_Shumaev за то, что он поверил в нас и всячески поддерживал!Конечно, в проектах бывают шероховатости, и порой, ты их просто не замечаешь. Но когда с тобой работает профессионал, то все эти шероховатости можно быстро устранить. Как раз-таки это и случилось с нами!Несколько раз в неделю мы созванивались в Zoom или Telegram с нашим ментором и обсуждали текущие изменения в проекте, новые идеи, а также составляли план работ на ближайшее время, распределяя задачи между участниками. Небольшие вопросы оперативно решали в общем Telegram-чате.Мы сильно подтянули бизнес-составляющую, отработали момент с актуальностью проекта! На сегодняшний день в Россиибольше 100 учебных заведенийс медицинскими направлениями, каждое из которых нуждается в обновлении парка лабораторного оборудования и спрос на него будет расти.Нам очень помогла подготовка к представлению нашего проекта в межвузовском конкурсе проектов выпускников «IT Академии Samsung».Наверное, самым хардом финала было то, что мы выступали первыми, а также вопросы жюри и слушателей финала.Кстати, воттутможно посмотреть наше выступление, тайминги там проставлены.Фото с церемонии награждения межвузовского конкурса проектов «IT Академии Samsung» слева направо: мы, Лиза и Андрей, авторы проекта, Владислав Крутских, наш руководитель из НИУ МЭИ и Сергей Певнев, вице-президент штаб-квартиры Samsung Electronics по странам СНГДомашняя лаборатория. Советы подрастающему поколениюНаверное, нам было бы сложнее работать над этим проектом, если бы у нас не было домашней лаборатории. Ещё с первых курсов мы инвестировали средства в оборудование и инструменты для неё.И эти инвестиции сыграли свою роль. Нужно исследовать выходной сигнал? Пожалуйста, под рукой всегда осциллограф. Нужно припаять что-то? – паяльная станция. Распечатать 3D-модель? – 3D-принтер! и т.д. Это упрощает многое!Кстати, вот кадры процессов работы:Кадры процессов работыКаждый был занят делом. Андрей больше специализировался на «железе», а я на исследовании функционала и рынка, а также готовила материалы для докладов. Но часто мы менялись ролями, и вообще, многое делалось совместно!Планы на будущееТут много чего можно расписать, ведь мозг постоянно генерирует новые идеи! Но о самом главном – мы бы хотели бы доработать схемотехническое решение и софт. Дальше патентование, чтобы можно было запускать серию (тут, конечно, еще много работы) и можно предлагать наше решение ВУЗам! Ведь стимуляторы нужны!А так было бы интересно реализовать лабник прямо в телефоне, мы же современное поколение студентов!ЗаключениеЧто сможет изменить наш прибор в образовательной и научной жизни? Для этого представляем визуализацию:Надеемся, что эта статья была полезна и замотивировала вас реализовывать ваши даже самые необычные идеи. И не боятся их! А также трудиться, выступать, показывать и раскрывать себя!Выпускники проекта «IT Академия Samsung» 2022 годаВ проектеSamsung Innovation Campusуже 31 вуз-партнер. Мы приглашаем вузы к сотрудничеству по следующим учебным трекам: «Искусственный интеллект», «Интернет вещей», «Мобильная разработка», «Большие данные».#SIC_Russia_2023 #enabling_people #SamsungInnovationCampus"
СберМаркет,,,Nature: создан моющийся HEPA фотокаталитический фильтр со сроком службы до 20 лет,2023-06-26T14:48:52.000Z,"Исследователи Samsung разработали  фильтр для очистки воздуха от пыли (PM) и летучих органических соединений (VOC) со сроком службы 20 лет, убирающий 95% PM и 82% VOC за проход, и моющийся при этом обычной водой почти до показателей первоначальной эффективности.Статьяопубликована в престижнейшем журнале Nature Communications.Чтобы вы понимали, сейчас чаще всего используют разные фильтры для PM и VOC. Оба этих фильтра одноразовые со сроком службы порядка полугода. По-моему, именно так и выглядит прорыв :)ЗагрязнителиМелкие взвешенные частицыaka пыль или Particulate Matter (PM). Чаще всего нужно следить за PM 10 и PM 2.5, то есть за частицами с размером меньше 10 мкм и 2.5 мкм, соответственно. 2.5 мкм раз в 30 тоньше человеческого волоса, но, как ни странно, примерно такой же длины, как крупная бактерия. Такая пыль может долго висеть в воздухе и быть супер-вредной (PM — самыйсильный фактор риска, увеличивающий преждевременную смертность). Регулярная уборка пылесосомпозволяетснизить концентрацию пыли дома, но только, если у вас хороший пылесос с герметичной системой и HEPA-фильтром. Но очиститель воздуха, всё же, будетэффективнее: он приведёт в норму показатели чистоты воздуха за час-другой.Летучие органические соединенияили Volatile Organic Compounds (VOC). Это то, что содержит в себе табачный дым,выделяетмебель из ЛДСП и МДФ, напольные покрытия, краски,благовонияи даже3D-принтеры. VOC также печально известны тем, что среди этих веществ многоканцерогенов. Но не всё так печально: их концентрацию можно держать на приемлемом уровне, постоянно проветривая помещение.Я хочу упомянуть еще два загрязнителя:Выхлопные газы— пересекающееся с VOC множество. Чаще всего, угарный газ, оксиды серы и азота. Чаще всего нейтрализуются фотокаталитическими фильтрами (об этом ниже).Углекислый газ. Дома главный загрязнитель — ты, username! Но не переживай, CO2можно убирать так, как это делаюткосмонавты, но лучше проветривать свою комнату!Как определяют загрязнения в воздухе:Мелкие взвешенные частицы(PM 2.5 и PM 10),чаще всего, определяют оптическим датчиком: фотоприёмник ловит свет, отраженный от пыли (смотрите рисунок 1). Как правило, измеряется в мкг/м3. Меньше 10 мкг/м3— хорошо, остальное — не очень. Иногда измеряют в миллионных долях ppm (parts per million), 1 ppm = 10-6(10-4%), это своего рода «процент для лилипутов».Рисунок 1. Так работает оптический датчик пыли.ИсточникЛетучие органические соединения(VOC) определяют более хитрым датчиком, в котором нагревается пластинка из оксида металла (смотрите рисунок 2). У её поверхности появляются ионы кислорода, которые вступают в реакцию с целевым газом и тем самым высвобождает электроны. Это приводит к изменению электрического сопротивления пластинки Сопротивление — это уже электрическая величина, которую можно измерить компактным аналогом мультиметра. На приборах, как правило, указывают показатель tVOC (total Volatile Organic Compounds) -  суммарная концентрация всей летучей органики в воздухе (не обязательно вредной). Эта величина измеряется в мг/м3, иногда в ppm. Упрощенно, всё, что меньше 1 мг/м3— хорошо (помните, что tVOC — общий показатель. Так-то для некоторых веществнормативы ВОЗгораздо строже (<0.001 мг/м3).Рисунок 2. А так работает датчик летучих органических соединений.ИсточникСуществуют бытовые модели таких датчиков (как на рисунке 3), но они недешевые. Они измеряют концентрацию CO2, VOC и PM 2.5. Как пользователь такого, могу сказать: обратите внимание, что при первом включении датчик VOC может прогреваться 4 часа, не спешите паниковать. И человек сам является источником летучих органических соединений, так что не удивляйтесь, если он будет завышать показания рядом с вами.Рисунок 3. Бытовой анализатор воздуха. Умеет измерять CO2, PM 2.5, tVOC.ИсточникЕсли нет бытового датчика, то часть показателей можно узнать в Интернете. В крупных городах есть станции экологического мониторинга. По ссылке можно посмотретьданные Мосэкомониторингаинародного ☭ мониторинга(вам нужно смотреть раздел: показ -> запыленность). Но самая большаякарта у сервиса Breezometer. Онаработает по всем городам мира (он, вероятно, экстраполирует данные со станций, оборудования в домах и автомобилях).Обратите внимание, Москва – довольно чистый город, по сравнению с мегаполисами Китая и Индии.Как работают обычные фильтры:PM. 2.5: HEPA-фильтры (High Efficiency Particulate Arrestance, высокоэффективное удержание частиц) (смотрите рисунок 4). В среднем, степень очистки >95%, срок службы порядка полугода.Рисунок 4. Микрофото волокон HEPA-фильтра.H11 – маркировка класса фильтра, этот чистит не менее 95% пыли.ИсточникТакже рекомендую прочитать прекраснуюстатьюо том, как работают HEPA-фильтры и почему их эффективность растет по мере загрязнения (хотя и падает их пропускная способность). Коротко и упрощенно принцип работы HEPA-фильтра можно объяснить через  описание механизмов фильтрации:«сито», когда частичка больше, чем пора фильтра (она просто не пролезает туда).адгезия (прилипание): частичка касается поверхности и практически навсегда прилепляется (работают силы межмолекулярного взаимодействия).аутогезия (слипаемость): то же самое, что и адгезия, но работает, когда частичка касается другой частички.Cовсем маленькие частицы пыли натыкаются на волокна фильтра благодаря броуновскому движению (так они с большей вероятностью “задевают” волокна), а сравнительно большие врезаются туда по инерции, не успевая обогнуть их с потоком воздуха. Win-win situation!Кстати, в Советском Союзе был свой HEPA –фильтры Петрянова-Соколова.VOC: угольные (адсорбционные) и фотокаталитические фильтры (как правило, используются вместе).Угольные фильтры (как на рисунке 5) улавливают практически все токсичные примеси воздуха с молекулярной массой более 40 атомных единиц. Эффективность очистки ~ 90%, срок службы ~ полгода.Рисунок 5. Угольный (адсорбционный) фильтр.ИсточникФотокаталитический фильтр (показан на рисунке 6) имеет пористый носитель с нанесенным ТiО2-фотокатализатором, который облучается светом и через который продувается воздух. Загрязнители адсорбируются на поверхности фотокатализатора и под действием света от ультрафиолетовой (УФ) лампы окисляются до углекислого газа и воды. Приятный бонус: УФ уничтожает некоторые бактерии и даже вирусы (но нужно смотретьмощность и время экспозиции). Угольный фильтр могут поставить второй ступенью после фотокаталитического (тогда он служит дольше). Степень очистки такой системы >95% (но по некоторым веществам ниже, например, по диоксиду серы ~15%). Срок службы ~ 2 года.Рисунок 6. Фотокаталитический фильтр.ИсточникИзстатьии комментариев к ней следует, что фотокаталитические фильтры имеют сравнительно невысокую производительность. Чтобы эффективно очищать от летучей органики целую квартиру, требуется большая установка (размером примерно с половину холодильника).Как работает новый фильтрКерамический фотокаталитический фильтр для очистки воздуха, о котором исследователи Samsungнаписалив журнале Nature, сделан по принципу «два в одном». Керамические HEPA секции в нем чередуются с секциями, покрытыми фотокатализатором Cu2O/TiO2и облучаемыми ультрафиолетом (смотрите рисунок 7).Рисунок 7. Устройство нового фильтра.Схема из статьи авторовКерамика – это необычный материал для HEPA. Обычно его используютдля водяных фильтровилинейтрализаторов выхлопных газов(обычные HEPA-фильтры делают из стеклопластиковых волокон). Но в этом случае это сделано, прежде всего, для того, чтобы фильтр можно было мыть.Более того, для фильтра используется не простая пористая керамика, а покрытая мембраной из неорганических материалов (в нашем случае, кордерит, алюмосиликат магния и железа). Это позволяет быстро переходить в режим фильтрации, при котором поступающая пыль задерживается первыми слоями налипших на фильтр собратьев (на английском это называется dust cake), это, в целом, эффективнее чем обычный механизм фильтрации (пояснение на рисунке 8).Рисунок 8.микрофотографииa - рамического фильтра, b - керамика, дополнительно покрытой мембраной.Справасхема, объясняющая понятие «dust cake»Ниже приведу в упрощенном виде таблицуиз статьи авторов, где сравниваются параметры керамического фильтра с кордеритовой мембраной и без неё.ПараметрКерамический фильтрКерамический фильтр с мембранойПадение давления при скорости движения потока 1 м/с, Па62136Степень фильтрации PM 2.5, %51.197.7Степень фильтрации PM 10, %53.698.0Из этой таблицы видно, что эффективность керамического фильтра с мембраной значительно выше, чем «обычного» керамического фильтра, хотя его воздушное сопротивление (падение давления на фильтре) тоже выше.Таким образом, воздух, попадая в секцию с HEPA-фильтром, натыкается на пробку и просачивается сквозь керамику с мембраной в фотокаталитическую секцию, покрытую Cu2O/TiO2и облучаемую УФ-лампой (подробнее об этом на рисунках 9 и 10).Рисунок 9. a - секции фильтра, чередующиесяв шахматном порядке, b – срезы фильтра–туда попали две секции очистки от пыли (левая верхняя и правая нижняя, они покрыты мембраной из кордерита) и две – фотокатализа (правая верхняя и левая нижняя, покрыты оксидом титана).ИсточникДобавление сокатализатора Cu2O улучшает фотокаталитическую активность традиционного катализатора TiO2,благодаря легкому разделению зарядов и высокой плотности носителей заряда.Рисунок 10. Схема воздействия УФ на катализатор. Оно приводит к освобождению нескольких форм кислорода, который разрывает VOC вплоть до воды и углекислого газа.ИсточникПосле оптимизационных расчетов был использован массив 2х2 УФ-светодиодов. Расчеты показали, что лучшие показатели интенсивности света 38,1 (в центре) и 40,8 мВт/см2(сбоку) достигаются при расположении ламп на расстоянии около 30 мм до поверхности фильтра (смотрите рисунок 11).Рисунок 11. Эффективность фильтрации VOC в зависимости от типа применяемых УФ-светодиодов. График изстатьи авторов.Почему новый фотокаталитическийфильтр эффективен:Удаляет >95% PM и >82% VOC за один проход потока воздуха через него.1 дм3фильтра может держать в себе 20 г пыли – это примерно в 4 раза больше, чем у обычных фильтров.Моется обычной водой.Может быть использован повторно 10 раз с сохранением эффективности фильтрации (другими словами, прослужит 20 лет).Регенерация фильтраИсследования показали, что простая промывка водой в направлении против улавливания пыли является наиболее эффективным способом регенерации такого фильтра. Утверждения о 20-летнем сроке службы фильтра следуют именно из этих данных (смотрите рисунок 12).Рисунок 12. Эффективность фильтрации почти не упала в течение 10 циклов регенерации, а сопротивление потоку даже немного снизилось. Фото и график изстатьи авторов.Апробация опытных образцов таких фильтровНовые фильтры тестировали в течение 30 месяцев в одном из зданий в Южной Корее. Исследователи подтвердили, что эффективность фильтрации PM 2.5 остается выше 98% в течение 30 месяцев без замены и регенерации, в то время как обычные фильтрующие элементы показали сравнительно низкую эффективность (62%) и требовали замены каждые 3-6 месяцев.Кроме того, в течение 12 месяцев исследовалась отдельно стоящая система очисткивоздуха с использованием нового фильтра на подземной парковке (подробнее нарисунке 13).Рисунок 13. Фото экспериментального стенда на одной из парковок в Южной Корее и результаты измерений двух показателей PM в течение года при расходе 4000м3/ч(это очень высокий показатель, этого хватит для проветривания семи квартир).ИсточникВсё вышеописанное создаёт оптимистичное впечатление. Кажется, что произошел технологический скачок пусть даже и в небольшой области знаний, связанной с воздушными фильтрами. Конечно, у этой технологии есть (или будут обнаружены) недостатки (например, наверняка, такие фильтры будут заметно дороже применяемых в данный момент), но, безусловно, эта разработка способна улучшить качество нашей жизни в ближайшее время.Спасибо за внимание!Вячеслав Шумаевкандидат физико-математических наук, ведущий инженер Samsung"
СберМаркет,,,Многоклассовая детекция клеток рака почки: нейросетевой ассистент врача-патоморфолога,2023-03-29T15:13:59.000Z,"Всем привет, меня зовут Арсений, я студент 2 курса магистратуры Сеченовского университета по специальности ""Наноматериал и биофотоника”. В октябре прошлого года я занял первое место во всероссийском межвузовском конкурсе “Samsung Innovation Campus” со своим проектом “Многоклассовая детекция ядер светло-клеточного почечно-клеточного рака” и хотел бы поделиться подробностями.Я учусь в медицинском, и когда возникла необходимость придумать тему выпускного проекта, чтобы завершить свое обучение по треку “Искусственный интеллект” Samsung Innovation Campus, я обратился к своим коллегам из Института регенеративной медицины Сеченовского университета с просьбой о каком-нибудь датасете. К счастью, у них один был. В итоге я смог не только сделать победивший в конкурсе проект, но и расшевелить тему анализа изображений раковых почек в стенах Сеченовского университета.Картинка для привлечения внимания с результатом работы StarDist из коробкиО задачеПоследнее время во всем мире, в том числе и в России, остро стоит проблема недостатка патоморфологов — врачей, в чьи обязанности входит анализ данных биопсий. Это рутинная и трудозатратная профессия, и от этого мало кто хочет в нее идти. Вместе с этим совершенствуются методы лечения и диагностики, из-за чего растет нагрузка на врачей этой профессии.Снизить нагрузку на врачей пытаются при помощи наработок компьютерного зрения. Особенностью применения ИИ в медицине является высокое требование к точности, и ориентирование на роль “ассистента” врача. Пока не представляется возможным замена врачей искусственным интеллектом полностью, большинство методов направлены на повышение скорости и удобства работы специалиста, на минимизацию его ошибок.Что касается диагностики видов рака, множество работ посвящены наиболее распространенным типам рака: молочной железы, простаты, легких и прямой кишки. Мой научный руководитель, к.м.н Файзуллин Алексей Леонидович, предложил мне посмотреть на проблему с другой стороны, взяв в качестве критериев выбора задачи не только распространенность, но и время анализа образцов одного пациента и сложность постановки диагноза.В итоге, идея применения ИИ для изучения светло-клеточного почечно-клеточного рака нам показалась наиболее перспективной, кроме того немаловажную роль играло мое желание внести полезный вклад в развитие именно области цифровой патологии.Светло-клеточный рак является самым распространенным раком почки, занимая около 70% от общего количества. При этом, специфично количество анализируемых образцов: при таком виде рака производится полная резекция органа, который потом разделяется на большое количество образцов для анализа*. Вследствие этого анализ и получение результатов одного пациента занимает больше времени. Возможность уменьшить время анализа и улучшить точность диагностирования могла бы значительно помочь патоморфологам в их работе.Гистологический скан представляет собой изображение размером примерно 30 000 на 40 000 пикселей, и благодаря такому высокому разрешению мы можем разметить каждую клетку. Большой размер снимков предполагает разделение на патчи и обучение модели на этих частях. Для таких малых патчей мы уже можем решать задачи компьютерного зрения посегментации и классификации, что в масштабе гистологического снимка значительно упрощает работу врачей.Стоит отметить сложность постановки диагноза при светло-клеточном раке. При анализе скана врач выбирает регионы, а потом считает в нем пропорции клеток различного класса. Т.о. оценка дается региону и для этого надо отсмотреть большое количество клеток (при этом специалисты не всегда приходят к единому мнениюhttps://link.springer.com/article/10.1186/s13000-021-01130-2).Наша идея заключается в том, что модель если и будет ошибаться на отдельных клетках, то для всего региона ошибка будет незначительной и оценка результата будет верной.ДатасетДанные достались мне от проекта другой научной группы, цель которого была схожей, но совершенно отличался подход. Изначально идея была в ручном создании интерпретируемых признаков и последующей подачи их в интерпретируемую модель табличных данных. Но оценка результата была низкой и этот проект не показал высоких значений метрик.Объем датасета составлял примерно 11к данных со следующим распределениемГистограмма распределения классов в датасетеУ разметки была одна особенность: чтобы создать хоть сколько-нибудь сбалансированный датасет, размечались не все ядра в регионе, а лишь представляющие интерес. То есть разметка была разреженной. Это не позволяло подавать снимки сразу в модель сегментации/детекции. Конечно, можно применить методы sparce-сегментации и считать лосс только по размеченным данным, но эта идея была отвергнута ввиду излишней сложности для стоящей передо мной цели. Поэтому я решил использовать двухуровневую структуру: сначала ядра детектировались моделью StarDist, затем распределялись классификатором по двум группам классов. Нулевой класс – ядра, не представляющие интереса, и 1-4 классы – раковые ядра с соответствующим грейдом.StarDist – модель для детекции клеток/ядер. В официальномрепозиторииесть предобученные модели, в том числе и для гистологии. В основе модели StarDist лежит архитектура Unet, которая предсказывает вероятность того что пиксель является центром ядра, а также 32 вектора до границы этого ядра. Потом это все пропускается через NMS и выдается конечный результат.Такой подход хоть и кажется запутанным и крайне неочевидным, но позволяет получить точные детекции клеток даже в самых сложных случаях.Пример сложной ситуации детектирования клеток (https://arxiv.org/abs/1806.03535)КлассификаторДля распределения ядер по классам я создал дополнительный классификатор, который помогал определить степень поражения ядер на изображениях.В начале я, как и многие, в качестве базовой архитектуры классификатора рассматривал EffNet и ResNet. И первая просто отказалась учиться, модель оказалась избыточной для имеющихся данных. ResNet показал результаты получше, но тоже далекие от идеала.После этого началось первое для меня исследование по решению задачи перебором различных моделей и аугментаций данных. Оказалось, что я не первый, кто столкнулся с тем, что на гистологических снимках модели семейства VGG обучаются лучше чем EffNet и другие глубокие модели. По результатам моего исследования на данный момент лучшей моделью для решения задач классификации в гистологии оказалась модельDenseNet из трех dense-блоков по четыре свертки без паддингов. Для выпускного проекта Samsung Innovation Campus я использовал DenseNet121 из стандартной библиотеки PyTorch, так как для конкурса не успел сделать кастомную версию, но для нашего внутреннего пользования презентовал уже кастом версию. При обучении этой модели изображения ядер обрезались с небольшим отступом, приводились к общему размеру и передавались в классификатор. Для улучшения результатов использовались небольшие аугментации, такие как смещение оттенка и поворот на случайный градус. Клетки не котики, у них нет верха и низа, мы можем крутить их на все 360°.Схема DenseNet для гистологического изображения (https://www.sciencedirect.com/science/article/pii/S1361841523000166)Интерфейс системыЯ не разработчик интерфейсов, однако в современном мире можно обойтись и без знаний JS и HTML. Я воспользовался открытой Python-библиотекой streamlit, которая позволяет быстро накидать простой интерфейс для запуска скрипта обработки изображений и вывода результата.Интерфейс программы v0.0.1Результат работыДля оценки качества модели применялись средние значения метрик точности (Precision) и полноты (Recall). Модель показала значения 0.90 по обоим параметрам.APrecisionARecallDensenet2010.900.90Densenet1610.850.84ResNet500.840.84Densenet1210.800.80ResNet1010.800.80EfficientNet_b00.800.80Конечно, эти показатели не идеальны, но исходная работа в первую очередь ставила задачу доказательства концепта, с чем она, на мой взгляд, прекрасно справилась.Нейросеть позволяет идентифицировать истинно опухолевые клетки. Точность классификации опухолевых клеток по ядерной градации производит впечатление. Решение имеет потенциальную ценность для специалистов микроскопической диагностики при анализе операционного материала светлоклеточного рака почки— к.м.н. А.Л. ФайзуллинОзнакомиться с проектом можнопо ссылке.Первые попытки визуализации результата работы модели. В дальнейшем мы исправили цветовое безумие и перешли к градации зеленый-желтый-красныйФинальный вариант визуализацииПланы на будущееМожно сказать, что проект появился очень вовремя. Он дал начальную мотивацию для исследования проблемы классификации клеток. Результат не только был отмечен жюри конкурса проектов Samsung Innovation Campus, но и продемонстрировал перспективы развития этой темы. В январе текущего года мы собрали новый датасет, который полностью посвящен теме многоклассовой детекции. Собранный датасет уже полноценен с точки зрения плотности разметки – в регионе интереса выделены все клетки и им присвоен класс. Работа по сбору и разметке данных оказалась очень трудозатратной, но результаты работы показали, что развитие идей применения поклеточной классификации нейронными сетями для диагностирования светлоклеточного рака могут быть применимы в реальных сценариях.Сейчас мы с коллегами завершаем работу над полноценным исследованием, и я думаю, что в скором времени мы сможем опубликовать научную статью, начало которой было положено в моем проекте.БлагодарностиХочу выразить благодарность своему научному руководителю к.м.н. Файзуллину Алексею Леонидовичу и преподавателю дисциплины “Искусственный интеллект” Валетову Дмитрию Кирилловичу.P.S.В процессе написания этой статьи у людей, не знакомых со спецификой диагностирования рака почки, возникло большое количество вопросов по поводу полной резекции органа. Да, орган удаляют полностью. Сначала рак диагностируют на МРТ, потом делают операцию. После чего врачу-патоморфологу необходимо уточнить диагноз, определив степень поражения. Это необходимо для прогнозирования и определения дальнейшего плана лечения.ГлоссарийБиопсия– взятие образца ткани для последующего исследования. Обычно такая операция делается специальной иглой, которая забирает часть тканиГистоподготовка— процесс подготовки и окраски образца, в ходе которого получают гистологические срезы толщиной 3-4 мкм. Малая толщина среза позволяет получить изображение на светлопольном микроскопе: мы видим свет, прошедший сквозь образец, что на выходе дает нам четкое и контрастное изображение.NMS– non maximum suppression,статья с объяснением принципа.На церемонии награждения призеров конкурса: слева я, справа -- вице-президент Samsung Electronics Сергей ПевневЗемеров Арсенийtg:@azemerov2 курс магистратуры Сеченовского университетаАссистент кафедры высшей математики, механики и математического моделирования Сеченовского университетаВ проектеSamsung Innovation Campusучаствует уже более 30 российских вузов-партнеров. Мы приглашаем вузы к сотрудничеству по следующим учебным трекам: Искусственный интеллект, Интернет вещей, Мобильная разработка, Большие данные.#SIC_Russia_2023 #enabling_people #SamsungInnovationCampus"
СберМаркет,,,Linux на смартфоне: считываем показания сенсоров и программируем «Куб времени» в Node-RED,2023-01-15T18:35:51.000Z,"И снова привет! Впрошлом урокемы посмотрели проект для смартфона “Погодная станция”. Сегодня будем делать более сложный проект: “Куб времени”. Это устройство для тайм-менеджмента, которое поможет вам вести лог собственных занятий и знать, сколько времени вы тратите на работу, учебу, чтение, спорт, игры и другие занятия, а также планировать ваш день. Интересный гаджет и хороший пример для обучения программированию.В ходе работы над проектом мы начнем писать свои собственные программы в Node-RED, более детально изучим возможности Termux (эмулятора среды Linux под смартфон) и запрограммируем сценарий, задействующий сенсорику смартфона!Этот урок - часть курсаGalaxy Upcycling - новая жизнь старого смартфона, практико-ориентированные сюжеты которого показывают как, в рамках концепции Upcycling, превратить старые Android-смартфоны в интересные, небанальные, красивые и полезные устройства.Текст написан по мотивам видео:Основы Node-REDДля начала - небольшое введение в среду визуального программирования Node-RED, которую мыустановилив прошлом уроке. Начнем писать в ней самые простые программы.В Node-RED программа называется потоком (flow) и состоит из отдельных узлов (node). Узлы соединяются друг с другом путём перетаскивания связи между ними. Через эти связи идут сообщения (messages), у которых есть названия (topic) и полезная нагрузка (payload).Слева находится палитра узлов:Самый важный узел - это отладочный узел (debug). Я рекомендую в любой непонятной ситуации пользоваться им. Если передадите в него сообщение, оно выведется в отладочной консоли, и вы сможете обнаружить и исправить ошибку.Сделаем самый простой поток из двух узлов: Inject (вбросить в поток сообщение) и Debug (отладка).После того как мы разместили и соединили несколько узлов, программу нужно развернуть (deploy), после чего начнется ее выполнение.Нажатием кнопки на узле “Метка времени” можно запустить поток на выполнение. Мы видим, что в отладочной консоли каждый раз появляется сообщение с указанием текущего времени (Timestamp).Там могут быть любые данные, не обязательно время. Вы можете заставить узел отправлять что угодно, к примеру цифру 5, это делается в настройках узла (двойной клик мышью).Один из самых полезных узлов - функция (function). В нем можно писать любой код на языке JavaScript.Например, я могу добавить туда простейший код, который умножает на два все, что приходит в узел, и тогда в узел debug будет приходить уже десятка, а не пятерка.Вся программа, написанная в Node-RED, представляет собой текст в JSON-разметке, то есть один большой текстовый файл. Поэтому, чтобы импортировать стороннюю программу, достаточно нажать на “Импорт” и вставить целиком ее текст. Так же легко можно экспортировать свою программу и поделиться ей.В целом, Node-RED удобно использовать в учебных целях, а также для быстрого прототипирования. Для сложных проектов визуальное программирование не очень подходит, для них обычно пишут код в текстовом формате. Но для небольших проектов Node-RED - дружественная к пользователю, простая и визуально приятная среда.Доступ к периферийным устройствам смартфона через Termux-APIЧтобы двигаться дальше у вас на смартфоне должна быть установлена программа Termux. Если вы не знаете, как это сделать, перейдите прямо сейчас куроку“Погодная станция”.Установим дополнительный программный пакет Termux-API, он скачивается с F-Droid:https://f-droid.org/en/packages/com.termux.api/. Нужно дать ему все возможные разрешения. Также установите в консоли Termux соответствующий пакет:apt install termux-apiВведите в консоли команду termux-sensor -l, и вы увидите потрясающий список сенсоров! Очень круто, что у сенсоров указаны конкретные модели, а, значит, вы можете о каждом из них подробнее узнать в официальной документации от производителя. Список на разных смартфонах получится разный, в зависимости от аппаратного обеспечения каждого конкретного устройства.Пример сенсорной начинки флагманского смартфона Samsung S22Чтобы вывести показания конкретного сенсора, воспользуйтесь командой с ключом -s и впишите название сенсора. В моем примере я вывожу показания акселерометра:termux-sensor -s ""LSM6DSO Accelerometer""Видно, что они идут в широко известном формате данных JSON, а, значит, их легко разобрать и выделить только интересующее нас значение. Остановить процесс можно, как и в обычном Линуксе, командой CTRL-C.Чтобы получить одно значение, можно запустить команду с ключом -n равным 1:termux-sensor -s ""LSM6DSO Accelerometer"" -n 1Выводим показания сенсоров в Node-REDВозвращаемся к разработке в среде Node-RED! Ставим в ней пакет узлов для работы с Termux-API:https://flows.nodered.org/node/node-red-contrib-termux-apiВ левой панели появятся новые узлы, относящиеся к функционалу смартфона.Для примера посмотрим, как работает узел vibrate - включение вибромотора телефона. Поставим timestamp и из него переход в узел vibrate. Как только кликнем на узел timestamp, увидим, что телефон завибрировал.Теперь сделаем так, чтобы показания нашего сенсора появлялись в Node-RED.Любую консольную команду можно запустить на выполнение через узелexecи получить результат. Выберите слева такой узел и перетащите его на панель программы. Выход узла exec соедините с узлом отладки (debug). Вот как это будет выглядеть:Теперь в окошке справа вы увидите показания сенсора:Видно, что в отладочном окне объект на выходе узла exec содержит текстовую строку из трех значений: x, y и z.Для удобства я сразу переименовала узел, чтобы он назывался “Получить данные с акселерометра”. Переименовать узел можно, открыв параметры узла двойным кликом и изменив строку “Имя”.Чтобы работать не со строкой, а с массивом значений, передадим нашу строку в функцию json. Это стандартная функция, она делает из строки в формате JSON объект JavaScript.В результате этого у нас на узле debug будет уже не текстовая строка, а объект JavaScript:Узнаем грань кубика из показаний акселерометраНапишем простую функцию, которая по полученным значениям x, y и z, –покажет, на какой из шести граней стоит наш кубик.Для этого сделаем узел function, в котором и начнем писать нашу функцию.Для начала “вытащим” значения x, y, z из массива. Есть небольшая тонкость. Поскольку в названии сенсора стоит пробел (""LSM6DSO Accelerometer""), то при попытке к нему обратиться в коде возникнет ошибка. Поэтому его нужно всегда указывать в кавычках, пример: msg.payload[""LSM6DSO Accelerometer""]:var accelerometer = msg.payload[""LSM6DSO Accelerometer""];
var x = accelerometer.values[0];
var y = accelerometer.values[1];
var z = accelerometer.values[2];Теперь я воспользуюсь готовым кодом, чтобы из показаний акселерометра вычислить сторону кубика. Этот код я не придумала сама, а взяла готовый извидео-инструкциина YouTube о том, как сделать “умный игральный кубик” от канала Playful Technology. Этот код очень наглядный и понятный.var accelerometer = msg.payload[""LSM6DSO Accelerometer""];
var x = accelerometer.values[0];
var y = accelerometer.values[1];
var z = accelerometer.values[2];

var side=0;

if (Math.abs(x) > Math.abs(y) && Math.abs(x) > Math.abs(z)) {
    if (x > 0) {side = 0}
    else side = 1;
}

if (Math.abs(y) > Math.abs(x) && Math.abs(y) > Math.abs(z)) {
    if (y > 0) { side = 2 }
    else side = 3;
}

if (Math.abs(z) > Math.abs(x) && Math.abs(z) > Math.abs(y)) {
    if (z > 0) { side = 4 }
    else side = 5;
}

msg.payload = side;
return msg;Запустив код, мы видим, что в консоль будет печататься грань, на которой стоит кубик.Теперь сделаем эту программу циклической. Пусть она в цикле опрашивает смартфон и выдает сообщение с гранью кубика, только если грань поменялась.Я добавила глобальную переменную prev_side и теперь печатаю грань кубика и обновляю глобальную переменную, только если грань поменялась.В Node-RED можно выводить отладочную информацию рядом с узлом, например, такой строкой:node.status({fill:""red"",shape:""dot"",text:""red""})В результате вместе с текстом будет выведен красный кружок, и это можно использовать для визуальной индикации.Чтобы не переполнять отладочную консоль текстом, советую выводить грань по ходу потока, рядом с узлом, где вычисляется грань.Подключаем трекер времени TogglОстался последний шаг: сделаем куб трекинга времени. Например, мы хотим отследить, сколько времени в день тратим на чтение, бытовые дела и занятия спортом. Нарисуем или напишем названия этих активностей на гранях кубика. Будем поворачивать кубик каждый раз, когда меняем занятие. Такое устройство очень полезно для хронических прокрастинаторов. Понаблюдав за собой при помощи этого кубика, вы поймете, куда уходит ваше основное время.Как учитывать время, затраченное на дела? Для этого существуют готовые решения. Я решила воспользоваться бесплатным веб-сервисомToggl, в нем уже есть необходимые мне функции: установка и сброса таймера, назначение меток таймерам, вывод статистики.Пример статистики по временным затратам в TogglЯ соединю этот сервис с Node-RED, чтобы установка и сброс таймеров в Toggl осуществлялись не нажатием кнопки на сайте или в мобильном приложении, а происходили как реакция на сигнал о переворачивании кубика на другую сторону.В Node-RED есть пакет node-red-contrib-toggl, установите его через “Управление палитрой”. После установки у вас в среде появятся новые узлы.В параметрах узла start timer увидите, что нужно добавить конфигурационный файл.Нажав на “карандашик”, увидите, что предлагается добавить новый API-ключ.Где его взять: захожу на сайтToggl, меня интересует продукт Toggl Track. Регистрация бесплатна. В настройках своего профиля я могу посмотреть API-ключ, при помощи которого очень легко настроить интеграцию со сторонним сервисом.Пропишите этот API-ключ в настройках узла Toggl и затем совершите развертывание потока (не обращайте внимания на сообщение об ошибке), чтобы в узле Toggl появился выбор Workspace (рабочей среды). Выберите свою среду и проект, который будет запускаться, например, “Чтение”.Всё почти готово! Теперь осталось реализовать следующую логику: остановка существующего таймера, старт нового таймера. Одну грань можно оставить без таймера - это будет сброс куба (остановка всех запущенных таймеров).Я ставлю узел “Get running timer” и в настройках включаю галочку на “send false if no running timer”, чтобы не генерировалось сообщение об ошибке. В нашем случае отсутствие таймера - это не ошибка, а вполне штатная ситуация. Если таймер запущен, то при смене грани кубика я его сбрасываю. Затем совершаю выбор, какой новый таймер запустить.Есть один не самый простой момент - это когда мы добавляем условный оператор и смотрим, не является ли имя таймера равным false. Здесь нужно сравнивать именно как выражение, а не как строку (по умолчанию).В завершение давайте добавим обратную связь. Можно добавить вибрацию, чтобы поворот куба сопровождался обратной связью от телефона.Корпус я сделала из картонной коробки, на гранях написала занятия, смартфон вклеила на двусторонний скотч. Это годится как самый первый прототип. Потом можете дорабатывать кубик дальше, если вам понравится устройство.Скачать мой код можно по ссылке:https://gist.github.com/tatyanavolkova/3b75d501fa0d67a62e2d4e1b11633abfЗаключениеВот такой куб времени у меня получился, он поможет вам измерять затраты самого ценного и невосполнимого ресурса – времени; планировать ваш день будет проще.Получается, что покупать дорогие одноплатные компьютеры необязательно, если вы хотите проверить какую-то гипотезу, протестировать несложный код или сделать любительский проект с киберфизическим устройством. Оказывается, что для некоторых таких задач вполне хватит  смартфона. Теперь вы можете сделать очень и очень многое, а наш курсGalaxy Upcyclingдаст вам массу интересных идей. Удачи!Другие статьи этого цикла:Несложные оптические трюки со смартфоном: голограмма и проекторСпектрометр из смартфона, картона и осколка DVD-диска: смотрим на спектры лампочек, фонариков, солнцаНовый год не за горами: делаем супергирлянду на базе ESP и WLED, управляем со смартфонаДелаем физическую лабораторию из смартфона своими рукамиLinux на смартфоне: делаем экран погодной станции, используя Termux и Node-REDТатьяна ВолковаКуратор образовательных программ Samsung Innovation Campus#SIC_Russia_2023 #enabling_people #SamsungInnovationCampus"
СберМаркет,,,"Linux на смартфоне: делаем экран погодной станции, используя Termux и Node-RED",2022-12-29T17:15:31.000Z,"Всем привет!Сейчас я покажу, как перенести на смартфон проект погодной станции, изначально сделанный под Raspberry Pi. Для этого мы установим программу Termux — эмулятор терминала Linux —, затем поставим на него среду визуального программирования Node-RED и за 5 минут реализуем готовый проект — погодную станцию, то есть экран, выводящий текущую погоду и прогноз. Использовать будем только Free/Open Source- инструменты.Этот текст основан на моем видео:И мы продолжаем погружаться в тему повторного использования техники вкурсе“Galaxy Upcycling - новая жизнь старого смартфона”. Чего мы с вами уже только не делали: ифизические эксперименты, иполезные гаджеты, изабавные спецэффекты. Но в сегодняшнем уроке будет важно не то, ЧТО мы сделаем, а КАК мы это сделаем.Что такое Termux: консоль Linux на смартфонеTermux имитирует консоль операционной системы Linux на смартфоне с Android, и даже Root-доступ ей не требуется.Открываются новые возможности:перепробовать много вариантов программного обеспечения, уже существующего под Linuxподнять свой сервер для резервного копирования или для игры в Minecraft с друзьямипроверять системы на уязвимостьхостить свою домашнюю страничкуи даже превратить смартфон в устройство Интернета вещей! Чем мы и займемся сегодняИсточник: https://github.com/AKXX/termuxУстановка TermuxTermux распространяется открыто и бесплатно. Достаточно просто скачать установочный APK-файл с сайта F-Droid:https://f-droid.org/en/packages/com.termux/Требуется ОС Android не ниже 7-й версии. На моем Galaxy S7 (2016) всё запустилось без проблем. Нужно дать приложению различные разрешения при установке. И вот, запустив Termux, вы видите знакомое окно консоли Linux! Здесь можно использовать все привычные команды, такие как перейти в директорию (cd), вывести листинг директории (ls), установить новые пакеты (apt), автодополнение (tab), история команд (↑, ↓). Пользовательский опыт при работе в консоли оказался вполне сносным, а уж если подключить Bluetooth-клавиатуру, то вообще не отличить от десктопа.Так выглядит домашняя директория.А так - файловая системаЧто полезного можно здесь сделать? Раз мы взялись делать погодную станцию, нужна какая-то среда разработки, простая и удобная для работы с физическими устройствами. Я предлагаю установить среду Node-RED.Что такое Node-REDNode-RED - это открытая и бесплатная платформа для интеграции систем между собой, своего рода универсальный клей для быстрого соединения программных и аппаратных компонентов. Ее часто используют при изучении Интернета вещей. Платформа поощряет сотрудничество: на официальном сайте выложено свыше 4000 готовых узлов и более 2000 готовых пользовательских программ.Программы в Node-RED создаются на визуальном языке, и готовая программа похожа на блок-схему. Среда приучает вас мыслить в парадигме событийно-ориентированного программирования, в ней широко используется многопоточность. Но такая организация кода имеет и свои особенности: в визуальной схеме легко запутаться, как только она хоть немного разрастается.Node-RED называется так потому что в основе своей он написан на языке серверного программирования NodeJS. Пользовательские узлы пишутся на языке JavaScript. В этом уроке мы будем использовать полностью готовый код, то есть программировать не придется.Устанавливаем Node-REDНа сайте Node-RED естьруководствопо установке под Termux, но вкратце все сводится к нескольким строчкам:apt update
apt upgrade
apt install coreutils nano nodejs
npm i -g --unsafe-perm node-red
node-redСервер стартовал:Сервер Node-RED будет доступен на смартфоне по адресуhttp://127.0.0.1:1880. Все достаточно понятно: адрес localhost и порт 1880. Можно открыть среду прямо в браузере в смартфоне, но там редактировать неудобно.Среда явно не предназначена для программирования через экран смартфонаЯ предпочитаю работать над кодом с ноутбука. Для этого ноутбук должен быть в той же WiFi-сети. Выйдем из Node-RED через стандартную для Linux команду прерывания процесса Ctrl+C. Узнаем IP адрес смартфона через стандартную команду ifconfig. И опять запускаем сервер node-red. У меня у смартфона оказался адрес 192.168.0.14, соответственно в браузере я перешла на адресhttp://192.168.0.14:1880/и увидела удобное большое окно среды Node-RED.Совсем другое дело!Обзор готового проекта - погодной станцииЯ решила взять в учебных целях готовый проект погодной станции, которыйнашлана официальном сайте Node-RED. Эта станция показывает текущую погоду, прогноз на будущее, влажность, облачность, и обладает симпатичным интерфейсом.Автор делал его изначально под Raspberry Pi, но оказывается, что ту же самую программу можно запустить и на смартфоне, ведь платформа разработки одна и та же - Node-RED.Так выглядел проект под Raspberry Pi.А так выглядит у меня на смартфонеЯ немного упростила этот проект: убрала сторонние иконки, чтобы их не пришлось устанавливать отдельно, и добавила свои параметры, которых мне здесь не хватало.Ссылка на изначальный проект:https://gist.github.com/djiwondee/b5b7d5da14d24e71de447e6aa290937eСсылка на мой проект:https://gist.github.com/tatyanavolkova/8e09e182f328e9486f635a3a3d6d8550Какие шаги нужно проделать, чтобы сделать точно такую же станцию у себя на смартфоне? Очень просто! Нужно импортировать мой код в Node-RED. Скачайте по вышеприведеннойссылкефайл flow.json и нажмите Import.Удобно, что весь проект в Node-RED представляет собой JSON-текст, и его можно импортировать даже простым copy-pasteПосле импорта программа сама напишет, каких еще пакетов ей не хватает. Все эти дополнительные пакеты легко установить через менеджер палитры. К примеру, это пакет dashboard, который отвечает за визуальный интерфейс вашего приложения.После того, как всё установлено и настроено, запускаю проект кнопкой Deploy и открываю визуальный интерфейс по адресуhttp://192.168.0.14:1880/ui. Чтобы всё отображалось красиво, включите темную тему в настройках Dashboard.Пока никаких показаний погоды не видно. Почему? Это легко понять, используя узел debug. Ставлю его и сразу по сообщению об ошибке понимаю, что необходим API-ключ.Погодная станция берет данные с сайта прогнозов погодыOpenweathermap. У этого сайта существует API для внешних запросов, то есть формат для общения с другими программами. Зная этот формат, можно получить показания погоды для конкретной географической точки.Чтобы пользователи не перегружали сервер погоды, каждому пользователю выдано строго ограниченное количество запросов, но нам их более чем хватит, у нас запросы будут нечастые. API-ключ у каждого пользователя свой, поэтому пройдите регистрацию на сайте Openweathermap, она бесплатная, и в настройках своего профиля увидите длинную строку с этим ключом. Важно, что в первые несколько часов после регистрации ключ может быть неактивен, тогда нужно подождать.С ключом возвращаемся в Node-RED. В настройках узла Current Weather я прописываю свои параметры: географическое положение (широту и долготу) и свой API-ключ. То же самое проделываю в настройках узла 5 Day Forecast.После чего всё будет работать!Разбор кода погодной станцииТеперь, когда у меня всё запустилось и работает, посмотрим, как же это всё устроено.Код достаточно простой. В узле Current Weather отправляется запрос на сервер погоды. От этого сервера приходит ответ в виде длинной строки в формате JSON.В JavaScript текст в формате JSON преобразуется в программный объект, содержащий поля с данными. Увидеть объект и его содержимое можно, если поставить сразу после узла Current Weather узел Debug. Посмотрим на данные. Они достаточно понятны: здесь есть и скорость ветра, и его направление, и текущая температура с текстовым описанием, и облачность, и многое другое.Дальше в узле Prepare Dashboard Data происходит следующее: мы берем эти данные и обрабатываем, как нам нужно. К примеру, если нам не принципиально знать точное направление ветра в градусах, мы можем преобразовать это в буквенное обозначение по сторонам света - Ю, З, ЮЗ и так далее. Многие из этих данных слишком подробные для повседневного использования. А еще здесь выбираем иконки для отображения на экране. Иконки берутся из готового базового набора для погоды, который уже встроен в NodeRed. Часть показателей здесь я добавила (изначально они не отображались): облачность, влажность, давление. Вы можете добавить что-то на свое усмотрение, все делается полностью по аналогии.А в узле 5 Day Forecast происходит ровно всё то же самое, только запрос делается уже не текущей погоды, а прогноза - на ближайшие часы, либо дни.Что получилось:На первом экране указано время рассвета/заката, и можно обновить данные (вручную послать запрос к серверу погоды)На втором экране текущая погода и прогноз в ближайшие часы, обновляется по таймеруЗаключениеМы рассмотрели программу Termux, и оказалось, что Linux ближе, чем вы думаете. Для мелких задач не обязательно устанавливать целую операционную систему на свой компьютер, вы можете просто иметь Linux у себя в кармане на смартфоне. Мы также установили Node-RED и поняли, что некоторые задачи обработки информации даже не требуют программирования, однако по-прежнему требуют алгоритмического мышления.Если показанного примера вам не достаточно и вы хотите научиться делать полноценные приложения под Android, то вам помогут практические навыки программирования на Java, которые можно получить в«IT Школе Samsung».Ну, а вам домашнее задание — добавьте свои собственные виджеты на панель инструментов так, чтобы отображать важные для вас показатели. Это может быть счетчик просмотров вашего канала на YouTube, или даты ближайших событий в календаре, или расписание электричек. А еще можно дальше развивать погодную станцию. Теперь, когда вы поняли принцип, вы можете подключить свои собственные самодельные или покупные датчики, чтобы отображать показания не только из Интернета, но и прямо из окружающей среды, например, измерять температуру и влажность внутри помещения.Всем пока! Продолжим эту тему в следующем уроке с совершенно другим проектом.Другие статьи этого цикла:Несложные оптические трюки со смартфоном: голограмма и проекторСпектрометр из смартфона, картона и осколка DVD-диска: смотрим на спектры лампочек, фонариков, солнцаНовый год не за горами: делаем супергирлянду на базе ESP и WLED, управляем со смартфонаДелаем физическую лабораторию из смартфона своими рукамиТатьяна ВолковаКуратор образовательных программ Samsung Innovation Campus"
СберМаркет,,,Делаем физическую лабораторию из смартфона своими руками,2022-11-30T17:52:46.000Z,"Всем привет! Продолжаем наш курсGalaxy Upcycling - Новая жизнь старого смартфона, где обсуждаем, как можно повторно использовать устаревший смартфон, иногда - в неожиданном качестве, как физическую лабораторию.Этому, например, посвящен небольшойкурс“Лаборатория в кармане” педагога Анатолия Шперха. В самом деле, современный смартфон оснащен большим количеством сенсоров, способных измерять параметры окружающей среды. Это акселерометр, компас, GPS (тоже можно использовать для измерения скорости), гироскоп, датчик Холла (магнитометр), датчик освещенности, датчик приближения.Я рассмотрю сегодня три урока из приложения для проведения учебных физических экспериментовPhyPhox. Эти уроки относятся к разделу «механика» и задействуют разные сенсоры смартфона:Скорость лифта (барометр и акселерометр)Центрифуга (гироскоп и акселерометр)Свободное падение (микрофон)Надеюсь, что учителя физики найдут в моем видео интересные сценарии для уроков, а все остальные - полюбят физику после такого несложного знакомства с ней!Этот текст основан на моем видео:О приложении PhyPhoxСтавить эксперименты будем в мобильном приложении PhyPhoxhttps://phyphox.org/. Звучит как “Фифокс”. Его название расшифровывается какPhysicalPhone Experiments. Его разработали в Германии, в Рейнско-Вестфальском техническом университете Ахена. Приложение отличается приятным интерфейсом и качественно снятыми видео экспериментов. Недавно они достигли отметку в 3 миллиона скачиваний, и это заслуженный успех.Сильная сторона проекта - методические руководства и даже печатные раздаточные материалы. На русский переведен только интерфейс и короткие описания экспериментов в мобильном приложении. Впрочем, авторы готовы сотрудничать с учителями физики со всего мира, и любой желающий может внести свой вклад в перевод.Чтобы начать,скачайтеприложение напрямую из Google Play Market.Скорость лифтаВидео эксперимента на PhyPhox:https://youtu.be/y-goBtfuXAMСейчас мы увидим, как с помощью датчиков телефона - барометра и акселерометра - измерить скорость лифта и понаблюдать за его ускорением. Для этого эксперимента понадобится телефон, в котором есть барометр – а он есть не во всех моделях.Нужно открыть “Лифт” в приложении, зайти в лифт, положить телефон на пол, нажать “треугольник” (запуск эксперимента) и проехать хотя бы три этажа. Телефон будет измерять три величины:высота (вычисляется из показаний датчика давления поспециальной формуле)вертикальная скорость (определяется через изменение высоты (то есть пройденное расстояние) в единицу времени)ускорение по оси Z (передаётся из показаний акселерометра)Я проехала на лифте с 1 по 15 этаж и вот что получилось.Рассмотрим результаты эксперимента.Первый параметр - высота, вычисленная из показаний барометра. Здесь может быть непонятно, почему она начинается с 0, ведь мы находимся не на уровне моря. Но в документации к эксперименту сказано, что измеряющая программа берет первое измерение за нулевое, видимо для наглядности. На графике видно, что относительная высота за время поездки на лифте изменилась примерно на 44 метра. Учитывая, что один этаж имеет высоту приблизительно три метра в современных стандартных зданиях, а проехала я 15 этажей, то это хорошо коррелирует с реальностью.Но вернемся к барометру и его показаниям. Как узнать, какое у нас на самом деле давление? Его можно измерить отдельно в PhyPhox (для этого откройте раздел «давление» в исходных датчиках). Я измерила, и на первом этаже у меня получилось 987,6 гектопаскалей. Не самая привычная единица измерения, проще перевести ее в миллиметры ртутного столба делением на 1,333. У меня получилось 740 миллиметров ртутного столба. Сравним с показаниями, например, наГисметео. Там сказано, что в день проведения эксперимента в Москве давление составляло 743 миллиметра ртутного столба, то есть в целом всё совпадает. А поднявшись на 15 этаж, я увидела, что давление стало 982 гектопаскаля, выходит, разница между этажами составляет более 5 гектопаскалей.Второй параметр - скорость лифта - считается по уже имеющимся данным высоты и времени. Видно, что скорость лифта начала расти от 0 м/с (лифт не двигался) до примерно 1 м/с, долгое время держалась так, а в конце поездки снова упала до нуля. Соответствует ли этот график реальности? Если вы изучите в Интернете сайты компаний-производителей, то увидите, что как раз 1 метр в секунду - это стандартная скорость пассажирского лифта. Скоростной лифт может разгоняться до 2-4 метров в секунду и такой лифт можно встретить в зданиях высотой от 15 до 30 этажей.Интересно посмотреть на третий параметр - ускорение. В начале поездки оно растёт примерно до 0.5 м/с2 далее большую часть поездки остаётся нулевым, затем меняется на противоположное (минус 0.5 м/с2). Получается, что большую часть поездки мы движемся примерно с постоянной скоростью и нулевым ускорением, то есть равномерно. Поэтому в лифте так комфортно.ЦентрифугаМетатели молота вращают снаряд, ускоряя его. Пока они держат ручку, трос создаёт силу, с которой связано центростремительное ускорение. На фото чемпионка мира Татьяна Белобородова. https://ria.ru/20130816/956848516.htmlВидео эксперимента на PhyPhox:https://www.youtube.com/watch?v=lLCf05Hc83YОчень простой эксперимент, в котором мы проверим справедливость формулыa = r·ω²,где a - это ускорение, м/с2, r - радиус вращения, м, ω (омега) - угловая скорость, рад/с. При помощи датчиков смартфона - акселерометра и гироскопа - мы измерим ускорение и угловую скорость, и посмотрим на соотношение этих двух параметров.Нам понадобится:СмартфонВращающийся предмет. Лучше всего подойдет маленькая компактная центрифуга для сушки овощей и зелени. Если нет такой центрифуги, можно использовать всё, что крутится: велосипедное колесо, офисное кресло, карусель.Полотенце. В него будем заворачивать смартфон, во-первых, чтобы он не повредился, а во-вторых, чтобы не болтался и радиус вращения был постояннымВторое устройство, будь то ноутбук, планшет или смартфон. На нем мы будем смотреть результаты эксперимента, пока смартфон с датчиками крутится. Это не обязательно, но желательно.Здесь нам желательно иметь удаленный доступ, потому что иначе не сможем наблюдать график, пока телефон крутится. Открываем эксперимент под названием “Центростремительное ускорение” и вверху справа, в настройках, ставим галочку на “Разрешить удаленный доступ”. Внизу появится информация об IP-адресе, куда нужно зайти для просмотра эксперимента. Открываем этот адрес в браузере и видим ровно то же, что на экране смартфона. Не забывайте нажать на “треугольник”, чтобы эксперимент стартовал.Заверните смартфон в полотенце, при этом не блокируйте экран. Повращайте его с разной скоростью.Увидите, как заполняются два графика - график ускорения и график зависимости ускорения от квадрата угловой скорости .Здесь интересен второй график - видно, что он представляет собой почти прямую линию, то есть ускорение линейно зависит от квадрата угловой скорости.Если мы посмотрим на формулу,a = r·ω², то увидим, что она соответствует действительности! В нашем случае радиус - константа.Вы можете заметить странные показания при больших значениях ускорения. Точки на графике перестают ложиться на прямую примерно при ускорении 30 м/с²и выше, а это очень много, больше 3 g., как на аттракционе Центрифуга (Гравитрон). Это связано с тем, что сенсоры смартфона не рассчитаны на такие значения, и поэтому их точность падает или, вообще, они начинают выдавать ошибкиСвободное падениеВидео эксперимента на PhyPhox:https://www.youtube.com/watch?v=zRGh9_a1J7sА сейчас перейдем к самому сложному, но и самому творческому эксперименту на сегодня - это “Свободное падение”! Этот эксперимент стал самым популярным среди всех роликов PhyPhox. Здесь нам предстоит повторить опыт Галилея, когда он изучал законы механического движения.Источник: https://bit.ly/3Vi6jzjМы вслед за ним найдем величину ускорения свободного падения. Благодаря тому, что в смартфоне есть микрофон, его можно использовать как акустический секундомер.Нам понадобится:СмартфонЛинейкаМаленький предмет, например, камешек или стеклянный шарик. Я взяла керамзитовый шарик из цветочного горшка.Ручка или карандашРулетка для измерения высотыОткрываем в PhyPhox раздел “Акустический секундомер” и не забываем запустить эксперимент нажатием на треугольник. Кладем линейку на стол так, чтобы ее конец свешивался, и кладем туда камешек. Ударяем авторучкой по линейке так, чтобы камешек упал на пол. Мы будем регистрировать микрофоном телефона два звука: звук выбивания линейки и звук падения камешка. Повторим этот эксперимент несколько раз, записывая высоту и зафиксированное время падения.Чтобы грамотно провести эксперимент, придется набрать достаточно измерений, равномерно распределенных в доступном вам диапазоне примерно до 2-3 метров. Попробуйте проявить фантазию и подумать, какие из поверхностей доступны вам. Подойдут столики, шкафчики, полки. Чтобы получить измерение на высоте 2 метра, мне пришлось использовать дверь: высота стандартных дверей в квартирах как раз составляет два метра.В результате у меня получилась таблица:В третьем столбце я посчитала ускорение из имеющихся данных. Мы здесь имеем дело с прямолинейным равноускоренным движением, и соответствующая формула выглядит так:где v0 – начальная скорость, м/с, t – время, с, a – ускорение, м/с2, s – пройденный путь, м. Поскольку начальная скорость v0 здесь равна нулю, то формула упрощается, и из нее уже просто вывести формулу для ускорения так, чтобы убрать параметр скорости (которую мы не измеряли):По полученной формуле, взяв имеющиеся у меня данные, я поделила 2S на квадрат времени, и у меня получился последний столбец с ускорением.Известно, что ускорение свободного падения - это константа и оно равно в среднем 9,81 м/с². Смотрим столбец с экспериментальным расчетом ускорения и видим, что да, ускорение постоянно и примерно равно этой величине! Причем чем больше высота падения, тем меньше ошибка.Откуда здесь берется погрешность? Во-первых, понятно, что здесь есть сопротивление воздуха - та самая причина, по которой перышко и железный шарпадаютс разной скоростью. Оно мешает телу разогнаться, то есть снижает ускорение. Во-вторых, скорость звука - мы ее здесь не учитываем (Звук, имеющий скорость 343 м/с, достигает микрофона в 2 метрах от источника за 5-6 миллисекунд). Наконец, есть погрешность самого измерительного прибора.Пусть вас не разочаровывает, что мы нашли величину g с ошибкой. В конце концов, сам Галилей считал, что значение g в два раза меньше истинного, но у него даже и секундомера не было. Мы тоже поставили свой эксперимент в очень простых условиях. Главное, что мы увидели - ускорение свободного падения постоянно, и это подтверждается нашими данными!На этом всё на сегодня! Пишите в комментариях, удалось ли вам повторить какой-либо эксперимент, что нового вы узнали, и стала ли физика вам ближе.Другие статьи из этого цикла:Несложные оптические трюки со смартфоном: голограмма и проекторСпектрометр из смартфона, картона и осколка DVD-диска: смотрим на спектры лампочек, фонариков, солнцаНовый год не за горами: делаем супергирлянду на базе ESP и WLED, управляем со смартфонаТатьяна ВолковаКуратор образовательных программ Samsung Innovation Campus"
СберМаркет,,,"Искусственный интеллект не создаст за вас крутую команду разработчиков, или Как мы недооцениваем наём джунов",2024-09-28T16:36:34.000Z,"Писать код — несложно, сложно писать хороший код.Когда мне было 19 лет, я бросила колледж и переехала в Сан-Франциско. Там я должна была выйти на работу сисадмином Unix в Taos Consulting. Но я не успела приступить — меня тут же переманил один стартап, где я стала работать разработчиком почтовых подсистем.Я всегда знала, что смогу найти работу. Предложений было множество, и что ещё важнее, требования работодателей не зашкаливали. Если ты умел выполнять sling для HTML или работать с командной строкой, рано или поздно находились желающие платить тебе зарплату.Была ли я гением, родившимся в обнимку с компьютерной клавиатурой? Конечно, нет. Я жила на задворках Айдахо и училась дома. Впервые прикоснулась к компьютеру в шестнадцать лет, когда пошла в колледж. Попала в университет благодаря стипендии по программе классического фортепиано, чуть позже разменяв его на вереницу гуманитарных дисциплин: классический греческий и латынь, теория музыки, философия. Всё, что я узнала о компьютерах, я узнала на работе, будучи системным администратором на кафедре информатики в университете.Вспоминая сейчас всё это, я понимаю, что попала в ИТ в очень удачный момент. Меня бросает в дрожь при мысли, что было бы, если бы я начинала позже на несколько лет. Все карьерные лестницы в ИТ, по которым взбирались я и мои друзья, давно развалились.ИТ взрослеетВ какой-то степени любая профессиональная отрасль взрослеет по мере своего развития. В самом начале любая сфера деятельности похожа на Дикий Запад — низкие ставки, законов нет, а стандарты в зачаточном состоянии. Даже если взять историю зарождения не айтишных отраслей — медицины, кинематографа или радио — сходство в развитии будет поразительным.У каждой молодой технологии есть такой волшебный момент, когда границы между ролями размыты, и каждый, кто достаточно мотивирован, любознателен и готов пахать, может захватить контроль над этой технологией.Но это очень скоротечный момент. Так и должно быть. Требования к объёмам предварительных знаний и опыту работы у новичков в отрасли растут как на дрожжах. Ставки повышаются, цели становятся более масштабными, резко возрастает цена ошибки. Появляются сертификаты, обучающие курсы, стандарты, правовые нормы. Мы начинаем ожесточённо спорить, можно ли считать разработчиков инженерами.Разработка ПО — это отрасль постоянного ученичестваСегодня мало кто захочет доверить график ротации дежурных сотрудников какому-нибудь подростку без диплома. Возросли требования к предварительным знаниям, которые нужны для старта в отрасли, темпы ускорились, а на кону стоит гораздо больше. Сейчас уже нельзя научиться буквально всему на рабочем месте, как когда-то делала я.Но и в университете заранее всё не выучишь. Профильный диплом поможет подготовиться скорее к исследованиям в ИТ-области, чем к каждодневной работе разработчика.Возможно, более практичная дорожка в ИТ — это хороший программистский буткемп, оттачивающий практические навыки решения задач и работы с современным инструментарием. В любом случае вы не столько учитесь, «как делать конкретную работу», сколько «изучаете основы, чтобы понимать и использовать инструменты, которыми нужно пользоваться, чтобы изучить работу».Для создания программ нужно постоянно практиковаться. Нельзя стать разработчиком, просто читая учебники. Научиться можно только на практике. Нужно делать, делать и ещё раз делать. Какое бы образование вы ни получили, большую часть знаний вы всё равно получите на работе. Причём учиться будете всё время. Учиться и учить нужно всю жизнь. В отрасли, которая меняется так быстро, — не может быть по-другому.Чтобы стать компетентным разработчиком, нужно лет семь, а то и больше. Чаще всего специалиста такого уровня называют «ведущим разработчиком», проще говоря, «сеньором». Для этого много лет нужно писать, ревьюить и деплоить код каждый день, в команде с куда более опытными разработчиками. Вот примерно сколько лет уходит на профессиональное становление.Что значит быть сеньоромЧасто слышу возмущённые возражения по поводу сроков, например:«Семь лет?! Да мне хватило два года!»«А я стал сеньором всего через четыре с половиной года!»Рада за вас. Нет, не то чтобы семь лет — это какая-то волшебная цифра. Но чтобы дорасти до зрелого разработчика, который станет якорем для своей команды, нужны время и опыт. Более того, для этого нужна практика.Мы стали называть сеньорами разработчиков, которые могут подготовить код и выйти в плюс в плане продуктивности. По-моему, это огромная ошибка. Вроде как получается, что другие разработчики работают менее продуктивно, а это не так. Мы упускаем из виду истинную природу разработки ПО, в которой написание кода занимает лишь малую часть.Для меня концепция сеньора в основном не завязана на способности писать код. Для сеньора скорее важно умение понимать, поддерживать, объяснять и управлять большим корпусом ПО в продакшне на протяжении долгого времени, а также способность трансформировать потребности бизнеса в технические решения. Огромная часть работы строится вокруг создания и поддержки больших, сложных социотехнических систем, а код — всего лишь одно из представлений этих систем.Так каково же это —быть сеньором? Это значит, что вы, прежде всего, научились учиться и учить; держать в голове эти модели и рассуждать о них, и можете на протяжении долгого времени работать с этими системами, знаете, как поддерживать и расширять их. Это значит, что вы здраво мыслите и можете доверять своим инстинктам.И это плавно подводит нас к вопросу об ИИ.Перестаньте разрушать собственное будущееОчень, действительно очень трудно впервые устроиться на работу разработчиком.Я не осознавала, насколько это трудно, пока моя младшая сестра не начала искать работу. Недавней выпускнице, отличнице, с небольшим практическим опытом, трудолюбивой и дружелюбной, понадобилось два (!) года, чтобы закрепиться на работе в своей сфере. Это было несколько лет назад. Как ни парадоксально, с тех пор устроиться на работу стало ещё сложнее.В прошлом году мне из раза в раз попадались статьи о том, чтов разных отраслях работу начального уровняпостепенно заменяет искусственный интеллект. В ряде случаев это начинание имеет свои плюсы. Если работа представляет собой рутинные действия: например, нужно преобразовать формат документа, прочитать и законспектировать множество текстов или заменить один набор иконок на другой — такие задачи действительно могут перейти к ИИ. Я не воспринимаю это как революцию — это просто продолжение тренда автоматизации, которая теперь справляется и с текстовыми материалами, и с математическими задачами.Но, похоже, недавно некоторые руководители ИТ-компаний и так называемые идейные лидеры искренне поверили в то, что генеративный ИИ (GenAI) вот-вот сможет взять на себя работу, которую сейчас делают джуны. Я прочитала немерено статей о том, что работу джунов-разработчиков полностью автоматизировали или что потребность в джунах стремится к нулю. Меня это просто сводит с ума.Всё это свидетельствует о глубоком непонимании того, чем, собственно, занимаются разработчики.Не нанимая и не обучая джунов, мы разрушаем собственное будущее.Это надо остановить.Писать код — это самое простоеПохоже, люди считают, что написать код — это самое сложное в разработке ПО. А вот и нет. Это не было и не будет самой сложной частью работы. Наоборот, это самые простые задачи в работе разработчика, и они будут становиться проще и проще.Самое трудное — это то, что вы делаете с кодом: как вы с ним обращаетесь, насколько вы его понимаете, расширяете и как вы им управляете на протяжении всего его жизненного цикла.Для начала джун учится писать строки, функции и сниппеты кода, выполнять их отладку. Он накапливает опыт и постепенно дорастает до сеньора. В это время он учится составлять из программ целые системы и управлять ими с помощью этапов изменений и преобразований.Социотехнические системы состоят из программного обеспечения, инструментов и людей. Чтобы в них разбираться, необходимо понимать взаимодействие между ПО, пользователями, продакшном и непрерывными изменениями. Это фантастически сложные системы, которым присущи хаос, нон-детерминизм и непредсказуемое поведение. Если кто-то утверждает, что он хорошо разбирается в системе, которую он разрабатывает и с которой работает, то либо это очень маленькая система, либо (что более вероятно) ему не хватает знаний понять, что именно он не знает. Код — вещь простая, а вот системы — сложная.Нынешняя волна инструментов генеративного ИИ открыла для нас возможность писать много кода, причём очень быстро. Простые задачи с удивительной скоростью становятся ещё проще. Но эти инструменты никоим образом не помогают понять код и работать с ним. Более того, с ними трудные задачи становятся ещё труднее.Писать код — несложно, сложно писать хороший кодВозможно, если вы читаете разные аналитические статьи, в вашем воображении разработчики задорно сочиняют промпты для ChatGPT или генерируют тонны кода в Copilot, а потом беззаботно отправляют коммит этого добра в GitHub и идут домой. Но реальность выглядит иначе.Правильнее было бы относиться к инструментам вроде Copilot, как к прикольной функции автодополнения или копипаста или как к детищу порочной страсти результатов поиска Stack Overflow и гугловского «Мне повезёт!». Каждый раз идёшь ва-банк.Эти инструменты полезнее всего, когда уже есть параллель в файле, и нужно просто скопировать что-то с небольшими изменениями. Или когда вы пишететесты, и у вас есть гигантский блок повторяющихся файлов YAML, а в нём повторяется схема со вставкой нужных названий столбцов и файлов, как в автоматическом шаблоне.Но сгенерированному коду нельзя доверять. Я не устаю повторять это снова и снова.Код, сгенерированный искусственным интеллектом, выглядит вполне вразумительно, но даже когда он вроде как «работает», он редко согласуется с тем, что вам от него нужно.ИИ бодренько генерирует код, который нельзя нормально спарсить или скомпилировать. Он создаёт переменные, названия методов, вызовы функций; он галлюцинирует и выдумывает несуществующие поля. Сгенерированный код не соблюдает ваши практики или стандарты программирования. Искусственный интеллект не собирается выполнять рефакторинг или подбирать для вас интеллектуальные абстракции. Чем важнее, сложнее или осмысленнее этот фрагмент кода, тем менее вероятно, что вы получите от ИИ годный артефакт.Возможно, вам удастся сэкономить время, не создавая код с нуля. Но вам придётся проверить результат строчка за строчкой и только потом закоммитить его или отправить в продакшн. Во многих случаях это занимает времени не меньше, чем само написание кода — особенно в наши дни, когда у нас в распоряжении такие продвинутые возможности автодополнения. Чтобы код, сгенерированный искусственным интеллектом, привести в соответствие с остальной базой кода, иногда нужно проделать ОГРОМНУЮ работу. Откровенно говоря, часто овчинка выделки не стоит.Не так уж сложно создать код, который можно компилировать, исполнять и успешно протестировать. Гораздо труднее смастерить базу кода, которая ещё много лет будет пригодна для того, чтобы с ней работали, изменяли её и рассуждали о ней отдельные люди, целые команды и будущие поколения команд.Как разработчики на самом деле используют генеративный ИИТакова правда жизни: можно очень быстро генерировать много кода, но доверять такому результату нельзя. Категорически. Но всё же бывают сценарии использования, в которых генеративный ИИ неизменно блистает.Например, можно попросить ChatGPT сгенерировать образец кода с использованием незнакомых API. Это проще, чем читать документацию по API — в конце концов, корпус обучался на репозиториях, в которых API использовались для реальных рабочих нагрузок.Удобно использовать генеративный ИИ для создания узкоспециализированного или понятного кода, писать который противно или утомительно. Чем предсказуемее сценарий, тем лучше эти инструменты справятся с написанием кода.Генеративный ИИ спокойно справится с задачей, если вам нужно эффективно копировать и вставлять данные в шаблон — как если бы вы каждый раз генерировали нужный код с использованием sed/awk или макроса vi.А ещё он отлично справляется, когда нужно написать небольшие функции, делающие что-то на языках или в сценариях, с которыми вы не знакомы. Если у вас есть сниппет кода на Python, а вам нужно получить то же самое на Java, с которым вы не работаете, ИИ подстрахует.Опять же, не забывайте, шансы 50/50, что результат может получиться совершенно от балды. До проверки результатов вручную всегда нужно исходить из предпосылки, что ИИ наделал ошибок. И всё же, эти инструменты, так или иначе, помогают ускорять работу.Генеративный ИИ чем-то напоминает джунаМой коллега Кент Квирк говорит: «ИИ — это шустрый джун-разработчик с очень высокой скоростью набора текста». Очень яркий образ, бьёт прямо в точку.Генеративный ИИ — в каком-то смысле действительно джун, потому что его код нельзя взять и с ходу выкатить в продакшн. Вы несёте за него ответственность — юридическую, этическую и практическую. Вам всё равно нужно тратить время, чтобы понять и протестировать, что он сделал, стилистически и тематически доработать результаты, чтобы они вписывались в вашу базу кода, а коллеги могли понять и поддерживать этот код дальше.На самом деле, это вполне уместное сравнение, но только если у вас автономный код одноразового использования, то есть если вы не планируете встраивать его в более крупный проект и если другим разработчикам не придётся читать и модифицировать его.Да, в отрасли есть и такие направления: для них подходит разовый технологический код, который никто не будет читать. Есть агентства, которые каждый год штампуют десятки одноразовых приложений, создаваемых для конкретного маркетингового мероприятия. После него этот код никому не нужен. Но к большинству программ такой подход не относится.Одноразовый код — это редкость; код, который должен прослужить многие годы — норма.Даже когда мы планируем использовать код однократно, часто оказывается, что мы ошибались.Но генеративный ИИ — не часть вашей командыНа код, который генерирует GenAI, нельзя положиться, в этом плане он действительно как джун. Но это единственный момент, в котором это сравнение верно. Потому что взять в команду человека, который пишет код — совершенно не то же самое, что создавать код автоматически. Такой код можно получить откуда угодно: из Stack Overflow, Copilot, мало ли откуда ещё. Да и это даже неважно. В таком случае у вас нет циклов обратной связи, на другом конце провода нет человека, который постепенно учится и работает над собой, нет влияния на вайб и культуру вашей команды.Констатирую максимально очевидную вещь: дать джуну обратную связь по проверенному коду — это совершенно не то же самое, что править сгенерированный код.Ваши усилия окупаются, когда вы вкладываете их в профессиональный рост другого человека. Это возможность передать кому-то уроки, которые вы сами извлекли за годы работы. Сама необходимость сформулировать обратную связь, чтобы донести свою мысль до другого человека, вынуждает глубже осмыслить проблему. В каком-то смысле это помогает вам лучше разобраться в материале.Когда у вас появляется новый джун, динамика команды сразу меняется. Возникает атмосфера, в которой задавать вопросы — это нормально и даже необходимо, а обучение — это постоянный процесс. Мы больше говорим о динамике команды в моменте.Время, которое вы тратите, чтобы подтянуть джуна, иногда окупается неожиданно быстро. Время летит ☺️ Когда мы нанимаем сотрудников, мы часто переоцениваем сеньоров так же сильно, как и недооцениваем джунов. От стереотипов пользы мало.Мы недооцениваем стоимость найма сеньоров и переоцениваем стоимость найма джуновЛюдям кажется, что сеньора можно закинуть в команду и он сразу же будет работать продуктивно, а только что нанятый джун будет бесконечно тянуть команду на дно. И то и другое — неправда. На самом деле большая часть работы, с которой сталкивается большинство команд, не так уж трудна, если разделить её на составные части. Она всегда предоставляет начинающим разработчикам пространство для профессионального роста.Вот упрощённый взгляд на вещи вашего бухгалтера: «Зачем платить $100k джуну и снижать общую скорость работы, если можно платить $200k сеньору, чтобы дела шли быстрее?» Это же бессмыслица!Но и я, и вы, и каждый неравнодушный разработчик знаем: разработка устроена иначе. Это ориентированная на практику отрасль, и здесь продуктивность определяется результатами и пропускной способностью каждой команды, а не отдельно взятого сотрудника.Человек может по-разному влиять на скорость работы команды. Впрочем, существуют и разные способы вытягивать из команды энергию, создавать помехи для работы и для всех вокруг. Это не всегда связано с личностью человека (по крайней мере, не в том виде, как принято считать), и написание кода — это только один из критериев, влияющих на скорость работы.Каждому разработчику, которого вы берёте на работу, нужно время и помощь с адаптацией, прежде чем его работа начнёт приносить ощутимый вклад в общее дело. Нанимать и обучать разработчиков — дорого, независимо от их уровня. Любому сеньору нужно время, чтобы сформировать в уме представление о системе, ознакомиться с инструментами и технологиями и выйти на нужную скорость. Сколько времени? Это зависит от наличия у сеньора опыта работы с вашими инструментами и технологиями, от того, насколько хорошо и понятно организована ваша база кода, налажен процесс онбординга, и от других факторов. В целом эта адаптация растягивается на 6–9 месяцев. Не исключено, чтосеньор выйдет на свою полную мощьтолько где-то через год.Да, джун будет набирать обороты дольше. И да, команде придётся вложить в него больше сил. Но это не навсегда. Джуны выйдут в плюс примерно за то же время: от шести месяцев до года. А вот развиваются они гораздо быстрее, чем их более опытные коллеги. Не забывайте, что вклад джуна в работу может быть значительно больше, чем просто написанный ими код.Необязательно быть сеньором, чтобы создавать ценностьПо моим наблюдениям, продуктивнее остальных с написанием кода и созданием разных функций справляются разработчики среднего уровня. Они ещё не погрязли в совещаниях, кураторстве, наставничестве, рекламе иархитектуре. Их рабочее время ещё не рябит переключениями на другие задачи, так что они могут спокойно заниматься программированием. Придя в офис, они первым делом надевают наушники, весь день пишут код и вечером уходят домой, проделав огромную работу.Мидлы пребывают в этом приятном временном состоянии, когда они уже набили руку на программировании и могут работать очень продуктивно, но всё ещё учатся, как создавать и поддерживать системы. Единственное, чем они занимаются — это создание тонн кода.И делают это с большим энтузиазмом. Они кайфуют от этого! Им не скучно в тысячный раз писать веб-форму или страницу для авторизации пользователей. Для них всё ново, всё интересно. Как правило, это значит, что они будут работать лучше, особенно под ненавязчивым руководством более опытного коллеги.Мидлы в команде — это изумительно. Но единственный способ заполучить их — взять на работу джунов.Команда, в которой есть мидлы и джуны, вырабатывает отличный иммунитет против чрезмерного и преждевременного усложнения решений. Они ещё не углубились в проблему настолько, чтобы точно представлять все пограничные случаи, которые им надо предусмотреть. У них всё просто, а ведь именно простоты добиться очень трудно.Аргументы в пользу найма джунов в долгосрочной перспективеЕсли спросить, то почти каждый искренне согласится, что нанимать джунов — это хорошее дело… Но этим должен заниматься кто-то другой. Всё потому, что в долгосрочной перспективе аргументы в пользу джунов звучат убедительно и понятно:Нам нужно больше сеньоров в отрасли.И кто-то должен их вырастить.Работа джунов дешевле.Они могут привнести столь необходимое разнообразие.Джуны часто очень преданы компаниям, которые согласились тратить силы и время на их обучение, и годами сидят на одном месте.Мы уже упоминали, что кто-то должен взять это на себя?Но мыслить на перспективу — не самая сильная сторона компаний в частности и капитализма в целом. Если ставить вопрос таким образом, получается, что взять джуна на работу с вашей стороны — это акт благотворительности, и он обойдётся вам очень дорого. А компаниям хочется, чтобы эти расходы взял на себя кто-то другой. Это и приводит нас в точку, в которой мы сейчас находимся.Аргументы в пользу найма джунов в краткосрочной перспективеНо и в краткосрочной перспективе есть доводы в пользу найма джунов — эгоистичные и сугубо практичные аргументы, почему это выгодно для команды и компании. Нужно просто немного сместить фокус внимания с отдельных личностей на целые команды.Давайте начнём вот с чего:нанять разработчика — не значит «выбрать лучшего человека для выполнения этой работы». Наём разработчиков — это про создание команд. Наименьшая единица владения программным продуктом — это не отдельный человек, а команда. Только команды могут владеть, создавать и поддерживать костяк программного обеспечения. По сути, это совместная, кооперативная деятельность.Если бы под наймом разработчиков подразумевался выбор только «лучших», стоило бы нанимать самых продвинутых и самых опытных специалистов, которых вы можете получить за имеющиеся у вас деньги, — здесь под «сеньором» и «опытным разработчиком» мы понимаем «самого продуктивного» сотрудника. Сомнительно, но окей. Но ведь продуктивность отдельного сотрудника — это не то, что нам нужно оптимизировать. Что на самом деле важно оптимизировать — так это продуктивность команды.Лучшие команды — это те, где сочетаются разные сильные стороны, точки зрения и уровни профессионализма. Монокультура может быть невероятно успешной в краткосрочной перспективе — иногда она даже превосходит разнообразие в команде. Но монокультура плохо масштабируется и не способна гибко адаптироваться к новым вызовам. Чем дольше вы отказываетесь от многообразия, тем сложнее будет навёрстывать.На работу нужно брать джунов, причём не один раз, а постоянно. Нужно укомплектовывать воронку кадрами с нижнего уровня. Джуны остаются джунами всего пару лет, а мидлы превращаются в сеньоров.Суперсеньоры — не самые подходящие кандидатуры для наставничества над джунами; лучше всего с этим обычно справляются сотрудники, которыестоят на ступеньку вышеи ещё хорошо помнят, каково это — быть новичком.В здоровой высокоэффективной команде есть сотрудники разных уровнейЗдоровая команда — это экосистема. Вы же не составляете команду по разработке продукта из шести спецов по БД и одного мобильного разработчика. Не стоит также формировать команду из шести сеньоров и одного джуна. Хорошая команда включает специалистов с разными навыками и грейдами.Вы когда-нибудь работали в команде, состоящей исключительно из ведущих или главных инженеров-программистов? Не то чтобы там было весело. Это неэффективная команда. Объём работы по созданию и планированию высокоуровневой архитектуры ограничен, как и количество больших решений, которые нужно принимать. Ведущие инженеры бо́льшую часть времени занимаются скучной и рутинной работой, так что они склонны слишком усложнять решения либо, наоборот, слишком всё упрощать, а иногда и то и другое одновременно. Они соревнуются друг с другом ради фана и придумывают поводы для технических стычек. Они вечно что-то не задокументируют и недостаточно вкладываются в то, что делает системы простыми и управляемыми.Команды, в которых работают или только мидлы, или только новички, или только сеньоры, будут страдать разными патологиями и всегда иметь схожие проблемы с конкуренцией и слепыми зонами. Сама работа имеет широкий диапазон трудностей — от простых, узкоспециализированных задач до важных и рискованных архитектурных решений. Логично, если у выполняющих эту работу людей будутразные уровни компетентности.В лучших командах никто не скучает, потому что каждый сотрудник работает над задачей, которая ему интересна, — таким образом он раздвигает для себя границы возможного. Единственный способ добиться этого — когда в команде работают люди с разным уровнем навыков.Узкое место компании — это наём, а не обучение сотрудниковУзкое место, с которым мы сталкиваемся сейчас — это не обучение новых джунов и формирование у них нужных навыков. Проблема не в том, что джуны недостаточно стараются; я встречала много добротных, благоразумных советов на эту тему. Но так проблему не решить.Затык в том, что джунов не берут на работу. Всё дело в компаниях, которые относятся к джунам, как к издержкам, которые хочется переложить на кого-то другого, а не как к инвестициям в будущее своей компании.После первого места работы разработчику, как правило, несложно трудоустроиться. Но найти первую работу — это смертоубийство. Это почти невозможно. Если только вы не окончили один из лучших вузов страны и не попали на стажировку в Big Tech, то это лотерея чистой воды, вопрос удачи или личных связей. Это было тяжело ещё до того, как химера «искусственный интеллект может заменить джунов» вылезла из болота. А уж теперь-то... Вообще ужас.Где бы вы были, если быне попали в ИТ в своё время?Я знаю, где была бы я, и это точноне здесь.В интернете любят высмеивать бумеров — поколение, которое по инерции отучилось в вузе, напокупало недвижимости и вышло на пенсию, а потом легонечко утянуло лестницу, по которой они сами взбирались, и теперь дразнит молодёжь «неженками». Мем «Ок, бумер», возможно, останется с нами. Но, может, мы хотя бы попытаемся не допустить, чтобы фраза «Ок, ведущий инженер» стала нормой?Никто не считает, что нам нужно меньше сеньоровМногим кажется, что нам не нужны джуны. Но никто не считает, что нам нужно меньше сеньоров или что нам понадобится меньше сеньоров в обозримом будущем.Думаю, можно смело предположить, что всё, что можно детерминировать и автоматизировать, в конечном счёте будет автоматизировано. Разработка программного обеспечения — не исключение: мы же нулевая точка. Ещё бы, мы всегда ищем способы автоматизировать и повысить эффективность чего угодно — собственно, этим мы и должны заниматься.Но большие программные системы недетерминированы, плохо поддаются прогнозированию и страдают непредсказуемым поведением. Уже сам факт наличия пользователей вносит в систему хаос. Компоненты можно автоматизировать, но сложность можно только регулировать.Даже если бы нам удалось полностью автоматизировать системы и передать их управление искусственному интеллекту, тот факт, что мы не понимаем, как ИИ принимает решения, — это огромная, возможно, непреодолимая проблема.Строить бизнес на системе, которую люди не могут ни понять, ни отладить, — это такой экзистенциальный риск, что никакая юридическая, финансовая или отвечающая за безопасность команда никогда под этим не подпишется.Может быть, когда-то нечто подобное и наступит, но из сегодняшнего дня такое будущее ещё не разглядеть. Если бы мне предложили поспорить об этом, я бы не поставила на кон свою карьеру или компанию.А пока что нам нужно больше сеньоров. Но единственный способ заполучить их — это подправить карьерную воронку, из которой они потом вырастают.Должна ли каждая компания брать на работу джуновНет. Компания должна быть в состоянии обеспечить джунам успех.Признаки, которые говорят о том, что вашей компании НЕ нужно нанимать джунов:Вы существуете меньше двух лет.У вашей команды вечный аврал, или у вас в системе не бывает затишья.У вас нет опытных руководителей или есть плохие руководители, или вообще нет начальства.В компании нет графика разработки продукта.У вас в команде никто не хочет отвечать за джунов и быть ихнаставником.Хуже отказа брать на работу джунов может быть только первая работа, на которой джуну нечему научиться. Найти вторую работу настолько проще, чем устроиться на первую, что, думаю, большинство джунов предпочли бы откровенно паршивую первую работу, чем вообще никакой.Работа на полной удалёнке — не то чтобы камень преткновения, который совсем нельзя обойти, но она заметно осложняет ситуацию. Я бы посоветовала джунам для начала всё-таки искать работу в офисе. Когда пропитываешься повседневными разговорами и технической болтовнёй в курилке, учишься намного быстрее. На удалёнке вы этого лишены. Если вы всё-таки работаете из дома, не забывайте, что компенсировать нехватку такого общения придётся более усердной работой. Я советую вам обращаться за советами к тем,кому это удалось(да-да, такие тоже есть).А компаниям рекомендую не начинать с приёма на работу одного джуна. Если вы планируете нанять джуна, берите сразу двух-трёх человек. Пусть каждый из них ощущает, что он не один, тогда ситуация для него будет менее стрессовой.Никто не придёт и не решит за нас наши проблемыЯ пришла к выводу, что единственный способ раз и навсегда изменить ситуацию — это взять дело в свои руки. Разработчики и их начальство должны продолжить борьбу и сделать эту битву личной.Насколько я знаю, в большинстве компаний, в которых есть программа найма и обучения новичков, она есть только потому, что разработчики решили за неё бороться. Именно разработчики, реже их начальники, доказали, почему эта программа нужна, нашли для неё ресурсы, разработали саму программу, проводили собеседования и нанимали джунов, а потом выделили им наставников. Это не какой-то экзотический проект. Такая программа вполне по силам большинству мотивированных, опытных разработчиков. И, кстати, для их карьеры это тоже хорошо.Финансовый отдел не станет лоббировать такую программу. Руководство компании не вступится. Чем больше должность подразумевает отношение к разработчикам, как к взаимозаменяемым ресурсам, тем менее вероятно, что они осознают важность такого подхода.ИИ не придёт и не решит за нас все наши проблемы, и весь код за нас не напишет. А если бы и написал, это не имело бы значения. Написание кода — это лишь малая часть работы профессиональных разработчиков, и, пожалуй, самая простая её часть.Только мы в контексте и обладаем авторитетом, необходимым для внедрения изменений. И только мы знаем, что эти изменения — краеугольный камень для создания отличных команд и профессионального роста.Великие разработчики вырастают в великих командах. Никто не знает это лучше разработчиков и их руководителей. Пора брать дело в свои руки.Чтобы расти, нужно выйти из привычной зоны и сделать шаг к переменам. Можно изучить новое, начав сбесплатных занятий и гайдов:Как начать работать с нейросетямии создать свой ChatGPT;За кем будут охотиться работодатели:профессии, актуальные в 2030 году;Делегирование: как руководителю научиться ставить задачи;5 шагов, которые помогут найти работу после обучения;Soft skills: ключевые навыки для успешной карьеры.Или открыть перспективы и получить повышение спрофессиональным обучениеми переподготовкой:Нейросети для каждого: как решать рабочие задачи быстрее;DevOps-инженер;Mini-MBA:Управление проектами и командами в IT;HR-аналитика и автоматизация;Deep Learning."
СберМаркет,,,С 80-х по 2024-й: как создавались и оптимизировались CI-тесты,2024-09-24T17:20:51.000Z,"Современные команды разработки тестируют каждое изменение кода перед мержем. Это не просто общепринятая традиция: наряду с ревью кода, это стандарт по умолчанию, применяемый практически во всех кодовых базах компаний. Мы называем его тестами CI (непрерывной интеграции). В результате их внедрения среднестатистическая организация запускает сотни наборов тестов в день.В прошлом непрерывное интеграционное тестирование было с нами не всегда, в отличие от обычного тестирования. По моим наблюдениям, CI — это результат того, что тестирование всё больше ускоряется. Разберёмся, как это произошло и как тестирование будет ускоряться дальше.Самый медленный способ тестирования кода — чтениеВ 1980-х годах тестирование программного обеспечения было медленным. Бо́льшая часть тестирования была сосредоточена прежде всего напоиске возможных ошибокв коде. Майкл Фейган популяризировал «инспекции Фейгана», во время которых группы разработчиков просматривали распечатки кода в поисках ошибок. Этот процесс отнимал много времени, выполнялся вручную и больше походил на интенсивное код-ревью, чем на то, что мы сегодня считаем тестированием программного обеспечения.В 1990-х годах стали более распространёнными юнит-тесты программного обеспечения. Но некоторое время юнит-тестыписалив основном специализирующиеся на этом тестировщики ПО с использованием собственных инструментов и методов компании. Некоторые считали, что у авторов исходного кода могут быть слепые пятна при тестировании собственного кода, и, честно говоря, мы по-прежнему не доверяем разработчикам проверять изменения в их собственном коде по аналогичным причинам.Тогда тесты запускали нечасто по двум причинам: во-первых, их не всегда писали сами авторы ПО, во-вторых, тесты могли выполняться медленно. Компьютеры того времени работали медленнее, и в зависимости от сложности тестов выполнение одного набора тестов могло занимать часы или даже целый день. Если же тесты писал отдельный разработчик, а набор тестов не запускался или не прогонялся до следующего вечера, могло пройти несколько дней, прежде чем разработчик узнавал, почему его изменение ломает сборку.Периодическое самостоятельное тестированиеОпубликованная в 1999 году книга Кента Бека «Экстремальное программирование» (XP) помогла изменить культуру разработки ПО. Разработчиков поощряли писать небольшие изолированные тесты для каждой новой части кода, которую они вносили.По мнению адептов XP, программисты могли научиться быть эффективными тестировщиками, по крайней мере на уровне отдельных частей кода, тогда как отдельная группа тестировщиков безнадёжно замедлила бы цикл обратной связи, который обеспечивали тесты. Важную роль здесь сыграл xUnit: он был разработан специально, чтобы свести к минимуму потери времени для программистов, пишущих тесты.Авторы кода начали писать собственные тесты. Благодаря этому новый код стал тестироваться чаще, чем на этапе интеграции. Ускорение тестирования привело к тому, что разработчики начали быстрее получать обратную связь. Но самостоятельное тестирование было добровольным, приходилось полагаться на то, что авторы тщательно выполняют локальные тесты перед мержем. Более того, успех теста зависел от локального компьютера автора, а не от некоторого эталонного сервера. Кодовые базы всё равно могли сломаться при следующей сборке и выполнении наборов тестов.Запуск автоматизированного сервера для тестирования измененийВ то время как Google началавтоматизировать свои тестысборок в 2003 году, отрасли разработки ПО в целом потребовалось немного больше времени, чтобы сделать то же самое. Но автоматизация была крайне необходима.Программные системы становятся больше и сложнее. Что ещё хуже, новые версии доставляются пользователям часто, иногда несколько раз в день. Это очень далеко от мира «коробочного ПО», которое обновляется лишь один или два раза в год.Способность людей вручную проверять каждое поведение системы не успевает за ростом числа фич и платформ в большинстве программ.Разработчики программного обеспечения в GoogleРазработчик Sun Microsystems Косукэ Кавагучи сыграл ключевую роль в открытии новой эры тестирования. В 2004 году он создалHudson, позже переименованный в Jenkins в результате конфликта с Oracle. На своей основной работе Косукэ «устал навлекать на себя гнев своей команды каждый раз, когда его код ломал сборку». Он мог бы вручную запускать тесты перед каждым внесением кода, но вместо этого Косукэ поступил как типичный программист: создал автоматизированную программу. Инструмент Hudson действовал как долгоживущий тестовый сервер, который мог автоматически проверять каждое изменение кода по мере его интеграции в кодовую базу.Косукэ открыл исходный код Hudson, и тот стал пользоваться огромной популярностью. Началось первое поколение автоматизированных тестов непрерывной интеграции, и впервые стало обычным делом тестировать каждое изменение кода по мере его написания. Быстро развернулись подобные инструменты, например Bamboo и TeamCity, но опенсорсный Hudson был популярнее.Платить кому-то другому, чтобы автоматически тестировать измененияК концу 2000-х годов хостинг кода переместился в облако. Раньше команды запускали собственные серверы Subversion для размещения и интеграции изменений кода, теперь же всё больше людейстали размещатьсвой код на GitHub. Тесты непрерывной интеграции последовали этой тенденции и также переместились в облако: в 2011 году появились такие продукты, как CircleCI и Travis CI. Теперь и небольшие компании могли передавать на аутсорс обслуживание самих обработчиков заданий CI. Более крупные старые компании в основном оставались на Jenkins, потому что могли позволить себе продолжать поддерживать серверы CI самостоятельно и потому что Jenkins предлагал более продвинутый контроль.В середине 2010-х годов мы стали свидетелями двух эволюций облачных систем CI.CI-системы, не требующие обслуживания, объединились со службами хостинга кода.GitLab был первым, кто предложил это универсальное решение: оно позволяло пользователям запускать свои тесты CI на той же платформе, на которой они ревьюили и мержили изменения.Microsoft приобрела GitHub в 2018 годуи добилась релиза GitHub Actions, который поддерживался продуктом Microsoft Azure DevOps. В обоих случаях две наиболее популярные платформы для размещения кода изначально предлагали интегрированное выполнение тестов CI.Крупные организации перешли с Jenkins на более современные варианты самостоятельного размещения.Buildkite был первымпопулярным современным решением, запущенным в 2013 году. Он дал компаниям возможность получить преимущества веб-панелей управления и координации, при этом по-прежнему размещая свой код и выполняя тесты на собственных компьютерах. Позже GitHub и GitLab предложили свои самостоятельные обработчики заданий CI, а некоторые компании со множеством ручных тестов решили выполнять собственные тесты в конвейерах CodeDeploy в AWS или на платформе DevOps в Azure.Процесс тестирования программного обеспечения можно рассматривать с точки зрения скорости и дешевизны:Дни и недели.В 1980-х годах изменения программного кода медленно ревьюились вручную, чтобы найти ошибки. Тестовые наборы могли запускать на ночь или только перед релизом.Дни и ночи.В 90-е годы автоматизированные тесты стали писать всё чаще — специализирующиеся на этом тестировщики либо сами авторы кода. Изменения кода стали тестироваться до, а не после мержа с остальной частью кодовой базы.Часы и минуты.В начале 2000-х годов появились и стали популярными первые серверы автоматического интеграционного тестирования, что привело к тестированию каждого изменения по мере его мержа с кодовой базой.Минуты.Примерно в 2011 году стали доступны сервисы тестирования CI, не требующие обслуживания. Теперь тестирование каждого изменения стало выгодно и небольшим командам.Сокращение времени на тестирование одного измененияЛучшие практики направлены на то, чтобывремя CI составляло около 10–15 минут: таким образом разработчики могут поддерживать короткие итерации. Но это становится всё сложнее, потому что кодовые базы и наборы тестов растут с каждым годом.Разработчики не ждут медленных тестов. Чем медленнее выполняется тест, тем реже его будут выполнять и тем дольше придётся ждать повторного прохождения теста после сбоя.Разработчики программного обеспечения в GoogleЕсть только три способа ускорить что-либо в программном обеспечении: вертикальное масштабирование, распараллеливание и кеширование. В случае с CI применяют все три способа, причём в последние годы всё больше внимания уделяется кешированию и распараллеливанию.Во-первых, десятилетиями закон Мура гарантировал, что всё более мощные процессоры смогут быстрее выполнять наборы тестов, хоть и с большими затратами. Используя облачные сервисы по требованию, разработчики могутпереключить настройкув AWS или GitHub Actions, чтобы оплатить более мощный сервер, и надеяться, что их набор тестов будет выполняться быстрее.Во-вторых, поставщики CI постепенно совершенствовались в распараллеливании. Buildkite, GitHub Actions и другие поставщики позволяют пользователямопределять графы зависимостей этапов тестирования, что даёт возможность разным компьютерам передавать контекст и выполнять тесты параллельно для одного и того же изменения кода. Облачные вычисления позволяют организациям выделять бесконечное количество параллельных хостов для выполнения тестов, не опасаясь нехватки ресурсов. Наконец, сложные инструменты сборки, напримерBazelиBuck, позволяют большим базам кода вычислять графы сборки, а также распараллеливать сборку и тестирование на основе графа зависимостей в самом коде.В-третьих, системы кеширования в CI эволюционировали, чтобы свести к минимуму повторяющуюся работу. Обработчики заданий CI обычно поддерживают удалённое кеширование шагов установки и сборки, позволяя тестам пропускать предварительную работу по настройке, если части кодовой базы не изменились.Увеличение скорости тестирования всех измененийКоманды разработчиков достигают теоретического предела скорости проверки одного изменения кода, если предположить, что требования к проверке заключаются в «выполнении всех тестов и сборке для каждого изменения кода».И всё же шаблоны разработки продолжают оптимизироваться для повышения скорости.Вопрос: что может быть быстрее, чем запуск CI при изменении кода с использованием быстрых компьютеров, параллельных тестов и интенсивного кеширования?Ответ: совсем не запускать для этого изменения некоторые тесты.Почти как во времена до CI: некоторые организации с высоким темпом разработки используют пакетирование и зависимости между пулл-реквестами, чтобы сэкономить вычислительные ресурсы и быстрее давать разработчикам обратную связь. У себя в компании мы видим, что это происходит в двух областях. Первая — очереди мержей компании. Внутренние очереди мержей в крупных компаниях, таких как Uber,предоставляют пакетирование и пропуск выполнения тестов. Суть в том, что если вы устанавливаете порядок внесения изменений в код, то вам больше не нужно тестировать каждое изменение в очереди так же строго, как раньше, — хотя здесь есть и некоторые недостатки.Chromium использует вариант этого подхода под названием Commit Queue, чтобы главная ветка была всегда зелёной. Изменения, прошедшие первый этап, отбираются для второго этапа каждые несколько часов, чтобы пройти большой набор тестов, который занимает около четырёх часов. Если сборка сломается на этом этапе, весь пакет изменений будет отклонён. Шерифы сборки и их помощники подключаются в этот момент, чтобы приписать сбои конкретным ошибочным изменениям. Обратите внимание, что такой подход приводит к релизу пакетов, а не отдельных коммитов.Второе место, где можно пропустить CI, — этомногослойные изменения кода, которые так популяризировал Facebook*. Если разработчик выстраивает серию небольших пулл-реквестов, он неявно описывает требуемый порядок мержа этих изменений. Как и в очереди мержей, CI можно объединить с этими изменениями и искать проблемное среди них бинарным поиском (git bisect), если будут обнаружены какие-либо сбои. Сбои в нижней части стека могут уведомить разработчиков ещё до начала выполнения изменений вверх по стеку.Если раньше графы зависимостей при тестировании предлагали раннее обнаружение отказов и экономили вычислительное время при тестировании одного изменения, то теперь они дают те же преимущества и для нескольких пулл-реквестов. Экономия вычислительного времени имеет большое значение: несмотря на то, что облачные ресурсы предлагают бесконечную горизонтальную масштабируемость, расходы на тестирование могут составлять до 10–20% от общих расходов компаний на облачные вычисления.Тестирование быстрее, чем выполнение кодаСамая быстрая форма тестирования CI, о которой я знаю на сегодняшний день, включает в себя пакетное тестирование множества изменений одновременно, а также максимально возможное распараллеливание и кеширование.До появления пакетного выполнения интеграционных тестов некоторые разработчики по-прежнему продолжали проверять изменения кода вручную, иногда просматривая распечатки за столом в поисках ошибок. Мы отказались от этого метода проверки, потому что с некоторых пор машины стали способны выполнять код быстрее, чем люди могли его прочитать и осмыслить.Возможно, это соотношение изменится из-за появления больших языковых моделей. Я подозреваю, что мы стоим на пороге быстрого и дешёвого анализа кода с использованием искусственного интеллекта. Ранее я говорил, что есть только три способа ускорить вычисления: более быстрые чипы, распараллеливание и кеширование. Технически есть и четвёртый вариант, если вы готовы смириться с нечёткими результатами: вероятностное предсказание результата. Интересный факт: процессорыуже делаютэто сегодня.Возможно, это и не заменит юнит-тесты и код-ревью, проводимые человеком, но ИИ может найти распространённые ошибки в предлагаемых изменениях кода за десять секунд и меньше. Он может указывать на проблемы с линтером, несогласованные шаблоны в кодовой базе, орфографические и другие виды ошибок. Существующие координаторы CI могут инициировать проверку с помощью искусственного интеллекта и возвращать результаты быстрее, чем другие тесты. В качестве альтернативы проверка с помощью ИИ может стать настолько быстрой и дешёвой, что начнёт выполняться в фоновом режиме пассивно в редакторах разработчиков, как и Copilot. В отличие от традиционных тестов CI, ИИ-ревью не требует полноты кода, чтобы выявить проблемы.Станут ли тесты в стиле ИИ популярными? Неясно, нокомпаниипытаютсядвигаться в этом направлении. Если ИИ-тесты когда-нибудь станут достаточно хорошими, чтобы получить широкое распространение, это может стать ещё одним технологическим примером выражения «новое — хорошо забытое старое».* Facebook — проект Meta Platforms, Inc., деятельность которой в России запрещена.Чтобы расти, нужно выйти из привычной зоны и сделать шаг к переменам. Можно изучить новое, начав с бесплатных занятий:Нейросети для работы: пошаговый план применения;Системный аналитик: первые шаги к профессии;Специалист по информационной безопасности: старт карьеры;Основы анализа данных в SQL, Python, Power BI, DataLens;Excel: простые шаги для оптимизации работы с данным.Или открыть перспективы с профессиональным обучением и переподготовкой:Инженер по автоматизации;Инженер по тестированию;DevOps-инженер;Сетевой инженер;Онлайн-магистратура «Кибербезопасность»."
СберМаркет,,,Пошаговое руководство по созданию синтетических данных в Python,2024-09-13T09:16:15.000Z,"Простое руководство для новичков: как самому генерировать данные для анализа и тестированияПредставьте: вы только что написали модель машинного обучения и вам нужно протестировать её работу в конкретном сценарии. Или вы собираетесь опубликовать научную статью о пользовательском решении в области Data Science, но имеющиеся датасеты нельзя использовать из-за юридических ограничений. А может быть, в рамках проекта машинного обучения вы занимаетесь отладкой и исправлением ошибок и вам нужны данные, чтобы идентифицировать и устранить проблемы.В этих, да и во многих других ситуациях могут пригодиться синтетические данные. Реальные данные часто недоступны: уже кому-то принадлежат или дорого стоят. Так что умение создавать синтетические данные — важный навык для дата-сайентистов.В этой статье я расскажу, с помощью каких приёмов и методов можно с нуля создать в Python синтетические данные, игрушечные датасеты и фиктивные значения. В некоторых решениях применяются методы из библиотек Python, в других — приёмы, основанные на встроенных функциях Python.Все методы, о которых я рассказываю, пригодились мне при обучении или тестировании моделей, а также для исследовательских задач и научных статей. В конце статьи приведён notebook — вы можете воспользоваться им как руководством к действию или сохранить на будущее.1. Используем NumPyNumPy — самая известная библиотека Python. Она поддерживает операции линейной алгебры и численные вычисления, а также полезна при генерировании данных.Генерирование линейных данныхВ этом примере я покажу, как создать датасет с шумом в данных, характеризующимся линейной взаимосвязью между входными и выходными переменными. Этот приём может пригодиться для тестирования моделей линейной регрессии.# Importing modules (импорт модулей)
from matplotlib import pyplot as plt
import numpy as np

def create_data(N, w):
 """"""
 Creates a dataset with noise having a linear relationship with the target values.
 N: number of samples
 w: target values
(Создаёт датасет, где есть шум с линейной взаимосвязью между входными и выходными переменными.
N: количество примеров
w: значения выходных переменных)
 """"""
 # Feature matrix with random data (создание матрицы со случайными значениями)
 X = np.random.rand(N, 1) * 10
 # Target values with noise normally distributed (значения нормально распределённых данных)
 y = w[0] * X + w[1] + np.random.randn(N, 1)
 return X, y

# Visualize the data (визуализация данных)
X, y = create_data(200, [2, 1])

plt.figure(figsize=(10, 6))
plt.title('Simulated Linear Data')
plt.xlabel('X')
plt.ylabel('y')
plt.scatter(X, y)
plt.show()Синтетические линейные данные (изображение автора)Данные временных рядовВ этом примере с помощью NumPy я сгенерирую синтетические данные временных рядов с линейным трендом и компонентом сезонности. Пример подойдёт для финансового моделирования и прогнозов фондового рынка.def create_time_series(N, w):
 """"""
 Creates a time series data with a linear trend and a seasonal component.
 N: number of samples
 w: target values
(Создаёт временной ряд данных с линейным трендом и сезонной составляющей.
N: количество примеров
w: значения выходных переменных)
 """"""
 # Time values (заданный период)
 time = np.arange(0,N)
 # Linear trend (линейный тренд)
 trend = time * w[0]
 # Seasonal component (сезонная составляющая)
 seasonal = np.sin(time * w[1])
 # Noise (шум)
 noise = np.random.randn(N)
 # Target values (значения выходных переменных)
 y = trend + seasonal + noise
 return time, y

# Visualize the data (визуализация данных)
time, y = create_time_series(100, [0.25, 0.2])

plt.figure(figsize=(10, 6))
plt.title('Simulated Time Series Data')
plt.xlabel('Time')
plt.ylabel('y')

plt.plot(time, y)
plt.show()Синтетические данные временных рядовОсобые данныеИногда возникает необходимость в данных с особыми характеристиками. Например, для снижения размерности нужен многомерный набор данных всего с несколькими информативными признаками. В примере ниже показан адекватный метод генерирования таких датасетов.# Create simulated data for analysis (создание синтетических данных для анализа)
np.random.seed(42)
# Generate a low-dimensional signal (​​снижение размерности данных)
low_dim_data = np.random.randn(100, 3)

# Create a random projection matrix to project into higher dimensions (создание случайной проекции данных)
projection_matrix = np.random.randn(3, 6)

# Project the low-dimensional data to higher dimensions (проекция массива данных низкой размерности на массив данных высокой размерности)
high_dim_data = np.dot(low_dim_data, projection_matrix)

# Add some noise to the high-dimensional data (добавление шума на массив данных высокой размерности)
noise = np.random.normal(loc=0, scale=0.5, size=(100, 6))
data_with_noise = high_dim_data + noise
  
X = data_with_noiseЭтот фрагмент кода создаёт датасет со 100 наблюдениями и 6 признаками на базе массива данных низкой размерности (всего с тремя размерностями).2. Используем scikit-learnПомимо моделей машинного обучения, в scikit-learn есть генераторы данных, которые могут создавать искусственные датасеты контролируемого размера и сложности.Метод make classificationС помощью методаmake_classificationможно создать рандомный датасет с n классами. Этот метод позволяет создавать датасеты с выбранным количеством наблюдений, признаков и классов.Он подходит для тестирования и отладки моделей классификации, таких как машины опорных векторов, деревья решений и наивные байесовские классификаторы.X, y = make_classification(n_samples=1000, n_features=5, n_classes=2)

#Visualize the first rows of the synthetic dataset (визуализация первых строк синтетического набора данных)
import pandas as pd
df = pd.DataFrame(X, columns=['feature1', 'feature2', 'feature3', 'feature4', 'feature5'])
df['target'] = y
df.head()Первые строки датасета (изображение автора)Метод make regressionАналогичным образом с помощью методаmake_regressionможно создать датасеты для регрессионного анализа. Он позволяет задавать количество наблюдений, признаков, предвзятость и шум датасета.from sklearn.datasets import make_regression

X,y, coef = make_regression(n_samples=100, # number of observations (количество наблюдений)
                           n_features=1, # number of features (количество признаков)
                           bias=10, # bias term (алгоритмическая предвзятость)
                           noise=50, # noise level (шум в данных)
                           n_targets=1, # number of target values (количество значений выходных переменных)
                           random_state=0, # random seed (начальное значение генератора)
                           coef=True # return coefficients (извлечение коэффициентов)
                           )Синтетические данные, созданные с помощью метода make_regression (изображение автора)Метод make blobsМетод make_blobs позволяет создавать искусственные blob-объекты с данными, которые подходят для задач кластеризации. В нём можно указать общее число точек данных в датасете, количество кластеров и стандартное отклонение внутри кластера.from sklearn.datasets import make_blobs

X,y = make_blobs(n_samples=300, # number of observations (число точек данных в датасете)
               n_features=2, # number of features (количество признаков)
               centers=3, # number of clusters (количество кластеров)
               cluster_std=0.5, # standard deviation of the clusters (стандартное отклонение внутри кластера)
               random_state=0)Синтетические данные в кластерах (изображение автора)3. Используем SciPyБиблиотека SciPy (аббревиатура от Scientific Python) считается одним из лучших инструментов для численных вычислений, оптимизации, статистического анализа и других математических задач. Статистическая модель SciPy может создавать синтетические данные из разных статистических распределений — например, из нормального, биномиального или экспоненциального.from scipy.stats import norm, binom, expon
# Normal distribution (нормальное распределение)
norm_data = norm.rvs(size=1000)Изображение автора# Binomial distribution (биномиальное распределение)
binom_data = binom.rvs(n=50, p=0.8, size=1000)Изображение автора# Exponential distribution (экспоненциальное распределение)
exp_data = expon.rvs(scale=.2, size=10000)Изображение автора4. Используем FakerА как насчёт нечисловых данных? Ведь зачастую нам нужно обучать модели на нечисловых или пользовательских данных, таких как имя, адрес и электронная почта. Для создания реалистичных данных, похожих на персональную информацию, подходит библиотека Faker.Библиотека Faker может генерировать убедительные данные, пригодные для тестирования приложений и классификаторов машинного обучения. В примере ниже я покажу, как создать фейковый датасет с именами, адресами, номерами телефонов и электронной почтой:from faker import Faker

def create_fake_data(N):
 """"""
 Creates a dataset with fake data.
 N: number of samples
 (Создаёт датасет с фейковыми данными
 N — количество примеров)
 """"""
 fake = Faker()
 names = [fake.name() for _ in range(N)]
 addresses = [fake.address() for _ in range(N)]
 emails = [fake.email() for _ in range(N)]
 phone_numbers = [fake.phone_number() for _ in range(N)]
 fake_df = pd.DataFrame({'Name': names, 'Address': addresses, 'Email': emails, 'Phone Number': phone_numbers})
 return fake_df

fake_users = create_fake_data(100)
fake_users.head()Фейковые пользовательские данные, созданные в Faker (изображение автора)5. Используем Synthetic Data Vault (SDV)А что, если в датасете не хватает наблюдений или вам нужно больше данных, похожих на имеющийся датасет, чтобы дополнить обучение ML-модели? Synthetic Data Vault (SDV) — библиотека Python, в которой можно создавать синтетические датасеты с помощью статистических моделей.В примере ниже мы используем SDV, чтобы расширить демонстрационный датасет:from sdv.datasets.demo import download_demo

# Load the 'adult' dataset (загружаем датасет с данными по взрослым)
adult_data, metadata = download_demo(dataset_name='adult', modality='single_table')
adult_data.head()Демонстрационный датасет с информацией по взрослому населениюfrom sdv.single_table import GaussianCopulaSynthesizer
# Use GaussianCopulaSynthesizer to train on the data (используем моделирование гауссовыми копулами для обучения модели на основе данных)
model = GaussianCopulaSynthesizer(metadata)
model.fit(adult_data)

# Generate Synthetic data (генерируем синтетические данные)
simulated_data = model.sample(100)
simulated_data.head()Синтетические выборки (изображение автора)Эти синтетические данные очень похожи на исходный датасет.Что со всем этим делатьВ статье я привёл пять способов создания синтетических датасетов, которые можно применять для машинного обучения, статистического моделирования и других задач, подразумевающих манипуляции с данными. Это простые примеры, которые нетрудно повторить. Советую изучить код, прочитать необходимую документацию и освоить методы генерирования данных, подходящие для других задач.Синтетические датасеты могут пригодиться дата-сайентистам, специалистам в области машинного обучения и разработчикам: с помощью этих датасетов можно повышать эффективность модели и снижать затраты на производство и тестирование приложений.Как обещал —notebookсо всеми методами из статьи.Чтобы расти, нужно выйти из привычной зоны и сделать шаг к переменам. Можно изучить новое, начав с бесплатных занятий:Основы анализа данных в SQL, Python, Power BI, DataLens;Промышленное программирование: что нужно знать инженеру по автоматизации;Excel: простые шаги для оптимизации работы с данным;Основы Python: создаём телеграм-бота;1С-аналитика: как стать специалистом по автоматизации процессов.Или открыть перспективы с профессиональным обучением и переподготовкой:Fullstack-разработчик на Python;Специалист по информационной безопасности;Android-разработчик с нуля;Python-разработчик: расширенный курс;DevOps-инженер."
СберМаркет,,,Сколько зарабатывают разработчики в 2024 году в России и чего ожидать в будущем,2024-09-06T07:18:42.000Z,"Рынок ИТ до сих пор нуждается в специалистах самых разных направлений. Спрос на айтишников растёт вместе с зарплатами. Так, информационное агентство Известия со ссылкой на данные Авитосообщает, что весной 2024 года спрос на сотрудников в ИТ-отрасли вырос почти в два раза по сравнению с весной 2023 года. Разбираемся, что сейчас происходит на рынке с зарплатами разработчиков.Поданным hh.ru, дефицит наблюдается на позициях сеньоров: в среднем 1,2 активных резюме на одну вакансию. Среди мидл-разработчиков — 6 резюме на вакансию, среди джуниоров самая большая конкуренция — более 16 резюме на вакансию.Исследование hh.ru «Рынок труда в сфере ИТ. 1 кв. 2024 года»Поданным hh.ru, которые опубликовали Известия, в среднем по России специалисты без опыта в ИТ рассчитывают на зарплату в 42 тыс. рублей, с опытом до года — 50 тыс. рублей, от 1 до 3 лет — 68 тыс. рублей, от 3 до 6 — 86,2 тыс. рублей. Айтишники с опытом более 6 лет рассчитывают на среднюю ежемесячную зарплату в 125,9 тыс. рублей.Средняя зарплата айтишников в январе — мае 2024-го составила 131,2 тыс. рублей, что на 18% больше, чем за тот же период 2023 года, следует изданных hh.ru. В то же время увеличилось и количество вакансий в сфере ИТ: в январе — мае 2024-го их стало больше на 19% по сравнению с предыдущим годом, отметили аналитики.Камиль КалимуллинИТ-предприниматель, основатель AdvantShop, «Стачки» и «Инженерки»Я бы не сказал, что есть какие-то большие изменения. В целом рынок стабилизировался, какого-то взрывного роста нет. Скорее идёт поправка на общую инфляцию. Оплата работы разработчиков индексируется, что коррелирует с общими тенденциями рынка. Самыми востребованными и высокооплачиваемыми исторически остаются направления узкие и требующие особой ответственности: архитекторы, управленцы, люди, которые принимают проектные дорогие решения, — эти специалисты обычно сто́ят на рынке дороже.Зарплаты по направлениямПо данным Хабр Карьеры, которыйпроанализировалзарплаты за первое полугодие 2024 года, самый ощутимый рост зарплат был у программистов 1С: на 18%, до 180 тыс. рублей. На 13% выросли зарплаты фронтендеров, на 12% — зарплаты разработчиков игр.Исследование Хабр Карьеры «Зарплаты IT-специалистов в первой половине 2024»Тамара КлимоваБизнес-аналитик Coding TeamРынок расширяется, особенно направление 1С и финтех, так как из-за санкций многие переходят на 1С и самописные доработки и решения. Появились и развиваются такие направления, как искусственный интеллект, различные боты. Автоматизация, связанная с этими направлениями будет в ближайшие годы только набирать обороты, а соответственно, будет расти зарплата.Средняя зарплата снизилась только у системных инженеров: в среднем на 5%, до 144 тыс. рублей. Зарплата архитекторов ПО остаётся самой высокой среди разработчиков — 388 тыс. рублей (+8%), меньше всех получают HTML-верстальщики — 66 тыс. рублей (+10%).Исследование Хабр Карьеры «Зарплаты IT-специалистов в первой половине 2024»Если опираться наданные hh.ru, самые высокие зарплаты в сфере ИТ предлагают руководителям групп разработки (252 тыс. рублей), DevOps-инженерам (245,7 тыс. рублей) и дата-сайентистам (242,8 тыс. рублей). При этом самой популярной позицией в начале 2024-го был программист-разработчик: 63,9 тыс. вакансий со средней зарплатой 153,3 тыс. рублей.Данные из статьи CNews «Зарплаты российских ИТ-шников растут гигантскими темпами» со ссылкой на данные банка ТочкаАльфред СтоляровCEO ИТ-компании EvAppsСамые востребованные сейчас — системные и бизнес-аналитики, девопсы, разработчики под мобильные устройства на iOS и Android. Я расположил их в порядке убывания востребованности.CNewsопубликовалинесколько факторов, которые влияют на рост заработных плат в ИТ:спрос на ИТ-специалистов увеличивается вместе с развитием рынка: квалифицированные специалисты становятся всё более ценными;компании конкурируют за лучших сотрудников и готовы предлагать более высокие зарплаты, чтобы привлечь и удержать таланты;на уровень зарплат также влияет инфляция и рост цен.Данные из статьи CNews «Зарплаты российских ИТ-шников растут гигантскими темпами» со ссылкой на данные банка ТочкаНесмотря на колоссальный рост зарплат, 1С-разработчики не могут похвастаться самыми высокими зарплатами в отрасли. В тройку специальностей с самыми высокими зарплатами входят архитекторы программного обеспечения (388 тыс. рублей), разработчики мобильных приложений (217 тыс. рублей) и баз данных (200 тыс. рублей).Исследование Хабр Карьеры «Зарплаты IT-специалистов в первой половине 2024»Зарплаты по языкам программированияСогласноисследованиюХабр Карьеры зарплаты выросли практически во всех языках программирования. Самый ощутимый рост был у разработчиков в Kotlin (+16%, до 290 тыс. рублей), C (+14%, до 180 тыс. рублей), также на 11% выросли зарплаты в Swift и Elixir.Зарплаты разработчиков в Objective-С остаются самыми высокими, но в первой половине 2024-го медиана зарплаты тех, кто пишет код на этом языке, снизилась на 10%, до 342 тыс. рублей.На втором месте зарплаты Elixir-разработчиков — 312 тыс. рублей. Они выросли на 11%. На третьем — зарплаты в Scala. Они остаются на уровне 300 тыс. рублей со второго полугодия 2023 года.Зарплаты в Ruby опустились с третьего места на седьмое, а оклад разработчиков в Kotlin поднялся на одну позицию с ростом в 16% — до 290 тыс. рублей, — как и зарплаты в Swift с ростом в 11% — до 272 тыс. рублей.Исследование Хабр Карьеры «Зарплаты IT-специалистов в первой половине 2024»Самые высокооплачиваемые языки программирования — Objective-С, Elixir, Scala, Golang и Kotlin, а самый низкооплачиваемый — C.Меньше всего заметны изменения у JavaScript-разработчиков, их зарплаты выросли примерно на 2% При этом у лидов здесь зарплата выросла почти на 3% и составила 350 тыс. рублей, а у джунов не изменилась вовсе — 80 тыс. рублей.Исследование Хабр Карьеры «Зарплаты разработчиков в первом полугодии 2024: языки и квалификации»В Python, напротив, выросли все, кроме лидов. Джуны зарабатывали 87 тыс. рублей (+16,2%), мидлы — 191 тыс. рублей (+5,9%), сеньоры — 324 тыс. рублей (+4,9%), лиды — 358 тыс. рублей (–1,2%).Исследование Хабр Карьеры «Зарплаты разработчиков в первом полугодии 2024: языки и квалификации»В PHP выросли все, но лиды без изменений: джуны зарабатывали 75 тыс. рублей (+7,1%), мидлы — 170 тыс. рублей (+13,3%), сеньоры — 290 тыс. рублей (+5,5%), лиды — 320 тыс. рублей (без изменений).Исследование Хабр Карьеры «Зарплаты разработчиков в первом полугодии 2024: языки и квалификации»В Golang выросли все, а джуны без изменений: джуны — 120 тыс. рублей (без изменений), мидлы — 250 тыс. рублей (+9,9%), сеньоры — 364 тыс. рублей (+11,2%), лиды — 419 тыс. рублей (+4,8%).Исследование Хабр Карьеры «Зарплаты разработчиков в первом полугодии 2024: языки и квалификации»В Kotlin выросли все: джуны — 100 тыс. рублей (+25%), мидлы — 233 тыс. рублей (+11,1%), сеньоры — 350 тыс. рублей (+5,4%), лиды — 433 тыс. рублей (+8,3%).Исследование Хабр Карьеры «Зарплаты разработчиков в первом полугодии 2024: языки и квалификации»От чего зависит зарплатаЗарплата программиста во многом зависит от его опыта иуровня квалификации. Начинающие разработчики обычно зарабатывают меньше, чем их болееопытные коллеги. Последние, как правило, обладают глубокими знаниями и навыками, которые позволяют им решать сложные задачи и принимать важные решения в проектах.Дамир СагдуллаевИТ-рекрутерЕсли выбирать основной фактор, то стоимость определяется тем, какого уровня задачи для бизнеса может решить специалист и сколько прибыли в итоге он может принести.Важную роль может игратьместоположение работодателя. Медианнаязарплата ИТ-специалистовв Москве — 200 тыс. рублей, в Санкт-Петербурге — 165 тыс. рублей, в регионах — 135 тыс. рублей.Разработчики с опытом работы в определённых областях, таких как создание игр или искусственный интеллект, могут зарабатывать больше, чем их коллеги с общими навыками.Специализация в конкретной областиможет значительно повысить вашу рыночную стоимость.Камиль КалимуллинИТ-предприниматель, основатель AdvantShop, «Стачки» и «Инженерки»Быстрыми темпами происходит автоматизация процессов, и, соответственно, те, кто владеет инструментами автоматизации, искусственного интеллекта, становятся более востребованными и высокооплачиваемыми специалистами, потому что более производительны. Влияют также опыт, образование, место проживания. В большей степени востребованность человека зависит от его знаний и опыта, полученных в боевых проектах. Образование влияет меньше.Если ты джун, здесь, понятно, будут смотреть на образование. Но если человек с опытом, конечно, образование — второстепенный фактор. Место проживания сейчас фактически сравнялось: распространена удалённая работа. Если компания в Москве и требуется, чтобы ты был в Москве, то зарплата будет больше. Но, в принципе, очень много тех, кто работает в регионах и получает похожие деньги.Разные отраслимогут предлагать разные уровни зарплат. Например, программисты, работающие в финансовом секторе или в сфере искусственного интеллекта, часто зарабатывают больше, чем те, кто работает в образовательных или некоммерческих организациях.Исследование Хабр Карьеры «Зарплаты IT-специалистов в первой половине 2024»Финансовые компании и стартапы в области технологий часто предлагают более высокие зарплаты и бонусы, чтобы привлечь и удержать таланты. Кроме того, работа в крупных корпорациях может быть более прибыльной, чем в небольших компаниях. Крупные компании часто имеют больше ресурсов для выплаты высоких зарплат и предоставления дополнительных бонусов и льгот.Альфред СтоляровCEO ИТ-компании EvAppsВыше всего в работе программиста ценится реальный опыт. Именно на базе такого опыта формируются грейды. Например, мы считаем джуном человека с 1–1,5 годами коммерческого опыта, а мидлом — с опытом до 3 лет. Имеет значение и разнообразие опыта: если разработчик все 3 года сидел на одном проекте и выполнял рутинные операции, его опыт едва ли соответствует требованиям к мидлу.Высшееобразованиеи наличие специализированных сертификатов также могут влиять на уровень зарплаты.Андрей ЛядковФаундер агентства Staff-HubНа зарплату, кроме классических требований, всё чаще влияет наличие софт-скиллов. Вторая больная история — ответственность. Особенно это касается «молодых, но перспективных».Чего ждать по зарплатам в ближайшем будущемДефицит ИT-специалистов — не общая ситуация на рынке: он касается именно опытных кандидатов. Компаниям нужнытимлидыисеньоры, которых на рынке труда почти нет: либо их уже разобрали максимально именитые конкуренты, либо они работают на зарубежные компании, либо трудятся на себя. Такие айтишники почти никогда не выходят на рынок, потому что сразу переходят из компании в компанию.Структура рынка постепенно меняется: компании чаще нанимаютджунови мидлов, чем сеньоров. Из-за этого наибольший прирост в 2020–2022 гг. показывали именно зарплаты мидлов, конкуренция за которых кратно усилилась.Данные hh.ruподтверждают, что больше всего востребованы специалисты с опытом работы от 3 лет — 47,5% от общего объёма объявлений о поиске ИТ-специалиста. На втором месте — специалисты с шестилетним опытом (35,7%), на третьем — без опыта вовсе (12%). Айтишников с опытом более шести лет ищут 4,5% работодателей.Андрей ЛядковФаундер агентства Staff-HubКвалифицированные позиции будут продолжать расти в зарплате. С джунами сложнее: не каждая компания готова их обучать и натаскивать, особенно за те деньги, которые они хотят, выйдя из вуза или окончив курсы. Средний уровень будет расти, деваться некуда. Но темпы роста невысоки и, скорее всего, будут отставать от сеньоров.Согласностатистике hh.ru, с января по май 2024 года ИТ-специалисты разместили на этой площадке приблизительно 1,3 млн резюме, из которых 20,7% принадлежало специалистам с опытом работы более 15 лет. Чуть меньше (19%) — соискатели с опытом работы от 3 до 6 лет. Затем идут специалисты с опытом 9–12 лет (11%). Резюме от соискателей без опыта в ИТ-сфере составили 10% от общего числа объявлений о поиске работы.Альфред СтоляровCEO ИТ-компании EvAppsСтабильный рост зарплатных ожиданий точно будет продолжаться. Говорить о скачкообразности этого роста не берусь: на данный момент не вижу предпосылок для этого. Ситуация в мире и на рынке разработки остаётся слабопредсказуемой. Если появятся, например, нововведения в законодательстве или в налогообложении, то возникнут и зарплатные скачки.Технологии развиваются, растёт количествоинструментов, ускоряющих и упрощающих разработку, требуются люди, умеющие работать с этим. Чем больше разработчик знает, чем он опытнее, тем выше его зарплата. Список требований в вакансиях растёт количественно и может отпугнуть даже опытных специалистов.Камиль КалимуллинИТ-предприниматель, основатель AdvantShop, «Стачки» и «Инженерки»Разработка никогда не умрёт, она будет развиваться. Соответственно, спрос на разработчиков сохранится. Не думаю, что он будет колоссально растущий. Скорее даже наоборот, немного падающий за счёт средств автоматизации. Произойдёт перетасовка компетенций: если раньше ты просто должен был программировать на каком-то языке — сейчас ты должен обладать средствами разработки, то есть чуть более современными инструментами, чуть более продуктивными техниками.Чтобы расти, нужно выйти из привычной зоны и сделать шаг к переменам. Можно изучить новое, начав с бесплатных занятий:Как сменить профессию: советы и кейсы;1С-аналитика: как стать специалистом по автоматизации процессов;Вводный курс магистратуры РАНХИГС «Управление проектами в IT и digital»;Специалист по информационной безопасности: старт карьеры;Открытые занятия по аналитике.Или открыть перспективы с профессиональным обучением и переподготовкой:DevOps-инженер с нуля;Онлайн-магистратура «Управление проектами в IT и digital»;Разработчик на C++;Android-разработчик с нуля;Инженер по тестированию."
СберМаркет,,,Топ-7 высокооплачиваемых профессий в сфере ИИ,2024-08-29T11:31:41.000Z,"О том, как появление нейросетей влияет на рынок труда,говорятвсе. Потому что не говорить об этом невозможно. Помнению экспертов(и не только их), некоторые профессии скоро отправятся на свалку истории, а другие станут невероятно востребованными. Давайте разберёмся, что ждёт работодателей и соискателей в ближайшем будущем и каким специалистам в сфере ИИ, или AI, готовы платить больше всего.Что в прошломНа протяжении тысячелетий людям приходилось тяжело работать, чтобы прокормить себя: буквально выращивать достаточное количество пищи. Изобретение приспособлений для обработки земли и прогресс в животноводстве упрощали жизнь людей и увеличивали их шансы на выживание. Однако разделение труда и технический прогресс поставили людей в неравное положение: некоторые продолжали производить хлеб насущный, а другие отправились на фабрики, где могли заработать больше денег.В 19-м веке, ближе к окончанию промышленной революции, работники фабрик испугались, что их заменят машины, и начали препятствовать научно-техническому прогрессу. От борьбы с машинами луддитовне удерживалдаже страх смертной казни. Однако развитие промышленности остановить было невозможно: инженеры создавали всё более сложные и производительные машины, а для их обслуживания по-прежнему требовались рабочие — правда, всё более квалифицированные.Что сейчасСегодняшняя ситуация чем-то похожа на положение дел в начале 19-го века. Нейросети как будто способны заменить большое количество профессий: от художников до программистов, которые и занимаются разработкой искусственного интеллекта. Но на самом деле всё не так просто.Рынок труда в 20-х годах 21-го века перестраивается. Причина этому — мировые экономические сдвиги и появление новых технологий. Это касается какглобальной ситуации, так и положения делв России.Если говорить о нашей стране, на рынке труда существует значительный разрыв. Крупные технологические компанииотказываютсяот «ручного» труда, автоматизируя рутинные операции. При этом в ряде компаний всё ещё велик спрос насотрудников, выполняющих работу, которую можно автоматизировать, — но управленцы не готовы вкладываться в реформацию. Кроме того, существует высокий спрос на работников, выполняющихфизические операции, которые сегодня не получается поручить машинам, — например, на курьеров.Но обсудим подробнее состояние рынка труда в технологической сфере. Здесь ощущается дефицит кадров, — об этом сейчас многоговорят. При этом работодатели ждут специалистов уровня middle и выше, аджуну нужно потрудиться, чтобы найти первое место работы. За опытными разработчиками рекрутеры ведут настоящую охоту — и их предложения куда интереснее «молодой амбициозной команды», печенек и ДМС со стоматологией.Кому готовы платитьОплата труда разных специалистов в сфере AI может различаться в зависимости от страны, компании и, конечно, грейда. Давайте посмотрим, кого ищут работодатели в России, какие требования они предъявляют к кандидатам и сколько готовы платить.Анна ВекличСооснователь AI-агрегатора нейросетей GPT4Telegrambot, медиа об AI «Hi, AI! | нейросети», автор курса «Нейрограмотность»Есть специалисты, которые разрабатывают AI: дата-сайентисты, специалисты по машинному обучению, математики, учёные. Есть те, кто знает, как адаптировать AI под потребности рынка и решать разные задачи с помощью уже готовых AI-сервисов или с помощью их дообучения под задачи компании: специалисты по дообучению LLM, разработчики на разных языках программирования и другие. И есть третий тип специалистов: все остальные. Они обучились работать с AI-сервисами как пользователи (как раньше мы учились пользоваться ПК) и стали ценными сотрудниками, которые могут быстрее выполнять задачи, делегируя многое нейросетям. Но при этом сами никак не влияют на работу сервисов, пользуются тем, что предоставляют авторы AI-ресурсов. Это маркетологи, продажники, редакторы, журналисты, иногда методисты, экономисты, юристы.Сейчас высокий спрос на всех этих специалистов. Просто он в разных секторах рынков. Первые — самые высокооплачиваемые, работают в топовых корпорациях или создают свои компании. Вторые — работают в среднем бизнесе, агентствах, а также руководителями направлений по цифровизации. А третьи — это необходимые специалисты, которые в скором времени должны появиться везде: от самых начальных позиций до руководителей проектов. Сотрудники многих отраслей сегодня должны уметь работать с AI, — так же, как мы умеем пользоваться интернетом, компьютером или документами Word.Отметим, что зачастую компании ищут человека ну пул обязанностей, которые находятся на пересечении разных профессий, — например, ML-инженера с глубоким пониманием принципов компьютерного зрения или data engineer, data analyst.Архитектор интеллектуальных систем (AI/ML architect)Источник:getmatchЧто делает:занимается проектированием сложных AI-систем и их интеграцией в бизнес-процессы компаний.Сколько зарабатывает:до 550 000 ₽.Что должен уметь:понимать принципы работы алгоритмов машинного обучения;знать языки программирования (чаще всего требуется Python, Java, C++);работать с фреймворками и библиотеками;иметь навыки проектирования систем;работать с базами данных.Инженер данных (data engineer)Источник:hh.ruЧто делает:разрабатывает системы хранения и обработки больших объёмов данных, необходимых для обучения AI-моделей.Сколько зарабатывает:до 500 000 ₽.Что должен уметь:проектировать, создавать и поддерживать хранилища данных;работать с облачными платформами для хранения данных;настраивать ETL-процессы (извлечение, преобразование и загрузка данных);разрабатывать и оптимизировать SQL-запросы;применять методы обеспечения качества данных (очистка, дедупликация, профилирование);использовать инструменты для работы с большими данными: Hadoop, Spark и подобные;обеспечивать безопасность данных.Владислав ШевченкоВедущий инженер разработки и внедрения моделей машинного обучения в Альфа-Банке, ментор программы онлайн-магистратуры «Инженерия данных» в НетологииПосле окончания университета я два года работал инженером в Роснефти. Именно там у меня появилось огромное желание создавать свои бизнес-решения и проекты. Дальнейший путь был довольно стандартным: сначала консалтинг и бизнес-аналитика, затем я решил, что хочу не просто создавать процессы и решения, но и понимать, как всё работает изнутри. Поэтому я начал обучение по программам для работы с большими данными, таким как Pentaho и Hadoop, и нашёл свою первую работу дата-инженером в одном из ведущих банков. Кстати, желание разбирать всё до мельчайших деталей у меня с детства: я часто разбирал игрушки.ML-инженер (machine learning engineer)Источник:hh.ruЧто делает:создаёт и обучает модели машинного обучения, которые лежат в основе многих AI-решений.Сколько зарабатывает:как правило, до 600 000 ₽.Что должен уметь:проектировать и обучать модели машинного обучения;работать с различными алгоритмами и методами машинного обучения: линейной регрессией, деревьями решений, нейронными сетями и т. д.;настраивать гиперпараметры моделей для повышения их точности и производительности;проводить эксперименты и анализировать результаты;разрабатывать и внедрять системы автоматического обучения и переобучения моделей;обеспечивать интерпретируемость и объяснимость моделей;применять методы борьбы с переобучением и другими проблемами машинного обучения.Наталья БаданинаData scientist в Чистой линии, эксперт ML и DS в Нетологии, соавтор курсов по DS и RЧтобы начать работать с ML, понадобятся навыки программирования на Python, иногда можно встретить в работе R и Julia. Их используют в основном для статистического анализа и высокопроизводительных вычислений. У Python нужно изучить ещё некоторые библиотеки, такие как TensorFlow, PyTorch, scikit-learn.На старте карьеры сильно поможет практический опыт. Например, участие в pet-проектах,стажировкахили хакатонах. На платформе Kaggle доступны соревнования по анализу данных, где можно применить и улучшить навыки в решении практических задач.Что касается теоретических знаний, для понимания алгоритмов машинного обучения будет полезно знать линейную алгебру, статистику и теорию вероятностей.Важно знать и специфику отрасли, в которой вы собираетесь применять AI, будь-то финансы, медицина, маркетинг или промышленность.NLP-специалист (natural language processing specialist)Источник:hh.ruЧто делает:занимается обработкой естественного языка, которая позволяет AI понимать и генерировать текст.Сколько зарабатывает:до 700 000 ₽.Что должен уметь:работать с текстовыми данными, используя методы и инструменты NLP;применять алгоритмы машинного обучения для обработки естественного языка;создавать модели для анализа тональности текста, определения ключевых слов, извлечения информации и других задач;использовать библиотеки и фреймворки для NLP: NLTK, spaCy, TensorFlow Text и другие;разрабатывать системы автоматического перевода, распознавания речи, генерации текста и другие приложения NLP.Специалист по компьютерному зрению (computer vision specialist)Источник:hh.ruЧто делает:работает над системами распознавания изображений и видео, которые используются в различных AI-приложениях.Сколько зарабатывает:до 600 000 ₽.Что должен уметь:работать с алгоритмами и методами компьютерного зрения;создавать и обучать модели для распознавания объектов, определения их местоположения, классификации и других задач;использовать библиотеки и фреймворки для компьютерного зрения: OpenCV, TensorFlow, PyTorch и другие;разрабатывать системы автоматического обнаружения, отслеживания, анализа и интерпретации визуальных данных.Аналитик данных (data analyst, data scientist)Источник:hh.ruЧто делает:анализирует большие объёмы информации, выявляет закономерности и тенденции, которые можно использовать для улучшения AI-алгоритмов.Сколько зарабатывает:до 550 000 ₽.Что должен уметь:работать с различными источниками данных;очищать, преобразовывать и обрабатывать данные;применять методы статистического анализа и машинного обучения;строить модели и делать прогнозы на основе данных;визуализировать результаты анализа в виде графиков и диаграмм;интерпретировать результаты и формулировать выводы.Руководитель проектов в области AI (AI project manager)Источник:getmatchЧто делает:координирует работу команды специалистов, отвечает за успешное выполнение проектов по разработке и внедрению AI-решений.Сколько зарабатывает:до 400 000 ₽.Что должен уметь:определять цели и задачи проекта;составлять план работ и оценивать ресурсы;координировать работу команды специалистов;контролировать сроки выполнения задач;обеспечивать качество результатов;управлять рисками проекта;представлять результаты заказчику.Что в ближайшем будущемНельзя сказать, что ситуация на рынке труда сейчас меняется стремительно: по-прежнему всегда востребованы специалисты с опытом, умением самостоятельно находить решения, способностями к самообучению и развитыми гибкими навыками. Таким кандидатам готовы предоставлять максимально комфортные условия работы и хорошо платить.Однако можно выделить тренды, отличающие сегодняшнее положение дел от прошлого десятилетия:Рутинные процедуры всё больше автоматизируются — компаниям всё меньше нужны люди для ручного труда и выполнения простых операций (в том числе интеллектуальных). Из-за этого начинающим специалистам сложнее найти работу.Новые технологии появляютсявсё чаще. Следствие этого — необходимость больше времени уделять обучению.Конкуренция среди работодателей растёт. В России этот факт усугубляется большим количеством уехавших за границу IT-специалистов.Работодатели ищут кандидатов с хорошей базой. Востребованы разработчики с математическим бэкграундом, а также специалисты смежных профессий, например биоинженеры или специалисты с глубоким пониманием физических процессов.У бизнеса есть отдельный запрос на специалистов, которые хорошо владеют нейросетями на уровне пользователей. Конечно, уровень их дохода может быть не таким высоким, как у профессионалов, разрабатывающих и внедряющих AI-решения, но чтобы оставаться конкурентоспособным в любой профессии, придётся осваивать написание промптов.Владимир ЛиРуководитель отдела машинного обучения в НетологииСошлюсь на недавнеевыступлениегенерального директора NVIDIA Дженсена Хуанга, где он сказал: «Сейчас уже нет смысла учить детей программировать, так как программистов скоро полностью заменят нейросети. Но одновременно с этим появитсяновая профессия: промпт-инженер, который умеет ставить правильные запросы для ИИ, — и в дальнейшем он один может заменить целый отдел программистов».В такое в целом верится, но надо понимать, что, в свою очередь, специальность промпт-инженера тоже будет расширять свою функциональность, так как надо уметь не только сделать правильный промпт, но и интегрировать созданное решение в продуктивную среду, внедрить его в соответствующие бизнес-процессы, поэтому инженерные и архитектурные навыки здесь всё равно нужны.С другой стороны, NVIDIA — монополист в сфере производства чипов, на которых работает ИИ. Благодаря этому капитализация компании выросла на 1 500% за последние 4 года. Поэтому прогнозы довольно обоснованы, но было бы странно, если бы управляющий компании — лидера в развитии AI предполагал иное развитие событий.Нейросети будут частью нашей жизни: спорить с этим и пытаться что-то изменить — уже бесполезно. Они не приведут к массовому исчезновению ряда профессий, но существенно изменят подход к рекрутингу и организации работы внутри компаний.Востребованность специалистов, занимающихся разработкой и внедрением искусственного интеллекта, будет только расти. Работодатели готовы предлагать таким работникам хорошие условия, но и требования предъявляют высокие. Из-за этого опытные специалисты должны постоянно повышать квалификацию и изучать новые технологии, а новички — как можно быстрее нарабатывать базу, параллельно изучая прикладные дисциплины.Чтобы расти, нужно выйти из привычной зоны и сделать шаг к переменам. Можно изучить новое, начав с бесплатных занятий:Нейросети для работы: пошаговый план применения;Как начать работать с нейросетямии создать свой ChatGPT;AI против спама: практическое руководство по разработке спам-фильтров;Мастер-класс «Анализ данных и нейросети»;Эффективное использование нейросетей для обучения.Или открыть перспективы с профессиональным обучением и переподготовкой:Нейросети для бизнеса и управленцев;Онлайн-магистратура «Прикладной искусственный интеллект»;Нейросети для каждого: как решать рабочие задачи быстрее;Deep Learning;Нейросети для жизни: как повысить личную эффективность."
СберМаркет,,,Функция setdefault() в Python: для чего нужна и как её использовать,2024-08-20T10:45:14.000Z,"Словари Python — мощные инструменты для работы с данными. Они поддерживают разные методы, но функция setdefault() выделяется способностью упрощать код и эффективно работать со значениями по умолчанию.Мы перевели для вас статью о функции setdefault(). В ней рассмотрим синтаксис, сценарии использования функции и покажем её пользу на практических примерах, а в подробном заключении сделаем основные выводы.Разбираемся в функции setdefault()Методsetdefault()в словарях Python позволяет извлекать значение по указанному ключу, если он существует. Если ключа нет, функция вставляет ключ с указанным значением по умолчанию и возвращает это значение. Метод особенно полезен при работе со словарями, в которых обязательно нужен ключ и при этом, если ключа изначально нет, вам нужно инициализировать его значением по умолчанию.СинтаксисТак выглядит синтаксис методаsetdefault():dict.setdefault(key, default=None)key: выполняется поиск ключа в словаре.default: если ключ не найден, задаётся и возвращается значение. Значение по умолчанию —None.Возвращаемое значениеМетодsetdefault()возвращает:значение указанного ключа, существующего в словаре;значение по умолчанию, если ключа в словаре нет.Практические примерыЧтобы понять, как функцияsetdefault()работает в разных сценариях, давайте рассмотрим несколько практических примеров.Пример 1. Базовый сценарийЭто пример базового использования методаsetdefault().# Creating a dictionary (создаём словарь)
fruits = {'apples': 5, 'oranges': 3}

# Using setdefault to get the value of an existing key (с помощью setdefault получаем значение существующего ключа)
apples_count = fruits.setdefault('apples', 10)
print('Apples:', apples_count)


# Using setdefault to add a new key with a default value (c помощью setdefault добавляем новый ключ со значением по умолчанию)
bananas_count = fruits.setdefault('bananas', 10)
print('Bananas:', bananas_count)


# Printing the updated dictionary (выводим обновлённый словарь)
print(fruits)Результат:Пример 2. Использование setdefault() в циклеВ этом примере с помощью методаsetdefault()подсчитаем частотность элементов в списке.# List of fruits (список фруктов)
fruit_list = ['apple', 'banana', 'apple', 'orange', 'banana', 'apple']

# Creating an empty dictionary to store the count (создаём пустой словарь для хранения подсчитанных значений)
fruit_count = {}

# Counting the frequency of each fruit (подсчитываем частотность каждого фрукта)
for fruit in fruit_list:
    fruit_count.setdefault(fruit, 0)
    fruit_count[fruit] += 1

# Printing the fruit count (выводим число подсчитанных фруктов)
print(fruit_count)Результат:Пример 3. Использование setdefault() с вложенными словарямиВ этом примере мы с помощью методаsetdefault()поработаем с вложенными словарями.# Creating an empty dictionary to store student grades (создаём пустой словарь для хранения оценок студентов)
grades = {}

# List of student data (список сведений о студентах)
students = [
    ('Alice', 'Math', 'A'), 
    ('Bob', 'Math', 'B'),
    ('Alice', 'Science', 'A'),
    ('Bob', 'Science', 'C')
]

# Inserting student grades into the dictionary (вставляем оценки студентов в словарь)
    for name, subject, grade in students:
    grades.setdefault(name, {})
    grades[name].setdefault(subject, grade)

# Printing the grades dictionary (выводим словарь с оценками студентов)
print(grades)Результат:Пример 4. Использование setdefault() для сложных структур данныхВ этом примере воспользуемся методомsetdefault(), чтобы сформировать словарь списков.# List of student scores (список баллов студентов)
student_scores = [
    ('Alice', 85),
    ('Bob', 92), 
    ('Alice', 88), 
    ('Bob', 95)
]

# Creating an empty dictionary to store scores (создаём пустой словарь для хранения баллов)
scores = {}

# Inserting scores into the dictionary (вставляем баллы в словарь)
    for name, score in student_scores:
    scores.setdefault(name, []).append(score)

# Printing the scores dictionary (выводим словарь с баллами)
print(scores)Результат:Что по итогуФункцияsetdefault()— универсальный эффективный инструмент работы со значениями по умолчанию в словарях Python. Она упрощает код, уменьшая необходимость в явных проверках и условных выражениях при работе с ключами в словарях. Научившись работать с методомsetdefault(), вы сможете писать болеечистый код, характерный именно для Python, особенно в сценариях, где требуется ключ, который инициализируется значением по умолчанию.Основные выводы:Базовый сценарий. Методsetdefault()позволяет получать значение по имеющемуся ключу или задавать значение по умолчанию, если ключа нет.Подсчёт частоты.setdefault()можно использовать для эффективного подсчёта частоты элементов в списке.Вложенные словари. Методsetdefault()особенно полезен при работе с вложенными словарями, так как позволяет с лёгкостью обрабатывать сложные структуры данных.Создание сложных структур данных. Методsetdefault()можно использовать для формирования словарей списков или других сложных структур данных без явных проверок на наличие ключей.Включив методsetdefault()в свой инструментарий Python, можно повысить удобочитаемость, эффективность и надёжность кода. Эта встроенная функция, безусловно, поможет и начинающему, и опытному Python-разработчику качественнее работать со словарями и эффективно управлять значениями по умолчанию.Чтобы расти, нужно выйти из привычной зоны и сделать шаг к переменам. Можно изучить новое, начав с бесплатных занятий:Основы анализа данных в SQL, Python, Power BI, DataLens;Вводный курсмагистратуры НИУ ВШЭ «Инженерия данных»;Введение в SQL и работу с базой данных;Основы Python: создаём телеграм-бота;1С-аналитика: как стать специалистом по автоматизации процессов.Или открыть перспективы с профессиональным обучением и переподготовкой:Python для анализа данных;Системный аналитик;Онлайн-магистратура «Инженерия данных»;1C-аналитик: старт в профессии;SQL и получение данных."
СберМаркет,,,Как бороться с микроменеджментом в ИТ (и нужно ли),2024-08-10T07:25:34.000Z,"Осознание, что ты микроменеджер, приходит не сразу, и чаще всего только после фидбэка от подчинённых. А фидбэк этот бьёт обухом по голове, когда у сотрудников заканчивается терпение. Стоит один раз с утра спросить, как дела, из лучших побуждений предложить свою помощь, повторить это ценное предложение часа через три, подстраховать уверенного мидла или сеньора, чтобы он вдруг ощутил, что с ним обращаются как со школьником, — и вот обратная связь от коллег помогает узнать о себе что-то новое.Даже если вы искренне хотите помочь и облегчить жизнь окружающим, микроменеджмент не перестаёт быть микроменеджментом и всё так же раздражает сотрудников.Вот что такое микроменеджмент, по мнениюГарри Чемберса:Это чрезмерное, нежелательное, контрпродуктивное вмешательство в работу и нарушение работы людей или процессов. Это любые действия, которые создают помехи для процессов, политик, систем и процедур, чужих обязанностей и полномочий. Микроменеджмент возникает, когда влияние и взаимодействие начинают отнимать ценность у людей и процессов.Как понять, что вы микроменеджер: основные симптомыДмитрий Федоров, начальник отдела разработки ПО в НИЦ «ИРТ», у себя на работе не сталкивался с микроменеджментом, но всерьёз воспринимает опасности, которые таит в себе этот стиль управления:Дмитрий ФедоровНачальник отдела разработки ПО в НИЦ «ИРТ»Гиперконтроль со стороны руководства, которое следит за каждой маленькой задачей подчинённого, тянет вниз не только подчинённого, но и весь сектор, за который отвечает «гиперруководитель». Подчинённые будут терять мотивацию к работе, и творческие люди или те, кто способен принимать самостоятельные решения, на этой позиции не задержатся с таким руководителем. Остаются только те, кто работает ради нужды работать. Это всё приводит к режиму «дали задачу, сделал, не дали — не сделал». Для компании это явный путь в никуда.Такой руководитель не способен трезво оценить процессы взаимодействия в коллективе, которые важны для достижения общей цели, не говоря уже о процессах взаимодействия со смежными подразделениями. Всё время тратится на контроль сотрудников и постановку им задач, так как руководитель думает, что он всё лучше знает. Но на самом деле всё с точностью наоборот.Обычно микроменеджеры вырастают из хороших исполнителей, поставленных на руководящую должность, или же из людей, не очень компетентных сфере, в которой им приходится быть руководителями. Такое тоже бывает. В обоих случаях неспособность видеть более широкие бизнес-процессы и средства достижения бизнес-целей коллективом как единым целым приводит к тому, что нужно имитировать бурную деятельность. А это порождает тотальный контроль подразделения.У микроменеджмента множество симптомов, по которым можно вовремя диагностировать этот стиль управления.Микроменеджер:Не видит леса за деревьями. Он настолько погружён в детали, что упускает общую картину.Помешан на согласованиях. Для него немыслимо предоставить подчинённым толику свободы, ведь он свято верит, что лишь он способен принимать эффективные решения. Необходимость спрашивать разрешения по любому поводу снижает у сотрудников уверенность в себе. А ещё она оказывается весьма контрпродуктивной: каждого специалиста нанимали, потому что он обладает подходящей квалификацией и способен самостоятельно принимать решения в рамках своих должностных обязанностей.Одержим отчётами. Сотрудники тратят больше времени на отчётность, чем на выполнение своих обязанностей, и работают в атмосфере бесконечных оправданий. Ежедневная отчётность — явный признак, что в компании прижился микроменеджер. Представьте, сколько рабочего времени сотрудники тратят впустую.Не делегирует. Это порождает две большие проблемы. С одной стороны, сотрудники задаются вопросом, можно ли им выполнять работу, для которой их изначально наняли. С другой стороны, микроменеджер становится настолько перегружен чужой работой, что не справляется со своей.Хочет быть в курсе всего. Живёт по принципу «нехватка информации рождает стресс». Опасается, что сотрудники принимают независимые решения, и требует, чтобы ему направляли копии всех писем, а то мало ли что. Фраза «И не забудьте указать меня в письме» — явный симптом микроменеджмента. Потребность отслеживать все каналы связи указывает на страх остаться не у дел или допустить дискуссии и действия сотрудников без ведома начальства.Слишком усложняет. Из-за его одержимости деталями даже простые вещи могут стать до смешного сложными. Микроменеджер создаёт такие подробные и запутанные инструкции, что они становятся непонятными. Часто за этим скрывается стремление сохранить контроль, ведь если сотрудникам что-то непонятно, им приходится обращаться к начальству за разъяснениями. Это ещё одна причина, по которой у сотрудников падает самооценка.Постоянно жалуется. Микроменеджеры жалуются на всё, даже когда жаловаться особенно не на что. Беда в том, что, если постоянно выискивать недостатки в других людях, они непременно найдутся. Иногда жалобы касаются даже мелочей, не связанных с выполняемой задачей. Микроменеджеру кажется, что так он помогает достичь идеала. На самом деле он просто подрывает мотивацию у сотрудников.Подавляет инициативу. Гиперконтроль не оставляет места для свободы творчества. Чем креативнее характер работы, тем сильнее сотрудников обескураживают указания о том, как именно её следует делать.Не делится знаниями. Начальник должен служить примером и наставником для молодых сотрудников. Если он не проявляет никакого интереса кменторству, это может не только разочаровать, но и сильно обескуражить. Такой начальник не поддерживает джунов, зато заваливает их жалобами и критикой.Задаёт нереалистичные стандарты. Микроменеджеры всегда сосредоточены на том, что нужно улучшить или изменить, и не стремятся выражать признательность сотрудникам за пользу, которую те приносят компании.Верит в постулат «Кто, кроме меня?». Такие начальники часто считают себя наиболее способными в коллективе: а иначе чем объяснить, что именно он здесь на руководящих позициях?Как микроменеджмент вредит компании: основные последствияНа первых порах может казаться, что микроменеджмент эффективен. Но на самом деле он приводит к неприятным последствиям для компании.Утрата контроля.Микроменеджмент ограничивает руководителя в инструментах управления, сводя всё к контролю. Парадокс в том, что именно эта ситуация и приводит к утрате контроля: такой подход к управлению командой не приносит результата.Утрата доверия. Из-за микроменеджмента исчезает доверие между руководителем и сотрудниками. В таком начальнике подчинённые видят деспота, единственное желание которого — загнать их в угол. А когда доверие исчезает, происходят две вещи: производительность серьёзно снижается и сотрудники уходят в другую компанию.Несамостоятельность сотрудников. В условиях микроменеджмента подчинённые теряют самостоятельность и уверенность в себе. У них формируется привычка постоянно быть под контролем начальника. А управлять несамостоятельными сотрудниками трудно и затратно по времени, тогда как самостоятельные думающие специалисты способны творить чудеса. Предполагается, что каждый сотрудник изначально должен привнести в работу нечто ценное: свои навыки, умения, экспертность.Выгорание руководителя. Микроменеджмент — весьма утомительное занятие. Гиперконтроль за целым коллективом быстро выматывает. В конце концов руководитель начинает ненавидеть свою работу, а то и компанию, которая его наняла. В самых тяжёлых случаях это грозит увольнением и даже нежеланием снова устроиться руководителем в другом месте.Большая текучка. Сотрудники недолюбливают микроменеджмент и часто уходят с работы, где они сталкиваются с таким отношением. Из-за постоянного обучения и переобучения новых кадров отдел не может нормально развиваться, а компанию покидают квалифицированные специалисты. На их место приходят кадры похуже, что сказывается на итоговых показателях компании и подрывает её боевой дух.Утрата инициативы. У несамостоятельных сотрудников пропадает желание работать с душой: они делают только то, что от них требует начальство, — ни шага вправо, ни шага влево. Никто не будет прыгать выше головы, чтобы справиться с поставленной задачей.Меньше инноваций. Одна из самых больших опасностей микроменеджмента — подавление творческого духа сотрудников. Конечно, не все инновации, которые предлагают подчинённые, всегда оказываются стоящими, но микроменеджмент сводит к нулю вероятность появления интересных идей. А ведь кто не рискует, тот не пьёт шампанского.Как предотвратить микроменеджмент, если вы руководительПредотвратить проще, чем лечить. Если вы только начинаете руководить коллективом, имеет смысл следовать советам знатоков о том, как создавать нетоксичную культуру управления в компании.Чтобы не пойти по кривой дорожке микроменеджмента, придерживайтесь этих рекомендаций:Начните делегировать. Это необходимый навык любого руководителя: чтобы добиться качественных результатов, ему нужно опираться на навыки и опыт своей команды. Эту суперспособность можно развивать шаг за шагом: сначала делегировать небольшие задачи, потом покрупнее. Чтобы делегировать рабочие задачи правильно, выясните сильные стороны своих сотрудников.Не воспринимайте ошибки как трагедию. Микроменеджмент часто порождается перфекционизмом, из-за которого руководитель пытается контролировать всё, чтобы избежать ошибок. Ноошибок не стоит бояться: они помогают нам учиться и совершенствоваться. А вот перфекционизм сказывается на эмоциональном состоянии и на затратах времени для выполнения задачи. Нужно просто признать, что промахи — нормальная часть рабочего процесса, и предоставить сотрудникам инструменты для самостоятельного поиска и устранения ошибок. Такой подход к работе повышает профессионализм сотрудников и их уверенность в себе.Распишите обязанности. Микроменеджмент часто возникает из-за путаницы в распределении обязанностей. Если в компании неизвестно, кто за что отвечает, сотрудникам будет сложно брать на себя ответственность за принятие решений и выполнение задач. Чётко пропишите обязанности каждого сотрудника. Объясняйте их новичкам в процессе онбординга и не отказывайтесь от открытого диалога в дальнейшем. На любом этапе каждый сотрудник должен точно понимать, за что он отвечает.Помните, что цель — всему голова. Именно цель отражает измеримый результат, к которому нужно прийти. Прелесть в том, что цель позволяет управлять работой сотрудников без гиперконтроля. Измеримая цель чётко определяет искомый результат, позволяет сотруднику решать задачи самостоятельно, при этом оставаться подотчётным начальству.Софья Гриханова, HR, эксперт на курсе «HR-менеджер» от Нетологии, тоже отмечает переход компаний на контроль результата, а не самого процесса:Софья ГрихановаHR, эксперт на курсе «HR-менеджер» от НетологииПринцип work-life blend — размытие границ между работой и «неработой» — позволяет ослабить контроль над сотрудниками в рамках одного-двух рабочих дней. Как мне кажется, это благотворно влияет на состояние сотрудников. Мы уходим от контроля сотрудника, но переключаемся на более тщательный контроль результатов: по проектам, их контрольным точкам и задачам. Такая концепция подходит скорее для творческих команд и удалённой работы, когда идеи и решения не могут рождаться как по будильнику, при этом она требует самоорганизации и самоконтроля.Отладьте процесс онбординга. При правильном подходе онбординг даёт сотрудникам инструменты и информацию, необходимые для успешной работы в новой должности. В этот период новые коллеги вживаются в свои обязанности, задают вопросы, могут совершать ошибки.Софья ГрихановаHR, эксперт на курсе «HR-менеджер» от НетологииАдаптацию проходят специалисты любого грейда. Всех новых сотрудников необходимо поддерживать на старте внимательнее, при этом не забывать о них и после адаптации. У сотрудников с разными грейдами разный уровень ответственности и полномочий на проекте, а за этим стоят и разные формы контроля в процессе и по результатам проекта.Правильная организация онбординга требует больших предварительных усилий, но позволяет избежать микроменеджмента в дальнейшем. Это ключ к доверию между руководством и сотрудниками. Если вы всему научили нового сотрудника во время онбординга, дальше вам не придётся постоянно контролировать его работу.Устраивайте дни без совещаний. Оказывается, ежедневные совещания — не лучший подход к тимбилдингу. Необходимость постоянноотвлекатьсяна встречи вызывает у сотрудников раздражение и повышает стресс. А вот дни без совещаний позволяют людям работать в комфортном ритме, сотрудничать с другими в удобном режиме и формируют доверие между сотрудником и начальником. Это отличная профилактика микроменеджмента и тренировка эффективного взаимодействия, когда сотрудники получают достаточно пространства для работы с полной отдачей.Укрепляйте доверие. Это ключевая проблема микроменеджмента. Чтобы спокойно распределять обязанности, давать сотрудникам задачи и свободу действий при их выполнении, нужно доверие. Так что лучшее лекарство от микроменеджмента — благоприятная рабочая атмосфера в коллективе, в которой царит доверие. Оно не возникает в мгновение ока, но, как говорила секретарша Верочка в «Служебном романе», «нет ничего невозможного для человека с интеллектом». Укрепляйте партнёрские отношения с сослуживцами, развивайте эмоциональный интеллект и прислушивайтесь к мнению подчинённых.«Подорожники» против микроменеджмента, если вы сотрудникЕсли вы по другую сторону баррикад и вами руководит микроменеджер — не отчаивайтесь. Мировая общественность уже разработала рекомендации для сотрудников, оказавшихся в этой непростой ситуации:Постарайтесь понять, почему начальник ведёт себя именно так. Стремление контролировать всех и вся объясняется разными причинами. Может быть, у вашего руководителя сильный стресс, он страдает от недоверия к миру или у него просто такой характер. Чтобы разрулить ситуацию, нужно выяснить, в чём дело.И снова о доверии. Как мы выяснили раньше, микроменеджмент — проблема доверия. Чтобы недоверчивый начальник перестал вас контролировать, нужно доказать ему, что вы не подведёте. Выдавайте качественный результат, регулярно сообщайте руководителю о своих успехах и попытайтесь наладить личный контакт — да, человеческие отношения никто не отменял.Не стесняйтесь обсуждать свои чувства. Как ни парадоксально, это один из самых действенных подходов к решению проблемы. Честно, вежливо и спокойно объясните, что микроменеджмент мешает вам работать и подрывает результаты. Избегайте оборонительной позиции и не поддавайтесь гневу.Формируйте здоровые границы и реалистичные ожидания. Чётко проговорите свои обязанности, выясните ожидания руководителя на ваш счёт, чтобы в процессе работы не возникало недопонимания. Установите границы и не забывайте сигнализировать, если их нарушают. Такие изменения могут привести рабочую атмосферу в норму.Общение, общение и ещё раз общение. К счастью или несчастью, в рабочих отношениях нельзя поставить точку раз и навсегда. Это постоянный процесс: вы не сможете изменить ситуацию в одночасье. Чтобы всё не вернулось на круги своя, нужна постояннаяобратная связь. Проявляйте инициативу и общайтесь с руководителем, чтобы понимать, доволен ли он вашими результатами.Грозит ли нам эпидемия микроменеджмента в ближайшем будущемАнглоязычный интернет бьёт тревогу. В своей книге «Микроменеджмент» (My Way Or The Highway. The Micromanagement Survival Guide) Гарри Чемберс приводит данные опроса, которые должны вызывать серьёзные опасения:Ситуации в компанииРядовые сотрудникиТимлиды, исполнительное руководство, менеджеры среднего звенаСейчас сталкиваются с микроменеджментом со стороны руководителя37%27%Сталкивались с микроменеджментом со стороны предыдущих руководителей67%71%Задумывались о смене работы из-за микроменеджмента69%62%Увольнялись из-за микроменеджмента36%32%Заявили, что микроменеджмент мешает им работать71%73%Заявили, что микроменеджмент негативно сказывается на их моральном состоянии85%77%Эти цифры наводят нас на мысль об эпидемии микроменеджмента: шутка ли, треть сотрудников меняли работу из-за микроменеджмента, а две трети не могут нормально работать, когда начальник стоит над душой.Мы решили провести небольшойопроси сравнить полученные данные с российскими реалиями:Почти у половины наших респондентов грамотно организовано управление подчинёнными, а десятая доля опрошенных хоть и сталкивается с микроменеджментом, но обладает такой силой духа, что им это жизнь не портит. Всего 14% опрошенных меняли работу из-за гиперконтроля руководства. Похоже, у нас с культурой труда всё не столь печально.Игорь МартюшевМидл-Python-разработчик в Alaris LabsЯ не сталкивался с микроменеджментом. Благо руководство у меня адекватное и понимающее.Контролировать сотрудника в определённых рамках нужно, без этого никак. Но чрезмерный контроль и ограничения могут вызывать у работника чувство неуверенности, страха перед ошибками и недостатка инициативности. В результате команда становится менее продуктивной и творческой. Поэтому важно, чтобы руководство доверяло своим сотрудникам и предоставляло им достаточно свободы для самостоятельной работы.Ситуация не безоблачная, но всё же не катастрофическая: эпидемии микроменеджмента, похоже, в ближайшее время не будет. Пользуйтесь нашими шпаргалками, чтобы сорняки микроменеджмента не пустили почву в плодородные земли вашей компании, и формируйте культуру труда, основанную на взаимном доверии и реалистичных ожиданиях.Поделитесь в комментариях к статье своими историями о том, как вы сталкивались с микроменеджментом, а ещё лучше — как вы его победили🤗Чтобы расти, нужно выйти из привычной зоны и сделать шаг к переменам. Можно изучить новое, начав с бесплатных занятий:1С-аналитика: как стать специалистом по автоматизации процессов;Excel: простые шаги для оптимизации работы с данным;Системный аналитик: первые шаги к профессии;Soft Skills: как мягко добиваться карьерных целей;Основы Python: создаём телеграм-бота.Или открыть перспективы с профессиональным обучением и переподготовкой:Системное управление процессами и командой;Продакт-менеджер;Mini-MBA:Управление проектами и командами в IT.Основы трейдинга и инвестиций;Python для анализа данных."
СберМаркет,,,"10 бесплатных курсов и мастер-классов, которые стоит пройти в августе",2024-08-01T17:43:25.000Z,"​​🤦 Посмотрели на календарь, а лучше бы не смотрели!Последний месяц лета решили провести с максимальной пользой. Начнём, пожалуй, с бесплатных занятий ↓Эффективное использование нейросетей для обученияПо мнению экспертов, в обозримом будущем образование невозможно представить без участия ИИ: его будут использовать все участники образовательного процесса на всех этапах.Серьёзно изменятся методы преподавания и способы обучения, повлияют технологии на доступ к знаниям и подготовку учителей. Умение работать с ИИ становится преимуществом, и важно научиться правильно его применять и сделать своим помощником.Эта встреча будет полезна всем, кто следит за трендами и хочет на конкретных примерах разобраться, как правильно использовать нейросети в обучении.Когда — 5 августа в 19:00 (Мск).Узнать, как ИИ упрощает процесс обучения→Пошаговое внедрение LegalTech-решения: кейс РостелекомаПо прогнозам экспертов, к 2026 году автоматизацию рабочих процессов внедрят 40% юридических отделов.LegalOps — молодая и перспективная профессия, сочетающая в себе знания и опыт юриста, работу проджект-менеджера и преимущества сферы IT.На примере кейса Ростелекома вы узнаете: с чего начинается внедрение LegalTech-решения в компании, какие этапы включает процесс автоматизации, как подготовить сотрудников к изменениям. Плюс познакомитесь с первой (!) магистерской программой по legal technology в России и узнаете, что нужно для поступления.Когда — 7 августа в 18:00 (Мск).Познакомиться с LegalTech→1С-программист: первые шаги в профессиюРешения 1С используют тысячи компаний России, например, ПАО КамАЗ, издательство «Эксмо», Газпромнефть, Яндекс Доставка, медицинские и образовательные учреждения. Такой спрос позволяет найти хорошую работу в любом регионе страны и претендовать на достойную зарплату уже на старте карьеры.На бесплатном курсе вы сможете попробовать себя в роли 1С-программиста и создать с нуля своё первое приложение для учёта финансов с документами, отчётами и дашбордом для рабочего стола. Вас ждут 5 занятий и 5 практических заданий. Для комфортного обучения потребуется около 6 часов в неделю.Старт — 7 августа.Стать повелителем 1С→Системный аналитик: первые шаги к профессииIT-сфере нужны специалисты, которые помогут правильно понять боли бизнес-заказчика, составить грамотное техническое задание и оптимизировать процесс разработки. Без такого специалиста компания может вложить деньги в невостребованные решения из-за обычного недопонимания между заказчиком разработки и исполнителем.На курсе-симуляторе вы узнаете:как взаимодействуют фронтенд- и бэкенд-части сервиса;что такое мэппинг данных и где он используется;как составить грамотное ТЗ для разработчика.Вас ждут 5 часов теории и 14 часов практики. По итогу вы составите техническое задание на доработку логистики интернет-магазина.Старт — 13 августа.Узнать о мэппинге данных→Как сменить профессию: советы и кейсыНа бесплатном занятии вы узнаете:Зачем мы меняем профессии и почему это так сложно.Как сменить профессию и остаться в выигрыше: правила безопасности.Что мешает смене профеcсии: страхи и барьеры.Какие профессии востребованы в IT, определите подходящую под ваш опыт и цели.Как бесплатно получить индивидуальную карьерную консультацию.Какие перспективы, ошибки новичков и доход в IT: разберёте с HR-специалистом.Вместе с экспертом разберёте несколько кейсов выпускников — познакомитесь с реальными историями смены профессии. А после — сможете задать вопросы.Решиться на перемены→1С-аналитика: как стать специалистом по автоматизации процессовЗанятие будет полезно всем, кому интересна специальность на стыке управления проектами, 1С-разработки и аналитики.Вы познакомитесь с профессией 1С-аналитика — специалиста, который занимается внедрением платформы 1С в компании. Это ключевое звено между бизнесом и разработкой: специалист выясняет, какие проблемы есть у бизнеса и как их можно решить, пишет техническое задание для разработчиков, а затем тестирует готовое решение.Когда — 14 августа в 19:00 (Мск).Освоить задачи 1С-аналитика→Инструменты UX/UI-дизайнера: работа с Figma и нейросетямиС помощью ИИ можно не только написать тексты и сгенерировать изображение, но и создать дизайн-концепты, прототипы сайтов и макеты приложений. Важно понимать, как правильно его использовать и какой результат необходим.Вы поймёте на примере кейсов, как команда СДЭКа интегрировала новые технологии в свои проекты. Сможете применить опыт других в своей работе. Примете участие в создании персонажа с помощью нейросети и получите конкретные рекомендации от приглашённого эксперта по работе с Figma.Когда — 14 августа в 19:00 (Мск).Узнать, как применять ИИ→Специалист по информационной безопасности:  старт карьерыСпециалист по информационной безопасности защищает данные компаний и отбивает кибератаки. В прошлом году число таких кибератак в России взлетело на 80%, поэтому спрос на специалистов резко вырос.На бесплатном курсе вы попробуете себя в качестве такого специалиста и решите его настоящие задачи. Вы узнаете:что представляет из себя профессия,какие инструменты и методы использует специалист,как строить карьеру в этом направлении и какие навыки развивать.А по итогу курса найдёте уязвимости в веб-сервисах и настроите компоненты операционной системы для обеспечения безопасности.Старт курса — 14 августа.Познакомиться со сферой ИБ→Основы разработки на JavaПредлагаем подобраться к сниженным процентным ставкам по ипотеке и свободе от НДС, а также прикоснуться к космосу — создать приложение с фотографиями с сайта NASA для своего портфолио. За две недели вы:изучите основы одного из самых востребованных во всём мире языков программирования;познакомитесь с обязанностями Java-разработчика и возможной зарплатой в бэкенд-разработке;создадите приложение и перенесёте его в Telegram-бота.Старт — 15 августа.Познать Java→Как начать карьеру в аналитике и Data ScienceБизнесу нужны специалисты, умеющие работать с данными. Они помогают принимать взвешенные решения о развитии компании. В условиях турбулентности потребность в таких специалистах только растёт. По даннымисследованияХабр Карьеры за первое полугодие 2023, специалисты сферы аналитики в среднем зарабатывали 150 000 ₽. При этом аналитиков не хватает — спрос превышает предложение на рынке труда.Приглашаем на день открытых дверей всех, кто хочет разобраться в карьерных перспективах сферы работы с данными. Все участники получат гайд «Как войти в сферу данных и найти своё направление» и специальные цены на все программы обучения направления Аналитики и Data Science.Когда — 26 августа в 19:00 (Мск).Начать карьеру в аналитике→"
СберМаркет,,,"Алгоритмическое мышление для дата-сайентистов: как писать код, который экономит время и место",2024-07-30T11:47:18.000Z,"Алгоритмическое мышление заключается в том, чтобы, объединив строгую логику и творческие способности, структурировать, решать и анализировать задачи, чаще всего с помощью компьютера. С алгоритмическим мышлением тесно связаны задачи на упорядочивание, поиск и оптимизацию — именно с ними часто приходится иметь дело в Data Science — проектах.Алгоритмическое мышление помогает решать такие задачи с эффективным использованием времени и места — дискового пространства или памяти компьютера. В результате получаются быстрые и экономичные алгоритмы.Хотя стоимость хранения и вычислительных ресурсов будет и дальше снижаться в обозримом будущем, алгоритмическое мышление не потеряет важности в Data Science — проектах, и вот почему.Во-первых, во многих коммерческих сценариях использования требования заказчиков превосходят возможности доступных решений при любой исходной сложности Data Science — пайплайнов: от работы с источниками и преобразования данных до моделирования и подготовки ресурсов. Другими словами, у заказчиков часто бывают завышенные ожидания. Они надеются, что задачи, которые занимают дни или часы, можно сделать за минуты или секунды, а те, что занимают минуты или секунды, — в мгновение ока.Во-вторых, растёт число аналитических задач, выполняющихся напрямую на локальном устройстве: в контексте встроенных систем, IoT, пограничных вычислений. Для них эффективное использование вычислительных ресурсов имеет принципиальное значение. Пространства и памяти всегда не хватает, при этом иногда нет возможности перенести вычислительные ресурсы в более мощную и централизованную облачную среду.В-третьих, на работу промышленных Data Science — пайплайнов часто уходят значительные объёмы энергии, что усугубляет климатический кризис. Уверенные навыки алгоритмического мышления помогают аналитикам создавать эффективные экологичные решения, которые учитывают эти особенности.Выпускники профильных вузов, конечно, знакомы с основными концепциями алгоритмического мышления. Но всё больше людей приходят в Data Science из других предметных областей — от естественных и социальных наук до искусства. Скорее всего, в ближайшие годы эта тенденция только усилится за счёт достижений в области генеративного ИИ и преподавания Data Science в школах и вузах. Эта статья написана преимущественно для тех, кто незнаком с алгоритмическим мышлением. Мы начнём с общего обзора алгоритмического процесса решения задач, а затем перейдём к формированию алгоритмического мышления на материале программистских задач с платформыHackerRank, где компании подбирают дата-аналитиков. Приведём и несколько полезных источников для дальнейшего изучения. В завершение кратко рассмотрим применимость алгоритмического мышления в разработке программного обеспечения с помощью искусственного интеллекта и подведём итоги.Как решать задачуНазвание этого раздела вторит названию книги, написанной Дьёрдем Пойа, профессором Стэнфордского университета, американским математиком венгерского происхождения, и опубликованной в 1945 году. В книге«Как решать задачу»Пойа предлагает обманчиво простой, но чрезвычайно эффективный подход, который можно применять к алгоритмическому решению задач. Он состоит из четырёх этапов:Понять задачу. Обстоятельно сформулируйте задачу, обращая внимание на любые ограничения самой задачи и её решения: допустимые типы и диапазоны данных на вводе, формат на выходе, максимальное время выполнения и т. п. Для проверки спросите себя: «Могу ли я переформулировать задачу собственными словами?», «Хватает ли у меня данных для реализации полезного решения?». Используйте конкретные примеры или датасеты, чтобы сделать задачу и её пограничные случаи более рельефными. Если как следует поразмыслить на этом этапе, можно заметно упростить выполнение следующих шагов.Разработайте план. На этом этапе лучше разбить задачу на подзадачи, для которых уже известны эффективные решения. Навык находить подходящие решения и применять их к подзадачам разных типов (например, при поиске или сортировке) приходит с опытом. Но бывает, что требуется и творческий взгляд на задачу: когда нужно объединить несколько подходов, придумать новый или по аналогии позаимствовать решение из другой предметной области. Пойа даёт несколько рекомендаций в помощь мыслительному процессу, например: рисовать схемы, идти в обратном направлении, от искомой цели. На этом этапе полезно хотя бы в общих чертах прикинуть, поможет ли разработанный план решить задачу.Реализуйте план.Внедрите решение, используя подходящий инструментарий. Применительно к Data Science — проекту это могут быть библиотеки scikit-learn, PyTorch или TensorFlow для машинного обучения, платформы AWS, GCP и Azure для хостинга и запуска пайплайнов. На этом этапе нужно очень внимательно относиться к деталям, ведь даже из-за небольших ошибок в коде могут возникнуть отклонения от разработанного плана, которые не позволят решить задачу. Не скупитесь на модульные тесты: тщательно проверяйте разные части кода даже для пограничных случаев.Обернитесь. Привычка смотреть назад — неотъемлемая часть валидации Data Science — проектов. На вопросы вроде «У новой модели машинного обучения производительность выше, чем у старой?» можно ответить только в ретроспективе, собрав и оценив необходимые метрики по каждому эксперименту. Но для доработки текущего проекта и оптимизации будущих принципиально важно проверить и другие аспекты Data Science — пайплайна (код ETL, тестовые случаи, скрипты коммерческого внедрения), и управление жизненным циклом ИИ (уровень автоматизации, вопросы конфиденциальности и безопасности, реализацию цикла обратной связи в продакшн-среде). Это обязательный этап, даже если в динамичной рабочей атмосфере трудно найти время для такого обзорного взгляда на вещи.В процессе решения задачи по Пойа особенно трудно выполнить правильно первый и второй этапы. Структурировать условия или решение концептуально логическим и непротиворечивым образом — во многих случаях нетривиальная задача. И здесь поможет знакомство сконцептуальными моделями— аналитическими структурами для представления абстрактных концепций. К концептуальным моделям относятся диаграммы процессов, матрицы, древовидные и реляционные диаграммы. В книгеConceptual frameworks: a guide to structuring analyses, decisions and presentations, написанной автором этой статьи, простыми словами объясняется, как понимать, создавать, применять и оценивать такие концептуальные модели.Сложность алгоритмаВ контексте алгоритмического решения задач следует отдельно остановиться на темесложности. Сравнивая два разных алгоритма, нужно учитывать их временную и пространственную сложность, то есть как время и пространство (память), необходимые для каждого алгоритма, соотносятся с размером задачи или данных. Ориентируйтесь на пять уровней сложности: от минимального (лучшего) до максимального (худшего). Чтобы упростить ход мысли, опишем только временную сложность:Константная. Независимо от масштаба задачи, этот алгоритм всегда выполняется за одно и то же время. Формально константные алгоритмы считаются самыми быстрыми. Например, чтобы определить, является ли целое число чётным, вне зависимости от размера целого числа можно просто выяснить, делится ли его крайняя правая цифра на два без остатка. Доступ к элементу списка по индексу тоже выполняется практически мгновенно, независимо от длины списка.Логарифмическая. Для датасета размеромnалгоритм выполняется вlog(n)интервалов времени. Не забудьте, что у логарифма бывают разные основания, напримерlog2(n)для бинарного поиска, когда объём данных уменьшается в два раза с каждой итерацией. Как и константные алгоритмы, алгоритмы с логарифмической сложностью привлекательны тем, что масштабируются сублинейно по отношению к размеру задачи.Линейная. Как ясно из названия, для датасета размеромnалгоритм с линейной сложностью выполняется примерно вnвременных интервалов.Полиномиальная. Для некоторого положительного целого числаmалгоритм выполняется вx^2(квадратичное время),x^3(кубическое время) или, в общем смысле, вx^mвременных интервалов. Чтобы проверить полиномиальную сложность в коде, можно прибегнуть к одной хитрости: посчитать количество вложенных циклов. Например, функция с двумя вложенными циклами (цикл в цикле) имеет сложность x^2, а функция с тремя вложенными циклами — x^3 и так далее.Экспоненциальная. Для некоторого положительного целого числаmалгоритм выполняется в2^x,3^xили, в общем смысле, вm^xвременных интервалов. В этих статьях на StackExchange (ссылка 1,ссылка 2) объясняется, почему экспоненциальные функции в итоге становятся больше полиномиальных, и, следовательно, в плане алгоритмической сложности они хуже для больших задач.Для некоторых алгоритмов характерныкумулятивныеилимультипликативныесочетания уровней сложности. Например, если за цикломfor loopследует бинарный поиск, мы имеем дело с сочетанием линейной и логарифмической сложности, которое отличается последовательным выполнением режима цикла и поиска соответственно. И наоборот, когда в циклеfor loopв каждой итерации выполняется бинарный поиск, мы сталкиваемся с мультипликативным сочетанием линейной и логарифмической сложности.Хотя мультипликативные сочетания обычно оказываются дороже кумулятивных, иногда они неизбежны. Но их всё же можно оптимизировать. Например, алгоритм сортировки слиянием (merge sort) с временной сложностьюn log(n)обходится дешевле, чем сортировка выбором (selection sort), которая характеризуется квадратичной временной сложностью. Таблицу сравнения уровней сложности разных алгоритмов сортировки можно посмотретьздесьили ниже.BIG-OСложностьПримерO (1)КонстантнаяПоиск по ключу в хеш-таблице, арифметическая операция с числомO (log2(n))ЛогарифмическаяБинарный поиск, сложность, вставка, сбалансированное бинарное деревоO (n)ЛинейнаяПоиск перебором, среднеквадратическое отклонениеO (n × log2(n))КвазилинейнаяСамые быстрые алгоритмы сортировкиO (n2)КвадратичнаяПростые алгоритмы сортировки, перемножение n-значных чисел столбикомO (nx)ПолиномиальнаяLU-разложение матрицы, мощность графаO (cn)ЭкспоненциальнаяЗадача коммивояжёра (динамическое программирование)O (n!)ФакториальнаяЗадача коммивояжёра переборомУчимся на примере нескольких задачДалее рассмотрим подборку задач, опубликованных наHackerRank— социальной платформе, которая предлагает задания разной сложности по программированию. Похожие задачи можно найти и на сайтахLeetCodeиCodewars. Изучайте задачи, которые выкладывают на этих платформах. Так вы натренируете мышцу алгоритмического мышления, будете лучше проходить технические интервью (а эйчары любят спрашивать претендентов на аналитические должности об алгоритмах) и соберёте коллекцию фрагментов кода, которые можно использовать в работе.Все примеры сниппетов кода ниже написаны автором статьи на C++. Разработчики быстрых дата-пайплайнов часто предпочитают этот язык программирования. При необходимости эти сниппеты можно легко перевести на другие языки, например Python или R. Чтобы упростить сниппеты кода, допустим, что вверху файла с кодом есть строки:#include <bits/stdc++.h>
using namespace std;Так мы сможем везде опустить std:: и сосредоточиться исключительно на алгоритмах. Конечно, в рабочий код на C++ войдут только соответствующие библиотеки и std::, написанный в строгом соответствии с правилами разработки.Когда хватит и формулыИногда задачу, для которой мы собираемся использовать итеративное решение с полиномиальной сложностью (например, циклы for loop и while loop, генераторы списков), можно решить алгебраически с помощью формулы, выдающей нужный результат мгновенно.Возьмём задачу«Прыжки по числовой оси». Два кенгуру находятся где-то на числовой оси — на позицияхx1иx2соответственно. Они перемещаются прыжками. Первый кенгуру может прыгнуть за один раз наv1метров, второй — наv2метров. Имеются вводные значения дляx1,v1,x2иv2. Надо определить, могут ли оба кенгуру в какой-то точке времени в будущем оказаться на одной и той же позиции на числовой оси, если допустить, что каждый кенгуру может в каждый шаг времени прыгнуть только один раз. Решение функции должно представлять собой ответ ДА или НЕТ.Допустим,x1меньшеx2. Тогда решение задачи заключается в реализации цикла, который проверяет, сможет ли кенгуру, стартующий сx1,когда-нибудь догнать кенгуру, стартующего сx2. Другими словами, мы проверяем, существует ли положительный (целочисленный) шаг по времени, при которомx1 + v1 × t = x2 + v2 × t. Еслиx1большеx2, можно просто поменять местами значения соответствующих переменных в этом подходе. Но при большомtтакое решение может выполняться долго и даже затянуться до бесконечности, вызвав тайм-аут или сбой, если один кенгуру так и не догонит второго.Есть куда более эффективное решение. Выразим t из приведённого выше уравнения, чтобы найти целое положительное число. Получаемt = (x1 – x2) / (v2 – v1). Это уравнение дляtне может быть решено, еслиv2 = v1, поскольку на ноль делить нельзя. Но в этом случае ответ будет ДА: если оба кенгуру стартуют с одной позиции, очевидно, что они добегут до одной и той же позиции на числовой оси на следующем же шаге по времени. Более того, если оба кенгуру прыгают на одинаковое расстояние с разных стартовых позиций, ответ будет НЕТ: кенгуру, стартующий слева, никогда не догонит кенгуру, который стоит справа. Наконец, если мы находим положительное решение дляt, нужно убедиться, что это целое число. Для этого нужно привестиtк целочисленному типу данных и проверить, равно ли оно исходному значению. Вот сниппет кода, который реализует это решение:string kangaroo(int x1, int v1, int x2, int v2) {
    if((v2 == v1) && (x1 != x2)) return ""NO"";
    float t = 1.*(x1 - x2)/(v2 - v1);
    return ((0 < t) && (t == (int) t)) ? ""YES"" : ""NO"";
}Выбор из нескольких вариантовОдну и ту же задачу можно решить несколькими способами. Если вы нашли одно решение, это не значит, что нет других стоящих решений. У каждого подхода есть свои за и против, за счёт которых он лучше или хуже вписывается в контекст задачи. Чтобы подтвердить этот тезис, рассмотрим три задачи с разной степенью углубления в детали.Первая задача —«Кино в прекрасный день». Изучив её условия, вы поймёте, что ключевая часть решения — это поиск функции для инверсии положительного целого числа. Например, 123 в обратном порядке будет 321, а 12000 — 21 (обратите внимание, что в этом случае нули опущены).Первое решение назовёмreverse_num_v1. Здесь используется сочетание операций деления и деления по модулю, с помощью которых крайняя правая цифра переносится в крайнее левое положение. Начальные нули по традиции опускаются: см. пример ниже. Поскольку количество цифр относительно размера числа растёт логарифмически,reverse_num_v1отличается сублинейной временной сложностью, его пространственная сложность тоже ничтожна. Поэтому это достаточно привлекательный подход.int reverse_num_v1(int x) {
    long long res = 0;
    while (x) {
        res = res * 10 + x % 10;
        x /= 10;
        // Check for integer overflow
        if (res > INT_MAX || res < INT_MIN) return 0;
    }
    return res;
}В другом решении (назовём егоreverse_num_v2) мы конвертируем целое число в строковый тип данных, меняем цифры местами, отсекаем начальные нули, преобразуем строку обратно в целое число и получаем результат:int reverse_num_v2(int x) {
    string str = to_string(x);
    reverse(str.begin(), str.end());
    // Remove leading zeros
    str.erase(0, min(str.find_first_not_of('0'), str.size()-1));
    int res = stoi(str);
    // Check for integer overflow
    return (res > INT_MAX || res < INT_MIN) ? 0 : res;
}Такоепреобразование типов— распространённый приём во многих языках программирования: C++, Python и прочих. Есть готовые библиотечные функции для инверсии строк и обрезания начальных нулей, а цепочки функций для формирования пайплайна операций преобразования данных — это типичный паттерн для Data Science — проектов.Так что в первую очередь многим аналитикам приходит в голову именно решениеreverse_num_v2. Но если памяти не хватает, возможно,reverse_num_v1подойдёт больше, поскольку строковое представление целого числа занимает больше места, чем само целое число (вэтойдокументации можно ознакомиться с требованиями к памяти для разных типов данных в C++).Далее кратко рассмотрим ещё две задачи:«Пересчёт времени»и«Рисуем магический квадрат». Хотя на первый взгляд кажется, что это совершенно разные задачи, для решения обеих можно использовать один и тот же приём — использование таблиц или схем подстановки.В задаче«Пересчёт времени»можно использовать таблицу подстановки, чтобы мгновенно определять соответствие между 12- и 24-часовыми форматами послеобеденного времени. Например, восемь часов соответствует 20, девять — 21 и так далее.Задача«Рисуем магический квадрат»ограничивается квадратами, состоящими из трёх строк и трёх столбцов. Всего таких квадратов 8. Сохранив конфигурации этих магических квадратов в таблице поиска, можно реализовать относительно простое решение задачи, несмотря на её средний уровень сложности на HackerRank.Предлагаю вам перейти по указанным выше ссылкам и самостоятельно продумать эти задачи. Для сравнения рассмотрим соответствующие сниппеты кода для каждого решения ↓«Пересчёт времени»string timeConversion(string s) {
    // substr(pos, len) starts at position pos and spans len characters
    if(s.substr(s.size() - 2) == ""AM"") {
        if(s.substr(0, 2) == ""12"") return ""00"" + s.substr(2, s.size() - 4);
        else return s.substr(0, s.size() - 2);
    }
    else {
        // PM means add 12 to hours between 01 and 11
        // Store all 11 mappings of afternoon hours in a lookup table/map
        map<string, string> m = {
            {""01"", ""13""}, {""02"", ""14""}, {""03"", ""15""}, {""04"", ""16""},
            {""05"", ""17""}, {""06"", ""18""}, {""07"", ""19""}, {""08"", ""20""},
            {""09"", ""21""}, {""10"", ""22""}, {""11"", ""23""}
        };
        string hh = s.substr(0, 2);
        if(m.count(hh)) return m[s.substr(0, 2)] + s.substr(2, s.size() - 4);
        else return s.substr(0, s.size() - 2);
    }
}«Рисуем магический квадрат»Обратите внимание, что, хотя во фрагменте кода используются три вложенных цикла for loop, для решения задачи нужно только 8 × 3 × 3 = 72 цикла простых операций.int formingMagicSquare(vector<vector<int>> s) {
    // Store all 8 possible 3x3 magic squares in a lookup table/matrix
    vector<vector<int>> magic_squares = {
        {8, 1, 6, 3, 5, 7, 4, 9, 2},
        {6, 1, 8, 7, 5, 3, 2, 9, 4},
        {4, 9, 2, 3, 5, 7, 8, 1, 6},
        {2, 9, 4, 7, 5, 3, 6, 1, 8}, 
        {8, 3, 4, 1, 5, 9, 6, 7, 2}, 
        {4, 3, 8, 9, 5, 1, 2, 7, 6}, 
        {6, 7, 2, 1, 5, 9, 8, 3, 4}, 
        {2, 7, 6, 9, 5, 1, 4, 3, 8},
    };
    int min_cost = 81;  // Initialize with maximum possible cost of 9*9=81
    for (auto& magic_square : magic_squares) {
        int cost = 0;
        for (int i = 0; i < 3; i++) {
            for (int j = 0; j < 3; j++) {
                cost += abs(s[i][j] - magic_square[3*i + j]);
            }
        }
        min_cost = min(min_cost, cost);
    }
    return min_cost;
}Разделяй и властвуйЕсли задача кажется слишком большой и сложной, чтобы решить её в один заход, имеет смысл разделить её на подзадачи, решить которые по отдельности будет проще. В каждом случае это будут разные подзадачи: на сортировку, поиск или преобразование, с разным отношением к исходной задаче как части к целому.В Data Science часто возникает задача очистки данных. Здесь каждая подзадача представляет собой отдельный этап процесса очистки: удаление стоп-слов, лемматизацию и пр. В задачах типа go/no-go каждая подзадача заточена на более мелкое решение, в результате сложения которых получается общее решение исходной задачи go. С логической точки зрения её можно представить как сложное логическое выражение видаA AND B.Чтобы посмотреть, как подход «разделяй и властвуй» работает на практике, разберём две задачи, которые на первый взгляд кажутся совсем разными.Сначала возьмём задачу«Магазин электроники». По сути это задача на оптимизацию в заданных пределах.Дано: общий бюджетbи неотсортированные прайс-листы на клавиатуры и флешки. Назовём ихKиDсоответственно. Нужно купить самую дорогую клавиатуру и флешку, не превышая бюджет. В этой задаче в прайс-листах содержится до 1 000 наименований, но в реальности встречаются и варианты подлиннее.Примитивный подход к решению — перебирать элементы прайс-листовKиDс помощью двух вложенных циклов в поискахi-ой клавиатуры иj-ой флешки по максимальной цене, вписывающейся в бюджет. Это решение легко реализовать, но на него уйдёт слишком много времени, если прайс-листыKиDдлинные и тем более неотсортированные. У этого примитивного подхода квадратичная временная сложность, и это не сулит ничего хорошего при масштабировании до больших датасетов.Вот как сработает более эффективный подход. Сначала отсортируем оба прайс-листа. Затем выберем для выполнения циклов прайс-лист покороче. Для каждого элементаxв циклическом списке выполним в другом списке бинарный поиск элементаy(если такой будет) так, чтобы суммаx + yне превышала заданный бюджетb. Сохраняем полученный результат в переменнойmax_spentза пределами цикла. В каждой последующей итерации циклаmax_spentобновляется, только если стоимость последней пары «клавиатура + флешка» вписывается в бюджет и при этом превышает нынешнее значениеmax_spent.Хотя в этой задаче в любом случае придётся выполнять поиск в обоих прайс-листах, при эффективном подходе значительно сокращается общее время поиска, поскольку для циклов мы выбираем более короткий прайс-лист. Очень важно: бинарный поиск выполняем в более длинном прайс-листе, то есть имеем дело с логарифмическим или сублинейным временем выполнения. Более того, сначала может показаться, что предварительная сортировка двух прайс-листов только усложняет решение. На самом деле сортировку можно выполнить довольно эффективно, например с помощью merge sort. Самое важное, что она позволяет произвести бинарный поиск по более длинному прайс-листу. В результате мы получаем алгоритм, который работает гораздо быстрее первого предложенного варианта. Вот пример реализации эффективного решения:int findLargestY(int x, int b, const vector<int>& v) {
    // Simple implementation of binary search
    int i = 0, j = v.size(), y = -1, m, y_curr;
    while (i < j) {
        m = (i + j) / 2;
        y_curr = v[m];
        if (x + y_curr <= b) {
            y = y_curr;
            i = m + 1;
        }
        else j = m;
    }
    return y;
}

int getMoneySpent(vector<int> keyboards, vector<int> drives, int b) {
    int max_spent = -1;
    sort(keyboards.begin(), keyboards.end());
    sort(drives.begin(), drives.end());
    // Use smaller vector for looping, larger vector for binary search
    vector<int> *v1, *v2;
    if(keyboards.size() < drives.size()) {
        v1 = &keyboards;
        v2 = &drives;
    }
    else {
        v1 = &drives;
        v2 = &keyboards;       
    }
    
    int i = 0, j = v2->size(), x, y;
    for(int i = 0; i < v1->size(); i++) {
        x = (*v1)[i];
        if(x < b) {
            y = findLargestY(x, b, *v2);  // Use binary search
            if(y != -1) max_spent = max(max_spent, x + y);
        }
        else break;
    }
    return max_spent;
}Переходим к задаче «Вверх в таблице результатов». Представьте, что вы играете в аркаду и хотите отслеживать свой рейтинг в таблице результатов после каждой игры. В таблице используетсяdense ranking, т. е.функция ранжирования ​​DENSE_RANK() в упорядоченном наборе данных продолжает ранжирование без пропусков (например, 1, 1, 2), так что игроки с одинаковым числом баллов получают одинаковый рейтинг. Допустим, в таблице результатов есть 100, 90, 90 и 80 баллов. Тогда игрок, набравший 100 баллов, получает рейтинг 1, два игрока, набравшие по 90 баллов, получают рейтинг 2, а игрок с 80 баллами занимает третье место. Таблица результатов представляет собой массив или список целых чисел — лучших результатов каждого игрока — в порядке убывания. Подноготная задачи в том, что когда в таблицу добавляется новый результат, то определить рейтинг игрока не так-то просто, ведь он может совпадать с рейтингом других игроков. В описании задачи есть наглядный пример.Хотя задачам«Магазин электроники»и«Вверх в таблице результатов»на HackerRank присвоены уровни «лёгкий» и «средний» соответственно, вторая задача в каком-то смысле проще, потому что таблица результатов уже отсортирована. В примере решения ниже используется как раз эта особенность. Мы выполняем бинарный поиск по отсортированной таблице результатов, чтобы рассчитать рейтинг после каждого нового результата:int find_rank(int x, vector<int>& v) {
    // Binary search of rank
    int i = 0, j = v.size(), m_pos, m_val;
    while(i < j) {
        m_pos = (i + j)/2;
        m_val = v[m_pos];
        if(x == m_val) return m_pos + 1;  // Return rank
        else if(m_val > x) i = m_pos + 1;  // Rank must be lower
        else j = m_pos;  // Rank must be higher since val < x 
    }
    if(j < 0) return 1;  // Top rank
    else if(i >= v.size()) return v.size() + 1;  // Bottom rank
    else return (x >= m_val) ? m_pos + 1 : m_pos + 2;  // Some middle rank
}

vector<int> climbingLeaderboard(vector<int> ranked, vector<int> player) {
    // Derive vector v of unique values in ranked vector
    vector<int> v;
    v.push_back(ranked[0]);
    for(int i = 1; i < ranked.size(); i++)
        if(ranked[i - 1] != ranked[i]) v.push_back(ranked[i]);
    // Binary search of rank in v for each score
    vector<int> res;
    for(auto x : player) res.push_back(find_rank(x, v));
    return res;
}Что почитать по темеЗадачи, которые мы рассмотрели, дают общее представление о том, что такое алгоритмическое мышление, но эта тема заслуживает углублённого изучения. Книга Даниэля Зингаро с говорящим названием «Алгоритмы на практике. Решение реальных задач» — отличный учебник, который поможет совершенствоваться дальше. Зингаро увлекательно пишет и доходчиво объясняет базовые понятия курса:рандомизированные таблицы,рекурсию,динамическое программирование,поиск по графуи прочее. В книге есть приложение оBig O notation, с помощью которого можно аргументированно определять сложность алгоритмов.Ещё одна книга, в которой автор понятно рассказывает о нескольких основных алгоритмах, — «Грокаем алгоритмы» Адитьи Бхаргавы. В книге есть несколько полезных иллюстраций и сниппетов кода на Python плюс полезная информация об основах алгоритмического мышления, которая пригодится на собеседованиях.Андрей Грехов записал и выложил на YouTube несколькороликов, которые составляют отличное введение в динамическое программирование. Динамическое программирование — это мощное оружие, которое стоит добавить в свой арсенал. Освоив его, вы научитесь находить несколько способов решения задач в Data Science — проектах: задач на оптимизацию, когда то или иное количество (скажем, расходы или прибыль) надо максимально уменьшить или увеличить, или задач на комбинаторику, где нужно что-то посчитать, ответив на вопрос «Сколькими способами можно сделать XYZ?».Динамическое программирование хорошо подходит для решения задач с двумя свойствами:оптимальная подструктура, когда оптимальное решение части задачи помогает решить задачу в целом или её более крупную часть;частичное совпадение подзадач, когда результат, полученный при решении одной подзадачи, можно использовать в решении другой подзадачи без его пересчёта — то есть с помощьюсохранения результатов в памятииликеширования.Алгоритмическое мышление в эпоху искусственного интеллектаВ октябре 2023 года Мэтт Уэлш, бывший профессор информатики и технический директор Google, прочитал в Гарварде интересную лекцию с провокационным названием: «Большие языковые модели и конец программирования». Из неё следует, что развитие генеративного ИИ в целом, как и больших языковых моделей в частности, может кардинально изменить подход к разработке программного обеспечения. Да, люди, скорее всего, продолжат работать в области управления продуктом: за ними останется решение,какиепрограммы нужно создавать ипочему, в области тестирования и контроля качества — ведь кто-то должен убедиться, что программа работает как надо. Но, как считает Мэтт Уэлш, само преобразование спецификации задачи в код, готовый к работе в продакшн-среде, можно будет автоматизировать с помощью ИИ уже в обозримом будущем.К концу 2023 года инструменты на базе ИИ, например GitHub Copilot, уже демонстрировали способность дописывать базовые типы кода: сценарии тестирования, простые циклы и условные конструкции. У таких инструментов явно виден потенциал, они повысят продуктивность работы программистов, а может быть, и вовсе заменят их. С тех пор ИИ неизменно добивается впечатляющих успехов, а прогнозы о его работе в мультимодальном режиме сбываются всё точнее.В этом контексте стоит задуматься, насколько востребованным навыком будет алгоритмическое мышление для дата-сайентистов в эпоху искусственного интеллекта. Если вкратце, то, скорее всего, алгоритмическое мышление станет более важным, чем когда-либо.Если отвечать чуть подробнее, стоит признать, что уже сегодня во многих случаях черновик алгоритма вроде сниппетов кода, которые мы использовали в разделах выше, можно создать с помощью инструментов генеративного ИИ, таких как ChatGPT или GitHub Copilot. В конце концов, такие ИИ-инструменты обучаются, собирая данные из интернета, и, конечно, в сети дефицита кода не наблюдается. Но это вовсе не означает, что каждый код качественный, так что мы вполне можем вернуться к ситуации «мусор на входе — мусор на выходе».Любой код, генерируемый ИИ, обязательно нужно тщательно проверять и только после этого использовать в Data Science — проекте. А это значит, что ревьюеры с соответствующими техническими навыками останутся востребованными специалистами.Более того, возможно, созданный ИИ код нужно будет оптимизировать или подстраивать под требования конкретного сценария использования, и, скорее всего, промпт-инжинирингом тут не ограничишься. На самом деле составить промпт, с помощью которого можно адекватно генерировать нужный код, уловив при этом неписаные ноу-хау и мотивацию промпт-инженера, иногда труднее и дольше, чем просто сразу написать код на выбранном языке.При этом ни один подход не отменяет необходимости правильно сформулировать задачу и составить разумный план для реализации решения.Для таких заданий, как формулирование и планирование задачи, доработка, кастомизация и ревью сгенерированного ИИ программного кода в соответствии с требованиями сценариев, скорее всего, и дальше потребуется изрядный уровень алгоритмического мышления наряду с глубоким пониманиемнамерениянаписания кода, то есть ответа на вопросы «зачем», «для чего». Маловероятно, что такую работу в скором времени удастся делегировать искусственному интеллекту, и не в последнюю очередь по этическим и юридическим соображениям. Только представьте, что программу по предотвращению столкновений с объектами в системе самоуправляемого автомобиля создаёт искусственный интеллект без надлежащего надзора людей.Чтобы расти, нужно выйти из привычной зоны и сделать шаг к переменам. Можно изучить новое, начав с бесплатных занятий:Аналитик данных и Data Scientist: как выбрать профессию;Системный аналитик: первые шаги к профессии;Специалист по информационной безопасности: старт карьеры;1С-аналитика: как стать специалистом по автоматизации процессов;Вводный курсбакалавриата «Аналитика и Data Science».Или открыть перспективы с профессиональным обучением и переподготовкой:Data Scientist;1C-аналитик: расширенный курс;Онлайн-бакалавриат «Аналитика и Data Science»;Дата-инженер;Системный аналитик PRO."
СберМаркет,,,Не лгите в своём резюме. Техлиды всё равно узнают,2024-07-16T11:47:04.000Z,"Рынок труда в сфере IT давно стал рынком кандидата: количество вакансийрастётбыстрее, чем количество резюме. Но для многих соискателей — особенно претендующих на начальные позиции — процесс трудоустройства всё ещё сложен. Из-за этого велик соблазн подтасовать факты и представить себя в резюме более компетентным, чем на самом деле. Давайте разберёмся, для чего люди так делают и как это влияет на индустрию.Почему разработчики врут в резюмеНам с детства говорят, что обманывать нехорошо. Но когда до идеально подходящей вакансии остаётся пара шагов, велик соблазн приукрасить действительность, чтобы пройти первичный отбор и попасть на собеседование в компанию мечты. Часто у HR-специалистов есть фильтры, которые не позволят кандидату даже попасть на собеседование, — например, возраст или наличие формального образования. Многие кандидаты стремятся обойти их, меняя информацию в CV.АнатолийРазработчик, предпочёл не указывать место работыЯ приукрашивал несколько пунктов в своём резюме, — мне хотелось бы использовать именно слово «приукрашивать», а не «врать». Мне хотелось казаться лучше и презентабельное, чтобы поскорее устроиться на работу. Я прибавил к своим навыкам несколько пунктов и опыта в надежде выделиться среди остальных кандидатов. В результате заветный офер был получен.Вряд ли такое приукрашивание стало ключевым в моём трудоустройстве, так как все промежуточные этапы — техническое собеседование, решение тестовых заданий — я проходил честно, своими силами. Но могу предположить, что моё резюме привлекло к себе внимание.Другой случай, когда специалисты приукрашивают своё резюме, — желание сменить направление работы или попробовать другой стек. В этом случае они могут написать об опыте, которого фактически не было, например описать задачи соседней команды.Ещё один момент, о котором нельзя не сказать. В разных компаниях могут быть разные критерии оценки хорошего владения технологиями. Если на предыдущем месте разработчик лишь поверхностно сталкивался с какими-то инструментами, он может предположить, что этого будет достаточно для отклика на вакансию, где требуется продвинутое владение этим навыком. Что интересно, иногда так и выходит.Екатерина БорушнаяРуководитель команды бэкенд-разработки в VK WorkSpaceЯ не раз собеседовала кандидатов, резюме которых оказывалось не соответствующим реальности. Самые частые проблемы — завышение опыта или указание технологий, которыми соискатели не владеют в полной мере. Если возникают подозрения, мы всегда обсуждаем их с HR: если и у того возникли сомнения, кандидат дальше не пройдёт. Иногда даём тестовое задание, но это не сильно показательно: слишком много возможностей сделать его не своими руками.Я бы очень советовала не врать в резюме: при недостатке опыта резюме могут передать другой команде, если кандидат подходит под требования. В крайнем случае можно будет попробовать ещё раз, через год-другой, — это нормальная практика.А вот пойманного на лжи могут добавить в чёрный список и больше не пригласят на собеседование в компанию ни через год, ни через два.Почему ложь в резюме — это проблемаЛожь в резюме может создать проблемы для всех: самого соискателя, работодателя и рынка в целом.Она может всплыть уже во время интервью с HR. В этом случае соискателю откажут в должности, плюс его могут внести в чёрный список компании. И хорошо, если этот чёрный список не будет открытым для других работодателей.Если же получится пройти собеседование и устроиться на должность, для которой не хватает навыков, работа окажется слишком сложной. В этом случае разработчик может быстро выгореть и потерять несколько месяцев на восстановление.Ксения МаловаРуководитель «Академии IT-рекрутинга», сооснователь карьерного центра Career with loveВ рекрутерских комьюнити иногда рассказывают о случаях обмана со стороны кандидатов. Правда, чаще речь идёт не о конкретных случаях, а о новых методах. Например, на собеседование пришёл другой человек с виртуальной маской реального кандидата, а в процессе разговора маска слетела. Об этом говорят, чтобы коллеги были более внимательными. Чтобы предупреждали о конкретном кандидате, случай обмана должен быть вопиющим.Убытки работодателя очевидны: рабочее время HR-менеджера и технического специалиста будет потрачено на общение с заведомо неподходящим кандидатом. С другой стороны, иногда компании могут найти потенциальную суперзвезду, которая без уловок просто не попала бы на собеседование.АнатолийРазработчик, предпочёл не указывать место работыЯ считаю недопустимым откровенное враньё о своих навыках и знаниях. Такое поведение забирает время у большого количества людей, включая самого вруна. Будет правильнее честно признаться и себе, и представителям компании, что ты чего-то не знаешь, и подготовиться лучше в следующий раз.В результате поведения кандидатов, желающих казаться лучше, чем они есть, на рынке возникает неразбериха. По-настоящему скиллованные разработчики подвержены комплексусамозванцаи не указывают в резюме навыки, которыми владеют, а полные энтузиазма и самоуверенности «волки» занимают чужие места и создают напряжённость.Когда ни в коем случае нельзя лгатьНе стоит изменять ключевую информацию о своихпрофессиональных навыках, особенно если речь идёт о самых важных компетенциях. Если вы никогда не использовали определённую технологию — не пишите о ней в резюме.Ксения МаловаРуководитель «Академии IT-рекрутинга», сооснователь карьерного центра Career with loveОднажды мы нашли кандидата, у которого якобы было полтора года в нужной нам специфике в разработке мобильных приложений. Он начал работать в компании, но на корпоративе признался, что соврал в резюме и на самом деле выполнял совсем другие задачи. Этот человек проработал в компании полгода, а потом пришёл к руководителю с требованием поднять зарплату в два раза, иначе он уволится одним днём. Естественно, после этого разработчика уволили.Другой важный пункт —опыт работы. Не указывайте в резюме компании, в которых вы не работали, даже если это может выгодно выделить ваше резюме. Такая информация легко проверяется, и пойманный на лжи кандидат рискует попасть в чёрный список. Единственное исключение — если вы работали на крупную компанию по договору аутстаффинга. Но предварительно лучше убедиться, что NDA разрешает рассказывать о вашем участии в подобных проектах.В каких случаях можно приукрасить действительностьВыше мы сказали, что врать плохо. Но иногда немного приукрасить действительность — единственный способ добиться желаемого. Ниже рассмотрим примеры допустимой лжи.Изменить или не указывать локацию и возрастЕсли вы нашли идеальную и по всем параметрам подходящую вакансию в другом регионе, на неё можно откликнуться: мало в каких IT-компаниях сейчас нет возможности работать наудалёнке. При этом свой город в резюме можно не указывать. Конечно, речь не идёт о случаях, когда в вакансии прямо прописано требование находиться в определённой стране, — это может быть определено политикой безопасности.Возраст для большинства нанимающих менеджеров не является значимым критерием даже при поиске сотрудника на начальную позицию. Однако часто кандидатам после 40 сложно устроиться на новое место. Иногда достаточно просто удалить год рождения из резюме, чтобы количество приглашений на интервью увеличилось.Указать навыки, которыми владеешь на базовом уровнеВ резюме можно указать навыки, которые вы раньше применяли в работе, но которыми не владеете в совершенстве. Во-первых, зачастую этого будет достаточно. Во-вторых, представление о продвинутом знании, например, Redux может сильно отличаться от компании к компании.Александр МужевЭксперт QAMID (quality assurance middle) в НетологииЕсли вы уже обросли каким-то набором навыков (даже если получили их во время учёбы и освоили недостаточно хорошо), можно попробовать приврать в плане уровня знаний. Например, почти все изучали английский в школе и понимают довольно много слов. Можно указать в резюме, что имеются знания английского на уровне А1, хотя это и не так.Ни в коем случае не нужно врать про те навыки, с которыми вы не встречались в жизни. Если вы не знаете ни одного иероглифа, не пишите о владении китайским.Екатерина БорушнаяРуководитель команды бэкенд-разработки в VK WorkSpaceВсего знать невозможно. Да и редко находятся люди, соответствующие вакансии «до последней буквы». Как правило, есть две-три ключевых для вакансии технологии, остальные — опциональны, даже если явно это не написано. Я бы посоветовала перед собеседованием почитать о требуемых технологиях: часто для «опциональных» навыков может хватить и общего представления, если обязательной частью кандидат владеет.Не указывать грейд, если он ниже требуемогоЕсли в вакансии требуется разработчик с опытом мидла, а вы работали только на джуниор-позициях, можно удалить из резюме упоминания о грейде. Здесь история такая же, как и с навыками: порой джун по факту оказывается более опытным, чем человек, пару лет занимавшийся устаревшими технологиями в статусе мидла.Описать пет-проект или волонтёрство как коммерческий опытКарьерные консультантыне советуюттак делать, но зачастую задачи, которые выполнялись на добровольных началах, дают не меньше, чем опыт разработки в коммерческом секторе. Если перенести этот опыт из раздела «Волонтёрство» в раздел «Опыт работы», трагедии не произойдёт.Удалить из резюме нерелевантный опытФактически это не будет ложью: вы просто избавите рекрутера от малоинтересной информации о том, что до IT 5 лет проработали водителем.Александр МужевЭксперт QAMID (quality assurance middle) в НетологииВ целом я отношусь ко лжи в резюме нейтрально. Но если лгать в резюме, то нужно делать это уверенно. А ещё важно уметь развернуть сценарий вокруг любого написанного слова в резюме. Не нужно писать о том, о чём вы понятия не имеете.Что делать на техническом интервью, если написали в резюме неправдуТайное всегда становится явным — и не только в случае, когда вы выбрасываете в окно манную кашу. На техническом интервью в большинстве случаев будет понятно, чего стоит кандидат. Если вам задали слишком сложный вопрос или дали задачу, решения которой вы не понимаете, признайтесь в этом. Выскажите предположение о том, в каком направлении стоит думать, — в конце концов, умение правильно гуглить — один из главных навыков для разработчика.Екатерина БорушнаяРуководитель команды бэкенд-разработки в VK WorkSpaceПроще всего поймать нечестного кандидата на лайфкодинге. Такая проверка позволяет оценить и знания, и опыт. Но её часто критикуют: писать код под пристальным взглядом ревьюера — х10 к сложности.Важнее, чтобы кандидат сам понимал границы своей компетенции и честно говорил о них. Это поможет понять, к каким задачам его можно привлекать сразу, а где придётся подключать кого-то в помощь.Ложь в резюме: во благо или во вред?Правду говорить легко и приятно. Тем не менее иногда кандидаты приукрашивают реальность. В некоторых случаях это некритично. Например, можно:не указывать возраст или локацию;указать вCVтехнологии, которыми вы владеете на базовом уровне;не писать грейд на предыдущем месте работы — software developer смотрится солиднее, чем junior software developer;описать пет-проекты и волонтёрство в качестве коммерческого опыта;удалить нерелевантный опыт.Избегайте прямой лжи — но в некоторых случаях можно осветить реальность под нужным вам углом. По основным вопросам — о ключевых навыках и компетенциях, опыте работы в конкретных компаниях — лучше быть максимально честными.Трезво оценивайте себя, развивайте профессиональные навыки — и вам обязательно встретится идеальная вакансия.Чтобы расти, нужно выйти из привычной зоны и сделать шаг к переменам. Можно изучить новое, начав с бесплатных занятий:Soft Skills: как мягко добиваться карьерных целей;Мастер-класс «Как нейросети помогают оптимизировать бизнес-процессы»;Бесплатная консультация «Индивидуальная подготовка к выступлению/презентации»;Мастер-класс «Анализ данных и нейросети»;Карьера без страха.Или открыть перспективы с профессиональным обучением и переподготовкой:DevOps-инженер;Продакт-менеджер: расширенный курс;Управление рисками;Онлайн-магистратура «Управление проектами в IT и digital»;Системный аналитик PRO."
СберМаркет,,,Как настроить воркфлоу: 7 дельных советов от опытного разработчика,2024-07-13T17:50:53.000Z,"Полезные рекомендации, как организовать процесс разработки программного обеспечения так, чтобы показатели эффективности и продуктивности выросли.Время — ценный ресурс.Рост эффективности на 1 час в день экономит 1 месяц в год.Давайте посчитаем вместе:1 час в день × 5 дней в неделю × 52 недели = экономия 260 часов в год.260 часов ÷ 8 часов в день = 32,5 рабочих дня в год.Таким образом, вы на месяц приближаетесь к повышению, отдыху или любому другому занятию на своё усмотрение.Я разработчик. Мне удалось так наладить свою ежедневную работу, что последние несколько лет в среднем я успеваю сделать 1–2 запроса pull каждый день.Не то чтобы это наилучший показатель, и необязательно стремиться именно к нему, но он позволяет получить общее представление об эффективности работы.Сегодня я хочу поделиться с вами рекомендациями, благодаря которым мне это удалось.💡 Главная мысльОптимизируйте задачи, которыми вы занимаетесь по несколько разкаждый день.Исходя из расчётов в начале статьи, каждые две минуты, сэкономленные за рабочий день в результате автоматизации или оптимизации процесса, складываются в один рабочий день за год.Можете свериться с табличкой ниже, чтобы решить, имеет ли смысл оптимизировать ту или иную задачу. Но я этого не делаю. Я просто оптимизирую повторяющиеся ежедневные задачи и надеюсь на лучшее 🤞Таблица оптимизации времени xkcd. Она не совсем точно соотносится с концепцией рабочего дня, поскольку в ней расчёты ведутся в сутках.ИсточникВ статье я рассматриваю самые распространённые этапы рабочего процесса в программировании. В каждом разделе описываю, что я делаю для повышения эффективности. Если вы уже обогнали меня по продуктивности — это отлично. Делитесь своими успехами в комментариях — я всегда стремлюсь становиться ещё лучше.💻  Рабочий процесс / терминал GitЗдесь можно сильно сэкономить время тремя способами:Автодополнение по истории команд.Алиасы для быстрого набора команд.Упрощение управления файлами в Git.Автодополнение по истории командЯ пользуюсь такой настройкой терминала:iTerm2— терминал;Oh My Zshи Zsh (аналог Bash) —руководство по настройке;Starship— настройка командной строки терминала (необязательно, но всё-таки это полезно знать).Если вы уже настроили Oh My Zsh, добавьте эти «плагины» в файл.zshrc.# in ~/.zshrc
plugins=(
  git
  zsh-autosuggestions <--- we'll talk about this one now
  zsh-syntax-highlighting
)С помощью подсказок Zsh-плагин предлагает дописать каждую команду, которую вы вводите.Вот как это работает:Чтобы принять предложение из подсказки, нужно просто нажать стрелочку вправо на клавиатуре.АлиасыЯ добавил этот плагин к.zshrc.# in ~/.zshrc
plugins=(
  git <--- we'll talk about this one now
  zsh-autosuggestions
  zsh-syntax-highlighting
)Плагин Git добавляет алиасы для быстрого набора команд Git. Вот для примера несколько алиасов, которые я использую:ga => git add,gc => git commit,gd => git diff,gs => git status,gps => git push,gpl => git pull.Это экономит мне по крайней мере пару минут в день. А за год эти минуты складываются в часы.Ещё я добавляю пользовательские алиасы в.zshrc.Вот два примера:# in ~/.zshrc
alias addalias='code ~/.zshrc' <-- open up zshrc file to edit it
alias reload='source ~/.zshrc' <-- reload zshrc file to apply changesНумерованные сочетания клавиш для файлов GitSCM Breezeупрощает работу с файлами в Git: он помогает создавать нумерованные шоткаты для путей к файлам.Можно называть файлыgit add 2-3илиgit reset 1.Приведу пример:🖥️  Написание кодаВот как лучше всего оптимизировать работу в редакторе кода:Поиск кода в стеке.Допустим, вы ищете вызов метода. Вам нужно посмотреть, что за код у него внутри.Не используйте глобальный поиск cmd + F по имени этой функции, выискивая определение.Вот что я советую:Создание сочетаний клавиш для поиска переменной, метода, класса.Клавиша быстрого доступа в VS Code:F12.Сочетание клавиш в JetBrains IDE:cmd + B.Навигация между файлами.Бывает, работаешь сразу в 2–3 файлах и в каждом файле нужно вернуться в определённую точку. Нажимая nav в верхней части редактора, ищешь нужный из 15 открытых файлов. Или работаешь в большом файле и всё время прокручиваешь его то вверх, то вниз. Но и здесь можно организовать работу удобнее.Вот что я советую:Создание сочетания клавиш для перехода вперёд или назад по истории файлов. Так вы сможете удобно перемещаться и между редактируемыми файлами, и между конкретными разделами в файле.Сочетание клавиш в VS Code:Ctrl + -иCtrl + Shift + -.Сочетание клавиш в JetBrains IDE:cmd + option + ⬅️иcmd + option + ➡️.Написание кода.С тех пор как несколько месяцев назад я начал пользоватьсяGitHub Copilot, он уже сэкономил мне целые дни. Он предлагает автодополнение почти для каждой строки кода, который вы пишете, включая тесты. Удивительно умный инструмент 🤯📓 Сохраняйте интересноеВ Notion я сохраняю на потом всю полезную информацию.Тьяго Фортепопуляризировал этот подход как «создание второго мозга». Его суть в том, чтобы сохранять всякие полезности там, где вы будетеих использовать, а не там, где вы на них наткнулись.Вот скриншот двух моих папок с заметками в Notion:Каждая тема объединяет заметки из множества разных статей, видео, книг. Так мне гораздо проще найти их, когда они мне понадобятся.Я планирую рано или поздно активнее делиться их содержимым. Но многие мои статьи и так представляют собой доработанные версии заметок.✅ Не загружайте мозг идеями и задачамиНазначение вашего мозга — анализировать и решать творческие задачи, придумывать идеи, ане хранить всё это в памяти.Я сохраняю идеи и задачи вTodoist, а также пользуюсь функцией «Сохранить на потом» (Save for later) в Slack — и сразу же выбрасываю их из головы.Раньше я всё время загружал себе мозг. Метался от одной задачи к другой, стараясь ничего не забыть, и на их выполнение уходило в десять раз больше времени.А вот как выглядит мой список задач в Todoist ↓ Но вы можете организовать всё по-своему.Мне особенно нравится фича добавлять сайты как задачи. Если вы наткнулись на видео или статью, к которым хотите вернуться позже, их можно добавить в список дел одним щелчком мышиНеважно, какую именно программу вы выбрали. Главное — использовать мозг для решения задач, а не для хранения всей той информации, с которой вы сталкиваетесь.👀 Визуальная коммуникацияВот несколько примеров, как визуальный сторителлинг и донесение идей с помощью визуальных эффектов улучшают общение и делают его более эффективным:удобно описывать pull request;легко объяснить PM’у через сообщения в Slack, какое изменение вы хотите внести;начальнику проще утвердить или подписать скриншоты, показывающие созданный вами новый флоу.CleanShotсэкономил мне немыслимое количество времени. Он заменяет инструмент для создания скриншотов для Mac. Кроме того, инструмент:позволяет быстро и просто редактировать только что сделанный скриншот, который потом можно отправить куда угодно;умеет делать GIF и запись экрана — именно с помощью этого универсального инструмента я сделал все изображения для этой статьи;имеет массу других функций — например, запись прокрутки или размытие части изображения.Признаюсь, пример ниже — немного притянутый за уши. На самом деле, я часто снабжаю аннотациями скриншоты запросов pull, и на утверждение таких запросов уходит меньше времени:Часто возникает необходимость на скорую руку сделать запись экрана для презентации или сообщения в Slack, которое я отправляю дизайнеру или начальнику, чтобы согласовать работу той или иной функции.🔑 Вход на сайтМенеджер паролей — полезное решение по двум причинам:Это удобно: вам не нужно всё время вводить адреса электронной почты или пароли, он сделает это за вас.Это безопасно: он сам создаёт сложные уникальные пароли для каждого сайта.Когда вы входите на сайт с помощью1Password, это выглядит так:🪟 Работа с окнамиРазработчикам часто приходится переключаться между окном браузера, терминалом, редактором и мессенджером.Раньше я тратил уйму времени, вручную меняя размер окон. А потом начал пользоваться специальной программой —Rectangle. У неё есть бесплатная версия, а платная стоит около $10. С её помощью можно быстро разместить одно окно справа, другое слева, а третье открыть на полный экран. Rectangle экономит мне уйму времени.Ко всем этим лайфхакам я пришёл за шесть лет. Да, я медленно осваиваю новые инструменты, иначе чувствую себя перегруженным.Если вдруг вы не пользуетесь этими инструментами — это не означает, что с вами что-то не так. В своей статье я просто поделился тем, что хорошо сработало у меня и сделало мою жизнь легче. Возможно, вы тоже возьмёте что-то себе на вооружение.Буду признателен, если поделитесь в комментариях, чем сами пользуетесь для повышения эффективности работы, что работает для вас 🙏Чтобы расти, нужно выйти из привычной зоны и сделать шаг к переменам. Можно изучить новое, начав с бесплатных занятий:AI против спама: практическое руководство по разработке спам-фильтров;Как стать бэкенд-разработчиком;1С-программист: первые шаги в профессию;Как стать разработчиком на С++ с нуля;Frontend-разработка: основы HTML, CSS и JavaScript.Или открыть перспективы с профессиональным обучением и переподготовкой:Управление по Agile, Scrum, Kanban, Lean;Power BI & Excel PRO;Fullstack-разработчик на Python;Системный аналитик PRO;Mini-MBA: Управление проектами и командами в IT."
СберМаркет,,,Повышаем надёжность промышленного оборудования с помощью компьютерного зрения,2024-07-04T17:45:23.000Z,"Привет, Хабр! Меня зовут Павел Криницин. Я работаю на крупном металлургическом предприятии по производству и переработке алюминиевой продукции, где слежу за работой оборудования. В этой статье я расскажу, как мы исследовали способы раннего диагностирования повреждений конвейерных лент с применением компьютерного зрения. Эта статья будет полезна широкому кругу специалистов, занятых в различных областях промышленности, где применяют конвейерные транспортные системы. Описанные в статье подходы делают диагностику оборудования и поиск дефектов более точными, а планирование ремонтных работ — более эффективным.Павел КриницинРаботает аналитиком на крупном металлургическом предприятии и применяет алгоритмы машинного обучения и цифрового зрения для оптимизации производственных процессов.Но прежде чем мы перейдём к содержанию исследования, скажу пару слов о самом оборудовании и его роли в производственном процессе.Транспортное оборудование занимает большую долю всех промышленных объектов на металлургических предприятиях. От работоспособности ленточных конвейеров зависит функционирование всего производственного процесса. Транспортёры обеспечивают непрерывность подачи сырья на очередной производственный участок для его дальнейшей обработки и перемещают готовую продукцию на следующие этапы производства или на отгрузку.Устройством, которое перемещает грузы, могут быть металлические пластины — траки — или транспортёрная лента. Сфера их применения различна, у каждого есть свои преимущества, недостатки и технические особенности: сложность ремонта и монтажа, дороговизна запасных частей, ограничения по месту установки. Часто на производстве используют конвейеры с транспортёрной лентой, изготовленной на ПВХ- или резиновой основе. Их доля среди транспортёрного оборудования составляет более 90%.Рисунок 1. Металлическая цепь транспортёра, собранная из отдельных стальных пластин — траков, — не боится воздействия посторонних твёрдых предметов, которые способны повредить мягкую транспортёрную лентуСрок службы резиновой конвейерной ленты — в среднем один-два года, металлических траков — десятки лет. Учитывая высокую частоту отказов оборудования для транспортировки и переработки нефтяного кокса, повышение надёжности транспортёрной ленты — приоритетная и особо актуальная задача на производстве.Рисунок 2. С точки зрения срока службы транспортёрной ленты чаще всего проблемы возникают у конвейерного оборудованияВот основные проблемы, которые возникают при эксплуатации транспортёрной ленты конвейеров:Износ ленты — со временем лента истирается о транспортируемые материалы и изнашивается, это приводит к образованию сквозного износа и деформациям.Попадание посторонних предметов, которое механически повреждает ленту, создаёт на ней разрывы и сквозные дефекты.Повреждение резинового слоя ленты экстремальными температурами от воздействия нефтяного кокса после прокалочной печи и холодильного барабана — на рабочей поверхности образуются трещины, их глубина и интенсивность постепенно увеличиваются, и лента рвётся.Рисунок 3. Стадии развития дефекта транспортёрной ленты в результате повреждения рабочего слояДля защиты от высокой температуры применяется лента марки 2Т3, но и она не выдерживает длительного контакта с разогретым прокалённым коксом.На срок службы транспортёрных лент могут влиять и другие факторы — например, фракционный и химический состав сырья: прокалённый и сырой нефтяной кокс, анодная масса. При перегоне сырого нефтяного кокса зимой на транспортёрную ленту воздействуют химические реагенты, которые уменьшают смерзание сырья. Чтобы снизить их вредное воздействие, используют маслостойкую транспортёрную ленту 2МС. Но постепенно её рабочая поверхность размягчается, теряет эластичные свойства и деформируется.На срок службы транспортёрных лент влияют и разнообразные условия эксплуатации оборудования. Часть оборудования работает непрерывно, другая — по установленному графику. Различается и загруженность транспортёров сырьём: постоянная загруженность или периодически изменяющаяся в процессе работы, эксплуатация на предельной производительности или в щадящем режиме. Из-за этого установить нормативный срок эксплуатации для транспортёрных лент невозможно: он сильно варьируется для аналогичного оборудования даже в пределах одного цеха.Специально обученный персонал сервисной службы выполняет регулярную классическую визуальную диагностику технического состояния лент в процессе эксплуатации. Но у этой процедуры тоже свои недостатки. Во время осмотра лента обычно находится под слоем материала, а при большом количестве оборудования невозможно одновременно остановить все транспортировочные системы. Проблема человеческого фактора тоже имеет большое значение: каждый пропущенный специалистом дефект может остановить конвейер и всю производственную линию, что приведёт к крайне негативным последствиям.Именно поэтому так важно внедрять дифференцированный подход к обслуживанию оборудования, который учитывает степень влияния конкретного конвейера на производственный процесс в целом. Повреждения ленты могут быть разными: одни требуют немедленного устранения, другие позволяют планировать ремонт в прогнозируемые сроки. Поэтому перед нами встала задача разработать и внедрить автоматизированную систему диагностики ленты в процессе эксплуатации. Такая система значительно повысила бы точность и своевременность обнаружения дефектов, обеспечила бы более стабильную и безопасную работу производственных линий.Чтобы справиться с характерными неисправностями ленты на нашем производстве: трещинами поверхности, деформациями и локальными разрывами или сквозными отверстиями, — мы изучили возможности самостоятельно реализовать алгоритмы компьютерного зрения, так как хотели оценивать состояние транспортного оборудования без привлечения ремонтного персонала, то есть в автоматическом режиме.Компьютерное зрение — это технология, позволяющая компьютерам «видеть» и интерпретировать визуальные данные путём анализа изображений или видео. Она применяет методы обработки изображений, машинного обучения и искусственного интеллекта, чтобы выполнять задачи детекции и сегментации объектов, которые раньше могли выполнять только люди.Так, с помощью видеокамер и алгоритмов анализа изображений мы отследили состояние транспортёрной ленты в режиме реального времени, а система автоматически выявила признаки износа — трещины, разрывы.Классификатор состояний ленты на основе предварительно обученной модели нейросети ResNet-18Мы обработали видеопоток с камеры со скоростью записи 250 к/сек и получили изображения транспортёрных лент с дефектами. Присвоили изображениям классы в соответствии с доминирующим дефектом на них. Например, если на ленте были разрывы и трещины, то изображению присваивался класс «Разрыв ленты» — как наиболее важный с точки зрения дальнейшей безотказной работы оборудования.Чтобы повысить универсальность работы модели, выполнили аугментацию изображений на обучающей выборке:transformed_dataset = []
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),  # Случайное отражение по горизонтали с вероятностью 50%
    transforms.RandomRotation(degrees=(-15, 15)),  # Случайный поворот на угол от –15 до 15 градусов
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)  # Изменение цветовых характеристик
])Исказили положение изображений и их цветовую гамму.Рисунок 4. Изменяем положение изображений — камера может быть установлена под разными углами над конвейером — и моделируем различную освещённость объектаДалее импортировали из пакета Torchvision библиотеки PyTorch предобученную модель ResNet-18, заморозили все слои модели и поменяли последний полносвязный слой на новый — с четырьмя выходными классами. Определили функции потерь и оптимизатора:model = models.resnet18(pretrained=True)
for param in model.parameters():
    param.requires_grad = False
num_classes = 4  # Количество классов в нашем датасете
model.fc = nn.Linear(model.fc.in_features, num_classes)
# Определение функции потерь и оптимизатора
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)Потом пробно обучили модель-классификатор на 20 эпохах, получили значение ошибки Loss — 7%. Учитывая, что на обучающей выборке было всего 224 изображения, мы получили неплохие результаты.Чтобы проверить работу модели, загрузили изображение из тренировочной выборки. А после обработки моделью получили заключение о состоянии ленты.Рисунок 5. Результаты работы моделиТаким образом, мы выяснили, что благодаря классификатору можно выявлять наличие или отсутствие дефектов на ленте и определять, какая визуальная диагностика нужна: с привлечением специалистов сервисной организации или с помощью алгоритмов детекции и сегментации. Такой алгоритм работы прост в реализации, его можно применять для диагностирования большей части конвейерного оборудования и контроля его исправного состояния.Детектор дефектов транспортёрной ленты с использованием Fast R-CNN из библиотеки PyTorchЧтобы обучить модель-детектор, нужен был набор изображений с размеченными дефектами. Мы размечали их в программе CVAT. Классы распознаваемых дефектов — трещины, порывы ленты, состояние стыка ленты.Рисунок 6. Результаты разметки дефектов транспортёрной ленты сохраняются в формате .xmlВ программе каждому изображению присвоили свой ID-номер, название дефекта и его координаты. Для одного изображения могло быть несколько отметок о дефектах.Затем с помощью методов Python создали датафрейм на основе XML-разметки. Для этого определили функциюdef parse_annotation: она анализировала XML-файл с аннотациями для изображений. Функцияparse_annotationпреобразовывала XML-данные в список словарей, каждый из которых содержал подробную информацию об одном объекте изображения: имя файла, размер изображения, метку класса и координаты точек полигональной линии.def parse_annotation(xml_file):
    tree = ET.parse(xml_file)
    root = tree.getroot()

    objects = []
    for image in root.findall('image'):
        filename = image.get('name')
        width = int(image.get('width'))
        height = int(image.get('height'))

        for polyline in image.findall('polyline'):
            label = polyline.get('label')
            points = polyline.get('points')

            objects.append({
                'name': filename,
                'width': width,
                'height': height,
                'class': label,
                'points': points
            })
    return objectsПроверили разметку изображения на основании созданного датафрейма.Рисунок 7. Три класса разметки на изображении участка транспортёрной лентыДалее создали класс нашего датасета (class CustomDataset(Dataset)), — таким образом мы подготовили данные в удобном формате для загрузки батчами с помощью DataLoader из PyTorch:class CustomDataset(Dataset):
    def __init__(self, root, df, transforms=None):
        self.root = root
        self.transforms = transforms
        self.dataframe = df
        self.imgs = list(self.dataframe['filename'].unique())
    def __getitem__(self, idx):
        img_name = self.imgs[idx]
        img_path = os.path.join(self.root, img_name)
        img = Image.open(img_path).convert(""RGB"")
        current_annotations = self.dataframe[self.dataframe['filename'] == img_name]
        num_objs = len(current_annotations)
        boxes = []
        labels = []
        for _, row in current_annotations.iterrows():
            boxes.append([row[""xmin""], row[""ymin""], row[""xmax""], row[""ymax""]])
            labels.append(row[""class""])

        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)
        image_id = torch.tensor([idx])
        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)
        target = {""boxes"": boxes, ""labels"": labels, ""image_id"": image_id, ""area"": area, ""iscrowd"": iscrowd}
        if self.transforms:
            img, target = self.transforms(img, target)
        return img, target
    def __len__(self):
        return len(self.imgs)И создали функцию обучения модели-детектора. Обучили модель:def train_model(num_classes, num_epochs, batch_size, device):
    dataset = CustomDataset(img_path, df, transforms=get_transform(train=True))
    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate_fn)
    class_weights = calculate_class_weights(df)
    criterion = torch.nn.CrossEntropyLoss(weight=class_weights)
    weights = FasterRCNN_ResNet50_FPN_Weights.COCO_V1
    model = fasterrcnn_resnet50_fpn(weights=weights)
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
    model = model.to(device)
    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0
        progress_bar = tqdm(data_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)
        for images, targets in progress_bar:
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
            loss_dict = model(images, targets)
            losses = sum(loss for loss in loss_dict.values())
            epoch_loss += losses.item()
            optimizer.zero_grad()
            losses.backward()
            optimizer.step()
            progress_bar.set_postfix(loss=epoch_loss / (progress_bar.n + 1))
        lr_scheduler.step()
        print(f""Epoch {epoch + 1} completed with loss {epoch_loss / len(data_loader)}"")
    print(""Training completed."")
    return modelОбученная модель распознавала объекты на изображениях из тренировочной выборки.Рисунок 8. Результаты работы модели-детектораМодель-детектор улучшила диагностику транспортёрной ленты: позволила не только выявлять неисправности, но и оценивать их масштаб. Например, теперь можно было рассчитать площадь выявленных повреждений. Эта информация особенно важна для определения дальнейшей стратегии в ремонте транспортёрного оборудования.Применение такой модели подходит для оборудования, которое установлено в местах с затруднённым для сервисного обслуживания расположением — стеснённые условия, недостаточная высота потолков, — и позволяет полностью исключить диагностику специалистами сервисной службы.Сегментация изображений участков транспортёрной ленты с использованием Mask R-CNN из библиотеки PyTorchС помощью сегментации изображений обычно выделяют объекты и их границы на изображениях. Так мы определили функцию создания сегментационной маски объектов и их последующую визуализацию:def create_segmentation_mask(height, width, points):
    mask = np.zeros((height, width), dtype=np.uint8)
    points = points.split(';')
    points = [list(map(float, p.split(','))) for p in points]
    points = np.array(points, dtype=np.int32)
    cv2.fillPoly(mask, [points], 1)
    return mask
def show_images_with_masks(images, masks, num_images=3):
    combined = list(zip(images, masks))
    random.shuffle(combined)
    images, masks = zip(*combined)

    fig, axes = plt.subplots(num_images, 2, figsize=(10, 15))
    for i in range(num_images):
        axes[i, 0].imshow(images[i])
        axes[i, 0].set_title(f'Image {i+1}')
        axes[i, 1].imshow(masks[i], cmap='gray')
        axes[i, 1].set_title(f'Mask {i+1}')
        
    plt.tight_layout()
    plt.show()Рисунок 9. Маски объектов на изображении дефектного участка лентыСоздали модель сегментации изображений для обработки датасета, обучили её:# Загрузка модели
weights = MaskRCNN_ResNet50_FPN_Weights.COCO_V1
model = maskrcnn_resnet50_fpn(weights=weights)
# Количество классов (например, 1 класс + фон)
num_classes = 3
# Замена box_predictor
model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(
    model.roi_heads.box_predictor.cls_score.in_features, num_classes)
# Замена mask_predictor
model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(
    model.roi_heads.mask_predictor.conv5_mask.in_channels,
    model.roi_heads.mask_predictor.conv5_mask.out_channels,
    num_classes
)
# Проверка модели
print(model)Чтобы оценить качество обучения модели, визуализировали предсказания модели на новых изображениях.Рисунок 10. Сегментированные дефекты на транспортёрной лентеСегментация дефектов на ленте расширяет возможности диагностики, позволяя более точно рассчитать площадь повреждения ленты и предсказать остаточный срок эксплуатации. Характер расположения объектов на изображении указывает на причины возникновения дефектов, помогает разработать корректирующие мероприятия, чтобы исключить или ослабить влияние этих дефектов.Конвейерное оборудование обычно устанавливают и эксплуатируют в местах с повышенной запылённостью и влажностью, поэтому камеры должны быть с достаточной степенью пылевлагозащиты. Мы использовали видеокамеры со степенью защиты IP64.После обучения алгоритма размеченный датасет с изображениями участков транспортёрных лент показывает хорошие результаты и отлично выявляет дефекты на ленте. Этого достаточно, чтобы детально определить техническое состояние ленты и решить, нужно ли проводить её визуальный контроль с привлечением специалистов по сервисному обслуживанию.Алгоритмы, обученные с помощью координат вершин ограничивающих прямоугольников, предоставляют гораздо больше информации о самом дефекте: его приблизительном размере, месте расположения, характере повреждений. По этой информации можно предсказать срок службы ленты, примерно оценить площадь её повреждения и зафиксировать воздействие на неё высокой температуры или химических реагентов.Сегментация объектов на ленте даёт более качественное представление о площади повреждения и точно фиксирует повреждённые области. Применять сегментацию целесообразно для особо ответственного оборудования, так как его безаварийная работа обеспечивает работоспособность всей технологической линии производства.В дальнейшем мы будем оптимизировать систему обслуживания оборудования по категориям важности для производственного процесса на основе полученной информации о работе моделей и интегрировать этот алгоритм в систему управления предприятием — MES.Чтобы расти, нужно выйти из привычной зоны и сделать шаг к переменам. Можно изучить новое, начав с бесплатных занятий:Системный аналитик: первые шаги к профессии;Промышленное программирование: что нужно знать инженеру по автоматизации;1С-аналитика: как стать специалистом по автоматизации процессов;Пошаговый план «Как стать аналитиком данных и стартовать в Data Science»;Митап «Аналитика в промышленности».Или открыть перспективы с профессиональным обучением и переподготовкой:Системный аналитик PRO;Data Scientist: расширенный курс;Машинное обучение;Онлайн-бакалавриат «Аналитика и Data Science»;Deep Learning."
СберМаркет,,,"Отвлекать программистов от работы — гораздо страшнее, чем кажется на первый взгляд",2024-06-20T11:49:26.000Z,"Я всегда знал, что если отвлекать человека во время работы, это вредит делу. Но только прочитав книгу «В работу с головой» Кэла Ньюпорта, я понял, насколько это серьёзная проблема.После эпидемии коронавируса культура труда стала меняться в целом к лучшему. Но были и минусы: например, количество совещаний на одного сотрудникавыросло на 13,5%.Я попробовал разобраться, что такого ужасного в совещаниях и постоянном дёрганье, а также что с этим можно сделать.Что такое погружение в работуТерминdeep workпридумал Кэл Ньюпорт в своей великой книге «В работу с головой. Паттерны успеха от IT-специалиста».Сосредоточенная работа — это состояние, при котором задействована большая часть умственных способностей и которое обычно приносит уникальный ценный результат. Таких результатов нельзя добиться, если вас отвлекают.Если вы занимаетесь задачей во время созвона в Zoom — это не погружение в работу.Противоположность сосредоточенной работы —shallow work, работа без концентрации, которой можно заниматься, не напрягая мозги на 100%. Например, так можно отвечать на сообщения в Slack, писать электронные письма или просматривать документы.Когда-то программирование было пристанищем людей, испытывающих удовольствие от работы с высокой концентрацией внимания. Неслучайно ведь до сих пор кто-то представляет себе, что программисты сидят в подвале и работают в гордом одиночестве, отгородившись от внешнего мира наушниками.Сегодня найти время для сосредоточенной работы становится всё труднее.Почему погружение в работу так важноПогрузиться в работу — это единственный способ войти в так называемое состояние потока, когда чувствуешь, что ты в ударе.На протяжении 10 лет McKinseyпроводилиодно исследование. По полученным данным, в состоянии потока продуктивность специалиста повышается впятьраз.Это значит, что если бы мы могли увеличить длительность нашего состояния потока на 15–20%, общая продуктивность работы в коллективе практически удвоилась бы.Этому естьнаучное объяснение: в состоянии потока выделяются норадреналин, дофамин, эндорфины, анандамид и серотонин. Все они влияют на производительность.Более длительное погружение в работу не только повышает продуктивность разработчиков, но и помогает:реже ошибаться, ведь их меньше отвлекают;сохранятьбаланс между работой и отдыхом: они успевают больше за меньшее время, так что у них остаётся больше свободного времени;оттачивать мастерство: только в периоды такой концентрации внимания удаётся решать наиболее сложные задачи, совершая прорыв в развитии профессиональных навыков.Так в чём, собственно, проблема?Удалённая работа должна была стать отличным решением: никакого общения, ничего не отвлекает. Но добиться желаемого удалось только наполовину.С 2020 года на 13,5% выросло общее число совещаний. А вот количество удалённых совещанийувеличилосьаж на 60%! Что неудивительно, 92% опрошенныхпризнаются, что они делают несколько дел сразу во время этих созвонов. Рискну предположить, что для программистов этот показатель достигает 100%.Это порождает две основные проблемы:1. Сосредоточенная работа превращается в работу без концентрации вниманияПомните простую проверку: если вы занимаетесь чем-то во время созвона в Zoom, это не погружение в работу.Отличный пример — ревью запросов pull. Если у вас весь день занят созвонами, когда лучше всего сделать ревью? Конечно же, во время созвона. Но проверка важного запроса pull требует максимум сосредоточенности. То же касается исправления багов или подготовки проектной документации.Выполнение таких задач во время совещаний раскручивает жуткий маховик:Качество работы падает, порождая новые проблемы → для решения проблем проводят новые совещания.Во время созвонов сотрудники отвлекаются и принимают неоптимальные решения → снова возникает потребность в совещаниях.В этом виноваты не многозадачные разработчики, а манера организации совещаний.2. Разработчики не входят в состояние потока15 минут нужно, просто чтобы приступить к работе, итолько на 45-й минутеможно глубоко сконцентрироваться на выполняемой задаче.Каждый раз, когда вас отвлекают, этот таймер сбрасывается: так действует механизмпереключения между контекстами. А поскольку среднестатистического сотрудника отвлекают31,6 раза в день, он прерывает работу каждые 15 минут. То есть он практически никогда не достигает состояния потока.Теперь осознаёте всю катастрофичность ситуации? Отвлекают 31(!!!) раз в день.И ведь впустую тратится не только время самих совещаний. Каждый раз, как вы отвлекаетесь, вас отбрасывает назад на 15–45 минут. Получается, за весь рабочий день вы успеваете продуктивно поработать всего 1–2 часа. И это в лучшем случае.Мне нравится сравнение, которое я позаимствовал вэтом комментарии с Reddit:Представьте, что разработчики — это шахтёры. Копать — наша работа. Чтобы попасть на каждое совещание, нам нужно остановить работу и вовремя выбраться из шахты на поверхность.После совещания нужно снова спускаться в шахту к тому месту, на котором мы остановились. При этом не забыть, где это место, и не заблудиться.Так что, если вы хотите, чтобы мы добылиалмазы, дайте нам спокойно работать.В известной статье «График менеджера и график творца» Пол Грэм пишет: «Когда вы живёте в графике творца, совещания — это катастрофа. Одна встреча может съесть у вас полдня, разбив время на две слишком малых части, чтобы в них что-то успеть сделать».Разработчикам нужно минимум 4–5 часов непрерывной работы в день. И задача руководителя как раз в том, чтобы создать для них такие условия.Как выделять время для сосредоточенной работыРаботайте над культурой совещаний в компанииСитуацию не так уж сложно исправить: нужно только заставить всех руководителей соблюдать базовые правила:Совещания должны бытьэффективными. Очевидно, так ведь? Но почему-то так бывает довольно редко. У совещания должна быть чёткая повестка и результаты.Нужно выделитьфиксированное время дня для совещанийи высвободить для работы продолжительные интервалы, когда не проводятся никакие митинги. В зависимости от организации это могут быть дни или часы. Вот что помоглоГрегору Ойстерсеку:Мы установили фиксированное время для совещаний. Так у всех появилась возможность лучше планировать свой день и понимание, когда «переключаться между контекстами»Приглашайте только тех сотрудников, которым действительно нужно быть на этом совещании.Один из недостатков удалённой работы — в том, что она позволяет без проблем позвать всех. Люди думают: «Поделаю тихонечко своё, а если понадоблюсь, так я на связи».Это ужасный подход. Помните: во время многозадачности невозможно войти в состояние потока.Пользуйтесь преимуществами асинхронной коммуникации. Большую часть совещаний можно заменить перепиской в Slack или по электронной почте.Подавайте примерЕсли вы уже наладили организацию совещаний, можно приступать к борьбе с другим вредительством.Лучший способ взрастить адекватную культуру труда — позаботиться о собственном погружении в работу и попросить коллег считаться с этими периодами. Должен признаться, что здесь я ещё не полностью одержал верх: я указываю время для погружения в работу у себя в календаре, но всё равно иногда отвлекаюсь и пишу сообщения в Slack.Понимаю, что подаю плохой пример коллегам: сосредоточенная работа — это святое, и сотрудники должны знать, что не отвечать в Slack пару часов — совершенно нормально.Как оттачивать навык погружения в работуИтак, вы обуздали совещания и вас никто не отвлекает. Но тут выясняется, что погрузиться в работу всё равно непросто.Работать сосредоточенно — это направлять все свои умственные способности на решение задачи. По даннымисследования Ньюпорта, современная жизнь изменила работу человеческого мозга. Проверьте себя: долго ли вы можете работать, не заглядывая в телефон и не открывая какой-нибудь сайт? Например, среднестатистический американец проверяет смартфон 352 раза в день.Работать не отвлекаясь — значит не переключаться на письма, посты в соцсетях, сообщения в Slack, WhatsApp и другие уведомления.Нам снова нужноучить свой мозг терпению.Ньюпорт советует в начале сосредоточенной работы ставить реалистичную, но труднодостижимую цель.Допустим, вам нужно погрузиться в конкретную программистскую задачу и вы собираетесь сосредоточенно работать 60 минут. Через 20 минут вы спотыкаетесь, но время работы без концентрации, когда можно переписываться с коллегами в Slack, начнётся у вас только через 40 минут. Не отвлекайтесь. Да, даже если на решение задачи уйдёт больше времени, вы получите долгосрочное преимущество — натренированный мозг.Если вы зашли в тупик и не можете обойтись без посторонней помощи (что на самом деле должно случаться довольно редко), передвиньте дедлайн, например на 20 минут, ноникогда не бросайте работу сразу же. Постепенно у вас начнёт получаться.Я абсолютно уверен, что если вы исправите ситуацию, ваша команда будет вам благодарна. Именно вы можете создать культуру, которая поддерживает погружение в работу и уважает священный «час тишины».Чтобы расти, нужно выйти из привычной зоны и сделать шаг к переменам. Можно изучить новое, начав с бесплатных занятий:Основы Figma;Системный аналитик: первые шаги к профессии;Soft Skills: как мягко добиваться карьерных целей;Открытые занятия по аналитике;Основы разработки на Java.Или открыть перспективы с профессиональным обучением и переподготовкой:Системный аналитик;UX-исследования;Специалист по информационной безопасности;Продакт-менеджер: расширенный курс;Машинное обучение."
СберМаркет,,,Где нас нет: как живут айтишники в Сибири,2024-06-13T11:35:44.000Z,"Продолжаем рубрику «Где нас нет» о жизни ИТ-специалистов в российских регионах. После рассказов оДальнем Востоке,Северо-Западе,КавказеиУралеснова переместимся на восток и узнаем, как живут и работают айтишники в Сибири.Сибирский федеральный округ (СФО) — второй по величине в России после Дальневосточного: он занимает четверть территории страны. Сибирь граничит с Казахстаном на юго-западе, Китаем и Монголией на юге, а на севере выходит к арктическим морям.Северная часть Сибири находится в арктической и субарктической климатических зонах с продолжительными суровыми зимами и коротким прохладным летом. Юг округа граничит с Центральной Азией и имеет более континентальный климат с тёплым летом и холодной зимой. Ландшафты Сибири разнообразны и включают в себя обширные равнины (Западно-Сибирская равнина и Среднесибирское плоскогорье), горные хребты (Алтайские и Саянские горы), тайгу, степи и арктическую тундру. Сибирь богата полезными ископаемыми. На её территориисосредоточено85% общероссийских запасов свинца и платины, 80% угля и молибдена, 71% никеля, 69% меди, 44% серебра, 40% золота.К Сибирскому федеральному округу относится 10 регионов: 5 областей, 3 республики и 2 края. Крупнейший город региона — Новосибирск, третий по величине в России после Москвы и Санкт-Петербурга. Другие города-миллионники, Красноярск и Омск,занимаютсоответственно 7-е и 13-е место в стране по численности населения. Всего в округе проживает более 16,5 миллиона человек.Уровень жизни в Сибири неоднороден. Выше всего он в Новосибирской области (23-е место врейтинге по странеза 2023 год). Красноярский край занимает 40-ю строчку в том же рейтинге; Томская, Кемеровская, Омская и Иркутская области — 49-ю, 55-ю, 57-ю и 59-ю соответственно. Хакасия находится на 62-м месте по уровню жизни в России, Алтайский край — на 69-м. Два региона СФО, Республика Алтай и Республика Тыва, расположены в конце рейтинга: 80-е и 85-е место соответственно.Стоимость жизни в большинстве регионов Сибирине превышаетсредних показателей по стране. Исключение составляют Красноярский край, где цены ниже, чем в Москве, но выше, чем во многих других крупных городах России, и Иркутская область, где прожиточный минимум сопоставим с Санкт-Петербургом. В четырёх регионах — Республике Алтай, Кемеровской области, Алтайском крае, Омской области — стоимость жизни заметно ниже среднероссийской.Москва, Санкт-Петербург и средний показатель по России представлены для сравненияЗарплаты в Сибири в основномниже, чем в среднем по стране. Исключение — Красноярский край, где уровень зарплат выше среднероссийского, но всё же значительно уступает Москве и Петербургу. В большинстве регионов Сибирского федерального округа средняя заработная плата более чем вдвое ниже, чем в Москве, а в Алтайском крае — даже втрое.Москва, Санкт-Петербург и средний показатель по России представлены для сравненияУровень развития инфраструктуры в Сибиричуть нижесреднероссийского. Этот показатель очень варьируется в зависимости от региона. К примеру, в Иркутской области и Красноярском крае ситуация с инфраструктурой лучше, чем в среднем по стране, в то время как в Тыве и Республике Алтай наблюдаютсяпроблемыдаже с элементарным водоснабжением.В СФО индекс развития инфраструктуры ниже среднероссийского уровняСерьёзной проблемой Сибири остаётся отток экономически активного населения: ежегодно из регионауезжаетпорядка 287 тысяч человек. Он усугубляется последствиями демографического кризиса 1990-х годов. Ожидается, что к 2030 году численность населения в возрасте 25–44 лет снизится на 20%, достигнув рекордно низкого уровня.Большинство проблем сибирских городов можно назвать общими: недостаточность или изношенность коммунальной инфраструктуры, слабая транспортная доступность, нехватка образовательных учреждений — от детских садов до колледжей и техникумов, — низкий уровень развития малого и среднего бизнеса. Эти факторы создают значительные препятствия для экономического и социального развития. Например, дефицит коммунальной инфраструктуры затрудняет строительство жилья, а отсутствие транспортной доступности препятствует мобильности населения. Низкий уровень развития предпринимательства снижает возможности для трудоустройства, а нехватка образовательных учреждений ограничивает доступ к качественному образованию.Из Сибириуезжаютв Москву, Санкт-Петербург, города средней полосы и юга европейской части России, часть молодёжи и высококвалифицированных специалистов едет за рубеж. Причины такой миграции очевидны: это и образовательные, и карьерные перспективы, и более мягкий климат без суровых морозных зим и больших перепадов температур.Озеро Художников, природный парк «Ергаки», Красноярский крайЭкологическую обстановку в городах Сибири тоже сложно назвать благоприятной. Самым грязным городом России с точки зрения содержания вредных веществ в воздухесчитаетсяКрасноярск, аналогичные проблемы существуют в Абакане, Братске, Канске, Норильске, Новокузнецке, Кызыле и других населённых пунктах. Другой актуальной проблемой остаётсявысокий уровень преступности: самая неблагоприятная обстановка в округе — в Республике Алтай.Развитие информационных технологий в Сибири сконцентрировано в основном в крупных городах. Новосибирский государственный университет (НГУ) —один из лучших вузовстраны по зарплатам выпускников в ИТ-сфере. Но из-за оттока населения существует проблема сдефицитом ИТ-кадровдаже в региональных центрах. Около 40% сибирских айтишников готовыуехать за рубежили в другие регионы России.В то же время в крупные сибирские городастекаютсялюди из соседних регионов Сибири, Крайнего Севера, Урала, Дальнего Востока, иногда — из Казахстана и Центральной Азии. Новосибирск и Томск считаются важными образовательными центрами, Красноярск — наиболее динамичным городом Сибири для инвестиций, Омск — место для размеренной жизни. Многим нравится здесь жить — рядом с самобытной природой, вне бешеного ритма жизни столиц, со снежной морозной зимой и жарким летом. И среди этих людей есть те, кто готов работать на благо малой родины.Мария АстанинаНачальник ИТ-отдела в здравоохранении, КрасноярскЯ живу в Красноярске и работаю руководителем ИТ-отдела в родильном доме. Могу сказать, что в бюджетных организациях есть трудности с квалифицированными специалистами. Зарплаты примерно равны средним по городу. В коммерческом секторе ситуация несколько иная: более высокие зарплаты и более гибкие профессиональные стандарты привлекают и начинающих, и опытных айтишников.В Красноярске есть техникум информатики и вычислительной техники, а также вузы, такие как Сибирский федеральный университет и Сибирский государственный университет науки и технологий, которые предлагают широкий спектр ИТ-специальностей.Многие айтишники предпочитают работать вне офиса, в том числе на компании из других городов. Примерно 10–15% моих знакомых в этой сфере работают удалённо, в основном в сфере разработки и администрирования баз данных. В городе в целом этот процент примерно такой же.В последние годы всё больше ИТ-специалистов уезжает из Красноярска. Многие переезжают в места с более тёплым климатом, такие как Краснодар, Сочи и Крым, а также в Москву и Санкт-Петербург. Примерно 30% возвращаются.Мне в целом нравится жить в Красноярске. Среди городов Сибири он лучший: по благоустроенности, ритму жизни; здесь приличный уровень образования — как на творческих, так и на технических специальностях. Много интересных мест, куда можно съездить: национальный парк «Красноярские Столбы» в черте города, парк «Бобровый лог» — рай для лыжников и сноубордистов; отсюда недалеко и до Хакасии с её историческими курганами.Но есть один огромный минус, из-за которого многие красноярцы уезжают из города, — экология. Какое-то время назад я думала о переезде в Питер, плюс мне предлагали перебраться в Крым и даже в Лаос. Но, взвесив все за и против, осталась: здесь семья, дети, родители, своя квартира. Возможно, уеду позже, но пока аргументы остаться перевешивают.Сергей БояркинИнженер по автоматизации, ИркутскЯ инженер по автоматизации в крупном ритейле, живу в Иркутске. С ИТ в городе дела обстоят неплохо, в плане роста и получения опыта — даже хорошо. Сейчас я работаю удалённо на Москву. Но, например, хостинговая компания, где я начинал карьеру, родом отсюда, и по сей день костяк специалистов находится здесь, в Иркутске.До 2020 года зарплаты в ИТ были на уровне средних по городу, но с массовым переходом сотрудников на удалённую работу во время пандемии они пошли вверх. Это связано с тем, что начался отток специалистов из местных компаний в московские, питерские и зарубежные. Поэтому местным работодателям пришлось повышать денежные довольствия, чтобы избежать потери специалистов.Часть айтишников уезжает из Иркутска. Самое популярное направление для переезда — Питер. Кому-то нравится более мягкий климат, кому-то доступ к инфраструктуре крупных городов, которой никогда не будет в Иркутске.Мне нравится мой регион, близость к Байкалу, возможность за 10 минут выехать из города на природу. Ритм жизни не такой бешеный, как в европейской части страны. Рядом находятся красивые места Бурятии, Тункинская долина, минеральные источники посёлка Аршан. Здесь чистый воздух, здесь семья и друзья. Иногда возникают неудобства из-за несоответствия часовых поясов с московскими коллегами. Но пока что плюсы тишины и спокойствия перевешивают, а погулять по Москве я могу и во время командировок пару раз в год.Семён ФакторовичТехнический писатель, НовосибирскЯ по профессии технический писатель, соучредитель небольшой компании по разработке технической документации на заказ. Живу в Новосибирске, точнее в Академгородке.Айтишных вакансий в Новосибирске очень много. Большинство моих друзей в городе — это коллеги по цеху, и ни у кого никогда не бывает проблем с тем, чтобы найти или сменить работу. Разнообразие компаний очень большое: можно работать в маленьком стартапе, можно в ИТ-гиганте.Вариантов, где учиться, тоже больше одного. Каждый новосибирский вуз имеет в своём составе ИТ-кафедру или факультет. Зарплаты в отрасли тоже превышают средние по городу. Высококвалифицированные специалисты в некоторых областях — фармацевтике, юриспруденции, нефтегазовой сфере — зарабатывают примерно вровень с айтишниками, но большинство других специальностей точно получают меньше. На удалёнке работает больше половины знакомых ИТ-специалистов — как на новосибирские компании, так и на другие города и страны.Новосибирск — третий по величине город России, где достаточно развит рынок труда. Здесь хорошая интеллектуальная атмосфера, довольно низкая преступность, и потому приток приезжих есть всегда, особенно из небольших городов. Из минусов отмечу суровый климат, грязный воздух, довольно изношенные дороги.Миграцию в другие города и страны среди моих сверстников, людей 35+, уже трудно назвать массовой. Уезжают обычно те, кто помоложе. Коллеги-айтишники переезжают в основном в Москву, Питер и на Запад — в США и Европу (от Германии до Кипра). Но у меня здесь большая семья, своё дело. Академгородок — место со своей атмосферой, которая сильно отличается от остального Новосибирска. Рядом лес, местное «Обское море» — Новосибирское водохранилище, где можно и купаться, и сёрфить, и ходить под парусом. От дома до воды мне 10 минут на машине или 25 минут пешком. В целом мне комфортно здесь жить.Сергей ГолдобинИТ-предприниматель, ОмскЯ владелец компании itБРАТ. Мы занимаемся разработкой ПО для автоматизации бизнеса. Основное направление работы — API, web, чат-боты. За 5 лет мы успели поработать как с крупными компаниями (такими как с Ростелеком), так и с малым бизнесом. Я постоянный технический партнёр Департамента молодёжи и спорта Омской области, член «Опоры России» как предприниматель и друг, также мы помогаем бизнесу в формате консалтинга.ИТ-отрасль в Омске развита очень хорошо. Каждый год выпускники ОмГУ, ОмГТУ, Омского государственного университета путей сообщения, частных учебных заведений начинают карьеру в ИТ, некоторые достигают позиций тимлидов. В городе более 50 крупных ИТ-компаний и множество мелких. Вакансий очень много. Нехватка специалистов — это тренд для всей России. Омская область прикладывает все усилия, чтобы решать такие задачи.Если посмотреть только на Ассоциацию «ИТ-Кластер Сибири», участником которой я являюсь, — каждый год мы проводим в городе межрегиональный ИТ-форум, куда приезжают представители Минцифры России. В прошлом году на форуме присутствовал лично министр связи Максут Шадаев.Многие омские айтишники работают удалённо, в том числе на фрилансе: так можно зарабатывать 150 тысяч рублей и более, что превышает средние зарплаты в городе. Такие специалисты работают с заказчиками со всей России, из стран СНГ и дальнего зарубежья.Конечно, есть те, кто переезжает в большие города, в основном в Москву и Питер. Некоторые возвращаются, да и те, кто уехал, часто приезжают проведать родственников и друзей на малой родине. Но я уезжать не хочу: мой вариант — развивать свою любимую область. Для меня есть один важный плюс, который перекроет экологию, инфраструктуру и остальное: здесь я вырос, здесь отец похоронен, душой я здесь себя чувствую лучше.В любом регионе есть свои проблемы, климат, неразвитость в какой-либо области. Но это точки роста, как минимум надо о них знать и думать, что с этим делать. Что мне нравится в моём регионе — мы всегда открыто обсуждаем проблемы и находим решения вместе: даже не в рамках области, а всей могучей Сибирью.Олег ЗмеевАкадемический руководитель Высшей IT-Школы (HITs) Томского государственного университета, ТомскЯ академический руководитель Высшей IT-Школы ТГУ. Для простоты понимания можно сказать, что моя должность аналогична должности декана факультета.Томск — это город ИТ: у нас даже проводится одноимённая ежегодная конференция, самая крупная за Уралом. В городе достаточно много ИТ-компаний самого широкого профиля и самого разного размера. Большая часть этих компаний разрабатывает ПО на заказ и сотрудничает со всем миром, поэтому многие айтишники в городе работают удалённо на другие города, регионы и страны. Подготовкой айтишников занимаются три университета: ТГУ, ТУСУР и ТПУ. Есть и вполне неплохой по современным меркам ИТ-техникум.Зарплаты в ИТ примерно в 2–3 раза выше, чем в среднем по городу. Хотя, конечно, уровень зарплаты зависит от квалификации специалиста.По структуре рынка вакансий Томск — транзитный город: здесь достаточно легко начать ИТ-карьеру. Многие здесь дорастают до мидла, а потом уезжают. Варианты самые разные: выпускники вузов часто возвращаются в свои родные регионы, мидлы — в основном едут в Москву, Питер, за рубеж. В этом смысле Томск — достаточно стандартный город, который активно участвует в круговороте айтишников в природе.В Томске спокойно. Это красивый старинный сибирский город. Здесь высокая плотность интеллектуальной коммуникации: он недаром считается университетской столицей России. Томск — вечно молодой город с очень светлой энергетикой, где каждый пятый житель — студент. Кому-то хочется уехать в мегаполис или за рубеж за новыми возможностями. Но сам я никогда не думал об отъезде: я как раз не очень люблю города-миллионники.Александр БатютинИнженер-программист, НовокузнецкЯ инженер-программист, работаю удалённо на компанию из Сочи уже 10 лет. Живу в Новокузнецке. Местный рынок труда знаю не слишком хорошо: знаю, что есть свои веб-студии, но сравнить зарплаты не могу, — как-то сложилось, что с самого начала работаю на другой город.По моим наблюдениям, сейчас на удалёнке около 95% айтишников, даже тех, кто до 2020 года работал в офисе: пандемия кончилась, а тенденция осталась.Из Кузбасса многие уезжают: либо в столицы, либо на юг — в основном, в Краснодарский край, где теплее. Раньше думал уехать за границу, но в связи с обстановкой в мире передумал. В тёплые края тоже не хочу: у нас и зима нормальная, на лыжах покататься можно, и лето достаточно жаркое. Живу и работаю за городом на природе, что мне тоже очень нравится: ощущаю некий баланс между работой и жизнью, который меня полностью устраивает.Никита МакарьевДиректор ИТ-компании, БарнаулЯ живу в Барнауле, у меня своя ИТ-компания. У меня нет целостной картины ИТ-рынка в городе, но вакансий хватает, люди работают, многие — удалённо на другие города и регионы.Про город могу сказать, что меня устраивает здесь жить. Радует доступность активного отдыха: зимой это лыжный курорт Шерегеш, летом — Алтай и его горные маршруты. Приемлемая экология, приемлемая инфраструктура. При наличии достойной зарплаты здесь можно и жить, и работать — сам об отъезде не думал. Массового оттока коллег в другие города и страны в своём кругу не вижу.Владимир [фамилия скрыта по просьбе спикера]Специалист по VAS-услугам, АбаканЯ работаю в МТС специалистом по дополнительным услугам, живу в Абакане. Вакансий для айтишников в городе немного: в основном это торговля, угольные разрезы, медицина, связь, госсектор. Учатся в основном в местных вузах: Хакасском государственном университете и Хакасском техническом институте. Удалёнка мало распространена.Плюсы жизни в Абакане те же, что и в любом небольшом городе: всё рядом, пробок почти нет. Здесь достаточно парков и деревьев, есть водоёмы, есть куда съездить, красивая природа, летом чистый воздух. Зимой существует проблема с загрязнением воздуха из-за добычи угля. Главный минус — мало перспектив для карьерного и финансового роста. Поэтому из Абакана многие уезжают, в основном в крупные города: Красноярск, Новосибирск, Москву, Петербург. Я периодически задумываюсь о переезде в Питер, но окончательно пока не решил.Вячеслав КанПрограммист 1С, Горно-АлтайскЯ 1С-ник. Работаю в этом направлении уже почти 13 лет. До этого был системным администратором. Но специального образования у меня нет: освоил всё на энтузиазме и при помощи наставников на работе.Вакансий — по крайней мере, в моей сфере — в городе довольно много. Я бы сказал, что в целом в регионе остро ощущается кадровый голод, но это связано с очень высокой стоимостью жилья и горнолыжным комплексом «Манжерок», с которым конкурировать в плане зарплаты местные работодатели не в состоянии.Зарплаты в ИТ заметно выше средних по региону. Опять же, если не принимать в расчёт «Манжерок» и игорную зону Altai Palace. Айтишников, проживающих в Горном Алтае и работающих на удалёнке, очень немного. Как правило, они переезжают в соседние регионы. Это связано с более доступной стоимостью жилья и развитой инфраструктурой.Из всех, кого я знаю, за границу уехали только двое, и то из моего региона — только один. Он работал на удалёнке в зарубежной компании, но она закрыла офисы в России и встал выбор: переезд за границу или увольнение. Он говорит, что намерен вернуться при первой возможности.Главные минусы, которые я могу выделить в жизни в Горно-Алтайске, — это дорогое жильё и недостаточно развитая инфраструктура. А так это прекрасное место. Комфортный климат, буквально в сотне километров от города расположены массивные горы и леса. Я жил в Новосибирске, Барнауле, Кемерово. Везде — Россия, Сибирь, но всё-таки мой дом здесь. Я здесь родился, здесь вся моя семья, родственники, могилы деда и бабушек. Остался один дедушка, который живёт в Алтайском крае. Но это чуть больше 100 км пути — около 2 часов езды на машине. Я добровольно Россию покидать не собираюсь. Считаю, что нигде я не нужен больше, чем здесь.Али КужугетiOS-разработчик, КызылЯ iOS-разработчик, старший член ассоциации «Институт инженеров электротехники и электроники». Сейчас занимаюсь своим проектом — русско-тувинским переводчиком и базой параллельных переводов. Также разрабатываю клавиатуры для iOS: тувинскую, алтайскую, удмуртскую. Иногда занимаюсь и разработкой на Android, чтобы поддерживать свои проекты, консультирую других коллег и бизнесменов.Интерес к ИТ в регионе растёт. Вижу рост числа вакансий, их качества и зарплат. Многие ребята начинают интересоваться профессией уже в школьном возрасте: таких много, например, в моём Государственном лицее Республики Тыва, лицее №15, Президентском кадетском училище. Эти ребята, как правило, поступают в топовые универы страны: МГУ, НГУ, МФТИ, Иннополис. Неплохую подготовку дают в местном университете (ТувГУ). Есть и талантливые самоучки, которые создают свои проекты и идут выигрывать гранты. Сам я учился в Новосибирске.Долгое время зарплаты айтишников не сильно отличались от средних по городу, но в последнее время стали расти. Если у программиста опыт работы больше пяти лет, то он всегда претендует на более высокую зарплату, чем большинство госслужащих. Многие специалисты работают удалённо в компаниях из крупных городов и получают заметно больше. В новых отраслях, связанных с fullstack-разработкой, ИИ, данными, разработкой игр, можно заработать около 200 тысяч — это в несколько раз выше средней зарплаты по городу.Начинающие айтишники обычно работают в офисах, опытные — на удалёнке. Многие ребята не хотят жить в мегаполисах, возвращаются к родным: шум, спешка и карьерная гонка — это не их выбор. Кому-то нравится работать после 12 часов дня, когда у центрального офиса в Москве или Питере утро, кто-то работает удалённо на Новосибирск и другие крупные города Сибири.Из тех, кто уезжает учиться в Москву, Питер и Новосибирск, часть остаётся в этих городах. Некоторые переезжают в более тёплый климат, где комфортнее зимовать — например, в Сочи. Но есть и те, кто возвращается.Тувинцы ценят родственные связи, многим хочется жить в среде своей культуры: летом купаться на озёрах, есть экологически чистую баранину, отмечать местные праздники и события. За рубеж не уезжают так массово, как в европейской части страны. Хотя некоторые ведут бизнес со странами ближнего и дальнего зарубежья, даже несмотря на все политические и экономические сложности.В Кызыле многое развивается. Становится классно. Молодые бизнесмены развивают новые услуги и открывают современные заведения, часто со свежим переосмыслением родной культуры. Я люблю такое. Меня радует общение на тувинском на улице: не во всех регионах страны распространены местные языки, хотя Конституция РФ даёт такое право. В Тыве очень красивая природа, нет городской суеты. При этом зимой очень холодно, а инфраструктуры, наподобие метро или крытых беседок, где можно спастись от мороза, нет. Не решена проблема со смогом в холодный сезон.Раньше я думал о карьере в крупных ИТ-компаниях Питера или Москвы, чтобы дальше двигаться в компании Кремниевой долины, но меня пригласили на работу в правительство республики, где я 2 года занимал должность начальника департамента по информационной политике. Сейчас ушёл с госслужбы и занимаюсь своими проектами, а за рубеж езжу на стажировки, продолжая работать на благо родного края.Несмотря на общие для региона проблемы, у Сибири есть потенциал для развития ИТ-сферы. Развитие кластеров и создание благоприятной бизнес-среды могут способствовать привлечению талантов и стимулированию ИТ-предпринимательства. Интеграция вузов в ИТ-индустрию создаст прочную основу для будущих коммерческих разработок.Стимулом для развития региона могут стать совершенствование ИТ-инфраструктуры, расширение доступа к высокоскоростному интернету, привлечение инвестиций в высокие технологии. Важными шагами могут стать совместные усилия государственного и частного секторов, партнёрство с соседними странами, увеличение числа удалённых вакансий в сочетании с развитием инфраструктуры, повышением интереса к национальным культурам, решением экологических и социальных проблем.Источники фото и данных на картинках:Фото озера Художников:Андрей Грачев, Поход в Ергаки (июнь 2013 г.), часть 2.Население:«Численность населения Российской Федерации по муниципальным образованиям на 1 января 2023 года» по даннымРосстата.Стоимость жилья:ПриказМинстроя Россииот 11 декабря 2023 г. №888/пр «О нормативе стоимости одного квадратного метра общей площади жилого помещения по Российской Федерации на первое полугодие 2024 года и показателях средней рыночной стоимости одного квадратного метра общей площади жилого помещения по субъектам Российской Федерации на I квартал 2024 года».hh.индекс в ИТ:сервис открытой аналитики рынка труда по даннымhh.ru.Средняя зарплата:«Среднемесячная номинальная начисленная заработная плата работников по полному кругу организаций по субъектам Российской Федерации с 2013 года (по месяцам), рублей» по даннымРосстата.Прожиточный минимум:Социальный фонд России.Открытые ИТ-вакансии:работа в отрасли «Информационные технологии, системная интеграция, интернет» по даннымhh.ru.Ближайшие курсы Нетологии по программированию:Разработчик на C++;Системный администратор;DevOps-инженер;Backend-разработка на Node.js;Android-разработчик с нуля."
СберМаркет,,,Эволюция сети Ethernet на витой паре,2024-06-07T11:59:58.000Z,"Привет! Меня зовут Максим Куприянов. Уже около тридцати лет я тесно работаю с глобальными и локальными вычислительными сетями. Главным образом на уровне маршрутизации и проксирования трафика, но раньше приходилось в том числе проектировать и прокладывать Ethernet-сети в офисах и серверных помещениях.Как у многих других компьютерных энтузиастов, у меня дома есть своя маленькая серверная, и иногда я задумываюсь, что бы мне в ней улучшить. На этот раз я решил задуматься об апгрейде сети, ведь гигабитный Ethernet, который у меня там живёт, — уже довольно старая технология.Я начал читать, что сейчас предлагается использовать в домашних и офисных сетях, и ситуация оказалась довольно запутанной. Если в сетях дата-центров уже активно используется 400 Гбит/с, на подходе 800 Гбит/с и главный вопрос — какой форм-фактор подключаемого модуля выбрать, то для медных сетей на базе витой пары всё не так очевидно. Есть много стандартов, но наиболее распространены всё те же, что и двадцать лет назад. При этом один из самых распространённых вопросов на просторах сети о домашнем/офисном Ethernet — «Какую категорию витой пары прокладывать?» Пришлось сесть за список опубликованных стандартов IEEE 802.3 (это всё, что про Ethernet) и основательно в нём покопаться.В этой статье я хочу немного погрузиться в историю сети Ethernet на базе витой пары, затронуть коаксиальный кабель и разобраться, куда же мы всё-таки движемся и почему всё происходит именно так. Для меня этот экскурс оказался очень познавательным, надеюсь, вам тоже будет интересно. В конце я попытаюсь дать мотивированный ответ на этот самый распространённый вопрос о выборе кабеля.Начнём с самого начала.1973 год. Идея сети для общего доступа к принтерамВ начале 1970-х корпорация Xerox задумалась над идеей создания проводной локальной сети, которая позволила бы подключать к одному лазерному принтеру сразу множество компьютеров. В 1973 году сотрудники исследовательского центра Xerox PARC Роберт Меткалф и Дэвид Боггс представили описание экспериментальной сети, которая передавала данные со скоростью 2,94 Мбит/с по толстому коаксиальному кабелю (шине). Назвали сеть Ethernet (эфирная сеть) из-за схожести механизма разделения среды передачи с радиосетью Гавайского университета ALOHA (ALOHAnet).1983 год. 10BASE5 thicknetXerox совместно с DEC и Intel решили использовать Ethernet в качестве стандартного сетевого решения, что привело к созданию в 1982 году спецификации Ethernet II. А в 1983 году Институт инженеров электротехники и электроники (IEEE) утвердил официальный стандарт Ethernet — IEEE 802.3. Там был в том числе описан стандарт физического уровня сети10BASE5, он же thicknet (толстая сеть). К слову, стандарт был официально опубликован лишь в 1985 году, после одобрения его институтом ANSI. Поэтому правильнее называть его IEEE 802.3–1985.10BASE5 подразумевал использование толстого (9 мм) коаксиального кабеля типа RG-8X с волновым сопротивлением 50 Ом. Клиентов сети предлагалось подключать непосредственно к этому кабелю с помощью специальных устройств, прокалывающих внешнюю оболочку, — вампирчиков. Центральный шип вампирчика контактировал с центральной жилой коаксиального кабеля, а два боковых шипа входили в контакт с экраном основного кабеля. Сами вампирчики присоединялись к приёмопередатчикам, а те — уже через AUI-интерфейс (DA15F) к компьютеру.На изображении ниже — как выглядела эта конструкция в сборе.10BASE5, приёмопередатчик Cabletron ST500-02Максимальная длина шины самого коаксиального кабеля была ограничена 500 метрами, что и отражено в названии стандарта. Подключить можно было до 100 участников, а пропускная способность в 10 Мбит/с делилась на всех. Интересно, что участники должны были подключаться к кабелю с точными интервалами, кратными 2,5 метра.Важный момент: 10BASE5 использовал так называемоеманчестерское кодирование. Его ключевая особенность — возможность самосинхронизации участников сети за счёт того, что каждый переданный бит — это переход в середине такта с одного уровня на другой. А это значит, что не нужна отдельная линия для передачи синхросигнала.Манчестерский код1985 год. 10BASE2 thinnetОчевидно, 10BASE5 thicknet был не слишком удобным в практическом использовании, да ещё и дорогим. На замену ему в 1985 году в рамках документа IEEE 802.3a вышел новый стандарт — 10BASE2, также известный как cheapernet (дешёвая сеть) и thinnet (тонкая сеть). Стандарт предполагал использование более дешёвого и удобного коаксиального кабеляRG-58. Но длина сегмента ограничивалась 185 метрами, а количество одновременных участников могло достигать 30. Для подключения участников к сети начали использовать простые и дешёвые BNC T-коннекторы.Два компьютера, объединённые сетью Ethernet 10BASE2Лирическое отступление: 10BASE2 была первой локальной сетью, которую я собирал и поддерживал. До сих пор без всякого удовольствия вспоминаю, сколько сил уходило на диагностику сети из-за плохо обжатых коннекторов или кабеля, проложенного так, что его могла задеть швабра уборщицы. Кстати, интересно, что 10BASE5 был официально признан устаревшим лишь в 2003 году, а 10BASE2 — вообще в 2011-м. Искренне сочувствую тем, кто продолжал его сопровождать всё это время.Что же важного для будущего развития привнёс 10BASE2? Во-первых, он помог значительно вырасти популярности Ethernet за счёт простоты развёртывания, невысокой стоимости решения и хорошей скорости. Во-вторых, он вызвал проблему — сокращение размера сегмента. Для её решения в том же 1985 году появился стандарт IEEE 802.3c, где было описано устройство репитер (повторитель). Повторители позволяли связать несколько сегментов сети в один. Для 10BASE2 разрешалось использовать до четырёх репитеров, и это позволяло разнести сеть на целых 925 метров. Повторители — это по сути Ethernet-концентраторы (хабы), без которых немыслима сеть на витой паре.1986 год. 1BASE5 StarLANА эта часть истории обычно остаётся за кадром. В 1986 году был опубликован документ IEEE 802.3e, который описывал сеть Ethernet на основе витой пары. Сеть назвалиStarLAN, поскольку стандарт использовал топологию звезды из центрального узла, в отличие от 10BASE2 и 10BASE5. Раннюю версию StarLAN разработали Тим Рок и Билл Арангурен из компанииAT&T Information Systemsещё в 1983 году.Почему вообще витая пара? А всё очень просто: в то время витая пара в США очень активно использовалась для обычной телефонии в офисах, так же, как в СССР использовалась «лапша». Представьте, если бы вместо прокладки дополнительной сети из коаксиального кабеля можно было воспользоваться уже существующей телефонной инфраструктурой. Это же какая экономия сил и средств.Так вот, даже модуляция сигнала и спаривание проводов, применяемые в StarLAN, были тщательно подобраны так, чтобы они не влияли на аналоговый сигнал обычного телефонного вызова и не были подвержены его влиянию.Более того, всем известный стандартTIA/EIA-568-B— это не что иное, как стандартAT&T 258A (Systimax), который был первоначально разработан именно для StarLAN. Пара 1 (синий) оставалась неиспользованной для размещения пары для аналоговых телефонов. Пары 2 и 3 (оранжевый и зелёный) обеспечивали передачу сигналов StarLAN. Да и телефонный коннектор 8P8C был изобретён Western Electric, ею владелаBell System, которую затем разбили на несколько меньших компаний. Одной из этих компаний и была AT&T Information Systems.Стандарты соединения проводников витой пары с контактами разъёмов 8P8CПочему StarLAN называлась 1BASE5? Первое число — максимальная скорость (1 Мбит/с), второе — максимальная длина линка (до 500 метров в зависимости от качества кабеля и условий). Для работы требовались две пары: одна — на приём, вторая — на передачу. Также StarLAN поддерживала использование до пяти концентраторов одновременно. Ну и плюс всё то же манчестерское кодирование сигнала.1987 год. LattisNetВ 1987 г. компанияSynOptics Communications, образованная выходцами из Xerox PARC, представила первый сетевой концентратор, поддерживающий передачу данных на скорости 10 Мбит/с по неэкранированной витой паре на расстояние до 100 м. А также собственную вариацию Ethernet-сети, работающей поверх. Сеть назвалиLattisNet. Очень что-то напоминает, правда?Модульный Ethernet-концентратор SynOptics LattisNet Model 30301988 год. StarLAN 10Вслед за LattisNet, StarLAN тоже доработали сеть до поддержки 10 Мбит/с поверх стометровых участков неэкранированной витой пары.Ситуация накалялась. А тут ещё и сетиToken Ringот IBM с их 16 Мбит/с.Ситуация, в которой оказались DEC, Intel и Xerox в конце 80-х гг.1990 год. 10BASE-TИ вот в 1990 году, собрав все наработки и заручившись согласием на передачу патентов, IEEE выпускает документ 802.3i, в котором определяет использование неэкранированной витой пары (UTP) для передачи данных со скоростью 10 Мбит/с (1,25 МБ/с) в топологии «звезда». Рождается тот самый10BASE-T, положивший начало всем стандартам NBASE-T. 10 в названии стандарта — максимальная скорость (10 Мбит/с), T — twisted pair, то есть витая пара.Что стратегически нового и важного приносит нам 10BASE-T?Он собирает воедино наработки StarLAN и LattisNet, освобождая их от патентных ограничений.Унифицирует сигнализацию и кодирование данных.Прописывает спецификацию топологии «звезда», хотя внутри это всё еще шина.Фиксирует требования к используемой неэкранируемой витой паре (UTP Cat 3).Добавляет так называемый сигнал link beat для быстрого определения наличия активного соединения между узлами.Концентратор 3Com OfficeConnect, позволяющий соединять сеть 10BASE-T с сегментом 10BASE-21990 год. KalpanaНапомню, что сеть Ethernet даже самого свежего стандарта 10BASE-T работала по методуCSMA/CD. Если во время передачи кадра рабочая станция обнаруживала другой сигнал,  занимающий передающую среду, она останавливала передачу, посылала сигнал помехи и повторно отправляла кадр через некоторое случайное время (вот весь алгоритм). Представьте себе базар, где куча покупателей и продавцов громко общаются, пытаясь перекричать друг друга. Вот примерно столь же эффективен был Ethernet при относительно большом количестве активных участников в одном коллизионном домене.Решать проблему предлагалось разделением сегментов сети маршрутизаторами.Приблизительно в 1990 году на рынок выходит новый продукт - коммутатор Kalpana Ethernet Switch EPS-700. Идея была не нова: еще в 1986 году компания DEC представила реализацию интеллектуального мостаLANbridge 100. Но EPS-700 позволял объединить до 7 сегментов сети в одном шасси, обеспечивая пропускную способность в 30 Мбит/сек и задержку лишь в 40 мкс. При этом стоимость порта была лишь в районе $1500, что было значительно дешевле (и намного быстрее) использования маршрутизаторов.Второй важной инновацией был полный дуплекс. Два из семи портов можно было использовать для подключения к другому коммутатору. При этом один из портов работал только на передачу, а второй только на прием.И в третьих, Kalpana изобрела технологию агрегации каналов EtherChannels.Забавно, что EPS-700 не проходил под требования стандарта, чтобы называться мостом (bridge), поэтому маркетологи просто придумали новое красивое название Switch. Так и появились коммутаторы. А сама компания Kalpana в 1994-ом была поглощена Cisco Systems.1995 год. 100BASE-TXПрошло пять лет, Ethernet постепенно завоёвывал мир, а потребитель хотел большего. И вот в 1995 году на свет появляется IEEE 802.3u, где описана серия стандартов физического уровня передачи данных в сетях Ethernet на скорости 100 Мбит/с (12,5 МБ/с) как по оптоволокну, так и по витой паре.Для витой пары 802.3u предлагал два варианта, оба со 100 Мбит/с на участках до 100 м. Различались требования к кабелю.100BASE-TX требовал две пары в кабеле пятой категории (Cat 5): одна пара работала на приём, а вторая на передачу в полном дуплексе.100BASE-T4 требовал четыре пары, но в кабеле категории три (Cat 3): одна пара была выделена на передачу, одна на приём, а вот две оставшиеся могли работать как на приём, так и на передачу, но не одновременно. Другими словами, никакого полного дуплекса тут не было.100BASE-T4 не прижился, но подарил идею параллельной передачи данных по нескольким проводникам, которую мы ещё увидим немного позже. Плюс 100BASE-T4 первым использовал PAM-3 модуляцию, которая тоже не пропала даром.100BASE-TX же был принят рынком, и его, как и витую пару пятой категории, начали активно внедрять. Кстати, он широко применяется даже в новых продуктах и по сей день. Что полезного, кроме скорости, принёс новый стандарт?В первую очередь это отказ от манчестерского кодирования. Оно требовало удвоенной частоты передачи символов, что для 100BASE-TX равнялось бы 200 МГц, а это очень много. На замену пришли комбинация 4B/5B для избыточного кодирования и трёхпозиционный MLT-3 (Multi Level Transmission — 3). Особенность этого подхода в том, что можно существенно сэкономить на необходимой для передачи данных частоте. В частности, для работы 100BASE-TX достаточно полосы в 31,25 МГц, хотя с учётом 25% избыточности реальная скорость передачи данных составляет 125 миллионов символов в секунду.Витая пара третьей категории не рассчитана на частоты более 16 МГц. Даже экзотическая сейчас четвёртая категория витой пары позволяла максимум 20 МГц. Пришлось менять стандарты. Отсюда и требования к наличию пятой категории витой пары (Cat 5), которая гарантирует прохождение сигналов с частотой до 100 МГц.В два раза бо́льшая частота витков в Cat 5 по сравнению с Cat 3И, наконец, полный дуплекс. Вместо работы на общей шине было предложено использовать одну пару только на передачу, а вторую только на приём. Это позволило значительно снизить задержки (по некоторым данным, до пяти раз) и получить совокупную пропускную способность в 200 Мбит/сек (100 в каждую сторону). Было лишь одно ""но"": полный дуплекс возможен только при использовании коммутаторов, которые на тот момент стоили неприлично дорого. Могу предположить, что технологию предполагалось использовать для уровня агрегации, но сложилось иначе - на рынке начали появляться более дешевые коммутаторы и коллизионные штормы, ко всеобщему счастью, постепенно ушли в прошлое.История 1995 года была бы неполной без этого момента: в том же году вышел конкурирующий стандарт от компании Hewlett-Packard: IEEE 802.12–1995, известный как 100BaseVG. Стандарт, так же как и 100BASE-T4, предлагал работу по четырём парам Cat 3 в режиме полудуплекса: все четыре пары или передавали, или принимали данные. VG — это voice grade, как раз про категорию витой пары. 100VG-AnyLAN, как его ещё называли, несмотря на полудуплекс, предлагал лучшую утилизацию сети за счёт использования так называемого маркера передачи данных, который ранее использовался в сетях Token Ring. В отличие от Token Ring, маркер в AnyLAN не покидал концентратора, что позволяло утилизировать до 95% теоретической ёмкости сети, при том что стандарты 100BASE-TX и 100BASE-T4 выдавали 45% при условии использования концентраторов. Хотя с коммутаторами эффективность Ethernet достигала уже 96%.1997 год. 100BASE-T2Очередная попытка переиспользовать существующую кабельную инфраструктуру была предпринята в 1997 году с появлением стандарта IEEE 802.3y, где был описан 100BASE-T2. Стандарт предлагал всё те же 100 Мбит/с, но по двум парам Cat 3. Достигалось это за счёт сложного кодирования сигнала и модуляции по методу PAM-5 (Pulse Amplitude Modulation with 5 levels). PAM-5 использует 5 уровней напряжения для кодирования сигнала, что позволяет закодировать в одном символе два бита данных и оставить ещё один уровень алгоритмам поиска/коррекции ошибок.Стандарт благополучно умер, а вот метод модуляции PAM-5 прижился.Метод модуляции PAM-51999 год. 1000BASE-TИ наконец мы достигли пика технологического совершенства прошлого века в части скорости и удобства передачи данных. В 1999 году появляется IEEE 802.3ab, где описывается 1000BASE-T — передача со скоростью в 1 Гбит/с (125 МБ/с) по неэкранированной витой паре пятой категории (Cat 5). Сразу добавлю, что инженеры были чересчур оптимистичны относительно качества продаваемой кабельной продукции, и уже в 2000 году требования к качеству кабеля ужесточили, введя новую категорию — Cat 5e. Следует отметить, что качественный кабель Cat 5 вполне подходил и под требования Cat 5e.Итак, как они это сделали? Смотрите, как всё сложилось.Стандарт использует все четыре пары проводников, то есть нет никакого полудуплекса, выделенных пар под приём/передачу или телефонного сигнала по первой паре.Все пары могут синхронно передавать и принимать данные благодаря хитрому эхоподавлению, фильтрации и компенсации перекрёстных наводок.Каждый байт передаваемых данных разбивается на четыре части по два бита, и эти части передаются параллельно по четырём парам (пришло из 100BASE-T4).Для модуляции используется 4D-PAM-5 (по сути взяли из 100BASE-T2), который позволяет уложить два бита в один символ. То есть за один такт передаётся байт по четырём парам целиком.Так как PAM-5 использует пять фиксированных уровней напряжения, а для кодирования двух битов достаточно четырёх, то пятый (ноль) используется для коррекции ошибок, что позволяет компенсировать расходы на избыточное кодирование.Таким образом, максимальная скорость передачи данных по одной паре растёт со 100 Мбит в 100BASE-TX до 250 Мбит в 1000BASE-T.Требования к частотному диапазону вырастают в два раза, до 62,5 МГц, но всё ещё остаются в пределах возможностей Cat 5 (и Cat 5e).Схема работы кодирования 4D-PAM-5 в 1000BASE-TСтандарт 1000BASE-T подсветил три основных последующих направления развития сети Ethernet:Параллелизация передачи. Для витой пары на стандартном разъёме четыре линии передачи — это уже предел, но для оптоволокна — вовсе нет.Увеличение максимальной частоты передачи данных и снижение помех. И это очень больная тема для витой пары.Усложнение алгоритмов кодирования. С одной стороны, это передача большего количества битов за такт. С другой — специальная предварительная подготовка сигнала, алгоритмы обнаружения ошибок и восстановления сигнала, а также сама модуляция снижают требования к качественным характеристикам кабеля.Вот по этому вектору Ethernet и развивался следующие 25 лет. Давайте посмотрим, как именно. Чтобы не уходить в дебри, я буду говорить только о сетях, построенных на основе неэкранированной (но иногда и фольгированной) витой пары.2006 год. 10GBASE-TИтак, прошло семь лет с выхода в свет гигабитного Ethernet. Стандарт на десятигигабитный Ethernet по оптоволокну (802.3ae) появился в 2002 году.Стоит добавить, что в этом же году появился стандарт на витую пару шестой категории (Cat 6). Подавался он под соусом: с шестой-то категорией вы точно будете готовы к десятигигабитным сетям, которые вот-вот появятся. Ведь Cat 6 гарантировал передачу сигнала на частоте уже до 250 МГц. Внимательный читатель тут же сопоставит 250 МГц Cat 6 и 100 МГц Cat 5 и Cat 5e и не увидит десятикратного роста. И будет совершенно прав: чуда не случилось. В итоге Cat 6 одобрили для работы на скорости 10 Гбит/с лишь для коротких участков.Сравнение кабелей Cat 6 и Cat 5e. Cat 6 толще за счёт крестовины и большего числа витков на дюймПродолжаем разбирать сложный путь рождения десятигигабитного Ethernet по витой паре. В 2004 году появляется стандарт передачи по твинаксиальному кабелю (10GBASE-CX4, 802.3ak), что хорошо принимается рынком, так как позволяет решить вопросы коммутации внутри стоек. Позднее их вытеснили более удобные DAC-кабели (Direct Attach Copper) с разъёмами SFP+ или QSFP на концах.Медный твин-коаксиальный кабель стандарта CX4И лишь в 2006 году наконец-то вышел 802.3an, он же 10GBASE-T. Четыре года инженеры потратили на размышления и эксперименты по утрамбовыванию сигнала в UTP. У них получилось, вот только очень сложно и дорого.10GBASE-T, SFP+ трансиверДля передачи сигнала на небольшие расстояния, до 55 метров, требовалась неэкранированная витая пара Cat 6 (максимальная частота передачи сигнала — 250 МГц). Для передачи на участки до 100 м требовалась уже категория 6a (да здравствуют инвестиции в будущее, сделанные кем-то ранее). Cat 6a обеспечивала передачу сигнала на частоте до 500 МГц. И да, это уже был прорыв. Но как его достигли?А достигли за счёт сложного кодирования данных: комбинации алгоритмов PAM-16 и DSQ128. Если 4D-PAM-5 из 1000BASE-T мог передавать два бита за такт, то PAM-16 способен передать уже четыре.В итоге на рынке 10GBASE-T появился в виде реально существующих портов ещё через два года, в 2008-м. К сожалению, он, пожалуй, так до сих пор успешным и не стал. На это есть как минимум три причины:Во-первых, дорогая и довольно неудобная Cat 6a. Кабели стали существенно толще за счёт фольгирования и появившейся разделительной крестовины. Коннекторы тоже немного поменялись. В офисах никто обновлять СКС под десять гигабит не станет — там и одного гигабита обычно более чем достаточно. В дата-центрах уже́, как правило, оптоволокно. Для домашних пользователей исключительно высокая цена. Кто же тогда потребитель?Во-вторых, интерфейсы 10GBASE-T потребляют довольно много электроэнергии и заметно греются.В-третьих, по сравнению с оптоволокном 10GBASE-T показывает в десять раз бо́льшую задержку на линк.Вот результатыпроведённых замеров, это подтверждающие.В итоге стандарт остался нишевым и как-то не прижился. Но не всё так плохо, инженеры сделали выводы, и сейчас мы посмотрим, что произошло десять лет спустя.2016 год. 2.5GBASE-T и 5GBASE-TЧерез 17 лет после возникновения гигабитного стандарта 1000BASE-T появилось то, что должно было появиться с удешевлением элементной базы для реализации сложных алгоритмов модуляции. Инженеры компаний, входящих в Ethernet Alliance, решили применить алгоритмические наработки 10GBASE-T к широко распространённым кабельным системам на базе Cat 5e и Cat 6. И да, модуляция PAM-16 (и всё остальное, что там есть в кишочках) поверх Cat 5e позволила получить 2,5 Гбит/с на участках до 100 метров, а поверх Cat 6 — 5 Гбит/с. Оба физических стандарта, 2.5GBASE-T и 5GBASE-T, были опубликованы в одном документе — IEEE 802.3bz.Коммутатор TP-Link SOHO-уровня с поддержкой портов 2,5 Гбит/сИнтересно и даже уникально, что в рамках того же документа была зафиксирована поддержка Power over Ethernet (IEEE 802.3at, PoE+). Таким образом, авторы документа очень толсто намекают, что оба этих стандарта в первую очередь предназначены для подключения Wi-Fi 5 и более современных точек доступа, требующих более чем гигабитных аплинков.Из собственной практики домашнего использования: я попробовал к имеющейся и довольно старой кабельной системе Cat 5e подключить устройства 2.5GBASE-T, и… всё прошло успешно, они прекрасно заработали. На мой взгляд, это тот самый современный sweet spot, на который стоит ориентироваться при построении домашних сетей.Но на этом 2016 год не закончился.2016 год. 25GBASE-T и 40GBASE-TДа, всё в том же 2016 году вышли не только стандарты Ethernet по витой паре на 2,5 и 5 Гбит/с, но и ещё два супербыстрых стандарта на 25 и 40 Гбит/с.«Как у них получилось?» — возможно, спросите вы. Но на самом деле никакой магии тут нет. Ну, кроме той магии, с помощью которой планируется найти потребителей этих стандартов. Самое значимое, что изменилось относительно 10GBASE-T, — это требования к кабелю.Теперь нужен кабель с частотным диапазоном до 2 ГГц. А это, напомню, в четыре раза больше, чем предлагает Cat 6a, которого требует 10GBASE-T.Вторым требованием, а точнее — ограничением, является максимальная длина участка. Оба стандарта предлагают максимум 30 метров против 100 метров ранее.Для стабильной работы с 40 Гбит/с ISO версия стандарта рекомендует использовать Cat 8.2, который, в частности, требует других разъёмов: TERA / GG45 / ARJ45.Различия между классическим коннектором RJ45 и GG45Уверен, что вы часто видели в продаже фольгированный кабель Cat 7, который рекламируется под лозунгом «инвестиции в будущее». Так вот, он не удовлетворяет требованиям обоих стандартов. Возникает вопрос: зачем же он нужен, если для 10GBASE-T достаточно Cat 6a, а 25GBASE-T уже нужен Cat 8? Я ответа не нашёл. Кажется, мы уже видели эту историю с Cat 6. Но успокаивает, что Cat 6 всё-таки вполне себе реанимировали для 5GBASE-T.Забавный факт: новый кабель Cat 8 значительно толще, чем Cat 6a. К примеру, кабель восьмой категории для внешней прокладки производства American Tech Supply имеет диаметр 9,9 мм, что на 0,9 мм больше, чем так называемый толстый коаксиальный кабель, с которого всё и началось.Оба стандарта используют более легковесную модуляцию PAM-4, что позволяет передавать два бита данных в одном символе, как и в PAM-5, но с меньшими энергозатратами. Для повышения помехоустойчивости и надёжности передачи данных используется алгоритм кодирования LDPC.Выживут ли эти стандарты? Довольно сомнительно. На сегодня (2024 год) я не смог найти в свободной продаже устройств с их поддержкой, хотя такие продукты точно есть. Кроме того, опять возникает вопрос замены кабеля и, вероятно, патч-панелей.Коммутатор Alpha Networks SNC-60x0-486T с поддержкой 48 портов 25GBASE-TКажется, что на этом история применения витой пары в среде Ethernet заканчивается, так как для коммутации в стойке проще и дешевле воспользоваться твинаксиальным кабелем или оптоволоконным патч-кордом, а от стоек у всех, кому важна производительность сети, уже давно идёт только оптика. Пользователи в офисах и домах, как и различные не требовательные к полосе устройства, висят на Wi-Fi.У Ethernet, конечно, есть туз в рукаве — Power Delivery. Ведь те же точки доступа, видеокамеры или киоски самообслуживания как-то нужно запитывать. Кроме того, если потребителей много, Wi-Fi не всегда будет хорошим вариантом. Но если вы думаете, что Ethernet смирилась с развитием в таком узком сегменте, то вы удивитесь.2016 год. 1000BASE-T1Сразу прошу прощения за небольшую манипуляцию. Я должен был написать в заголовке «2015 год», но это несколько ломало хронологию повествования.Итак, в 2015 году был опубликован документ IEEE 802.3bw, а в 2016 году — IEEE 802.3bp с описаниями стандартов 100BASE-T1 и 1000BASE-T1 соответственно. Эти стандарты описывают работу сети Ethernet по витой паре. По единственной витой паре, правда, ограниченной длиной в 15 метров.Зачем это нужно? Автомобили! В современных автомобилях много датчиков, которые передают показания в соответствующие контроллеры через какую-нибудь шину, например CAN. А шина CAN работает — вот ведь удачно получилось! — поверх витой пары. Как правило, все эти контроллеры и датчики принадлежат разным контурам. И, к примеру, камеру заднего вида нельзя просто так взять и подключить к новому потребителю, например к приложению, которое будет распознавать номера едущих сзади машин. Появилась идея унифицировать сеть передачи данных внутри отдельно взятого автомобиля. Кроме того, потребность в скорости передачи данных между устройствами растёт. Да и вообще рынок уже большой, а конкуренция — это замечательно.Кстати, интересный факт: 100BASE-TX появился внутри автомобиля ещё в 2008 году — на седьмой серии BMW для подключения к порту диагностики OBD2.Welcome to the wonderful world of automotive Ethernet!SFP-модуль Technica Engineering 1000BASE-T12019 год. 10BASE-T1L и 10BASE-T1SВ заголовке нет никакой ошибки: скорость новых стандартов действительно 10 Мбит/с. И да, это 2019 год, почти 30 лет после появления 10BASE-T. Но на всё есть свои причины. Давайте посмотрим, что это за стандарты.Оба они работают только в полном дуплексе по одной паре проводов, способной обеспечить частотный диапазон в 20 МГц (что близко к 16 МГц, которых требовал 10BASE-T).10BASE-T1S предназначен в первую очередь для использования в автомобилях и может работать на расстоянии до 15 м в режиме шины, да ещё и использует манчестерское кодирование. Ethernet 10 Мбит/с в режиме шины? Какой сейчас год? Ещё добавлю, что сеть использует что-то вроде токена для работы без коллизий, и напомню о VG-AnyLAN и Token Ring. Снова привет из восьмидесятых. Но очевидно, что шина в автомобилях более удобна, чем «точка-точка» соединения, вот и пришлось пойти на такой шаг.10BASE-T1L работает на участках до 1 000 метров и поддерживает технологию доставки питания Power over Data Line (PoDL). В качестве модуляции используется PAM-3, который пришёл из 100BASE-T4 (1995 год). Очевидно, что стандарт направлен на продвижение Ethernet в промышленных сетях. Автомобилей нам уже мало. Напомню, что Ethernet Alliance — это консорциум очень крупных поставщиков и потребителей сетевых решений (список). Так что рискну предположить, что на горизонте следующих 10 лет Ethernet глубоко проникнет в промышленные сети, несмотря на звучащий сейчас скептицизм.FC621 USB 10BASE-T1L StickСамое интересное для нас как потребителей в этих ответвлениях — то, что это всё тот же хорошо знакомый нам Ethernet. И мы можем подключать его к простой домашней, офисной и прочей сети, просто добавив соответствующий порт в коммутатор. Посмотрите, как предлагается организовывать автомобильную сеть: там есть и традиционный Ethernet, и новый 10BASE-T1S. Аналогичные схемы можно придумать и с 10BASE-T1L. Например, подключить все умные датчики к сети здания с единым управлением.Пример организации локальной сети автомобиляЧто же будет дальше?2020 год. 2.5GBASE-T1, 5GBASE-T1, 10GBASE-T1Проходит четыре года — и от 1000BASE-T1 мы прыгаем к 10GBASE-T1 и его мультигигабитным подмножествам. Эти стандарты были опубликованы в документе IEEE 802.3ch.Все три стандарта используют те же алгоритмы кодирования и модуляции (LDPC и PAM-4), что и 25GBASE-T и 40GBASE-T. Да и требования к частотным характеристикам кабеля у них схожие, явно необходимо что-то близкое к Cat 8.2.5GBASE-T1: до 600 МГц. Тут и Cat 7 как раз должен подойти.5GBASE-T1: до 1 250 МГц.10GBASE-T1: до 2 000 МГц.Важно не забывать, что все T1-стандарты (кроме T1L) пока рассчитаны на очень короткие отрезки кабеля — до 15 метров.Single Pair Ethernet коннекторы2023 год. 25GBASE-T1В 2023 году IEEE 802.3cy утверждает суперскоростной 25-гигабитный стандарт 25GBASE-T1 по одной витой паре. Всё те же PAM-4 и LDPC, что и в 10GBASE-T1, и всё то же требование к кабелю (до 2 000 МГц), но на ещё более коротких отрезках — 11 метров.Пока живых образцов на рынке нет, но тем не менее смотрите, как интересно получается: индустрия требует в автомобильную сеть скорости, сравнимые со скоростью чтения с хорошего NVMe-диска. Есть идеи, для чего? Неужели всё-таки self-driving cars?2025 год. Планы Ethernet AllianceВ ближайших планах на 2025 год следующее:IEEE 802.3da — увеличение максимальной длины сегмента 10BASE-T1S с 15 до 50 метров;IEEE 802.3dg — увеличение максимальной длины сегмента 100BASE-T1 и 1000BASE-T1 до 500 метров.Очень похоже на то, что Ethernet Alliance сделал ставку на однопарную сеть, и в ближайшие несколько лет мы увидим там много интересного. Да, в первую очередь это появится в автомобилях и в промышленных сетях, но что-то придёт и в дом, офис. В конце концов, Wi-Fi 8 (IEEE 802.11bn) уже не за горами, а там обещают пропускную способность до 46 Гбит/с (в теории), которые куда-то нужно будет подключать.ЗаключениеМы познакомились с основными этапами появления и развития сети Ethernet на базе кабеля типа «витая пара». Осознали, что главная проблема, с которой приходится бороться разработчикам её новых, более скоростных стандартов, — это частотные характеристики кабеля. Фактически неэкранированная витая пара исчерпала свои возможности, и единственный рабочий в настоящее время путь развития — это совершенствование алгоритмов подготовки данных и модуляции, которые позволяют снизить требования к среде передачи. С учётом того, что сейчас Ethernet Alliance очень активно вкладывается в automotive и промышленный Ethernet, где борьба с помехами — одна из важнейших задач, можно оптимистично смотреть на развитие в этих областях. А это значит, что мы, скорее всего, увидим менее требовательные к кабелю высокоскоростные стандарты и в классическом Ethernet по четырём парам.И попробую ответить на вопрос, какую же категорию витой пары тянуть дома или в офисе.Если у вас уже протянута сеть на базе витой пары категории 5e, вы можете воспользоваться устройствами стандарта 2.5GBASE-T и, не трогая кабельную составляющую, значительно поднять производительность сети.Если вам нужна недорогая, но надёжная сеть для быстрой передачи данных и доставки питания к устройствам, Cat 6 даст вам гарантированные 5 Гбит/с, а иногда и все 10 Гбит/с. Продажи кабеля этой категории, кстати, сейчас на пике. Правда, советую обращать внимание на толщину проводников и не использовать тоньше, чем 23 AWG (0,573 мм). На всякий случай: 24 и 26 AWG — это тоньше. Кроме того, к местам размещения ёмких по части требований к пропускной способности устройств — точек доступа, NAS, видеосерверов и т. д. — лучше подвести хотя бы два кабеля, чтобы можно было при необходимости объединить их в один логический порт (port channel) и в некоторых случаях удвоить доступную пропускную способность.Если вы можете потратить больше и хотите быть готовы к предстоящему появлению Wi-Fi 8, стоит рассмотреть Cat 6a. Он толще, жёстче, но уже сейчас позволит вам получить 10 Гбит/с, а в будущем, думаю, при соответствующем развитии алгоритмов обработки сигнала, и 25 Гбит/с.Ультимативный вариант на сегодняшний день — Cat 8. Очень дорогой, фольгированный кабель позволит вам использовать все доступные современные скорости передачи данных — конечно, если сможете найти соответствующее оборудование. Но я бы обратил внимание на оптоволокно. Тем более что сейчас в рамках автомобильной линейки Ethernet обсуждается передача данных по пластиковому волокну, что явно должно положительно сказаться как на стоимости кабеля, так и на его надёжности.И подумайте об однопарном Ethernet. Мне кажется, тот же стандарт T1L с выносами до 1 км и одновременной подачей питания может пригодиться не только на предприятиях. Постепенно появятся продукты на его основе, затем они станут дешевле и наконец придут в каждый дом. У проводного Ethernet грандиозные планы: посмотрите насвежую дорожную картуEthernet Alliance.Получите востребованную специальность или расширьте компетенции на онлайн-программах бакалавриата, магистратуры и профессиональной переподготовки, разработанных с ведущими вузами России:Программные системы и автоматизация процессов разработки(+ бесплатныйвводный курс);Кибербезопасность(+ бесплатныйвводный курс);Разработка IT-продукта(+ бесплатныйвводный курс);Инженерия данных(+ бесплатныйвводный курс);Прикладной искусственный интеллект(+ бесплатныйвводный курс)."
СберМаркет,,,Самые распространённые ошибки на собеседовании у джунов-разработчиков,2024-06-04T11:25:30.000Z,"За последний месяц на hh.ru для джунов без опыта было доступно всего около 6% от общего числа опубликованных вакансий для разработчиков. Такое соотношение обусловливает высокую конкуренцию среди начинающих специалистов: дело доходит до сотен претендентов на одно место. Это приводит к тому, что рекрутеры становятся требовательнее и любая ошибка или оплошность на собеседовании может стать причиной отказа.Спросили у экспертов, какие требования в условиях жёсткого отбора сейчас предъявляют к джунам и на чём чаще всего «валятся» кандидаты. Узнали у наших выпускников — недавних соискателей, а теперь трудоустроенных специалистов, которые в своё время оставили по несколько сотен откликов на вакансии и прошли через десяток собеседований, — в чём были сложности и как им в итоге удалось получить офер.Какие резюме не пройдут через фильтры рекрутеровСчитается, что самое трудное для джуна — пройти техсобеседование. В реальности же бо́льшая часть кандидатов даже не получает приглашения на интервью.За время поиска первой работы в мобильной разработке Никита оставил более 300 (!) откликов на вакансии, а приглашений на интервью получил всего 10.До этого Никита около 10 лет занимался промышленной автоматизацией: разрабатывал ПО для систем управления промышленным оборудованием электростанций и не только. Ему нравилась работа, но приходилось часто ездить в длительные командировки, а это плохо сочеталось с семейной жизнью. Никита начал думать о новой сфере деятельности.В 2020 году он решил заняться разработкой Android-приложений, потому что видел перспективность этого направления. Учился сначала самостоятельно: пользовался онлайн-сервисами вроде JavaRush, смотрел видеоролики на YouTube. А потом прошёл полноценный курс по Android-разработке.Никита ЗарубинR1 Android developer в Magenta TechnologyЯ разместил резюме на нескольких популярных платформах. В первую очередь искал вакансии без требований по коммерческому опыту, но таких было совсем мало. Со временем я начал откликаться и на вакансии с требованием небольшого опыта: максимально подтянул к IT опыт на предыдущем месте работы, указал обучение в Нетологии и сотрудничество с ней (я остался проверять домашки студентов на курсе), дал ссылку на репозиторий на GitHub. Я ежедневно по несколько раз обновлял своё резюме, просматривал новые вакансии и оставлял отклики, писал сопроводительные письма, но приглашений на интервью не было.Павел КомягинТимлид разработки внутренних продуктов в НетологииСейчас джунам особенно сложно попасть куда-то, так как вакансий катастрофически мало. Если они и появляются, то рекрутеры сразу тонут в сотнях откликов. Требования повышаются всегда, когда есть из чего выбирать. Все хотят лучших из лучших. Поэтому то, что раньше могло не играть роли (профильное высшее образование, опыт в опенсорсе или наличие пет-проектов), сейчас может быть той мелочью, из-за которой выберут одного джуна и откажут другому.На что в резюме рекрутеры смотрят в первую очередьОпыт коммерческой разработкиНесмотря на то, что джун — начинающий специалист, многие компании рассчитывают, что он — уже вполне боевая единица, способная самостоятельно решать рабочие задачи. А для этого нужны не только теоретические знания, но и практические навыки. Поэтому если в резюме не указан опыт в создании или доработке реального продукта, который либо уже имеет своих пользователей, либо активно разрабатывается, то, скорее всего, оно не пройдёт фильтр на позицию джуна-разработчика, даже если вакансия открыта для соискателей условно без опыта.Опрос65 работодателей, проведённыйhh.ruв мае 2023 годаЕсли опыт есть, но описан он скупо и бесструктурно, скорее всего, приглашения на собеседование также ждать не придётся.Ксения МаловаДиректор по продукту Академии IT-рекрутинга компании Tech-recruiterВажно понимать, что у джунов уже должен быть какой-то опыт. Желательно не только в пет-проектах, но и в продакшне, в реальных зарелизенных проектах, то есть настоящий практический опыт. Нужно как можно подробнее и без воды этот опыт описать. Чем структурнее, тем лучше. Укажите чётко, с какими проектами работали, как они называются, что вы делали в этом проекте, за какие задачи отвечали.Наталья ЛукиныхМенеджер проектов в СКБ Контур, сертифицированный карьерный консультант, сертифицированный специалист по оценке OntargetВ резюме обязательно нужна структура, где есть описание задач глаголами — кратко, ёмко, понятно — и результаты. При этом резюме не должно быть более 2–2,5 листов.Дарья ПарадироваРуководитель направления подбора и адаптации персонала в НетологииНовичкам, особенно тем, у кого нет опыта, действительно трудно найти свою первую постоянную работу. Поэтому важно найти возможность получить опыт коммерческой разработки. Стажировка, практика, любое участие в коммерческом проекте — это большое преимущество при поиске работы на старте карьеры. Важно фокусироваться не на условиях и соцпакете, а на задачах и возможности профессионально развиваться.Григорий ВахмистровГлавный инженер-программист в «ПСБ Лаб»Нужно понимать, что джуниор — это, по сути, младший специалист, а не совсем новичок. То есть соискатель — начинающий разработчик, но он уже может работать самостоятельно, хоть и под присмотром. Можно и после стажировки выйти на джуниорскую позицию, если есть сильная хардовая база, но в среднем по больнице работодатели хотят коммерческий опыт от года.«Проскочить» мимо такого ограничения можно только в качестве исключения: например, работодателю-бюджетнику понравилось резюме соискателя, нужно срочно закрыть штатную единицу и сверстать ФОТ на следующий год. Но это исключение, а не правило.Стажировка, особенно оплачиваемая, может считаться коммерческим опытом. Если на стажёров тратили время и они занимались чем-то полезным, то даже месяц такой стажировки очень важен. А если человек ходил для галочки и просто чтобы получить отметку о стажировке, то в таком опыте, конечно, ценности мало.Пожалуйста, не указывайте курсы в опыте коммерческой разработки. Во-первых, обучение не имеет к ней отношения: формально коммерческая разработка — та, за которую платят. Во-вторых, большинство специалистов расценят это негативно. Но если решением кейса в рамках обучения будут пользоваться клиенты, то есть кейс был на «боевом» продукте, то такой опыт можно считать стажировкой и, соответственно, расценивать его как опыт коммерческой разработки.Профиль на GitHub с портфолио проектов и примерами кодаНа техническом собеседовании нанимающий специалист — скорее всего, это будет тимлид разработки — может не погружаться в детали проектов, которые вы когда-то делали: сосредоточится на оценивании вас здесь и сейчас. Однако примеры в репозитории на GitHub важны для первичного отбора резюме эйчаром: считайте это фильтром, сквозь который пропускают всех без исключения.Дарья ПарадироваРуководитель направления подбора и адаптации персонала в НетологииРазработчику желательно иметь профиль на GitHub. Здорово, если у джуна есть примеры кода. Важно оформлять и пополнять портфолио проектов. Можно добавлять туда тестовые задания, которые вы выполняли, когда откликались на вакансии, и вообще всё, что возможно упаковать и показать, в том числе пет-проекты. Обычно даже на курсах дают задачи, которые максимально приближены к реальным коммерческим.Ксения МаловаДиректор по продукту Академии IT-рекрутинга компании Tech-recruiterОбязательно приложите ссылку на GitHub, чтобы рекрутер мог пройти и посмотреть, в чём у вас есть опыт.Григорий ВахмистровГлавный инженер-программист в «ПСБ Лаб»Пет-проекты, на мой взгляд, переоценены. Я честно пытался посмотреть один раз у одного соискателя, в итоге проект просто не собрался, а я уже потратил время. На потоке пет-проекты не смотрят: слишком долго плюс нет гарантий, что тот код, который выложен в вашем репозитории, писали именно вы. Поэтому, если хотите что-то сделать для себя или обучаться на пет-проекте — пожалуйста, но не стоит рассчитывать, что такой проект будут смотреть. В лучшем случае дочитают до этого места в резюме 🙂Никита ЗарубинR1 Android developer в Magenta TechnologyЯ старался выполнять все тестовые задания, которые мне предлагали, даже если заранее знал, что не приму офер этой компании из-за локации в другом городе. Тем самым я, во-первых, пополнял свой репозиторий новыми проектами, а во-вторых — получал обратную связь от ревьюеров и улучшал качество своего кода.   Возможно, благодаря тому, что репозиторий с проектами начал наполняться тестовыми работами, я начал получать приглашения на интервью. Наверное, работодатели обращали внимание на список проектов и активность репозитория и отмечали это как плюс.Профильное образованиеДля многих работодателей в IT первичны знания, навыки и опыт, а образование не имеет большого значения. Однако маловероятно, что в условиях повышенной конкуренции на собеседование пригласят кандидата, у которого в графе «Образование» одни лишь курсы — и те краткосрочные.Дарья ПарадироваРуководитель направления подбора и адаптации персонала в НетологииНужно понимать, что рекрутер просматривает сотни резюме, особенно когда речь идёт о позиции младшего специалиста. Для примера: на позицию стажёра мы получаем около 500 откликов. Техническое высшее образование, безусловно, будет большим преимуществом кандидата. Убеждена, что рекрутер обратит внимание на пройденные профильные курсы: это один из индикаторов проактивной позиции в обучении и мотивации развиваться.Оформление резюме и сопроводительное письмоПод фильтр эйчара, скорее всего, попадёт такое резюме ↓Слишком длинное и неструктурированное, с нерелевантным опытом.Пустое, по которому не разобрать ни навыков, ни опыта.Без фото или с неподходящим фото (в чьей-то компании, в солнечных очках и так далее).С эмоциональным или панибратским сопроводительным письмом.С безликим и безадресным сообщением, похожим на массовую рассылку.Дарья ПарадироваРуководитель направления подбора и адаптации персонала в НетологииСопроводительное письмо всегда приветствуется. Это хорошая возможность подсветить, почему соискателя заинтересовала эта позиция, и обратить внимание работодателя на свои сильные стороны.Ксения МаловаДиректор по продукту Академии IT-рекрутинга компании Tech-recruiterЖелательно, чтобы при отклике на вакансию вы написали сопроводительное письмо, которое не будет дублировать резюме. В целом ваш отклик должен отвечать на вопрос, почему работодателю стоит пригласить именно вас. Для этого нужно изучить требования к соискателю и ответить на эти вопросы.Например, вы знакомы с конкретным фреймворком или у вас есть пет-проекты по тематике, в которой работает компания. Или вам интересно само направление компании и вы хотели бы в нём развиваться. При этом важно написать, чем вы можете быть полезны компании, а не почему так хотите там работать.Ещё хорошо бы в резюме указывать контакты. И конечно, если вы хотите получать приглашения на собеседования, проверяйте, правильный ли номер телефона указали в резюме, берите трубку и заглядывайте в почту — иначе поиски работы затянутся.Александр перешёл в IT из автобизнеса. Всего он оставил более 600 (!) откликов, прежде чем нашёл подходящую работу.До того как задуматься о карьере в IT, Александр, помимо постоянной работы у автодилера, пробовал заниматься бизнесом. Он открыл торговую точку, но через полтора года был вынужден её закрыть, так как она не приносила дохода. В итоге Александр остался с кредитом более миллиона рублей и начал поиски стабильного для заработка и самореализации направления деятельности.После беседы с другом, который работал в IT-компании DevOps-специалистом, Александр заинтересовался профессией тестировщика. В IT его привлекла стабильность и потенциал отрасли, востребованность опытных специалистов. Область тестирования понравилась более низким порогом входа в профессию, чем это было в разработке, DevOps и так далее. Александр прошёл несколько бесплатных курсов, стал самостоятельно изучать теорию по статьям и видеолекциям на YouTube, затем пришёл учиться в Нетологию. После защиты диплома остался в команде курса и стал его экспертом.Александр МужевСтарший инженер по тестированию в «Альянс АйТи Технолоджи»С первого дня, как начал обучаться, я активно откликался на все интересующие меня вакансии на hh.ru, Зарплата.ру, даже не думая о том, подхожу или нет. Во время обучения и различных практик я постоянно модернизировал своё резюме. Количество ключевой информации становилось всё больше и больше при получении определённых навыков: например, прошёл Git и сразу добавил в резюме.Через полтора месяца обучения я, откликаясь на вакансии, попал на собеседование на должность QA-инженера в Сима-ленд, но в итоге получил отказ.Фидбэк от эйчара: как сильно не понравиться IT-рекрутеруОтбор в IT-компанию, как правило, проводят в несколько этапов, и до технического собеседования кандидата могут пригласить на интервью с эйчаром. А бывает, что эйчара зовут поприсутствовать на техсобеседовании или сам тимлид задаёт несколько общих вопросов, чтобы оценить ваши софт-скиллы и понять, сработаетесь ли вы с командой. В любом случае эта софтовая часть собеседования не менее важна, чем хардовая.Спросили у нанимателей, чего не стоит делать на собеседовании ↓Нарушать субординацию и базовые правила приличия: проявлять токсичность, вести себя по-хамски, некорректно отзываться о прошлых работодателях и так далее.Дарья ПарадироваРуководитель направления подбора и адаптации персонала в НетологииВ моей практике был случай, когда кандидат подключился на техсобеседование в тельняшке и с бутылкой пива. Он отлично справился с вопросами и задачами, но офер так и не получил.Соблюдение субординации и поведение, адекватное ситуации, очень важны.Другой кейс: на интервью подключился разработчик в кигуруми панды, по результатам собеседования он получил офер и принял его. Однако корпоративная культура в компаниях разная, и я всё-таки не рекомендую ничего ярче, чем кежуал:)Теперь о токсичности. В Нетологии очень ламповая корпоративная культура. Развита культура открытой обратной связи, взаимовыручки и поддержки. Оценке софт-скиллов уделяют внимания точно не меньше, чем оценке хард-скиллов. При найме сотрудника на позицию джуниора личностные качества однозначно выходят на первый план.На интервью важно быть доброжелательным, отвечать корректно, показывать свою заинтересованность и мотивацию. Да, действительно, в эмоциональном негативном ключе о прошлом работодателе лучше не отзываться. Всегда можно корректно подсветить какие-то зоны дискомфорта.Ксения МаловаДиректор по продукту Академии IT-рекрутинга компании Tech-recruiterОднажды на собеседовании мне встретился кандидат, который был откровенно токсичным. Когда я спросила, объяснял ли он на прошлом месте работы свою позицию по отладке оборудования, он ответил: «Да чего им объяснять, они же идиоты! С ними всё понятно: через пять лет загнутся». Конечно, мы с ним больше не увиделись.На собеседовании желательно вести обычный нормальный диалог, не агрессировать, не нарушать личных границ. У меня были ситуации, когда кандидат мог после собеседования сказать, мол, слушайте, в компанию я к вам работать не пойду, конечно, но вы такая классная, может, сходим на свидание? С такими кандидатами я больше не общаюсь и никому их не рекомендую.Подключаться к онлайн-собеседованию из шумного места, общаться по видеосвязи на ходу, опаздывать.Дарья ПарадироваРуководитель направления подбора и адаптации персонала в НетологииВажно подготовиться к собеседованию, договориться о комфортном времени, чтобы вас ничего не отвлекало.   Место форс-мажору всегда есть. Если вы заранее понимаете, что опоздаете, лучше предупредить HR-менеджера и, возможно, перенести встречу. В остальном в опоздании на несколько минут критичности нет, обстоятельства бывают разные.Ксения МаловаДиректор по продукту Академии IT-рекрутинга компании Tech-recruiterОпоздание не в каждой компании будет воспринято как что-то плохое. Кто-то на это даже не обратит внимания, а для кого-то будет критично. Важно всегда предупреждать, что вы задерживаетесь.Однажды кандидат не пришёл к нам на собеседование, потому что не нашёл парковку. Он просто сказал: «Ой, всё, я не приду». Хотя я говорила, что мы можем подождать, и заранее предупредила, что это центр города и ситуация с парковками сложная, порекомендовала выехать заранее.Врать об опыте.Павел КомягинТимлид разработки внутренних продуктов в НетологииНе стоит врать о своём опыте. Практически всегда это можно проверить: знания языка или технологий — вопросами и задачами, опыт работы в компаниях — звонками в эти компании. Если ложь всплывёт после найма в процессе работы, последствия могут быть печальными.Ничего не знать о вакансии и компании.Ксения МаловаДиректор по продукту Академии IT-рекрутинга компании Tech-recruiterНужно изучать информацию о компании и вакансии, сразу отмечать, в чём вы проседаете, а в чём, наоборот, хороши. На собеседовании подсвечивайте свои сильные стороны и рассказывайте об опыте, опираясь на требования, озвученные в вакансии.Большой ошибкой будет прийти на интервью, понимая, что у вас совсем другой стек или вам совершенно неинтересно то, чем занимается компания.Показывать незаинтересованность и низкую мотивацию.Дарья ПарадироваРуководитель направления подбора и адаптации персонала в НетологииОдин из крайне важных факторов — мотивация кандидата. Будьте готовы ответить на вопросы: «Почему решили прийти в IT?», «Как определились с профессией?», «Как видите свой вектор развития?», «Что для вас важно в работе?» и «Почему готовы работать конкретно в этой компании?».Ксения МаловаДиректор по продукту Академии IT-рекрутинга компании Tech-recruiterЕсли человек пришёл сюда, потому что ему посоветовал какой-нибудь сеньор-разработчик, пообещав большие деньги и быстрый старт в профессии, и это единственная мотивация, то у меня к этой мотивации вопросы. Выглядит так, будто это вообще что-то навязанное: сказали идти — и он пошёл.А вот как джуну, по мнению нанимающего тимлида разработки, стоит себя вести на собеседовании ↓Григорий ВахмистровГлавный инженер-программист в «ПСБ Лаб»На собеседовании можно делать всё, что позволяет ситуация, воспитание и софты. Я бы советовал держаться в конструктивном русле, максимально расслабленно и непринуждённо, так, чтобы было комфортно и вам, и собеседующему. Спросите заранее о тайминге и возможности его превышения, о том, будет ли оцениваться ход мыслей или требуются только точные быстрые ответы. Чем лучше вы будете понимать, что от вас хотят и какие есть возможности у другой стороны, тем больше вероятность максимально показать себя и свои навыки с лучшей стороны.Не стесняйтесь предлагать нестандартные варианты. Например, вы долго отвечали на вопросы и не успеваете закончить собеседование, а интервьюер ограничен по времени (следом встреча, которую он не может перенести). Попробуйте предложить варианты — например, созвониться ещё раз и ответить на те вопросы, на которые не успели. Это кардинально ситуацию не изменит, но интервьюер точно отметит вашу мотивацию и попытку найти выход из ситуации. Сильно креативить, наверное, не стоит, но, как говорится, за спрос денег не берут. Если чувствуете, что можете что-то предложить и это поможет вам получить желанный офер, то почему бы и нет? 🙂Опрос65 работодателей, проведённыйhh.ruв мае 2023 годаКак завалить техсобеседованиеАлександр МужевCтарший инженер по тестированию в «Альянс АйТи Технолоджи»К первому собеседованию в Сима-ленд я не готовился, но тестовое задание смог выполнить на ура. На собеседовании спрашивали много определений, было жёстко. В итоге ответил верно на 80% вопросов, но на должность не попал, так как не нашёл ключевой баг по безопасности приложения: безопасность на тот момент я ещё не прошёл и не знал, как проверять.Я немного порасстраивался, а потом придумал новое занятие: постепенно прорабатывать (учить наизусть) вопросы к собеседованиям на позицию QA-инженера. Нашёл неплохойсайтсо списком вопросов по градациям знанийи начал специально вручную, не используя готовые шпаргалки, создавать свою персональную шпаргалку «вопрос — ответ». Пока создавал, тут же учил, повторяя. Делал всё не в один момент, а постепенно и методично: один вопрос прорабатывал и учил в течение одного-двух дней. В итоге за 8 месяцев выучил 113 вопросов и был готов практически к любому собеседованию в части теории.Ксения МаловаДиректор по продукту Академии IT-рекрутинга компании Tech-recruiterВажно понимать, что провальные собеседования для начинающего специалиста — это нормально. В практике карьерного консультанта мне часто встречались ребята, которые очень хорошо знали теорию, а на собеседовании начинали сильно волноваться, теряться и забывали элементарные вещи. Конечно, у рекрутеров создавалось впечатление, будто соискатели ничего не знают. Умение проходить собеседования — это навык. Чем больше вы его тренируете, чем больше рефлексируете и отслеживаете, что сделали не так и на какой вопрос не ответили, тем более подготовленными вы становитесь.Никита ЗарубинR1 Android developer в Magenta TechnologyТехнические интервью были сложны поначалу. Чем больше собеседований я проходил, тем увереннее себя чувствовал. Спрашивают обычно одно и то же, вопросы стандартные. К большинству из них я был готов, а к чему не был готов — записывал и готовился.Все интервью я записывал и потом пересматривал. Сложнее всего давались вопросы не на знания, а на сообразительность. Также нужно было работать над контролем эмоций. Например, один интервьюер задал мне вопрос, связанный с особенностью многопоточности в Android. Это было собеседование в крупную компанию, что вызвало у меня бурю волнения, которую я не смог подавить. Тогда я ответил неправильно, хотя вопрос был достаточно простой.Вот как, по мнению нанимающих специалистов, можно успешно завалить техническое интервью ↓Демонстрировать полное непонимание и нежелание разбираться.Григорий ВахмистровГлавный инженер-программист в «ПСБ Лаб»В целом, как и с софтами, скорее всего, соискателя будут оценивать по совокупности правильных ответов. Условно, вы можете завалить один-два базовых вопроса, но продемонстрировать отличное понимание всего остального. В таком случае сложится впечатление, что либо вы переволновались и перепутали, либо, даже если вы не знаете, пару пробелов реально закрыть за испытательный срок. Если же в целом уровень очень слабый, то в какой-то момент приходит понимание, что соискателя слишком долго и дорого будет дотягивать до требуемого уровня и проще посмотреть других кандидатов.Основная ошибка у бэкенд-разработчиков по Java — непонимание того, как работает программа: как работает JVM, загрузчики классов, сборщики мусора, области памяти и прочая внутренняя кухня. Кандидат разобрался, как и какие буквы писать в IDE, но не разобрался, как это будет работать. Проведём условную аналогию с архитектором: можем нарисовать красивое здание, а посчитать его устойчивость не можем, не понимаем, как оно будет стоять и не упадёт ли. С SQL аналогично: самая частая ошибка — незнание механизмов работы. Я бы рекомендовал готовиться к трём базовым вопросам: о порядке выполнения операторов (нет, select выполнится не первым), признаках транзакции и уровнях изоляции транзакций. Также, скорее всего, будет вопрос об устройстве индексов: чем кластерный отличается от некластерного, а B-tree — от хеш-индекса.Не задавать вопросов.Павел КомягинТимлид разработки внутренних продуктов в НетологииЕсли на собеседовании дали закодить задачку, а кандидаты не задают уточняющих вопросов, пытаются сделать так, как поняли, то часто они могут зайти в тупик. А для джуна важно всё спрашивать и уточнять то, что непонятно и в чём есть сомнения.Как в итоге джуну получить оферСпросили у экспертов, каков их идеальный кандидат на должность джуна-разработчика. Вот какому соискателю при прочих равных отдадут предпочтение ↓Григорий ВахмистровГлавный инженер-программист в «ПСБ Лаб»На мой взгляд, основной скилл — этоосознанность. Если говорить про резюме мидлов (но для джунов тоже актуально), то там каждое первое резюме содержит строку «писал фичи, фиксил баги». А когда спрашиваешь соискателя о том, что именно он делал, не все могут даже просто сформулировать и рассказать, чем же они занимались.Одних хардов недостаточно, так как любому разработчику работать в коллективе. Брать соискателя без софтов — выстрел в ногу соискателю, себе (руководителю) и команде. Зачем кого-то нанимать, если понимаешь, что не сработаешься с человеком, каким бы замечательным специалистом он ни был? В первую очередь критерии отбора будут хардовые, то есть кандидат должен соответствовать по техническим навыкам, но про софты тоже не стоит забывать. Придётся много общаться с коллегами: уточнять требования у аналитиков, обсуждать задачи с PM, макеты — с дизайнерами, API — с фронтами, — а также погружаться в доменную область: разбираться в том, что ты автоматизируешь.«Горящие глаза» и общий энтузиазм приветствуются, но это должно быть подкреплено (идеи из разряда «давайте попробуем новую технологию» должны иметь под собой технико-экономическое обоснование), а заявление об интересе к разработке в целом может породить соответствующие вопросы о самообразовании, — нужно быть в теме и уметь ответить на вопрос о последней прочитанной книге или блоге, посещённой конференции или изученной технологии.Павел КомягинТимлид разработки внутренних продуктов в НетологииМожет, прозвучит банально, но в джунах я ценюдрайв. Ведь им нужно за короткое время впитать в себя очень много информации и получить опыт. Если человек не горит своим развитием, то его рост до следующей ступени может растянуться, а для компании это может проявиться в бо́льших расходах. Ещё важно, чтобы кандидат уже имел неплохой кругозор: был в курсе, какие технологии есть на рынке, знал основы одного из популярных языков (JavaScript, если мы говорим о фронтенде). Для меня знание конкретного фреймворка некритично, так как, может быть, наоборот, сможем научить сразу как надо, а не переучивать.Наталья ЛукиныхМенеджер проектов в СКБ Контур, сертифицированный карьерный консультант, сертифицированный специалист по оценке OntargetЗдесь многое зависит от портрета и от ценностей заказчика. В приоритете — опыт, нужная для компаниимотивация, софты: целеустремлённость, обучаемость, инициативность.Дарья ПарадироваРуководитель направления подбора и адаптации персонала в НетологииВ разработчике очень ценножелание постоянно обучаться, совершенствоваться, потому что технологии не стоят на месте. Здесь важна проактивная позиция и мотивация. Но если вам действительно интересно, вы справитесь с любым вызовом.Помню кейс: проект изначально был на DotNet Core, а потом приняли решение переехать на Node.js. Команда с позитивом приняла этот вызов и успешно освоила новый стек.Ксения МаловаДиректор по продукту Академии IT-рекрутинга компании Tech-recruiterВажный навык —самостоятельность. Человек должен уметь гуглить, должен разбираться в проблеме сам, а не искать постоянно решения у старших коллег. Ещё важны интерес и желание развиваться, делать что-то нешаблонное — когда мы не просто берём и делаем как под копирку какую-то задачу, а перестраиваем её под себя, делаем уникальной.На вакансию, по которой Никита в итоге получил офер, откликнулось более 130 человек, а на интервью пригласили около 30 из них.Никита ЗарубинR1 Android developer в Magenta TechnologyВ компанию Magenta Technology, где я работаю уже год, я попал только со второго раза.Первый раз я собеседовался в августе 2022 года. Прошёл небольшой скрининг с HR и был допущен до технического интервью. Тестового задания не было: сказали, что дипломной работы на курсе (это было приложение-чат, аналог соцсети) было достаточно для оценки скиллов. На встрече было два разработчика. Сразу рассказали флоу собеседования: по каким темам пройдёмся, с чего начнём, чем закончим. Встреча длилась около двух часов. Не сказать, что вопросы были нестандартными, но не ко всем я был готов. Результата ждал дня три. Получил обратную связь с уверенной оценкой J2–J3, но не хватило опыта по архитектурным паттернам. В итоге — отказ.Спустя полгода в этой же компании вновь открылась вакансия по Android Dev. У меня уже были контакты эйчара, и я напрямую с ней связался. Как оказалось, меня вспомнили. На этот раз получил тестовое задание, в котором нужно было продемонстрировать, как я усвоил тему, на которой провалился в первый раз. Я выполнил его досрочно и был приглашён на техническое интервью. На нём тоже было два разработчика, но вопросы были очень нестандартными. Мне даже показалось, что в первый раз я показал себя лучше. Но практически сразу после интервью я получил положительный результат от эйчара. Далее была встреча с непосредственным руководителем — директором по развитию Magenta Technology. Обсудили проекты, над которыми мне придётся работать, и мою мотивацию. Спустя несколько дней я получил офер.Александру пришлось пережить более 600 (!) отказов на отклики в связи с его особыми требованиями: чтобы уйти с прежней работы в дилерском центре, без подработок, нужна была зарплата не ниже 150 тысяч рублей.Александр МужевСтарший инженер по тестированию в «Альянс АйТи Технолоджи»Через два месяца обучения на курсе по ручному тестированию и автотестированию я чудесным образом попал в компанию «Анкор» (Яндекс). Выполнил тестовое задание на основе уже имеющихся базовых знаний — и начал работать по совмещению младшим ручным тестировщиком.Летом 2023 года из-за большой нагрузки на различных курсах и нехватки времени ушёл из «Анкора». Но я был доволен рабочей практикой и тем, что получил возможность пополнить резюме четырёхмесячным опытом в IT-компании.В июне 2023 года успешно завершил командный проект по Java на курсе, а в августе выполнил очень сложный курсовой проект по автоматизированному тестированию на Java.Начал учиться на курсе «Инженер по тестированию», и мой координатор в связи с моей активностью предложила мне стать аспирантом блока тестирования в каналах Discord. Эта работа не оплачивалась, но давала возможность обучаться на других курсах и, помогая студентам, не забывать пройденный материал. Я с удовольствием согласился и добавил новый статус в резюме.С сентября по декабрь 2023 года я активно помогал студентам, обучался сам, платил огромные кредиты и работал в дилерском центре. В декабре увидел сообщение в Discord, что требуется эксперт-наставник на проверку домашних заданий в блок ручного тестирования мобильных приложений. Написал координатору и предложил свою кандидатуру. Меня одобрили, и с этого момента Нетология стала для меня не только образовательной платформой, но и работой, хоть и не основной. За проверки домашних заданий я стал получать деньги.Также в конце сентября 2023 года, откликаясь на вакансии в своём Екатеринбурге, я получил приглашение на должность преподавателя QA в компьютерной академии TOP (очный формат). Успешно прошёл собеседование и хорошо ответил на все вопросы (благодаря своей подготовке к вопросам и используя шпаргалку). Стал преподавать очно два раза в неделю.Я продолжил откликаться на вакансии и в январе 2024 года получил приглашение в компанию «Альянс АйТи Технолоджи» в своём городе. Сходил на собеседование и после пятидневного ожидания получил наконец-то офер на позицию «Старший тестировщик ПО» с окладом 85 тысяч рублей. Принять решение было сложно, так как на работе в дилерском центре я зарабатывал около 200 тысяч рублей, но я согласился.На прежнем месте мне предложили остаться на совмещении, и я стал раз в неделю выходить на подработку, хоть как-то компенсируя риски по своим долгам (кредит + ипотека).В марте 2024 года я начал писать диплом по всему расширенному курсу «Инженер по тестированию» и сразу стал интересоваться проведением лекций в Нетологии. Вакантных мест сначала не было, но из-за своей активности я был на хорошем счету у менеджера проектов. К апрелю оказалось, что некому вести лекцию «Особенности тестирования веб-приложений и работа с DevTools», предложили мне — я с удовольствием согласился. 19 апреля, немного волнуясь, я успешно дебютировал как эксперт-лектор. Вдобавок освободилась позиция на проверку домашних заданий по всему блоку ручного тестирования — я также согласился.3 мая 2024 года я наконец-то получил зачёт по диплому, и вот кем я сегодня выступаю:старшим инженером по тестированию в компании «Альянс АйТи Технолоджи»;экспертом продвинутого курса «Инженер по тестированию» в Нетологии;преподавателем QA (очная форма) по направлению «Ручное и автоматизированное тестирование ПО» в компьютерной академии TOP;репетитором по ручному и автоматизированному тестированию ПО (расширенный курс): провожу индивидуальные занятия со студентами;QA-fullstack-фрилансером.Как подготовиться к собеседованиюПавел КомягинТимлид разработки внутренних продуктов в НетологииЕсли есть время, то я бы почитал статьи из разряда «100 вопросов к собеседованию по...», чтобы была какая-нибудь насмотренность. Это может не помочь, но лучше, чем ничего. Никогда не угадаешь, что спросят в конкретной компании.Дарья ПарадироваРуководитель направления подбора и адаптации персонала в НетологииПосле того, как структурировали всю информацию по опыту и проектам в резюме, написали сопроводительное письмо, необходимо активно откликаться на вакансии, быть готовым выполнить тестовые задания.Я рекомендую походить на различные интервью. Даже неудачные интервью — это опыт. После собеседования можно смело запрашивать обратную связь. Нередко руководитель готов дать рекомендации: на что стоит обратить внимание, какую литературу изучить, какие курсы пройти.В целом важно набраться терпения и упорно идти к своей цели.Григорий ВахмистровГлавный инженер-программист в «ПСБ Лаб»Лучшая подготовка — практика 🙂 Завалите 10 собеседований — и к 11-му не только разберётесь, что же от вас хотят услышать, но и сможете ответить на большинство вопросов. Проблема здесь только в сроках, потому что джунов не так часто зовут на собеседования.Если говорить о теоретической подготовке, то я обычно всем советую список вопросов вототсюда(вопросы по теории Java и немного по SQL). Сейчас есть очень много публичных разборов собеседований, после теоретической подготовки я бы рекомендовал часть времени потратить на их просмотр. Станет понятнее, что спрашивают и как, плюс будет возможность посмотреть на ситуацию со стороны.Решать задачи имеет смысл только в том случае, если вы знаете, что они будут (например, планируете трудоустройство в Тинькофф или Яндекс). Для задач подойдёт любой популярный сайт (для Тинькофф или Яндекса, наверное, лучшеLeetCode). Мне нравитсяHackerRank, так как сам когда-то на нём что-то решал.Codewarsещё хороший, многие советуют.Для обучения я бы советовал найти уже готовый маленький пример использования той технологии, с которой вы пытаетесь разобраться. У пет-проектов большой минус — они убивают мотивацию. Очень сложно заставлять себя делать в стол то, чем никто никогда не будет пользоваться. Маленькие примеры в этом плане гораздо лучше: они простые, понятные и в них можно быстро разобраться. В качестве источника примеров могу порекомендоватьBaeldung. Во-первых, информация всегда актуальная, во-вторых — отлично структурирована, а в-третьих — в конце статьи всегда есть ссылка на GitHub с рабочим актуальным примером.А вот что можно сделать, если собеседование уже завтра ↓Павел КомягинТимлид разработки внутренних продуктов в НетологииВ резюме может быть почти пусто, поэтому важно иметь какой-то текст. Как минимум можно подготовить стройный рассказ о себе и своём опыте: что читаете, смотрите (книги, сайты, авторы), какие современные технологии интересны. Если собеседование завтра, то на большее вряд ли можно рассчитывать.Григорий ВахмистровГлавный инженер-программист в «ПСБ Лаб»Отлично, вам повезло, вас позвали. Вы уже на полпути к цели! Вне зависимости от результатов вы получите бесценный опыт, который точно пригодится на следующем собеседовании. Если вы примерно понимаете, какие вопросы будут, то можно попробовать их повторить, но в целом один вечер вряд ли что-то глобально изменит. Лучше всего погулять, вкусно поесть и выспаться 🙂Советы и пожелания от тех, кто уже прошёл или провёл не один десяток собеседованийНикита ЗарубинR1 Android developer в Magenta TechnologyЯ желаю одного — не отчаиваться. Кто ищет, тот всегда найдёт. В моём случае от первого отклика до выхода на работу прошло около года, а от начала изучения Android-разработки прошло несколько лет. Самое главное — это мотивация. Вам должно нравиться то, что вы делаете, и обязательно найдётся работодатель, который оценит этот порыв по достоинству. Хочу пожелать всем, кто находится в поисках, удачи, терпения и сил! Ваша настойчивость обязательно будет вознаграждена. Оно того стоит!Александр МужевСтарший инженер по тестированию в «Альянс АйТи Технолоджи»Главное — победить себя. Тогда всё получится, и каждый сможет изучить новое. Не нужно, идя на курсы, думать, что вам дадут готовое и любого научат. Без собственного желания и попыток развивать ключевой навык «гуглить» ничего не получится: деньги просто уйдут на ветер. Нужно потратить определённое количество времени на самостоятельную работу над задачами, чтобы вступить в «зону комфорта IT» и выбирать компании с достойной работой.Григорий ВахмистровГлавный инженер-программист в «ПСБ Лаб»Я бы не советовал сильно переживать. Даже если у вас сейчас что-то не получается, вы обязательно с этим разберётесь. Никто из нас при рождении не умел ни ходить, ни говорить — эти навыки мы приобрели со временем. Так же и с карьерой, и с техническими навыками. Если у вас есть возможность сделать следующий шаг — делайте. Достичь можно любой цели, главное — не сдавайтесь. Когда всё получится, будете себе очень благодарны 🙂Курсы по программированию для получения новой специальности или повышения:Инженер по автоматизации;DevOps-инженер с нуля;Системный администратор;Инженер по тестированию;Bitrix-разработчик.Топ бесплатных курсов и онлайн-встреч:Специалист по информационной безопасности;День открытых дверей магистерской программы «Прикладной искусственный интеллект»;1С-программист;Системный аналитик;Основы разработки на Java."
СберМаркет,,,"Хорошие программисты совершенствуют навыки, великие — своё мышление",2024-06-01T17:33:33.000Z,"За свою карьеру я прошёл через множество испытаний:В первые годы мне с трудом давался функциональный стабильный код.Потом меня напрягали сложные разговоры о стилях архитектуры, масштабировании, автоматизации.Затем мне было трудно общаться с непохожими на меня коллегами, которых я совсем не понимал.А теперь мне сложно быть неиссякаемым источником вдохновения для команды и помогать сотрудникам справляться с новыми проблемами, не падая духом.Анализируя свою карьеру и карьеру моих коллег и подопечных, я понял одну вещь: профессиональный рост — всегда трудная задача. Мы снова и снова повторяем цикл формирования навыков. Но если разобраться, что к чему, движение вперёд можно сделать более приятным и менее хаотичным.Я бы выделил три фазы в освоении нового навыка:Фаза подстройки. Когда приходишь на новую работу, часто не хватает опыта и представлений о том, как лучше всего решать возникающие задачи. Поэтому в первую очередь мы изучаем правила, по которым следует действовать.Фаза наития. По мере накопления опыта мы начинаем полагаться на собственное чутьё. Но осторожнее: не путайте интуицию с самонадеянностью.Фаза стагнации. Что мы делаем, когда обжились на своей должности: уверенно идём по проторённой дорожке или переходим к чему-то новому?Дальше рассказываю, как и почему мы проходим каждую фазу и как извлекать максимум пользы на каждом этапе.Мышление ростаС неврологической точки зрения процесс обучения можно представить как протаптывание новой дорожки в мозге. Чем больше мы ходим по этой дорожке нейронов, тем шире она становится и тем проще на неё попасть. Нужно много сил, чтобы проложить такую дорогу. Чтобы добиться роста, надо выйти из зоны комфорта, прыгнуть выше головы.Когда вам предстоят такие усилия, очень полезно настроить мышление роста. В соответствии с этой установкой наши способности — это не данность, их можно развивать и совершенствовать. А ошибки и неудачи следует воспринимать как трамплин для дальнейшего роста. Этот подход позволяет позитивно воспринимать любые тяжёлые усилия, которые мы тратим на обучение чему-то новому.На каком бы жизненном этапе мы ни находились, расти всегда трудно. Но при правильном настрое и решительности со временем становится легче.1. Фаза подстройкиМне очень тяжело дался переход на руководящую должность. Этот вызов, один из самых трудных в моей жизни, пошатнул моё отношение к разработке программного обеспечения.Мне всегда нравилось решать трудные программистские задачи, и именно от этого умения зависела моя профессиональная самооценка. С переходом на управленческую должность я стал тратить значительную часть времени на решение новых задач. Конечно, я и раньше руководил рабочими группами и был наставником для других разработчиков. Но при переходе на новую должность я понял: если заниматься управлением ресурсами, проектами и людьми как полноценной работой, на мою главную страсть будет оставаться гораздо меньше времени. Если вообще будет оставаться.Мой KPI нужно было менять. Я привык оценивать только собственный вклад в работу. Теперь же основой оценки моей эффективности стала работа всей команды. Работает ли она качественно? Довольны ли разработчики? Этот переход дался мне с большим трудом и растянулся на несколько месяцев. Я разочаровался и начал сомневаться в себе — словил тяжёлую форму синдрома самозванца.Потребовалось несколько циклов карьерного роста, чтобы я наконец осознал: это всего лишь очередная фаза. Трудности — это нормально, и я это переживу.Поймите: синдром самозванца — естественное состояние, когда у вас появляются новые обязанности, будь то первая работа после института или повышение по службе.Всё, что нам нужно делать, — не сворачивать с выбранной дорожки, формировать новые нейронные связи, и дела пойдут в гору.2. Фаза наитияВ какой-то момент мы перестаём чувствовать себя застрявшими в одной точке. Жизнь налаживается. Да, время от времени трудности и неудачи снова будут появляться на горизонте. И всё же эти перемены и понимание, что дела идут на лад, воодушевляют как ничто другое. Но на этом этапе можно стать жертвой излишней самоуверенности — когда мы переоцениваем свои навыки из-за недостатка опыта (как показано на рисунке ниже).Источник:Эффект Даннинга — КрюгераЧтобы сформировать высокую компетентность, нужно развивать интуицию, которая подскажет, как лучше всего справляться с новыми вызовами.На старте мы штудируем правила, которые дают нам наставники, коллеги и другие люди с подходящим опытом работы. Но жизнь редко идёт по прямой, и правила не могут охватить все ситуации. Только эксперименты и своевременная обратная связь помогают натренировать мозг для интуитивного принятия решений вне рамок.Когда я начал менторить программистов и проводить встречи один на один, я глубоко погрузился в нюансы работы: какие вопросы и как часто задавать, как долго беседовать и так далее. Именно так я построил надёжный фундамент, который лёг в основу дальнейшего развития интуиции.После нескольких встреч я научился адаптироваться к потребностям каждого сотрудника. Например, некоторые разработчики охотно делились мыслями и переживаниями, но другие были достаточно замкнуты: приходилось бережно подталкивать их к большей откровенности. Мне потребовалось некоторое время, чтобы научиться интуитивно угадывать, держит ли разработчик в себе свои переживания, почему он не хочет об этом говорить и как лучше всего быть в такой ситуации.Я много раз ошибался. Но проявлял инициативу и работал над собой. Сотрудники верили мне, потому что видели, что я воспринимаю их обратную связь всерьёз.Не попадайте в ловушку самоуверенности, продолжайте собирать фидбэк и двигаться вперёд.Иногда ощущаешь себя на вершине мира, а временами всё сложно. Развитие интуиции, чтобы понимать, как и что лучше работает, — требует времени и сил, так что не пропускайте никакие этапы.3. Фаза стагнацииКак понять, что мы миновали фазу чрезмерной самоуверенности и стали настоящим экспертом? Обычно это происходит, когда становится проще работать. Иногда бывает трудно из-за объёмовработы, но сама работа больше не напрягает. Мы уже сто раз всё это видали, так что нас нечем удивить. Можем положиться на интуицию. Мы достигли вершины!В этот момент каждому разработчику нужно принять трудное решение: остаёмся на этом уровне, где всё уже более или менее знакомо, наслаждаемся отсутствием большой умственной нагрузки и стресса, при этом выдаём высококлассный результат,илипринимаем от жизни новый вызов, берём на себя новые обязанности в текущей или другой компании и в каком-то смысле снова начинаем всё с нуля?Я, как старший разработчик, задаюсь главным вопросом: готов я строить карьеру руководителя или мне хочется оставаться высококлассным разработчиком? Это трудный выбор между принципиально разными ролями. Я был хорошим разработчиком и любил свою работу. Но честно признался себе, чего действительно хочу, и с поддержкой команды пошёл в начальники.Обсудите свои перспективы с наставниками и коллегами, но следуйте тому, что действительно важно именно для вас.В идеале можно объединить продвижение по службе со своими профессиональными целями и интересами. А чтобы принять обоснованное решение, нужно изучить все возможности.Хорошие новости: обучение имеет накопительный эффектВ течение всей карьеры мы учимся и развиваемся — это захватывающее путешествие, которое приносит и радость, и боль. Если понимаешь, что впереди будут трудности и это нормально, учишься ценить этот опыт.Что особенно приятно, по мере развития становится проще решать новые задачи. Срабатывает своеобразный эффект снежного кома — когда навыки наслаиваются друг на друга. Если вы уже научились общаться с разработчиками, вам будет проще научиться общаться с людьми бизнеса. А благодаря этому вам будет проще освоить искусство презентаций. Эта цепочка может продолжаться бесконечно.Кстати, даже мне бывает чертовски тяжело делиться своими мыслями, открываться миру и давать другим шанс доказать мою неправоту. Но я научился ценить, насколько это заставляет меня расти и развиваться.В следующий раз, когда вы возьмёте на себя новые обязанности, держите в голове общую картину и постарайтесь насладиться предстоящими испытаниями: синдромом самозванца в начале, затем всплеском самоуверенности на этапе экспериментов и обучения, а когда достигнете вершины — стагнацией и размышлениями о следующих шагах. Все они помогают нам расти.Курсы по программированию для получения новой специальности или повышения:Fullstack-разработчик на Python;Python-разработчик: расширенный курс;Инженер по тестированию;DevOps-инженер;Разработчик игр на Unity.Топ бесплатных курсов и занятий:Тестировщик;Excel: простые шаги для оптимизации работы с данными;День открытых дверей магистерской программы «Прикладной искусственный интеллект»;Путь к здоровому сну;1С-программист."
СберМаркет,,,"10 вредных привычек, которые программисты втайне обожают",2024-05-24T12:36:12.000Z,"У программистов очень странные отношения с правилами. С одной стороны, программный код — это не что иное, как огромный свод правил — правил, которые бесконечно применяются покорными кремниевыми вратами без страха и упрёка и почти всегда без сбоев из-за альфа-частиц. И да, мы хотим, чтобы транзисторы безукоризненно выполняли эти правила.Но есть и другой, не столь сакральный уровень. В отличие от законов для ПО, правила, которые мы создаём для себя, довольно гибкие. Некоторые из них сформулированы скорее из соображений стиля, другие же делают кипу строптивого кода системной. Это набор правил, которые применяются к нашим делам и поступкам, а не к работе оборудования.Вопрос вот в чём:могут ли люди нарушать собственные правила?Есть ли у нас право на лету менять эти правила? Может быть, те или иные правила — это выходцы из другой эпохи. Может быть, некоторые правила с самого начала были сырыми. Может, кое-какие казались в своё время очень классными. Может, некоторые правила на самом деле — не правила, а привычки.В этой статье хочу рассказать вам про десять привычек программирования, настолько вредных, что иногда они оказываются полезными.Код без комментариевВсе знают, что разбираться в незадокументированном коде и заниматься его отладкой — это кошмар на улице Вязов. На уроках программирования нас учат, что писать понятные комментарии очень важно.Грамотное программирование— стиль программирования, объединяющий код и человеческий язык, — придумал Дональд Кнут, возможно, величайший из всех программистов на Земле. Кто мы такие, чтобы спорить с этим авторитетом?Но горькая правда состоит в том, что иногда комментарии всё только портят. Порой документация не имеет ничего общего с кодом.Может, команда, ответственная за документацию, расположена далеко от команды разработчиков. Буквально на другой планете. Может, авторы кода выкатили критически важное обновление, но забыли сказать об этом техписам, или техписы в курсе, но у них ещё руки не дошли обновить документацию. Иногда разработчики не обновляют даже общий комментарий про метод, который они поменяли. Нам остаётся только гадать.Но есть и другие проблемы. Может, комментарий написан на иностранном языке. Может, на описание концепции уйдёт как минимум несколько абзацев, а разработчик заканчивает agile-спринт. Может, тот, кто писал комментарий, просто ошибся.По всем этим и не только этим причинам некоторые разработчики считают, что лучше всего не перебарщивать с бесполезными комментариями, а то и вовсе обходиться без них. Вместо этого они отдают предпочтение простым, довольно коротким функциям, в которых в качестве подсказки используют длинные описательные имена переменных а-ля «горбатый регистр». Если компилятор не содержит ошибки, код должен быть наиболее точным отражением того, что делает компьютер.Медленно работающий кодБыстрый код = простой код. Но очень быстрый код — это сложный код. Поиск золотой середины в этом вопросе — задача нетривиальная.Так или иначе, это компромисс. В целом, нам хочется, чтобы наши программы работали быстро. Но если позднее никто не сможет разобраться в коде, такая сложность приведёт к медленной работе.Если скорость не так уж критична, имеет смысл писать код, который работает немного медленнее, но в котором проще разобраться.Иногда проще и медленнее — это лучше, чем суперинтеллектуально и супербыстро.Витиеватый кодОдному моему коллеге нравится применять все новые умные операторы в JavaScript, например, ellipsis. Код получается более компактным — в его понимании он проще и лучше. Любое ревью кода в исполнении таких программистов пестрит предложениями о том, где ещё можно переписать код, чтобы вставить в него новый синтаксис.А другие коллеги не разделяют мнение, что чем проще код, тем легче в нём разобраться.Чтобы прочитать такой код, надо распаковать новые операторы, часть которых может использоваться совершенно по-разному. Чтобы понять, как именно использовался оператор, приходится брать паузу и погружаться в размышления — здесь недостаточно пройтись по верхам. Чтение кода становится головной болью.Нелюбовь к сверхкомпактному коду имеет исторические предпосылки. Языки программирования, вроде APL, задуманные как очень компактные и эффективные за счёт применения собственных символов, почти исчезли. А вот другие языки, вродеPython, которые изобилуют разными скобками, продолжают набирать популярность.Любители новейших и величайших абстракций будут и дальше вставлять новые компактные функции везде, где получится, и хвалиться своей краткостью. Они же современные и модные. А их коллеги будут и дальше вставлять в стек удобочитаемый код подлиннее: в конечном счёте такой код читать легче.Старый добрый кодСоздатели языков программирования любят придумывать умные абстракции и синтаксические структуры, с помощью которых те или иные задачи можно решить в мгновение ока. Эти языки переполнены абстракциями, и именно по этой причине мануалы к ним иногда переваливают за тысячу страниц.Некоторые специалисты верят, что применять такие функции — это к лучшему. Дьявол кроется в деталях, а без практики навыки не отточишь. Разве нам стоит отказываться хоть от одной крупинки синтаксического сахара, описанного в этом рецепте на тысячу страниц? И всё же это правило не всегда идёт на пользу.Чрезмерное разнообразие свойств может всё запутать.Сейчас появилось так много умных синтаксических штук, что знать их все просто невозможно. Да и зачем? Сколько способов нужно, чтобы проверить поле на пустое значение или сделать наследование многоуровневым? Неужели только один из этих способов правильный и намного лучше остальных? Конечно, в команде всегда найдётся сотрудник, который устроит скандал по этому поводу и испортит обеденный перерыв или летучку.По крайней мере, одна группа разработчиков языков программирования решила ограничить набор характеристик. Создателиязыка Goговорят, что хотели создать язык программирования, который можно освоить очень быстро, буквально за день. То есть все разработчики в команде смогли бы читать весь код. Чем меньше свойств, тем меньше путаницы.Код собственными рукамиЭксперты по эффективности любят повторять: «Не изобретайте велосипед». Используйте готовые протестированные библиотеки. Не брезгуйте унаследованным кодом — он проверен временем.Но иногда не стоит отказываться от нового подхода. Библиотеки чаще всего пишут для специалистов широкого профиля и распространённых сценариев применения. Они перегружены «перестраховочными» тестами, позволяющими убедиться в согласованности данных и в том, что пользователь не застопорит всю систему, если введёт не те параметры.Но если у вас особый случай, несколько строк специализированного кода могут кардинально ускорить работу. Он не сможет делать всё, на что способна библиотека, зато то, что вам нужно, он сделает вдвое быстрее.Конечно, тут есть свои риски. Бывает настолько сложный и эзотерический код (например, в системах шифрования), что лучше его не трогать, даже если вы более или менее понимаете, как он работает. Но когда это уместно (например, когда библиотека заметно увеличивает рабочую нагрузку), несколько удачных строк кода от себя творят чудеса.Предварительная оптимизацияПрограммисты часто пишут код на скорую руку и оправдываются давним нравоучением о том, что предварительная оптимизация — это просто пустая трата времени. Они считают, что только после запуска всей системы станет понятно, какая именно часть кода окажется проблемной. Глупо тратить часы работы на идеальную функцию, если её будут вызывать только раз в год.Простое и надёжное, как швейцарские часы, правило. Да, некоторые проекты так и не доводят до конца из-за бесконечного планирования и избыточной оптимизации. Но, с другой стороны, есть множество примеров, когда капля предусмотрительности кардинально изменила бы ситуацию.Неправильно подобранные схемы и структуры данных порождают архитектуру, которую в дальнейшем едва ли можно будет оптимизировать.Иногда структура разбросана по стольким частям кода, что даже грамотный рефакторинг не спасёт ситуацию. И такую проблему решила бы предварительная оптимизация.БеззаботностьПереходя дорогу, хорошие программисты смотрят сначала налево, потом направо. Они добавляют многочисленные строки кода, которые перепроверяют данные дважды, а то и трижды. И только потом с этими данными выполняются те или иные действия. Мало ли, а вдруг сюда пробрался пустой указатель!Увы, все меры предосторожности могут заметно снизить скорость выполнения кода.Иногда из соображений производительности лучше в упор не видеть потенциальных рисков и просто написать код, который перепроверяет всё не слишком тщательно. Если нам нужно, чтобы код работал быстро, придётся обойтись абсолютным минимумом.НесогласованностьЛюди любят порядок, а программисты убеждены, что для всей массы кода нужно использовать одни и те же приёмы, алгоритмы и синтаксис в каждой его части. Такие старания упрощают жизнь каждому, кому придётся работать с этим кодом впоследствии.У такой согласованности есть своя цена. Прежде всего, это время, а иногда ещё и сложность.Чтобы устранить различия, придётся вернуться и переписать весь код, для которого применяли другие приёмы и алгоритмы. Из-за этого можно выйти далеко за рамки намеченного бюджета.Более серьёзная проблема возникает в отношениях между разными разделами. В некоторых проектах используется унаследованный код. В других — есть зависимости от библиотек. Многие проекты не смогут работать без API, написанных совершенно другими людьми в других компаниях.Часто сгладить различия между этими группами невозможно, а переписать весь стек в соответствии с новейшим видением можно только ограниченное число раз. Странный закоулок нашего мозга требует добиться идеального порядка, но иногда лучше заключить перемирие с несогласованностью.Любовь к техническим новинкамСогласованность встаёт на пути у инноваций, ведь она подразумевает строгое следование заведённому порядку вещей.Иногда, чтобы добавить новые функции, встроить новые библиотеки или интегрировать стек с новыми API, нужно отойти от старых схем.Да, это усложнит жизнь тем, кому при чтении кода придётся перейти на другую скорость, но такова цена прогресса. Ведь именно это делает работу программиста такой интересной.Несоблюдение правилРади шутки я спросил у GoogleGemini, нарушали ли программисты какие-то правила, когда создавали его. Gemini ответил:«Я бы не сказал, что программисты нарушили те или иные правила. Скорее, они расширили границы передовых методов работы при создании больших языковых моделей, таких как я».«Большие языковые модели обучаются на массивных объёмах данных, и в том, как модель обучается на этих данных, всегда присутствует элемент неизвестности», — сказал Gemini. — «Некоторые приёмы, используемые при создании больших языковых моделей, могут быть очень эффективными. Но трудно точно понять, как именно модель формулирует ответы».Вот оно! LLM (с англ. large language model — большая языковая модель) лучше нас знает, что правила устаревают и меняются. Возможно, если можно загрузить датасеты для обучения, уже не нужно тратить столько времени на понимание алгоритмов. Так что идите вперёд и будьте людьми. А о правилах пусть думают LLM.Курсы по программированию для получения новой специальности или повышения:Django: создание backend-приложений;React: фреймворк фронтенд-разработки;DevOps-инженер;Разработчик игр на Unity;Специалист по информационной безопасности.Топ бесплатных курсов и занятий:Проектирование интерфейсов с AGIMA;Основы Adobe Photoshop;Профессия Мемолог;Основы Figma;Карьерная консультация."
СберМаркет,,,Создаём безукоризненную автодокументацию кода на Python с помощью Sphinx,2024-05-21T11:45:02.000Z,"У дата-сайентиста множество обязанностей в каждом проекте, но именно подготовку документации он чаще всего оставляет на последний момент. Возможно, вы добросовестно пишете строки документации для классов и функций (молодцы!), но достаточно ли этого?На мой взгляд, документация не должна зависеть от вашего кода. Ни ваша команда, ни вы сами (а такое тоже возможно) не должны по прошествии некоторого времени продираться сквозь сотни строк кода в модулях Python в отчаянных попытках разобраться, что к чему. А ведь можно создавать прекрасную, стандартизированную документацию в едином стиле, просто применяя строки документации (docstrings) за несколько простых шагов. И проект будет говорить сам за себя.В этой статье я расскажу огенераторе документации Sphinx, с помощью которого можно автоматически создавать документацию для модулей Python. Кроме того, я буду использовать шаблон проектаCookiecutter Data Scienceв Visual Studio Code (VS Code), поскольку он легко интегрируется в Sphinx и имеет стандартизированную структуру директорий.Официальное пособие по использованию Sphinx— отличный ресурс для пользователей, которые хотят углубиться в детали. А моя статья — это краткое руководство по началу работы с этим инструментом.Пара слов о строках документацииDocstrings — ключ к качественной документации. Это блоки комментариев, которые размещаются в каждом классе, методе класса и функции и описывают природу кода наряду с данными на входе и выходе, а также выявленными ошибками.Есть три основных формата docstring: Google, reStructuredText (reST) и NumPy. Все они содержат одну и ту же информацию, различия сводятся только к формату.Здесьможно посмотреть примеры каждого формата docstring.Я буду использовать формат Google, так как его легко открыть и посмотреть и он занимает меньше места, чем остальные. Этот блок кода — типичный пример docstring в Google:""""""Description of the function, class or method etc.

Args:
    varA (str): Description of varA
    varB (bool): Description of varB

Returns:
    list: Description of returned list

Raises:
    ValueError: Description of raised error
""""""Загрузите autoDocstring — Python Docstring Generator в VS Code, чтобы автоматически генерировать строки документации при вводе трёх двойных кавычек. Создавайте docstring, только когда закончили с функцией, чтобы все вводные данные, результаты и ошибки вошли в генерируемый шаблон docstring.Приступим к созданию документации.Для этой статьи я создала модуль Pythondemo.py, который содержит класс и три базовых функции. Все они, кроме одной функции, снабжены аннотациями в виде docstrings. Именно в этом модуле я и буду создавать документацию для статьи. Вот содержание модуляdemo.py:Содержание модуляdemo.py, к которому мы создаём документацию. Снимок состояния сделан с помощью расширения CodeSnap в VS Code1. НастройкаДля начала нужно всё настроить. Установите VS Code и настройте новый проект вместе с Sphinx. Здесь есть два варианта:Настроить новый проект с помощью Cookiecutter, где соответствующая настройка Sphinx выполняется в момент создания стандартизированных директорий.Создать собственную структуру проекта и установить Sphinx отдельно.Вариант 1: установка CookiecutterВ терминале выполните pip install Cookiecutter и создайте новый проект:pip install cookiecutter
cookiecutter https://github.com/drivendata/cookiecutter-data-scienceПотом ответьте на вопросы, которые появятся в окне терминала.Готово, новый проект создан. Фреймворк Sphinx хранится в директории проекта /docs.Вариант 2: быстрый запуск SphinxЕсли вам не особо нравится шаблон Cookiecutter, создайте структуру проекта с нуля и установите Sphinx. Вполне разумно создать директорию для документации и установить в ней Sphinx. В терминале это выглядит так:mkdir docs
cd docs

pip install sphinx
sphinx-quickstart2. Структура папки SphinxПосле установки Sphinx тем или иным способом в директории проекта для документации появится несколько файлов.Файлconf.py— это основной файл конфигурации. Именно в него вносят изменения, чтобы отразить особенности документации, — об этом мы подробнее поговорим в следующем разделе.Файлindex.rst— своего рода оглавление документации.Здесьможно подробнее узнать об этом файле.Файлыgetting-started.rstиcommands.rst— предлагаемые шаблоны документации. Если они вам не нужны, удалите их.Файлы make (make.batиMakefile) нужны для создания документации. Эти файлы не нужно редактировать. Их вызывают в окне терминала в момент создания документации.Файлы Sphinx, установленные по умолчанию3. Файл conf.pyВсё самое важное происходит именно в файле конфигурации. Он используется в процессе сборки, так что очень важно настроить его правильно. Последовательность действий при изменении файлаconf.py:Преобразовываем комментарий к строке sys.path обратно в код(строка 20):#  sys.path.insert(0,  os.path.abspath('.'))Меняем путь os.path.abspathк соответствующему месту размещения кода, который нужно задокументировать (от файлаconf.py). Допустим, модули Python, документацию для которых нужно создать, находятся в директории проекта src/. Соответственно, я меняю os.path.abspath на место в директории /src, которая находится в родительской папке файлаconf.py. Можно указать относительное местоположение с помощью синтаксиса. and /:sys.path.insert(0,  os.path.abspath('../src'))

""""""
# you can use the following syntax to specify relative locations:

'.' # current path of conf.py
'..' # parent path of conf.py
'../..' # parent of the parent path of conf.py
""""""Относительное местоположение директории, в которой содержатся модули Python для папки с документацией. В этом примере нужно составить документацию к модулюdemo.py, который находится в директории src/data/Добавляем необходимые расширения. Вам понадобится добавить к файлуconf.pyнесколько расширений, чтобы пользоваться дополнительными функциональными возможностями при создании документации. Всё это необязательно, но поперебирать разные расширения можноздесь. Я рекомендую минимум пять расширений:Sphinx.ext.autodoc— использует документацию из docstrings.Autodocsumm— вверху HTML-страницы генерирует таблицу со сводной информацией из всех docstrings, удобен, если у вас много docstrings.Примечание: вам понадобится выполнить pip install autodocsumm в терминале.Sphinx.ext.napoleon— позволяет Sphinx выполнять парсинг строк документации в формате Google.Sphinx.ext.viewcode— добавляет ссылку на HTML-страницу, которая содержит исходный код каждого модуля.Sphinx.ext.coverage— выдаёт сводную информацию о количестве классов или функций, у которых есть docstrings. Если процент таких классов и функций довольно высок, база кода снабжена достаточным объёмом пояснений.Вот как включить эти расширения в файлconf.py(строка 29):#  add in the extension names to the empty list variable 'extensions'
extensions = [
      'sphinx.ext.autodoc', 
      'sphinx.ext.napoleon', 
      'autodocsumm', 
      'sphinx.ext.coverage'
]

# add in this line for the autosummary functionality
auto_doc_default_options = {'autosummary': True}Меняем тему. По умолчанию для создания документации используется довольно лаконичная тема. Но если вдруг вы захотите поэкспериментировать с разными вариантами, для этого нужно заменить значение ‘default’ переменной html_theme (строка 94) на какую-нибудь стандартнуютемуилитему стороннего разработчика. В этой статье я покажу тему по умолчанию и тему Read the Docs.html_theme = 'sphinx_rtd_theme' # read the docs theme. This variable is 'default' by default.Не забудьте, что для установки нестандартных сторонних тем нужно будет выполнить pip install.4. Делаем HTML-страницыИтак, файлconf.pyготов. У вас в коде прекрасные docstrings, мы вот-вот займёмся скрейпингом и создадим HTML-страницы.Генерируем файлы .rst пакетов PythonЭти файлы — родной формат Sphinx. Именно из них формируют HTML-страницы. Создаём их до HTML-файлов. Для этого выполняем командуsphinx.apidoc, которая использует расширение autodoc для поиска всех модулей Python (например, любых файлов .py) в местоположении sys.path, указанном в файлеconf.py. При выполнении команды apidoc можно задавать дополнительные параметры, описанные вдокументации, но я использовала следующий шаблон:Не забудьте в терминале поменять директорию на корневую директорию проекта, чтобы выполнить следующий код.sphinx-apidoc -f -o output_dir module_dir/-f— принудительная перезапись любых имеющихся сгенерированных файлов.-o output_dir— директория для размещения выходных файлов. Если её ещё нет, то как раз на этом этапе она будет создана. На забудьте заменить output_dir именем директории на ваше усмотрение. Я указала директорию /docs.module_dir— местоположение пакетов Python, документацию к которым мы создаём.После выполнения этой команды в папке docs должны появиться новые .rst-файлы.Содержимое папки с документацией после выполнения команды sphinx-apidoc для создания файлов .rstОбратите внимание, что появилось два новых файла .rst:data.rstиmodules.rst. В дополнение кmodules.rstдля каждой директории, которая содержит по крайней мере один модуль Python, будет создан файл .rst. В моём примере генерируетсяdata.rst, так как я сохранила файл demo.py в директории src/data.Если у вас несколько директорий для модулей Python в местоположении, указанном в sys.path в файлеconf.py, будет создано несколько файлов .rst.Примечание: эти файлы ещё не содержат извлечённую документацию, в них есть только информация, необходимая autodoc для создания HTML-файлов на следующем этапе.Редактируем файл index.rstПомните, чтоindex.rstфункционирует как страница с оглавлением, — надо отредактировать этот файл и включить в него все модули Python, для которых нужна документация.К счастью,modules.rstссылается на исходное местоположение всех модулей Python, идентифицированных в sys.path, поэтому можно просто добавить этот файл вindex.rst. Для этого откройте файлindex.rstи добавьте modules в раздел toctree (дерево содержания). Проверьте, есть ли строка между параметром :maxdepth: и именами файлов .rst.getting-started и commands уже будут включены в файл index.rst. Их можно удалить из файла, если вы не планируете создавать HTML-страницы. Хотя создать страницу getting-started — не такая уж плохая идея.Содержание файла index.rst. Я добавила modules, чтобы файл modules.rst использовался в процессе создания HTMLСоздание HTML-файловТеперь для сборки HTML-файлов мы можем использовать файлы make в директории с документацией. Эти файлы появятся в директории_build/html/в папке с документацией. Предварительный просмотр этих файлов доступен в VS Code, если загрузить расширение HTML Preview.Измените директорию для размещения файла make.bat и выполните следующую команду в терминале cmd:make htmlЕсли вы работаете в терминале Windows PowerShell, а не cmd, используйте следующий синтаксис:.\make.bat htmlЕсли при выполнении команды make html возникает предупреждение autodoc: failed to import module, скорее всего, autodoc не удалось найти модули из-за ошибок в конфигурации sys.path в файле conf.py. Убедитесь, что вы верно указали директорию, в которой находятся модули Python.Редактируем HTML-файлыЕсли вы собираетесь редактировать docstrings и вносить изменения в HTML-файлы, запустите команду:make clean htmlПосмотрим на нашу документациюНапомню, что я оформила документацию для модуля Pythondemo.pyв двух стилях (см. скриншоты ниже). На первом скриншоте — тема по умолчанию, на втором — тема Read the Docs.Пример созданной в Sphinx HTML-страницы с документацией к модулю Python: тема по умолчаниюПример созданной в Sphinx HTML-страницы с документацией к модулю Python: тема Read the DocsСодержание в обоих случаях одинаково, но выглядит документация по-разному. Давайте рассмотрим элементы, из которых состоят оба варианта:панель навигации — слева;сводная информация по всем классам или функциям модуля представлена в таблицах вверху страницы (благодаря расширению autodocsumm);под сводной информацией располагается подробный перечень компонентов docstring по всем функциям и классам.При создании HTML-страниц появится несколько иерархических HTML-страниц. В частности, появится домашняя страница и страницы по каждому пакету и модулю. Просмотрите страницы, изучите их структуру и почитайте официальную документацию, чтобы выяснить, как ещё можно кастомизировать эти страницы.Например, чтобы в документации можно было просматривать raw code каждой функции, добавьте расширение sphinx.ext.viewcode в файлconf.py. Напротив каждой функции или метода класса появится гиперссылка, при нажатии на которую будет показан raw code. Таким образом можно легко найти код, не углубляясь в базу программного кода.Готово! Простая красивая документация к модулям Python, для создания которой понадобилось несколько несложных действий в Sphinx. Надеюсь, вам было интересно, а инструмент для автоматического создания документации пригодится вам в ваших проектах. Если у вас есть полезные советы, делитесь ими в комментариях :)Курсы по программированию для получения новой специальности или повышения:Инженер по тестированию;Системный администратор;Разработчик на C++;Сетевой инженер;Frontend-разработчик.Топ бесплатных курсов и онлайн-встреч:Основы разработки на Java;Как победить синдром самозванца и построить карьеру;День открытых дверей магистратуры «LegalTech: автоматизация юридических процессов»;Основы Python: создаём телеграм-бота;День открытых дверей магистратуры «Финансовые технологии и аналитика»."
СберМаркет,,,Что значит быть хорошим разработчиком: 11 полезных советов от сеньора,2024-05-16T10:43:46.000Z,"Поданнымплатформы «Авито Услуги», 23% жителей России хотели бы начать карьеру в сфере информационных технологий. Но состояться в профессии сразу же после пары курсов и стажировки не получится. Путь от студента образовательной платформы до разработчика, которого можно назвать хорошим, непростой. Мы собрали рекомендации от тех, кто добился успеха в сфере информационных технологий, чтобы помочь новичкам построить свою карьеру.Чем хороший разработчик отличается от посредственногоПочему стоит стремиться стать хорошим разработчиком? Ответ очевиден: у специалиста, постоянно совершенствующего свои навыки, шире окно возможностей. Он может работать в компаниях с лучшими условиями, выбирать более интересные проекты и, конечно, получать соответствующую зарплату. Нельзя забывать и о том, что сфера IT крайне динамичная, и без постоянного развития можно обнаружить, что твои навыки стали невостребованными.Хороший разработчик не обязательно занимает высокую ступеньку в корпоративной иерархии. Во-первых, в разных компанияхгрейдымогут серьёзно различаться: требования к джуну в одном месте могут соответствовать позиции хорошего мидла+ в другом (уровня дохода, кстати, это тоже касается). Во-вторых, стремление развиваться в профессии, интерес к своему делу и перспективы нужны как новичку, так и разработчику со стажем.Дмитрий ФедоровНачальник отдела разработки ПО в НИЦ «ИРТ»Хороший разработчик не стоит на месте. Каждый год выходят новые технологии или новые подходы к решениям задач. Необходимо быть в курсе того, что происходит в профессиональном мире, в противном случае через несколько лет разработчик может стать невостребованным.Важно, чтобы человек стремился к изучению нового, осваивал какбазу, так и новые технологии. И нельзя забывать о мягких навыках — умении взаимодействовать с командой и заказчиком, распределять рабочее время, анализировать ситуацию.Григорий ВахмистровГлавный инженер-программист в «ПСБ Лаб»Понятие хорошего разработчика весьма субъективно. На мой взгляд, такой специалист автономен и может самостоятельно разобраться практически в чём угодно: как в технологиях, так и в бизнес-требованиях. При этом он не стремится выполнять подряд все указания руководства или старших коллег, а умеет обосновать, почему не стоит браться за те или иные задачи — например, внедрение технологий, которые затруднят поддержку продукта, неверные архитектурные решения, ошибки проектирования бизнес-требований и так далее.Почему вообще стоит прислушиваться к сеньорамКаждый тимлид, сеньор или технический менеджер когда-то был стажёром и джуном. Исключения, конечно, есть, но они достаточно редки. Каждый из опытных разработчиков совершил достаточное количество ошибок, которые и помогли ему получить опыт. Благодаря советам старших коллег можно ускорить своё профессиональное развитие, осознанно выбирая лучшие практики.Ещё один важный момент: опытные разработчики заинтересованы в развитии молодых специалистов. На рынке остро ощущается дефицит квалифицированных кадров, способных решать задачи самостоятельно — Минцифрыоцениваютэтот показатель в 500–700 тысяч человек. Поэтому тимлиды искренне заинтересованы в расширении команды перспективными новичками.При этом стоит отдавать себе отчёт в том, что опыт одного человека не может быть универсальным. К любому совету стоит подходить с долей здорового скептицизма. Иногда на задачи лучше взглянуть с нескольких точек зрения.Совет 1. Слушайте советы старших коллег, но не забывайте о критическом мышлении.Какие пути развития есть у хорошего программистаПрежде чем приступать к каким-то действиям, нужно определиться с вектором своей карьеры. Для этого нужно понять, в каком направлении вам интереснее развиваться. Одним разработчикам интереснее техническая часть, другие хотят расти как управленцы, третьи мечтают создать собственный продукт. Для каждого из направлений нужно развивать собственный скиллсет.Совет 2. Составьте дорожную карту своего профессионального развития.Григорий ВахмистровГлавный инженер-программист в «ПСБ Лаб»Если вы хотите составить индивидуальный план развития для роста в конкретном проекте, должности или зарплате, подойдите с этим вопросом к руководителю. Сделать это можно на встрече 1-1 или в рамках отдельного обсуждения. По итогу встречи должен быть сформирован документ, в котором чётко прописаны цели, сроки и критерии их достижения. Если такие документы в компании не предусмотрены, сформулируйте то же самое устно. Лучше сразу договориться с руководителем о регулярном совместном отслеживании прогресса. Можно назначить для этого ежемесячные встречи 1-1. Такие митинги помогут выровнять ожидания как сотрудника, так и руководителя. У обеих сторон будет понимание, возможно ли достижение договорённостей в указанные сроки, либо же есть какие-то отклонения от графика. Заодно сразу можно обсудить причины отклонения и способы их коррекции.Будем честны: не во всех компаниях внедрены процессы, направленные на развитие сотрудников. В этом случае заниматься составлением дорожной карты придётся самостоятельно. Определите для себя, в какую сторону вы хотели бы расти, составьте список необходимых навыков и подумайте, как можно их развить. Обязательно ставьте себе внутренние дедлайны — например, через полгода изучить новую технологию или через месяц прочитать книгу по тайм-менеджменту.Можно попросить помощи в профессиональных сообществах — более опытные коллеги подскажут, в каком направлении двигаться и где искать нужную информацию. При этом не забывайте о критическом взгляде на чужие советы.Дмитрий ФедоровНачальник отдела разработки ПО в НИЦ «ИРТ»В первую очередь необходимо развивать те навыки, которые трудно даются. Можно составить список задач, которые не получается выполнять самостоятельно, обобщить их и подтягивать скиллы, которых не хватает для решения.Важно определиться с направлением работы. Например, если разработчик пишет на С++ в компании, которая занимается ИИ, нужно изучать, что нового происходит не только в С++, но и в ИИ, а также в смежных областях.Дорожная карта развития будет инерционной: получение первоначальных знаний — устранение пробелов, мешающих работе — изучение профессиональной области — применение полученных знаний на практике — снова изучение.Если фирма, в которой работает программист, может не дать ему профессионального развития (например, пилят старый проект на старом стандарте, и новое в нём не появляется), нужно выбирать: возможно, стоит выбрать другую компанию, в которой будет больше простора для развития, или в этой компании искать другие пути.Что делать, чтобы стать хорошим разработчикомДорожная карта позволит понять, на развитие каких навыков нужно сделать упор. Среди них будут как знания технологий, так и мягкие навыки.Конкретные знания и умения будут зависеть от разработанной вами дорожной карты, но есть набор навыков, которые пригодятся любому разработчику. Например, если вы хотите публичности, стоит подтянуть умение выступать перед аудиторией и создавать эффектные презентации, а если вам интереснее менеджмент, углубить знания психологии.Совет 3. Изучайте базовые технологии.Некоторые технологии быстро изменяются, но существует база, знание которой необходимо на любом этапе карьеры.Игорь СимдяновАрхитектор в НетологииРяд технологий можно смело изучать в любой момент, хоть в начале карьеры, хоть в зените:любой императивный или объектно-ориентированный язык программирования;реляционные базы данных и язык запросов SQL;операционная система UNIX (в 2024 году речь исключительно о Linux);регулярные выражения;система контроля версий git;технология Docker;объектно-ориентированное программирование;NoSQL-базы данных, хотя бы Redis.В зависимости от интересов разработчика этот список можно расширять дополнительными технологиями и возможностями. Однако перечисленные выше будут жить долго, и поэтому потраченное на их изучение время многократно окупится.Совет 4. Учитесь писать хороший код.Довольно очевидный пункт, но его нельзя не упомянуть. Хороший разработчик в первую очередь должен уметь писать хороший код. При этом не нужно боятьсяделать ошибки: без них не получится освоить новые навыки. Однако каждая ошибка должна учить вас чему-то.Игорь СимдяновАрхитектор в НетологииРазработчику нужна постоянная тренировка в написании кода. Не можете придумать себе задач? Идите на codewars.com, решайте задачи оттуда. Не думайте о том, что у вас получается хуже, чем у других: главное — решить задачу. Мастерство придёт. Остальные зачастую совершенствовали свои навыки годами, и у вас тоже, наверняка, на это уйдёт изрядное количество времени.Часто новички (и не только они) боятся разрабатывать плохой код, и навык в кодировании не совершенствуется. Человек годами живёт в мечте: когда-нибудь я освою язык программирования и буду программировать как профи, но до этого я писать код не буду. Это очень плохой подход. Никто не может вам запретить тренироваться в программировании, ссылаясь на то, что у вас неэффективный или не соответствующий соглашениям код.Совет 5. Полюбите процесс написания кода.Чтобы развиваться в профессии, нужна сильная мотивация.Причём одной лишь финансовой составляющей для мотивации недостаточно — важно искренне интересоваться технологиями. Пробуйте необычные подходы к работе — например, геймефицируйте рабочий процесс — изучайте новые стеки, ищите направление деятельности, которое вам наиболее близко. Возможно, для этого придётся несколько раз менять сферу, чтобы попробовать разные подходы. В больших компаниях это можно сделать внутренним переводом.Совет 6. Изучайте не только свою предметную область, но и соседние.Важно знать общие принципы создания программного продукта. Если вы в будущем хотели бы возглавить подразделение, важно иметь представление о задачах и процессах каждого участника команды. Но даже если вы не стремитесь в управленцы, такие знания позволят более эффективно взаимодействовать с коллегами из других подразделений, а также откроют новые карьерные перспективы.Григорий ВахмистровГлавный инженер-программист в «ПСБ Лаб»Если говорить про развитие в целом, то рано или поздно разработчик приходит к тому, что бывают ситуации, когда необходимо взять на себя ответственность за ту или иную часть проекта, которая изначально не входила в его зону ответственности. Например, ушёл аналитик и нужно собирать требования с бизнеса самостоятельно; архитектор в отпуске, а нужно срочно согласовать архитектуру нового сервиса и приступить к реализации; пришли в стартап/новый проект, где ещё нет DevOps, и нужно настроить хотя бы какую-то базовую инфраструктуру, чтобы обеспечить процесс разработки для себя и коллег... На мой взгляд, должно быть понимание всего процесса разработки программного обеспечения, начиная со сбора и формирования требований, заканчивая инфраструктурой, эксплуатацией и поддержкой. Безусловно, основной фокус должен быть на разработке, но важно понимать не только свою работу, но и работу коллег, а также весь процесс разработки ПО в целом.Совет 7. Спокойно относитесь к рутине.Программирование — это не только решение интересных задач. Разработчику приходится сталкиваться со скучными однообразными задачами или нудным поиском ошибок. Это часть профессии, в которой тоже нужно искать положительные моменты. Изучая непонятные и запутанные области знаний, вы тренируете нейронные связи. В дальнейшем осваивать новые технологии будет быстрее.Игорь СимдяновАрхитектор в НетологииЗапаситесь терпением и не гипнотизируйтесь историями успеха. Все любят рассказать про то, как они за неделю освоили новый язык программирования. Однако почему-то нигде не встречаются рассказы, как в программе на 60 строк двое суток можно искать пропущенную точку с запятой, попутно поссорившись со всеми друзьями и родными из-за скверного настроения. Но только такие ситуации вас приближают к профессиональному уровню. Причём поначалу это может показаться бесполезной потерей времени. Это не так. Это и есть процесс обучения, именно через неприятности, отладку вы и становитесь профессионалом. И не смотрите на часы и рекламу. Идите к своей цели, даже если это займёт 10 лет. Оно того стоит.Совет 8. Развивайте навык понимания задач.Если руководитель просит вас сделать что-то, убедитесь, что вы правильно уяснили цель задачи. Для этого нужно уметь правильно формулировать вопросы — и не стесняться их задавать.Дмитрий ФедоровНачальник отдела разработки ПО в НИЦ «ИРТ»Хороший руководитель должен уметь поставить задачу, а хороший разработчик — правильно понять её. Если что-то непонятно, не нужно стесняться задавать вопросы и просить объяснить подробнее. Здесь разработчик и руководитель должны работать совместно.Парадоксально, но с опытом этот навык может стираться. Когда разработчик чувствует себя достаточноуверенно, он считает, что подсказки ему не нужны — и в результате на получение нужного результата тратится больше времени. Такое вот проявление эффекта Даннинга — Крюгера.В дальнейшем придётся развить ещё одно умение, связанное с задачами — а именнопостановка. Всем известно, какой результат получается без внятного ТЗ. По возможности стоит избегать этого.Совет 9. Учитесь писать хорошие тексты.Разработчику приходится много писать — письма коллегам и клиентам, комментарии к коду, техническая документация. Важно, чтобы тексты были понятными. Это существенно сократит объём коммуникаций, освободив время для решения других задач. Можно развить навыки в написании текстов, прочитав книги Максима Ильяхова — «Пиши, сокращай», «Новые правила деловой переписки», «Текст по полочкам».Совет 10. Тренируйтесь искать информацию.Для многих задач уже существуют готовые решения. Хороший разработчик не будет изобретать велосипед и воспользуется другими наработками. Умение находить ответы на свои вопросы — важный скилл, развитию которого нужно уделять внимание.Дмитрий ФедоровНачальник отдела разработки ПО в НИЦ «ИРТ»Умение гуглить — важное качество разраба. Хороший разработчик умеет найти информацию для решения той или иной проблемы. Благо сейчас ресурсы позволяют это сделать. Если бегать по каждой мелочи к старшим коллегам, то хорошим это точно не закончится.Совет 11. Не забывайте о soft skills.Любой программный продукт — это не техническая система, а социально-техническая система. Эффективное взаимодействие с коллегами и понимание потребностей заказчиков не менее важно, чем написание качественного кода.Григорий ВахмистровГлавный инженер-программист в «ПСБ Лаб»Чтобы добиться успеха в профессии, нужно 100% уметь в софты — выяснять требования у бизнеса, согласовывать их с заказчиком, планировать выполнение задач с менеджером, защищать свои решения на design review, онбордить коллег, менторить новичков. Программные продукты — это не техническая, а социально-техническая система, и эффективное взаимодействие с коллегами не менее важно, чем написание качественного кода. Его, кстати, тоже нужно уметь писать: требования по софтам не отменяют требований по хардам.ЗаключениеПуть разработчика, возможно, не проще пути джедая. Важно понимать, чего вы хотите, и слушать советы наставников и старших коллег.Слушайте советы старших коллег, но не забывайте о критическом мышлении.Составьте дорожную карту своего профессионального развития.Изучайте базовые технологии.Учитесь писать хороший код.Полюбите процесс написания кода.Изучайте не только свою предметную область, но и соседние.Спокойно относитесь к рутине.Развивайте навык понимания задач.Учитесь писать хорошие тексты.Тренируйтесь искать информацию.Не забывайте о soft skills.Ваш план развития может меняться со временем — почти каждый разработчик на своём карьерном пути проходит ряд трансформаций. Но знания и умения, которые вы приобретёте в процессе, помогут вам добиваться профессиональных целей и стать тем, на кого будут равняться джуны нового поколения.Курсы по программированию для получения новой специальности или повышения:Fullstack-разработчик на Python;Backend-разработка на Node.js;Разработчик игр на Unity;Django: создание backend-приложений;React: фреймворк фронтенд-разработки.Топ бесплатных курсов и занятий:Основы Figma;Excel: простые шаги для оптимизации работы с данными;Тестировщик: быстрый старт в IT;Frontend-разработка: основы HTML, CSS и JavaScript;Основы Adobe After Effects."
СберМаркет,,,10 ошибок новичка-продакта: взгляд со стороны нанимающего менеджера,2024-08-27T08:32:44.000Z,"Устроиться на первую работу в IT непросто: конкуренция на стажерские позиции у продакт-менеджеров высокая (например, мы только на одну вакансию получили 400 откликов). Но раскрою секрет — нанимающий менеджер сразу отсеит большую часть соискателей.Меня зовут Евгения Баранова, я работаю менеджером по продукту в Учи.ру и совсем недавно сама нанимала стажера-продакта. Хочу рассказать об этом опыте и о типовых ошибках новичков, чтобы вы их не допустили. Ведь предупрежден — значит, вооружен. Ну что, погнали!КонтекстНаша команда разработкимобильного приложения для учителейрасширялась, и мы решили нанять стажера, потому что рабочих рук не хватало. Для кандидатов придумали тестовое задание — ответ на вопрос: «Как бы вы изменили приложение Учи.ру для учителей?»Скажу честно, у нас была отличная вакансия для начинающего продакта. Предстояло отвечать за приложение целиком, а не за локальный сценарий, участвовать в расчете юнит-экономики и разработке growth-стратегии, работать с командой маркетинга и руководить отдельной командой разработки. Стажера ждал колоссальный опыт и карьерные перспективы.Вакансия стала популярной, мы получили на нее 400 откликов. Но представьте, лишь 80 кандидатов выполнили тестовое задание. И только пять (!) человек с ним справились успешно и прошли входное интервью.По итогам двое вышли в финал, один получил оффер и приступил к работе. От публикации вакансии до оффера прошло четыре недели.По итогам этого месяца я сформулировала десять рекомендаций для тех, кто планирует откликаться на вакансии менеджера по продукту. Они кажутся вполне логичными, но, как я убедилась, следуют им далеко не все. А это точно увеличит шансы на трудоустройство.В лотерее не обойтись без билетаНе игнорируйте условия вакансии — в них четко написано, что нужно делать соискателю! В моей ситуации некоторые кандидаты пытались пойти окольными путями: например, писали мне социальных сетях, минуя рекрутера. Хотя мы специально указывали, куда отправить резюме.И помните, что даже самое блестящее резюме (плюс личное сообщение нанимающему менеджеру) не избавляет от необходимости выполнить тестовое задание. Иначе получается как в анекдоте про человека, который очень хотел выиграть в лотерею, но не купил лотерейный билет.Не будьте тем, кто создает проблемыЕсли вы все-таки нашли потенциального руководителя в социальных сетях и хотите обратить на себя внимание (в целом, это не возбраняется и действительно может сработать) — пишите коротко и информативно, без долгих вступлений и неуместных вопросов.Когда кандидат что-то просит: например, отправить ссылку на вакансию или написать фидбек на профиль в LinkedIn — создается впечатление, что этот сотрудник будет не решать проблемы, а создавать новые.Увы, реально сообщение мне в одной из соцсетейПотрудитесь над сопроводительным письмомОдного соискателя мы пригласили на интервью, несмотря на среднее тестовое задание, потому что cover letter нам понравилось.Так что же написать о себе? Коротко расскажите, что делает вас уникальным и почему вы будете полезны на новом месте работы. Перечислите свои реальные достижения, укажите мотивацию. А вот подробности о хобби лучше опустить, если бизнес напрямую с ними не связан.Используйте «инсайдера» правильноПопулярный совет при поиске работы — заручиться рекомендацией человека изнутри компании. Считается, что если за кандидата ручается действующий сотрудник, то это залог успеха. Спойлер: совсем не факт!Но это может сработать, если использовать «инсайдера» по-другому. У знакомого сотрудника можно узнать больше о культуре компании, рыночном контексте, продукте. Учитывая эту информацию на собеседовании и при выполнении тестового задания, вы сможете впечатлить нанимающего менеджера куда больше, чем просто рекомендацией.Отвечайте на звонкиПункт в стиле Капитана Очевидность, но вы не поверите, как много людей не попадают на собеседование просто потому, что не отвечают по указанным контактам (или делают это слишком поздно).Укажите актуальные контакты и проверяйте свой телефон (электронную почту), чтобы не пропустить приглашение на интервью. Вы не Гарри Поттер, и почтовых сов вам не отправят.Не генерите тестовое задание через AI...Тех, кто выполняет тестовое задание «для галочки», видно сразу. А тех, кто сгенерировал ответ с помощью нейросетей — особенно.Такая небрежность демонстрирует неуважение и снижает шансы на трудоустройство. Более того, кандидата могут запомнить и оставить негативный комментарий во внутренней системе рекрутмента.И наоборот, соискатели, которые потрудились больше других, тоже всегда заметны. Их выделяют как наиболее перспективных и стремящихся получить работу.Отмечу, что можно использовать искусственный интеллект при выполнении тестового задания, но как вспомогательный инструмент. Например, попросить нейросеть дополнить ваш ответ или проверить его на ошибки....и не придумывайте его самиСнова о тестовом задании. Если оно есть, нужно выполнить его именно так, как указано.Напомню, у нас оно звучало так: «Как бы вы изменили приложение Учи.ру для учителей?». Из 80 кандидатов, выполнивших его, только 20 установили приложение для учителей, изучили его и написали комментарии по существу.Остальные вносили предложения по улучшению платформы, веб-сайта, приложениядля учеников. Таких невнимательных соискателей мы не рассматривали.Наше приложение для учителейНе входите в роль Стива Джобса…При выполнении тестового задания помните, что вы устраиваетесь на стажировку. И нанимающий менеджер хочет увидеть стройные и логичные рассуждения (что вы конкретно предлагаете сделать и зачем), а не набор гениальных идей и далеко идущую (и невыполнимую) стратегию.Уделите больше внимания обоснованию и логике вашего решения, а не  попыткам придумать что-то эдакое и удивить.…и в роль Гордона РамзиЕсли в тестовом задании вам (как в нашем случае) предложат придумать варианты улучшения продукта, не критикуйте его слишком яростно. Мы не на прожарке!Тактично изложите свое мнение, укажите точки роста продукта. Разнести можно любое программное решение, но это правда не лучший вариант. И не из-за того, что в компании обидятся, а просто потому, что вы ведь пока не знакомы с внутренней кухней, целями, задачами продукта и другими его характеристиками.Это не основано на реальных событиях, это реальные события*Сделайте задание со звездочкойВ 2016 году я сама устраивалась на стажировку. Кандидатам предлагалось тестовое задание, где в числе прочего была необязательная задача — надо было найти 20 человек, выстроить их в форме звезды, встать в центре и сделать фотографию сверху.Это упражнение проверяло многие качества, необходимые менеджеру продукта. А необязательность определяла тех, кто действительно хотел на стажировку.В нашем тестовом задании такого необязательного задания не было, но кое-кто из кандидатов сделал больше, чем мы просили. Один провел UX-тестирование с учителями, а другая подробно расписала гипотезы и образ решения в текстовом документе.После собеседования именно последняя девушка получила оффер, сейчас она уже крепкий джун. А мы с радостью наблюдаем, как стажеры быстро развиваются и становятся полноправными членами команды! И гордимся тем, что наша продуктовая команда растет и укрепляется благодаря перспективным специалистам.Надеюсь, мои советы помогут вам уверенно начать карьеру продакта. Вот последний: не бойтесь пробовать свои силы! Не бойтесь пробовать свои силы и может быть в один день мы будем работать над лучшим школьным ЭдТехом вместе 💜"
СберМаркет,,,Как редизайн платежного сценария увеличил средний чек на 30%,2024-07-02T07:07:43.000Z,"Меня зовут Надя Терехова, я отвечаю за продуктовый дизайн в Учи.ру. Сегодня я расскажу, как наша команда изменила дизайн платежного сценария и тарифную линейку, почему решилась на кардинальное обновление и какой результат получила (спойлер в заголовке).Наш кейс будет полезен дизайнерам и менеджерам, которые планируют  крупные изменения в зрелом продукте, но не знают, с чего начать, боятся ошибиться и уронить текущие метрики.В статье описали наш путь и собрали полезные советы для редизайна. Приятного чтения!Когда изменения назрелиИзначально мы хотели поменять только страницу с тарифами, так как обратили внимание, что новые курсы на ней не отображаются.Страница выглядела так:По умолчанию при нажатии на кнопку «Открыть доступ» пользователям предлагался только комплект базовых предметов, хотя на платформе появилось большое количество новых образовательных игр, комиксов, дополнительных дисциплин.Кроме того, если клиент хотел приобрести только один курс, он сразу попадал на страницу оплаты. А мы хотели, чтобы в таком случае ему демонстрировались комплекты курсов с выгодным оффером по стоимости и возможностью приобрести пакет.Сначала мы вносили в дизайн точечные улучшения. Но тестирования показывали либо минусовые, либо статистически незначимые результаты. Тогда продуктовая команда замахнулись на редизайн.Первый этап: анализируем текущий платежный сценарийДля начала мы запустили дизайн-дискавери. Дизайнер в рамках спринтов работал не только над быстрыми гипотезами, но и внимательно изучал продукт и анализировал, какой опыт лучше предложить пользователю на определенном этапе CJM.Кроме того, команда собрала все текущие сценарии покупки и визуализировала их. Они отличались в зависимости от сегмента пользователя и пути на платежку.В итоге мы поняли несколько важных вещей:стоит смотреть шире на все этапы платежного флоу, чтобы сформировать целостный опыт;задача затрагивает почти все команды, нужно привлекать их как можно раньше для обмена знаниями;ранее мы не согласовывали коммуникации и визуал между e-mail-каналом и интерфейсом продукта, поэтому по-разному доносили информацию о тарифах.Затем мы изучили результаты предыдущих внутренних качественных исследований и экспериментов, аналитику по воронке, обращения в техподдержку, чтобы сформулировать первые гипотезы и варианты решений.В поисках лучшего дизайнерского решения мы также рассмотрели варианты флоу покупки у других компаний. Это позволило создать пул гипотез, которые ранее не приходили нам в голову.Кстати, мы анализировали решения не только из сферы EdTech, но и продукты с похожими задачами, например, в e-commerce. Особенно нас интересовала визуализация скидок и предложений купить в рассрочку, разные механики допродаж с прогрессивной выгодой.Забегая вперед, скажу, что потом по этим гипотезам удалось провести успешные A/B-тестирования, а добавление информации о рассрочке повысило конверсию в покупку.Второй этап: создание прототипа и исследования с пользователямиВ процессе редизайна мы проводили несколько итераций UX-исследований с пользователями, которые включали глубинные интервью и юзабилити-исследования. Мы комбинировали как классические, так иитеративные юзабилити-тесты. И дизайнеры, и продакты были максимально вовлечены в процесс общения с пользователями (половину всех UX-тестов проводил сам дизайнер).Вот как стала выглядеть страница тарифов после редизайна. Теперь пользователь может увидеть все продукты на платформе:На интервью мы выяснили основные ожидания родителей, которые выбирают комплекты курсов. Они хотели, чтобы ребенок не отставал от школьной программы и всесторонне развивался.Поэтому мы сделали два новых пакетных предложения, — «Только школа» и «Всё на Учи.ру».Страница с тарифами спроектирована таким образом, чтобы пользователю былаочевидна ценность пакетных предложений по сравнению с покупкой отдельных курсов. Визуально (иконками) показаны курсы в пакете. Ниже можно в деталях изучить информацию о каждом курсе и еще раз убедиться, что по отдельности их покупать менее выгодно, чем пакетом. При проверке на UX-исследованиях пользователи считали эту мысль.Таким образом, с помощью дизайна нам действительно удалось донести ценность пакетного предложения. При этом у пользователей осталась возможность собрать комплект курсов под свой индивидуальный запрос.Еще мы узнали, что наша аудитория больше всего интересуется школьными предметами — математикой, русским и английским языками, окружающим миром. Поэтомуобъединили развивающие курсы и игры в комплекты, соответствующие этим предметным областям. Например: «Математика и логика», «Языки и чтение», «Мир вокруг», «Навыки XXI века». Это подчеркивает, что все активности на Учи.ру направлены на успешную учебу в школе и формирование полезных навыков.Пример подборки курсов по группе навыков:Кроме того, мы изменили сценарии покупки так, чтотеперь из каждого курса пользователь попадает на единую страницу тарифов с возможностьюпокупки пакета.Третий этап: A/B-тестирования и результатыПоскольку тесты, связанные со сценариями покупки, крайне рискованные (ведь они непосредственно влияют на денежные метрики), мы подкладывали соломку везде, где только можно. Например, разбили флоу покупки на шаги и редизайнили их по отдельности, запуская изменения на прод в разное время и в рамках отдельных экспериментов. Таким образом, мы подстраховались, чтобы запустить максимально эффективный вариант осенью, в «горячий» сезон.Пакет «Всё на Учи.ру», который дает доступ ко всем занятиям на платформе, мы сперва выкатили на старой странице тарифов. Вот как это выглядело:Увы, результат оказался нестатзначимым. Успокоили себя тем, что он не минусовой, и решили, что новый дизайн позволит улучшить показатели.Самый первый шаг с выбором доступа не было смысла запускать весной, когда мы работали над задачей: он актуален для новых пользователей, основная часть которых приходит на Учи.ру осенью. Поэтому отложили эту часть на более поздний период.Разработку экрана с пейволлом, который появляется у пользователя при окончании бесплатных заданий, совместили с задачей из техдолга (вынести интерфейс из монолита). А заодно проверили еще ряд гипотез, часть из которых не дала успешных результатов.К примеру, мы думали, что лучше всего сработает «детский» пейволл с забавной картинкой и понятной для ребенка коммуникацией. Но ошиблись: этот вариант показал менее значимые результаты, чем «взрослый» пейволл, в котором мы делаем акцент на скидке и премуществах полного доступа.Пейволл до редизайна:«Детский» пейволл:«Взрослый» пейволлКлючевой узел в сценарии — страницу с тарифами — решили протестировать на весеннем трафике, чтобы не уронить метрики осенью.В весеннем запуске тест был плюсовой, достаточно быстро вышел на плато и стабильно показывал десятипроцентный прирост среднего чека.Тут важно понимать, что весной совершается намного меньше новых покупок, чем осенью, поэтому мы ждали лучшего результата в сентябре. И действительно,осенью, когда раскатали эксперимент на всех, увидели рост среднего чека на 30%. Конечно, нельзя исключать накопительный эффект и от остальных фич, которыми мы занимались в прошлом сезоне (например, от релиза новых геймификационных механик).Как видите, процесс получился долгий и итеративный, но результат стоил того.ВыводыКрупный редизайн вызывает много страхов и опасений. Тем не менее, иногда для получения качественного результата все-таки нужны масштабные, а не локальные изменения.Вместе с коллегами собрали общие рекомендации, которые могут быть полезны командам, планирующим масштабный редизайн:Работа становится проще, если выполнять ее с теми, кого изменения тоже касаются: с другими командами, руководством, исследователями, специалистами поддержки. Важно обсуждать изменения вместе на демо, собирать максимально широкую обратную связь.Не стоит на 100% верить выводам из прошлых экспериментов. В итоге перезапуска тестов, в которых мы увидели потенциал, удалось получить положительный прокрас.Важно учитывать опыт пользователя в разных каналах коммуникаций. В рамках редизайна в рассылках мы начали транслировать ожидания, которые сами пользователи озвучивали в качественных исследованиях, а также переиспользовать эффективные визуальные презентации пакетов. Думаем, это тоже повлияло на результат.При исследовании конкурентов стоит рассматривать более широкий круг компаний: не только в своей сфере (например, EdTech), но и всех, предоставляющих похожие услуги.Хорошая идея — максимально декомпозировать редизайн на эксперименты. Лучше подстраховаться и раскатать эксперимент заранее, не в высокий сезон.А еще эта масштабная задача научила нас лучше планировать крупные дискавери-проекты, которые требуют продолжительной кросс-командной работы и большого количества дизайн-итераций. В процессе ее выполнения мыусовершенствовали коммуникацию между различными направлениями компании, включая смежные продуктовые команды, службу поддержки, CRM-маркетинг. Я думаю, что проект стал успешным во многом благодаря кросс-командной работе и обширной обратной связи."
СберМаркет,,,"Как благодаря переезду хранилища данных прокачать стек, архитектуру и скиллы команды",2024-06-13T10:23:23.000Z,"Приветствую всех читателей! Меня зовут Николай Самсонов. Я являюсь руководителем платформы данных в команде Учи.ру. В своей работе часто сталкиваюсь с ситуацией, когда бизнесу нужны метрики и показатели здесь и сейчас, в то время как автоматизация получения и обработки терабайт данных для их расчета может занимать значительное количество времени.Правильный стек, правильная архитектура и правильное видение процесса ELT — залог успешной аналитики, с этим никто не спорит. Но как прийти к ним и как найти баланс между затратами времени на исследование и поддержкой уже сделанного в бесконечном потоке A/B-тестирований, дашбордов, метрик и Ad hoc-запросов?Точного ответа у меня нет, но хочу рассказать про наш опыт переездов хранилища данных, за счет которых мы смогли качнуть баланс равновесия между задачами операционными (Run) и связанными с изменениями (Change) в пользу вторых. В наше хранилище грузятся обезличенные данные пользователей, необходимые для расчета показателей и метрик. Наш опыт показывает, что переезды не «пожар», а возможность прокачать технологии и скиллы людей, которые занимаются построением DWH.Переезд начинаетсяНа протяжении долгого времени весь стек DP работал достаточно беспроблемно, но при этом SLA доезда прода был размыт и с ручным подключением дежурного дата-инженера занимал время с 13:00 до 15:00 ежедневно.Как мы поняли в дальнейшем (и это небольшой спойлер всей истории), мощности железа позволяли сгладить многие шероховатости и неоптимальность кода. На нашем объеме, (если измерять ежедневной дельтой, то это цифры 14-16 TB), все сложности были связаны исключительно с этой неоптимальностью и человеческим фактором (последствиями внедрений).Впоследствии для устранения этих неоптимальностей мы приняли решение о замене провайдера. Переезд к новому поставщику услуг дался нам непросто. Сначала сетевое окружение и виртуальные роутеры просто не были готовы к нашим объемам данных при сохранении текущего стека. Когда мы от тестовых ограниченных запусков перешли к полноценному нагрузочному тестированию, то стали постоянно ловить S3 throttling алерты. Затем нам перенастроили сеть, дали более мощную ноду под виртуальный роутер — наша Big Data наконец-то поехала.Однако появились рандомные ошибки при записи файлов в S3-бакеты. При этом, если мы ловили их на Core-таблицах нижнего слоя, то ломались все витрины верхних слоев, и восстановление согласованных данных было возможно только полной перегрузкой прода.По итогам множества встреч с представителями облачного хранилища и постоянных пересылок многогигабайтных логов мы получили достаточно честный диагноз происходящего от самого провайдера: «Нашли косвенные доказательства того, что наш сервис вернул неполный листинг в момент интенсивной записи в контейнер. Мы храним листинги в реплицированной БД и такое поведение возможно, если при создании объекта не обновилась одна из реплик. А при чтении листинга запрос оказался именно на ней. Это соответствует гарантиям Eventual Consistency, которые дает наше хранилище».Мы стали размышлять, как с этим справиться и какие действия стоит предпринять в дальнейшем.Начали с того, чтопроапгрейдили нашу систему data quality,а именно:фреймворк сравнения количества записываемых в моменте файлов Spark с количеством файлов в S3 после записи вместе со связкой ретраев (на практике вышли на достаточность трех);систему ранних алертов, которая позволяла понять, что в данный конкретный деньвсе пропалосистема ретраев по той или иной причине не дала результат (с дальнейшими действиями на выбор дежурного DE — от точечного перезапуска дагов до остановки прогрузки в CH в состоянии целостности данных в нем на Т-2 и одновременным перезапуском всего прода).Затем мы сформулировали план по глобальному изменению инженерной архитектуры. Он был связан с внедрением одного из S3-коммитеров, которые должны были снизить нагрузку на хранилище при записи, либо должен был осуществить переход от листинга файлов из папки S3 к использованию меты записанных файлов (фактическая реализация с помощью Apache Iceberg). Внедрение такого инструмента, как Iceberg, требует множества изменений, ведь меняется сам подход работы с файлами. Как следствие, требуется перерасчет всех слоев DWH. В дальнейшем в этой статье я опишу, как и с каким эффектом мы его внедрили.Для решения проблемычастичного недоездачасти данных мы пробовали применять несколько различных коммитеров:Дефолтный S3-коммитер. Он медленный и требует строгой консистентности данных. В случае с нашим провайдером он только увеличил время сохранения файлов в S3 и не решил проблему.Staging directory-коммитер. С ним запись осуществляется быстрее, чем с дефолтным, но его нельзя использовать в партицированных таблицах. Он значительно сократил время записи файлов по сравнению с дефолтным, но мы продолжали периодически ловить ошибки.Magic-коммитер. Гораздо бодрей и надежней, однако при его использовании остались проблемы с партициями (их постоянный пересчет и пуш в Hive Metastore в больших таблицах мог занимать до 10–15 минут).Итак, после использования коммитеров наша проблема неполной записи данных в хранилище, не поддерживающем строгую согласованность, уменьшилась, но полностью не исчезла. Это было связано с тем, что коммитер должен хранить информацию о том, что записывают другие воркеры в S3. И если при команде ls драйвер не видит чего-либо, значит, для него это и не существует.В итоге частичные проблемы неполной записи данных не удалось исключить полностью, поэтому приняли решение перейти к новому провайдеру. К моменту второго переезда мы уже знали «что делать», какие тесты запускать, на какие показатели и алерты смотреть, какие вопросы задавать облачному саппорту.Далее я подробней остановлюсь на всех изменениях в разрезе архитектуры и технологий, которые мы поменяли для оптимизации DWH при наших переездах.Немного об архитектуре DWH Учи.руКогда в компании еще не было платформы данных, первые аналитики установили древнюю версию AirFlow. Таким образом, вся архитектура состояла из того самого первого подхода (сделать «здесь и сейчас») и последующих итераций на эту же тему.При переезде мы оставили общий подход хранения данных с разделением его на «холодное хранение» (будь то наши parquet’ы в S3 или не часто используемые Jupyter Notebook) и «горячее хранение» в виде ClickHouse на SSD, на который смотрят BI-дашборды Tableau или техническая учетка А/Б-тестера.Сам S3-бакет данных в облаке делится на RAW (грузим как есть из источников) и STORAGE (трансформируем и рассчитываем). Также  в STORAGE выделяется слой data mart (из которого load-даги уже производят выгрузку в Clickhouse). Помимо data mart существуют продуктовые «песочницы» — следствие микросервисной архитектуры (созданы для данных, которые нет необходимости мерджить с данными других источников).Что изменилось в архитектуре DWHВо-первых, в Storage был невыраженный слой плоских данных и вырожденный слой произвольного набора агрегатов. Первому мы дали основной приоритет (сделали условным DDS) с вынесением в него многих плоских витрин из data mart, второй полностью заменили на слой основных переиспользуемых данных — CDM.Во-вторых, мы постарались по максимуму убрать таблицы транзакций из data mart. Сейчас data mart используется только для агрегатов, на которых строятся дашборды в Tableau. Ранее дашборды в основном создавались с помощью Custom SQL на самом сервере Tableau. Мы максимально ввели в продакшен экстракты Tableau и написали фреймворк, чтобы они загружались на сервер по мере готовности витрин, которые для них используются. Это значительно улучшило качество жизни финальных пользователей Tableau (NPS мы, правда, не измеряли, но «спасибо» получали).В-третьих, с помощью Grafana мы поигрались с очередями запуска слоев и распределением дагов между слоями. Итого получилось пять основных слоев DWH: raw, ushi (DDS), dict, cdm, data mart.Кроме этого, мы привнесли на прод возможность делать версионированные таблицы (Slowly Changing Dimensions).Что изменилось в стеке DWHНаши инженеры и DevOPS-инженеры значительно видоизменили и улучшили многие инстансы и инструменты стека.Заменили YARN на K8sЭта идея была бы осуществлена нами и без переезда — в силу преимуществ, которые дает «кубер» по сравнению с «еще одним менеджером ресурсов».Он позволяет:подключать дополнительные ноды из квоты провайдера при высокой нагрузке с последующим их отключением;запускать разные версии Spark и Python в одном кластере и обеспечивать беспроблемное совместное использование ресурсов;поддерживать множество фреймворков, а также гибко их настраивать.Дополнительно нужно сказать, что весь кейс с K8s явился триггером множества других доработок, о которых будет написано ниже.Обновили ClickHouse до последней версииА также отточили и оптимизировали настройки балансировщика, плюс — подняли небольшой тестовый ClickHouse рядом с основным (как расширенную песочницу для аналитиков и пробных обновлений версий самого CH).Усовершенствовали ZeppelinМы сделали две ноды в нод-группе, которая предназначена для Zeppelin (вместо одной), а также установили лимиты на отдельные интерпретаторы, чтобы они не потребляли всю доступную память ноды (ранее при этом периодически страдала работоспособность самого сервера Zeppelin). В данный момент, если даже произойдет подобная ситуация на одной из нод, сервер должен пересоздаться на второй и продолжить работу бесшовно для пользователя этого инстанса.Внедрили OpenLDAP-серверУ нас достаточно много сервисов для работы с Big Data, и в каждом есть свои отдельные скрипты и файлы конфигурации для добавления новых пользователей. Это показалось нам неудобным: хотелось найти инструмент, который собрал бы всех пользователей, а сервисы могли обращаться к нему за аутентификацией. Удобным казался Google SSO, однако не все приложения его поддерживают (или нужно было доработать его какими-то сторонними решениями). Зато LDAP-интеграция была у всех приложений из коробки. Дело было за малым: написали Ansible, протестировали и внедрили OpenLDAP-сервер.Также с момента появления LDAP (а с ним пришла на прод и возможность заведения ролей с помощью SQL вместо config-файла) сделали автоматический revoke и grant для учеток аналитиков в ClickHouse в связи с тем, что CH по дефолту не поддерживает acid (кроме экспериментальных функций). Это существенно упростило жизнь аналитиков, которые могли получать все время разные данные днем в момент, когда мы шатали отдельные таблицы на проде по тикетам других аналитиков, а также жизнь дата-инженеров, которые обычно в этом случае писали предупреждающие сообщения.Задеплоили VaultДля работы с конфиденциальными данными внутри нашего Kubernetes-кластер мы развернули Hashicorp Vault в  и используем плагин Vault-CRD для чтения секретов подами прямо из Vault DB. Также в отдельном разделе мы храним ключи, которые могут понадобиться нашим data и devops инженерам, куда есть доступ через веб-интерфейс с аутентификацией через LDAP, разграничением прав доступа к ключам и настройке хранилища секретов в зависимости от роли пользователя.Усовершенствовали JupyterLabПосле переезда в Kubernetes пользователям JupyterLab часто не хватало выделенного ресурса для хранения данных, а вечно делать ресайз тома не казалось удобным решением. Что хотят пользователи, хранящие гигабайтные архивы и не желающие в гит? Правильно — безразмерное объектное хранилище S3. Из коробки JupyterLab не позволял монтировать S3-объекты в корень рабочей директории, поэтому мы взяли драйвер S3 от Yandex, добавили StorageClass и в деплойменте описали новый PV. Теперь пользователи JupyterLab могут хранить свои ноутбуки в папке S3 безразмерно и вечно.Заменили Thrift на KyuubiДля работы с данными в S3 первоначально использовался такой инструмент, как Thrift-сервер, поднятый на отдельных виртуалках. Но у него были критические недостатки: медленная скорость работы даже на простых запросах и катастрофическая отказоустойчивость. Запустив Kyuubi, мы получили масштабируемость и возможность автовосстановления сервиса, а также улучшили перфоманс до 3–4 раз на больших запросах.Обновили AirFlowЗаменили старую версию на последнюю, что существенно отразилось как на скорости (таски стали запускаться сразу же, а не через 2–3 минуты), так и на основном изменении de-стека — появились возможности интеграции AF c «кубером». Изменение версии AF (как и его работа внутри K8s) повлекло за собой изменение в написании декораторов и подхода к деплою.Как мы внедряли IcebergApache Iceberg — это, по сути плагин к Spark, который позволяет работать с данными в особом формате Apache Iceberg Open Table Format. Его основными особенностями являются отдельное хранение меты и версионность (со своим Rollback и Time Travel). Внутри каждого каталога появляется два подкаталога — дата и метадата, где в последней лежат файлы json и avro, описывающие parquet’ы в первой.Последнее обстоятельство делает этот инструмент необходимым в случаях, когда требуются быстрые откаты к предыдущему состоянию таблиц. Это важно аналитикам, работающим в Data Lake с A/B-тестированиями на больших объемах данных. Мы же в основном используем его, чтобы убедиться, что все наши данные «доедут до цели».Одновременно хочу заметить, что коммитер, находящийся под капотом в Apache Iceberg, дал значительный прирост скорости. А его мета позволила хранить историю апдейтов и откатов. Из относительных сложностей стоит упомянуть необходимость дополнительных настроек хранения меты (забор актуальных данных из других сервисов) и периодической чистки истории (снэпшотов), это позволяет не раздувать S3 до бесконечности.При сверке таблиц, загруженных традиционным способом в старое хранилище и с помощью Iceberg, мы заметили, что некоторые наши источники достаточно сильно меняют информацию задним числом. При том, что инструмент по версионированию у нас был разработан, но не получил большой поддержки снизу у непосредственных заказчиков, каждый случай такого расхождения был разобран как отдельный кейс с вариантами решения: обновить, перенести как есть, выявить причину у разработчиков.ИтогиИтак, подвожу итоги и отвечаю на вопрос, вынесенный в заглавие статьи: почему переезд хранилища данных — не «пожар», а уникальный опыт для прокачки стека?Большинство улучшений в стеке (появление новых инструментов и оптимизация существующих настроек), произведенных нами при переезде, были ответной реакцией на новую среду. Их можно условно разделить на две большие группы:1. Раз переезжаем, то сразу берем самую новую версию инструмента.2. Попробовали как раньше, не вышло, начали искать лучший вариант.Очень сомневаюсь, что при обычном Run-процессе можно было бы бесшовно и безболезненно поменять столь многое.Переезд потребовал большего перекоса в сторону Change-задач, а также более долгосрочного и прицельного планирования, чем просто работа по спринтам. Не скажу, что мы полностью перешли на каскадную разработку: скорее, стали использовать разные методологии для разных типов задач. А еще мы неожиданно поняли, что благодаря стоящим перед нами вызовам удалось привлечь специалистов высокого уровня, которым были интересны очень нетривиальные задачи.Любой переезд DWH не может быть долгим, ведь это период двойных костов для компании, которая продолжает тратить ресурсы на старый прод, для непрерывного получения актуальной информации в старом хранилище, и на установку и перенос данных в новое.Также при одновременном заборе данных из определенных внешних реплик мы были вынуждены выбирать окна и перестраивать процесс, чтобы не привести их к состоянию даунтайма. Когда что-то делаешь второй раз, то уже становится понятно, куда нужно подстелить соломку.Важно отметить, что при переезде невозможно значительно оптимизировать архитектуру хранения данных. Да, мы сделали все для снижения SLA доезда прода, создали тестовые пространства с расширенными правами для аналитиков и внедрили SCD. Но большинство таблиц нашего хранилища пока далеки от третьей нормальной формы, о Data Vault говорить не приходится.Чтобы этого достичь, нам еще предстоит:на время отойти от переездов и изменения стека, чтобы перестраивать (согласно правильным подходам) давно существующие витрины, на которых держится большинство дашбордов;после этого постепенно переключать их со старых витрин на новые.То есть осуществить такой «внутренний переезд». Мы ждем окончания внешнего переезда, чтобы вплотную заняться этим вопросом."
СберМаркет,,,Оптимизация и автоматизация в бэкенд-разработке мобильных приложений: как ускорить разработку в четыре раза,2023-12-20T10:40:29.000Z,"Привет, Хабр! Меня зовут Александр Меркулов, я занимаюсь разработкой более 20 лет, с 2011 года пишу на Ruby. Сейчас я — backend-техлид в Учи.ру. Наша команда использует большой монолит, который написан на Rails. Также мы создаем и поддерживаем микросервисы и у нас есть нативная мобилка: целых четыре приложения по два на платформу.Сегодня расскажу как раз про разработку бэкенда мобильного приложения и про полезные лайфхаки, которые помогут вам ее ускорить, минимизировать ошибки и сделать процесс более прозрачным.Расширенный взгляд на Code OwnershipНачнем с принципа Code Ownership, который позволяет упростить разработку и четко разделить зоны ответственности между командами разработчиков. Важно подчеркнуть, что Code Owners — это именно команды, а не отдельные разработчики (поскольку сотрудники могут переходить из проекта в проект, а структура команд остается относительно постоянной).Схема 1. Code OwnershipВ условиях крупных проектов со множеством команд ручная проверка файлов на соответствие определенным владельцам кода становится неэффективной. Именно поэтому мы реализовали автоматическую систему, которая при каждом новом pull request анализирует, какие файлы были изменены, и сверяет это с базой данных Code Owners. Если изменения не соответствуют файлу Code Owners, разработчик должен внести соответствующие коррективы перед тем, как его изменения будут влиты в основную ветку кода.Как это организовано у нас:owner:
  runs-on: ubuntu-latest
  steps:
    - uses: actions/checkout@v2
    - uses: uchiru/codeowners-validator@v0.8.8
      with:
        checks: ""files,duppatterns,syntax""
        experimental_checks: ""notowned,avoid-shadowing""
        owner_checker_allow_unowned_patterns: true
        owner_checker_owners_must_be_teams: true
        not_owned_checker_skip_path_patterns: ""db/migrate""Система Code Ownership также позволяет нам реализовать тактику Bug Ownership, когда за каждым обнаруженным багом автоматически закрепляется ответственная команда. Это способствует более быстрому и эффективному решению проблем.Схема 2. PR ReviewКроме того, автоматически назначается ответственный за ревью PR, что значительно ускоряет процесс проверки кода и его интеграции в основную кодовую базу. В этом контексте Code Ownership становится инструментом для разделения кода на логические домены — они управляются соответствующими командами.Эти методики позволяют не только ускорить процессы разработки и интеграции нового кода, но и сделать их более прозрачными и управляемыми на всех уровнях.Аутентификация: четыре аспекта интеграцииПервоначально мы столкнулись с проблемой интеграции различных сервисов между веб-версией и мобильным приложением. Это произошло потому, что мобильное приложение использовало JWT для аутентификации, а веб-приложение полагалось на cookie.Сложности были преодолены с помощью аутентификационного Middleware. Программный слой, реализованный на Roda/Sequel, принимает данные аутентификации от мобильного приложения и генерирует соответствующие заголовки (headers) для веб-сервисов. Это позволяет интегрировать в сервисы подходящую для мобильного приложения систему аутентификации без необходимости переписывания большого количества существующего кода.Схема 3. Аутентификация с MiddlewareОдин из основных плюсов использования Middleware в данном контексте — его гибкость. В будущем можно легко добавить поддержку других методов аутентификации. Это особенно полезно при дальнейшем расширении функционала или при интеграции новых сервисов.Стратегия обновления в мобильных приложенияхОбновления мобильных приложений — это не всегда простой процесс выкладывания новой версии в магазин приложений. Реальность гораздо сложнее: истекающие сертификаты, смена платежных провайдеров, критические баги. Все эти факторы требуют быстрого и централизованного управления обновлениями, чтобы мотивировать пользователей переходить на новую версию.Существует несколько подходов к решению этой проблемы.Первый — Hard Update или Force Update. В этом случае при старте приложения система на бэкенде проверяет версию приложения у пользователя. Если она не соответствует последней стабильной версии, сервер отправляет команду на принудительное обновление, и у пользователя на экране возникает соответствующее сообщение. Соответственно, либо пользователь обновит приложение, либо не сможет его открыть.Схема 5. Hard UpdateАльтернативный вариант — это Soft Update. В этом случае у юзера появляется всего лишь рекомендация обновиться, но пользователь может просто закрыть это сообщение и полноценно пользоваться приложением.Этот метод менее навязчив, но и менее эффективен. В большинстве случаев он остается неиспользованным, так как существуют более гибкие инструменты для управления версионностью: например, Feature Flags и А/B-тесты.Схема 6. Soft UpdateЧтобы координировать различные типы обновлений, мы используем административную панель. В ней можно выбирать конкретное приложение, его платформу и версию, с которой будет начинаться либо Soft Update, либо Hard Update. Это делает управление процессом более гибким, позволяет быстро реагировать на различные ситуации, которые могут возникнуть в жизненном цикле мобильного приложения.Работа с Push-уведомлениями в мобильных приложенияхГрафик 1. ПушиРабота с push-уведомлениями в мобильных приложениях может быть непредсказуемой и сложной задачей. Скорость их появления зачастую зависит от плана маркетологов или менеджеров по продукту, а разработчикам приходится с этим планом считаться.Для агрегации и отправки push-уведомлений мы применяем event-driven-архитектуру. То есть, события агрегируются на специализированном сервисе и отправляются пачками на другой сервис, который занимается их фактической отправкой. Это позволяет нам сгладить пики и поддерживать стабильность работы.Ключевой момент в организации отправки — асинхронность. Синхронная отправка может привести к высокому проценту ошибок, например, из-за устаревших push-токенов или недоступности провайдера. Асинхронная отправка снижает подобные риски и делает систему более устойчивой к сбоям.Мы разделяем уведомления на категории в зависимости от их критичности. Срочные push-уведомления высылаются как можно быстрее, а остальные, менее важные уведомления могут быть отправлены позже. Это позволяет нам оптимизировать процесс и учитывать различные бизнес-потребности.Схема 7. Сегрегация пушейВажным аспектом является управление push-токенами. Пользовательские токены могут устаревать (и нужно такой токен исключать) и заменяться на новые токены (их важно сохранять). Этот процесс у нас также автоматизирован и интегрирован в общую систему управления push-уведомлениями, что значительно снижает риски сбоев и улучшает общую производительность мобильного приложения.Контрактная разработка в мобильных приложенияхСхема 8. Контрактный процесс разработкиВ мобильной разработке мы активно применяем контракты. Это означает, что перед началом работы над новым функционалом (или над изменением существующего) команды фронтенда и бэкенда согласуютконтракт. Это позволяет вести параллельную разработку, ускоряя процесс и уменьшая риски несоответствия между клиентом и сервером.Схема 9. Параллельный процесс разработки по контрактуКонтракт в нашей практике — это не просто документ, описывающий взаимодействие между фронтендом и бэкендом. Он является ключевым инструментом для поддержания обратной совместимости. При любых изменениях в контракте есть возможность вернуться и перепроверить, как эти изменения отразятся на уже существующем функционале. Это снижает риск «поломки» мобильного приложения при изменениях, особенно учитывая тот факт, что веб-разработка обычно движется быстрее мобильной.Схема 10. Обратная совместимостьТакже контрактная разработка позволяет автоматизировать ряд процессов. Например, она делает возможной автоматическую проверку тестов на бэкенде при создании PR или merge в master.Схема 11. Автоматическая проверка контрактовАвтогенерация клиента также значительно упрощает разработку: позволяет ускорить ее и уменьшить вероятность ошибок.Схема 12. Автоматическая генерация клиента при помощи контрактовВ итоге контракты становятся не просто методологией, но и инструментом для более эффективного и безопасного процесса разработки мобильных приложений.Оптимизация процесса тестирования: интеграция и автоматизацияТестирование в мобильной разработке у нас проходит по принципу доменного flow. Это значит, что весь процесс тестирования разбит на домены или функциональные блоки — например, «олимпиады» или «карточки». При внесении изменений в определенный домен QA-инженер фокусируется именно на этом блоке, что не только ускоряет процесс, но и повышает его эффективность, так как позволяет более глубоко проникнуть в детали конкретной функциональности.Автотесты играют ключевую роль в оптимизации тестирования. Они позволяют минимизировать время, затрачиваемое QA-инженером на ручное тестирование, и выявлять проблемы на ранних этапах разработки. Если автотест «падает», инженеру по тестированию и разработчику сразу становится ясно, где требуется исправление.Схема 13. Запуск автотестовОдним из наиболее перспективных направлений в оптимизации тестирования является использование контрактов. Контракты позволяют не переписывать с нуля всю логику автотестов при изменении функциональности. Их можно интегрировать в существующие фреймворки для автоматической проверки различных механик. Это особенно удобно, когда в команде работают T-shaped-специалисты, которые могут быть как разработчиками, так и QA-инженерами.Вышеописанные методы и инструменты создают сильный фундамент для надежного и эффективного процесса тестирования в условиях постоянно меняющегося процесса разработки мобильного приложения.Успехи и прогресс: реальные цифрыОдним из ключевых успехов, которым я хотел бы поделиться, является сокращение среднего времени выкатки новых фич для мобильного приложения. Ранее этот процесс занимал около 60 дней с момента начала разработки до полного релиза. Однако благодаря оптимизациям на всех этапах — от интеграции веб-функционала в мобильные приложения до улучшения механизмов push-уведомлений и тестирования — сейчас это время сократилось до 16 дней. Это улучшение почти в четыре раза демонстрирует эффективность применяемых нами подходов.Схема 14. Автоматический запуск автотестовВ планах на будущее у нас — создание дополнительных автотестов. Полагаем, что это поможет еще быстрее проходить этапы тестирования и улучшит качество покрытия нашего кода.В заключение хочу подчеркнуть, что успех в разработке мобильного приложения возможен только при комплексном подходе, который включает в себя эффективное взаимодействие между фронтендом и бэкендом, а также интеграцию между веб-версией и мобильными платформами. Ведь они, по сути, являются частями одного большого продукта.Следуя этой стратегии и оптимизируя каждый этап разработки, можно значительно сократить время выкатки новых фич и повысить эффективность работы над мобильным продуктом.Наша команда разработки растет. Если хочешь развивать школьный EdTech вместе с нами,присоединяйся!"
СберМаркет,,,"Как написать проект по автоматизации на Python, Pytest и Playwright и настроить запуск автотестов в CI с нуля",2023-12-14T10:54:37.000Z,"Привет, Хабр! Меня зовут Александр Бехтерев, я работаю QA-инженером в Учи.ру. Не так давно перед нами встала задача перехода с библиотеки Selenium на новую и мощную библиотеку Playwright. И тогда я решил взять наш проект на Selenium и переписать его с использованием Playwright.В этой статье я постараюсь подробно рассмотреть процесс создания проекта на Python, Pytest и Playwright с применением паттерна Page Object, а также оставлю шаблон yml, который позволит запускать автотесты в CIВажно: данная инструкция написана под MacOS, и она предусматривает, что у вас уже установлена IDE и Python. В этой статье я использую PyCharm, Python 3.11 и встроенный терминал в MacOS.Возможности  PlaywrightPlaywright — это не просто инструмент, а мощное средство автоматизации тестирования веб-приложений, дающее широкий набор возможностей для взаимодействия с такими браузерами, как Chrome, Firefox и WebKit.Playwright — относительно новый инструмент для автоматизации тестирования, но при этом она обладает несколькими преимуществами по сравнению с Selenium, такими как:Высокая производительность.Возможность перехватывать сетевые запросы.Гибкость: библиотека поддерживает большое количество языков.Удобство использования: в ней предусмотрены встроенные ожидания до выполнения целевых действий с веб-элементами, а встроенный класс expect упрощает работу с ассертами.Снизу приведена пошаговая инструкция.1. Создаем и клонируем пустой репозиторий с Git. (Этот шаг можно пропустить, если хотите хранить проект только локально).Войдите в свой аккаунт на Git (если у вас его нет, нужно будет зарегистрироваться). После успешной авторизации нажмите на свою аватарку в правом верхнем углу и выберите разделYour repositoriesв выпадающем меню.Далее нажмите кнопкуNewна странице ваших репозиториев. На открывшейся странице укажите название репозитория (например, pw_autotests) и его видимость (public или private) в зависимости от ваших предпочтений.Далее нажмите кнопкуCreate repository. После создания репозитория вы увидите страницу с инструкциями по его использованию. Важно, что здесь размещается URL нашего репозитория.(Расположение URL репозитория)Чтобы клонировать ваш пустой репозиторий на локальную машину:откройте терминал (или командную строку);перейдите при помощи командыcdв тот раздел, куда вы хотите, чтобы сохранился ваш репозиторий (например,cd Desktop);выполните командуgit clone, приложив ссылкуhttps://github.com/your-username/repository-name.gitДалее, если система попросит ввести логин и пароль от Git, то введите их — и репозиторий будет скопирован на ваш компьютер.2. Создаем проект в PyCharm.Если вы выполнили пункт 1, откройте PyCharm и нажмите на кнопкуOpenи откройте репозиторий, который вы клонировали с Git, если нет, тогда  нажмите на кнопкуNew Project.(Создание нового проекта)В полеNew environment usingвыберитеVirtualenv. ПолеLocationзаполнится автоматически — главное, чтобы путь был указан такой же, как и в полеLocationвыше, только с добавлением/venv.Далее вBase interpreterвыберите установленный на компьютереPython 3.11. Снимите галочку сCreate a main.py welcome script, он не понадобится, и нажмите на кнопкуCreate.Вы увидите, что открылся новый пустой проект.3. Устанавливаем зависимости и Playwright в нашем новом проекте.Для удобства управления зависимостями и их версиями нужно создать файлrequirements.txtв корневой папке нашего проекта. Для этого кликните правой кнопкой мыши по нашей корневой папке (в левом верхнем углу). Откроется меню, в котором нужно выбратьNew→File, ввести название файла: requirements.txt — и нажать Enter.Далее добавьте в файлrequirements.txtсписок необходимых библиотек и их версий:pytest
playwright
requests
qase-pytest==4.2.0
python-dotenvТеперь откройте терминал в PyCharm и установите наши зависимости командой:python3 -m pip install -r requirements.txt.Если по какой-то причине не сработало, попробуйте командуpip3 install -r requirements.txt.Теперь необходимо установить драйверы браузеров, Playwright требует их для взаимодействия с браузерами. Для установки драйверов вводим в терминале следующую команду:playwright install.Теперь наш проект настроен для использования Playwright.4. Создаем тело проекта.Добавьте в корневую директорию несколько файлов:conftest.py — позволяет управлять фикстурами.README.md — позволяет добавить описание нашего проекта..gitignore — в нем указываются файлы и каталоги, которые Git будет игнорировать при пуше в master. Это поможет не засорять наш проект временными и ненужными файлами.pytest.ini — конфигурационный файл. Я использую его для определения пользовательских маркеров и интеграции с плагинами и расширениями. (При помощи маркеров удобно разделять автотесты, например, на smoke и регрессионные — в зависимости от ситуации прогоняться будут нужные нам тесты).Содержимое для файла.gitignoreя уже заранее подготовил: вам нужно только перейти поссылке, скопировать его и перенести в свой проект в файл с этим названием.Теперь заполним файлpytest.ini:[pytest]
markers =
    regression: run regression tests
addopts = -v -sЧтобы было удобнее работать над проектом, создайте несколько папок в корневой директории:Locators — тут будут находиться локаторы для разных страниц.data — тут будут храниться окружение, константы, ассерты.fixtures — тут будут располагаться модули с фикстурами Pytest.pages — здесь будут храниться файлы с методами для взаимодействия с элементами веб-страницы.tests — здесь будут находиться автотесты.Теперь заполните файлconftest.py:pytest_plugins = [
    'fixtures.page'
]Тут нужно указать путь, откуда должны подгружаться модули с фикстурами (для удобства модули с фикстурами лучше хранить в отдельной папке).5. Добавляем модуль page.Для начала в папке fixtures создайте файлpage.py.У меня уже заготовлен универсальный модуль page, который позволяет запускать автотесты в браузерах Chrome, Firefox и Chrome на удаленном сервере. Вам нужно только перейти по ссылке врепозиторийи скопировать содержимое модуляpage.pyк себе в проект.6. Создаем модуль с настройками окружения.В этом разделе покажу, как можно упростить работу с окружениями. Для примера я взял сайт-песочницуhttps://saucedemo.com, поскольку там есть авторизация — а это хорошая возможность создать отдельный модуль с фикстурой авторизации, которую мы будем вызывать перед запуском основных тестовых функций.Но это мы рассмотрим дальше, а сейчас вернемся к настройке окружения. Для начала создайте в папке data файл с названиемenvironment.py.И добавьте в него следующий код:import os

class Environment:
    SHOT = 'shot'
    PROD = 'prod'

    URLS = {
        SHOT: 'https://example.ru/',
        PROD: 'https://saucedemo.com/'
    }

    def __init__(self):
        try:
            self.env = os.getenv('ENV')
        except KeyError:
            self.env = self.PROD

    def get_base_url(self):
        if self.env in self.URLS:
            return self.URLS[self.env]
        else:
            raise Exception(f""Unknown value of ENV variable {self.env}"")

host = Environment()Класс Environment предназначен для управления окружением и определения базового URL в зависимости от окружения. Таким образом, если вам нужно будет прогнать ваши автотесты на тестовом или продовом окружении, просто подставьте свои urls в словаре URLS и укажите окружение ENV=prod или ENV=shot через терминал в PyCharm командой export ENV=prod (для MacOS).Но я советую получать все окружения из файла.envпутем использования библиотекиpython-dotenv. Вы ее, кстати, установили в самом начале! :-)Осталось сделать только следующие действия:Создайте в корневой директории файл с названием.env.Напишите в этом в файле ENV=prod (если локально нужно будет запустить на шоте, то вместо prod напишите shot).Теперь, чтобы окружения подгружались при запуске автотестов, внесите небольшие изменения в файлconftest.py. Откройте его и добавьте туда код:from dotenv import load_dotenv


load_dotenv()Это нужно, чтобы переменные окружения (такие как логины, пароли, api tokens) не хранились в коде, а подгружались из файла.env. Это актуально только для локальных прогонов; для запуска через CI переменные будут храниться и подтягиваться из git-secrets.7. Создаем родительский класс base для взаимодействия с элементами на страницах веб-приложений.Сейчас покажу, как создать и заполнить базовый родительский модуль base. В нем хранятся основные методы, при помощи которых происходит взаимодействие с элементами на страницах веб-сайтов.Для начала в папке pages создайте файлbase.py.Теперь заполните его кодом.Для начала импортируйте классы Page, TimeoutError, Response из модуляplaywright.sync_api; класс host из ранее созданного модуля environment.from playwright.sync_api import Page, TimeoutError, Response
from data.environment import hostСоздайте сам класс Base и добавьте конструктор класса, у которого будет аргумент, который принимает на вход класс Page из модуляplaywright.sync_api. Потом создайте внутри конструктора класса экземпляр класса Page, чтобы он был доступен во всех методах класса Base, что позволит нам выполнять действия на веб-страницах.class Base:
    def __init__(self, page: Page):
        self.page = pageДалее нужно добавить основные методы, которые пригодятся для написания простого e2e-автотеста для сайта-песочницы. Вообще, я написал достаточно много разных методов, которые пригодятся вам в будущем. Вы можете забрать их из моегорепозиторияна Git и добавить в файлbase.py.Разберу основные из них.Метод openосуществляет переход на веб-страницу:def open(self, uri) -> Response | None:
    return self.page.goto(f""{host.get_base_url()}{uri}"", 
    wait_until='domcontentloaded')Метод inputосуществляет ввод значений в поле ввода:def input(self, locator, data: str) -> None: 
    self.page.locator(locator).fill(data)Метод clickосуществляет клик по элементу:def click(self, locator) -> None: #
    self.page.click(locator)Метод get_textдостает текст из элемента по локатору:def get_text(self, element) -> str: #
        return self.page.locator(element).text_content()Метод click_element_by_indexпозволяет кликнуть на нужный элемент, указав его индекс (если на странице есть несколько элементов с одинаковым локатором):def click_element_by_index(self, selector: str, index: int):
    self.page.locator(selector).nth(index).click()Метод input_value_by_indexпозволяет кликнуть на нужное поле с инпутом, указав его индекс:def input_value_by_index(self, selector: str, index: int, data: str): #вводим данные в нужные поля по индексу
    self.page.locator(selector).nth(index).fill(data)Итак, вы создали класс Base и добавили в него основные методы, при помощи которых будете взаимодействовать с элементами на веб-страницах.Класс Base будет родительским классом, который будет наследоваться другими классами: иначе говоря, для каждой страницы веб-сайта создается отдельный модуль и класс в нем, которые наследуют родительский класс Base.8. Добавляем модуль с основными ассертами.В этом разделе расскажу, как добавить отдельный модуль, в котором хранятся методы с основными ассертами. Для начала в папке data создайте файлassertions.py.Теперь этот модуль нужно заполнить кодом. Для начала импортируйте все необходимые зависимости:from playwright.sync_api import Page
from data.environment import host
from playwright.sync_api import expect
from pages.base import BaseСтоит обратить внимание на импорт функции expect. Она предоставляет обширный набор встроенных ассертов и будет использоваться для проверки различных условий в ходе выполнения автотестов.Далее создайте класс Assertions, который будет наследовать родительский класс Base. Для корректной инициализации родительского класса и его атрибутов и методов добавьте конструктор класса и вызовите в нем встроенную функциюPython super(), через нее — конструктор родительского класса Base, затем передайте ему аргумент page.class Assertions(Base):
    def __init__(self, page: Page) -> None:
        super().__init__(page)Теперь давайте добавим методы с основными ассертами: их я тоже заранее уже подготовил врепозитории. Вы можете скопировать их и добавить к себе в проект.Те, которые пригодятся для пробного автотеста, я опишу ниже:check_urlпроверяет, что находится на нужной нам странице:def check_URL(self, uri, msg):
        expect(self.page).to_have_url(f""{host.get_base_url()}{uri}"", timeout=10000), msgcheck_presenceпроверяет, что элемент присутствует на странице (если есть — то assert True).def check_presence(self, locator, msg):
        loc = self.page.locator(locator)
        expect(loc).to_be_visible(visible=True, timeout=12000), msgcheck_absenceпроверяет, что элемент отсутствует на странице (если нет — то assert True).def check_absence(self, locator, msg):
        loc = self.page.locator(locator)
        expect(loc).to_be_hidden(timeout=700), msghave_textпроверяет, что у элемента нужный текст.def have_text(self, locator, text: str, msg):
        loc = self.page.locator(locator)
        expect(loc).to_have_text(text), msg9. Пишем первый автотест на авторизацию.В этом разделе я покажу, как написать автотест для авторизации на сайте-песочницеhttps://www.saucedemo.com. (А в следующем разделе мы сделаем метод авторизации фикстурой, чтобы в дальнейшем использовать ее перед запуском других автотестов).Для начала создайте следующие модули:файл auth.py в папке Locators;файл constants.py в папке data;файл main_page.py в папке pages;файл test_auth.py в папке Tests.Нужно добавить префикс test_ в название файла, чтобы Pytest автоматически обнаруживал и выполнял функции в модуле как тестовые сценарии. Это связано с соглашениями и стандартами, которые использует Pytest для автоматического обнаружения тестов.Теперь нужно в свежесозданном файлеtest_auth.pyсоздать класс Auth и подобрать локаторы для формы авторизации. Откройте сайт-песочницуhttps://saucedemo.com, наведите мышку на поле инпутаusernameи нажмите правую кнопку мыши. В появившемся окне выберитеПросмотреть код.У вас откроется панель разработчика на вкладке Elements с выделенной частью html-разметки для инпута Username.Теперь нажмитеcommand + F, чтобы открыть строку поиска по html-разметке страницы.В этой статье в основном я буду завязываться на Data attributes, так как разработчики сайта их нам оставили. Вообще, Data attributes является самым лучшим вариантом для того, чтобы на них завязать автотесты: с ними легче добиться стабильности тестов, так как маловероятно, что они могут измениться.Вообще, по возможности, я стараюсь следовать следующему приоритету локаторов:На первом месте — Data attributes.На втором — CSS-селекторы.На третьем — XPath (использую при более сложных сценариях поиска, как один из примеров — если хочу привязаться к какому-то элементу по тексту).После того, как вы нажали наcommand + F, у вас открылась строка поиска по html-разметке страницы. Введите в строке поискаdata-test='username'— это и есть Data attribute. Нажмите Enter — и вы увидите, что элемент выделился.Скопируйте его, затем создайте в своем файлеtest_auth.pyи в созданном классе Auth новый атрибут USERNAME_INPUT = ""[data-test='username']"". Аналогично добавьте PASSWORD_INPUT и LOGIN_BTN.Получиться должно так.class Auth:
    USERNAME_INPUT = ""[data-test = 'username']""
    PASSWORD_INPUT = ""[data-test='password']""
    LOGIN_BTN = ""[data-test='login-button']""Далее в файлconstants.pyдобавьте код:import os

class Constants:
    try:
        login = os.getenv('AUTH_LOGIN')
        password = os.getenv('AUTH_PASSWORD')
    except KeyError:
        print(""LOGIN OR PW WASN'T FOUND"")Это нужно, чтобы не хранить авторизационные креды в коде. Логин и пароль при запуске локально будет подтягиваться из файла.env, а при удаленном запуске — из папок secrets.Значит, нужно добавить логин и пароль в файл.env:AUTH_LOGIN = standard_user
AUTH_PASSWORD = secret_sauceТеперь можно приступать к заполнению модуляmain_page.py. Я расскажу, как написать метод, который будет производить авторизацию через форму авторизации.Добавим следующий код:from pages.base import Base
from data.constants import Constants
from Locators.auth import Auth
from data.assertions import Assertions
from playwright.sync_api import Page

class Main(Base):
    def __init__(self, page: Page) -> None:
        super().__init__(page)
        self.assertion = Assertions(page)

    def user_login(self):
        self.open("""")
        self.input(Auth.USERNAME_INPUT, Constants.login)
        self.input(Auth.PASSWORD_INPUT, Constants.password)
        self.click(Auth.LOGIN_BTN)
        self.assertion.check_URL(""inventory.html"", ""Wrong URL"")В методеself.open("""")передача """" означает, что мы не передаем никакой дополнительный uri к базовому url. Другими словами, мы просто открываем базовый url —https://www.saucedemo.com.В конце я также добавил проверку на то, что авторизация прошла успешно. Это именно пример, проверку можно сделать какую хотите.Теперь, чтобы автотест запустился, нам нужно прописать его в модулеtest_auth.py. Добавим следующий код:import pytest
from pages.main_page import Main


@pytest.mark.smoke
class TestLogin:
    def test_user_login(self, browser):
        m = Main(browser)
        m.user_login()Этот тест помечен маркойsmokeдля того, чтобы разбить автотесты на разные блоки и, в зависимости от ситуации, прогонять нужные.Также был создан экземпляр класса с главной фикстурой browser, а дальше вызван методuser_login()из модуляmain_page.py.Осталось только запустить автотест. Но для начала в терминале PyCharm нужно написать командуcd Tests, чтобы перейти в папку с тестовым модулем.Теперь в терминале PyCharm напишите команду для запуска автотестаpytest -s test_auth.pyи нажмите Enter. Флаг-sнам тут нужен для вывода дополнительной информации о выполнении тестов. Должен появиться статус PASSED, это означает, что тест прошел успешно.10. Пишем второй автотест.В этом разделе расскажу, как написать простенький e2e-тест, который включает в себя авторизацию, добавление в корзину товара и совершение покупки.Для удобства предлагаю сначала сделать из теста авторизации фикстуру, чтобы было легче вызывать ее перед запуском других автотестов.Для этого в папке fixtures создайте файл с названием user_auth.pyи добавьте туда следующий код:import pytest
from pages.main_page import Main


@pytest.fixture(scope='class')
def user_login(browser):
    m = Main(browser)
    m.user_login()Чтобы Pytest подгружал данную фикстуру, перейдите в файл conftest и добавьте путь до нее:pytest_plugins = [
    'fixtures.page',
    'fixtures.user_auth'
]Теперь можно приступать к написанию самого автотеста. В этот раз для главной страницы я покажу, как использовать CSS-селекторы, и что делать, если есть несколько элементов с одним локатором.Для начала дополните проект несколькими модулями:В папке Locators создайте два файла: первый — с названиемmarket_page.py, второй — с названиемbasket_page.py.В папке pages создайте файл с названиемmarket_main_page.py.В папке Tests создайте файл с названиемtest_buy_product.py.Теперь вернитесь на сайт-песочницуhttps://www.saucedemo.comи авторизуйтесь. Используйте логин standard_user, пароль secret_sauce.На открывшейся странице вы увидите карточки товаров. Вам нужно навести мышкой на кнопкуAdd to cart, нажать правую кнопку мыши и выбратьПосмотреть код.Далее нажмитеcommand + F, чтобы вызвать поисковую строку по html-разметке и введите в нееbutton.btn. Вы увидите, что элементов с таким локатором целых шесть. Значит, нужно обращаться к нужному элементу по индексу.Теперь перейдите в файлmarket_page.py, который находится в папке Locators, и добавьте туда код:class Market:
  ADD_TO_CART = ""button.btn""
  FOLLOW_TO_BASKET = ""[id='shopping_cart_container']""Далее добавьте в корзину любой товар. Затем в правом верхнем углу нажмите на изображение тележки, чтобы просмотреть флоу с нажатием на кнопку checkout, заполнением формы и оформлением покупки.Я уже подготовил все нужные локаторы, вы можете просто добавить их в файлbasket_page.pyв папке Locators:class Basket:
    CHECKOUT_BTN = ""[data-test='checkout']""
    FIRST_NAME = ""[data-test='firstName']""
    LAST_NAME = ""[data-test='lastName']""
    ZIP = ""[data-test='postalCode']""
    CNT_BTN = ""[data-test='continue']""
    FINISH_BTN = ""[data-test='finish']""
    FINAL_TEXT = ""//span[text()='Checkout: Complete!']""Далее добавьте методы в файлmarket_main_page.pyв папке pages для взаимодействия с элементами:from pages.base import Base
from Locators.basket_page import Basket
from Locators.market_page import Market
from data.assertions import Assertions
from playwright.sync_api import Page


class MarketPage(Base):
    def init(self, page: Page):
        super().init(page)
        self.assertions = Assertions(page)
    def add_to_cart(self): 
        self.click_element_by_index(Market.ADD_TO_CART, 0)
        self.click(Market.FOLLOW_TO_BASKET)

    def checkout(self): 
        self.click(Basket.CHECKOUT_BTN)
        self.input(Basket.FIRST_NAME, ""Ivan"")
        self.input(Basket.LAST_NAME, ""Ivanov"")
        self.input(Basket.ZIP, ""123456"")
        self.click(Basket.CNT_BTN)
        self.click(Basket.FINISH_BTN)
        self.assertions.have_text(Basket.FINAL_TEXT, ""Checkout: Complete!"", ""no"")</code></pre><p>Далее необходимо заполнить файлtest_buy_product.pyв папке Tests следующим кодом:@pytest.mark.regression
@pytest.mark.usefixtures('user_login')
class TestBuyProduct:
    def test_buy_product(self, browser):
        p = MarketPage(browser)
        p.add_to_cart()
        p.checkout()Как вы видите, в коде использована фикстура авторизации посредством команды@pytest.mark.usefixtures('user_login')— она будет выполняться до нашей основной тестовой функции.Предлагаю в этом убедиться, запустив автотест. Введите в терминал PyCharm командуpytest -s test_buy_product.pyи нажмите Enter.Оцените успешное завершение автотестов. Если что-то не получается, сравните смоим итоговым репозиторием.11. Настраиваем CI.Предусловие: Перед прочтением данного раздела запуште итоговый проект к себе на Git.В этом разделе мы добавим авторизационные креды (логин, пароль) в git-secrets и настроим CI, который будет запускать автотест на удаленном сервере.Первое, что для этого нужно сделать — добавить логин и пароль в git-secrets. Для этого перейдите на страницу своего проекта на Git и нажмите на кнопку Settings.На открывшейся странице слева, в разделе Security, нажмите наSecrets and variablesи в выпадашке выберитеActions.Затем на открывшейся странице нажмите на зеленую кнопкуNew repository secret.Заполните поле Name: AUTH_LOGIN, — поле Secret: standart_user, — затем нажмите кнопкуAdd secret.Аналогично создайте второйNew repository secret. Заполните поле Name: AUTH_PASSWORD, — поле Secret: secret_sauce, — затем нажмите кнопкуAdd secret.Теперь у вас есть две переменные в хранилище secrets, в которых хранятся логин и пароль от тестовой учетки.Далее для подключения CI вам нужно зайти в свой репозиторий и кликнуть по кнопкеActions.На открывшейся странице кликнитеConfigureв разделеPublish Python Package.На новой странице удалите содержимое предложенного workflow и поменяйте название у yml-файла наregression.yml.Затем скопируйте и перенесите настройки CI, приведенные ниже, в yml-файл, и нажмите кнопкуCommit changes.name: TestRegression

on:
  #push: {}
#  schedule:
#    - cron: '0 22 * * *'

  workflow_dispatch:
    inputs:
      environment:
        required: true
        default: ""prod""
        description: ""Set environment, e.g. `ENV=prod or ENV=shot`""
      arguments:
        required: false
        default: ""-m regression""
        description: ""pytest additional arguments, e.g. `-m regression`""


jobs:
  playwright:
    name: Regression tests
    runs-on: ubuntu-latest
    container:
      image: mcr.microsoft.com/playwright/python:v1.41.0-jammy
    env:
      AUTH_LOGIN: ${{ secrets.AUTH_LOGIN }}
      AUTH_PASSWORD: ${{ secrets.AUTH_PASSWORD }}
    strategy:
      matrix:
        browser: [remote_chrome]
    outputs:
      env_name: ${{ steps.pytest.outputs.env_name }}
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: python3 -m pip install -r requirements.txt
      - run: |
          if [ ""${{github.event_name}}"" = ""schedule"" ]; then
            export args=""-m regression""
            export ENV=""prod""
          else
            export ENV='${{github.event.inputs.environment}}'
            export args='${{github.event.inputs.arguments}}'
          fi
            echo ""::set-output name=env_name::${ENV:-prod}""
            pytest ${args} --bn ${{ matrix.browser }}Разделonопределяет события, которые будут запускать workflow.Если вы хотите, чтобы ваши тесты запускались в определенное время, то раскомментируйте раздел schedule и подберите нужное время через сервисcronitor(и раздел push тоже, если хотите, чтобы тесты запускались при пуше изменений в ваш репозиторий на Git).workflow_dispatchпозволит нам запускать workflow вручную. Разделjobsопределяет, какие шаги должны быть выполнены в рамках workflow, чтобы запустить автотесты с меткойregression. Если вы захотите запустить тесты с другой меткой, просто замените меткуregressionв аргументах запуска на нужную вам.Для продолжения работы снова кликните по кнопкеActions, и на открывшейся странице кликните по созданному нами workflow.Затем кликните по кнопкеRun workflow.После этого workflow должен запуститься.Можно зайти в него и кликнуть по Job’e, чтобы следить за всеми этапами workflow.Должно появится уведомление об успешном завершении автотеста.ВыводыВ этой статье мы рассмотрели процесс создания проекта по автоматизации с использованием Python, Pytest и Playwright. Мы начали с установки необходимых зависимостей, создания базовой структуры проекта, настройки окружения. Затем мы шаг за шагом создали автотесты, используя паттерн Page Object — это сделало наш код более организованным и обеспечило его легкость поддержки.Однако главной ценностью этой статьи является возможность интеграции автотестов в вашу CI-практику. Мы подробно рассмотрели настройку CI для автоматического запуска тестов, используя разные триггеры в зависимости от ситуации.Мы также выявили преимущества использования Playwright перед Selenium. Гибкость, высокая производительность и удобство использования делают Playwright отличным выбором для автоматизации тестирования.Надеюсь, что эта статья была вам полезна, спасибо за внимание! В следующей статье я разберу интеграцию с tms qase.io и расскажу, как добавить ее в CI workflow.Сейчас в нашу команду мы ищем двух инженеров по тестированию. Если хочешь развивать школьный EdTech вместе с нами,присоединяйся!"
СберМаркет,,,Как контракты помогают QA-специалисту не «сгореть» от багов в приложении,2023-12-07T13:54:40.000Z,"Привет, Хабр! Меня зовут Ирина Иванова, я работаю QA-инженером в команде разработки внутренних проектов в Учи.ру. Мы занимаемся такими сервисами, как виджет поддержки, личный кабинет сотрудников пользовательской поддержки, карьерный сайт и так далее.Все проекты разные, у них разные задачи. И трудности, с которыми приходилось сталкиваться во время работы над ними, тоже были разными. Но одной из повторяющихся проблем стало застревание фич на этапе контроля качества, неравномерная нагрузка на инженера по тестированию и, как следствие, замедление скорости релизов. Ниже я расскажу, какое мы нашли решение.Кто виноват и что делатьВы когда-нибудь сталкивались с тем, что при тестировании не работаетвообще ничего? Возможно, потому, что фронтенд-разработчик ожидает от API один формат данных, а бэкенд-разработчик реализовал другой. Или формат данных на бэке меняется в процессе разработки (только никто об этом не знает).Да, реальный процесс разработки часто отличается от теории. Ведь предполагается, что сначала происходит постановка задачи, затем параллельная разработка бэкенда и фронтенда — и все передается на тестирование и релиз.Но на практике обычно иначе: после постановки задачи разрабатывается бэкенд, потом (на его основе) — фронтенд, а после все передается инженерам по тестированию. В любом случае, контроль качества происходит уже после полной реализации фичи.Эмодзи с книгами обозначает контекст, а другой смайлик — баги.Представим, что при разработке бэкенда возникло какое-то количество багов, но их вовремя не нашли и не поправили. При разработке фронтенда число багов удвоилось, и в таком виде задача попала на тестирование. В идеальном мире у инженера по тестированию есть в наличии вся документация и четко сформулированные задачи. Но так бывает далеко не всегда.Получается, что с каждым переходом у задачи количество багов увеличивается, но при этом теряется контекст. Когда задача прилетает на тестирование, контекст уже минимальный, а число багов зашкаливает. Контроль качества оказывается узким горлышком, на котором все тормозится. Что делать — фиксить все баги? А если дедлайн уже горит — катить, как есть?Чтобы жить было легче, процесс можно выстроить по-другому: начинать тестирование одновременно с разработкой бэкенда и фронтенда. Это обеспечивает раннее выявление багов и удешевляет их устранение. Такой подход называетсяShift Left.QA-инженер подключается на каждом этапе разработкиРасскажу, как он применяется у нас в компании. Команда вырабатывает решение, затем фиксирует его в доступной и понятной всем форме. После этого начинается разработка на основе зафиксированных артефактов, на этом этапе можно описывать тест-кейсы и писать автотесты.Артефакты нужны, чтобы обозначить договоренности между всеми участниками команды разработки.Семь бед — один ответНо что, если в процессе разработки действительно стало понятно, что нужно изменить API? Как мы уже выяснили, делать это в одностороннем порядке — плохая идея.Конечно, можно собрать встречу и договориться заново. Но это не всегда необходимо, а если речь идет о кросскомандной разработке, то это еще и дорого.Тут нам помогают контракты. Они позволяют предложить изменения так, чтобы все участники узнали об этом, могли оставить комментарии, заблокировать или одобрить изменения. Такой подход называетсяContract First.Контракт принадлежит не конкретному человеку, а всем, заключившим его. Чтобы опубликовать новую версию контракта, необходимо согласие всех владельцев.Контракт содержит в себе некую договоренность: например, декларативное описание API. Мы используем самый популярный формат — OpenAPI.С помощью OpenAPI можно генерировать:интерфейс для провайдера API;клиент для потребителя API;тесты и документацию;Mock-данные.Если API уже есть, а контракта нет, можно сгенерировать OpenAPI-схему из API. Сгенерированная схема дополняется, добавляется в контракт и далее используется по стандартному сценарию.Подход Contract First  помогает обеспечению качества на всех этапах:на этапе проектирования — проверить API на соответствие требованиям;на этапе разработки —  использовать примеры данных из OpenAPI для тест-кейсов  , писать тесты для API  , проводить автотесты для фронтенда на Mock’ах;на этапе тестирования фичи — автоматически запускать изолированные тесты на Mock’е;на этапе регрессов — для тестирования API (при необходимости).Как видите,  такой подход,   позволяет  выявлять дефекты на каждом этапе разработки проекта.Для небольших изолированных проектов и команд (например, для разработки MVP) может использоваться подходCode First. В этом случае схема API, как правило, автоматически генерируется из кода бэкенда. Результат нельзя назвать контрактом, поскольку он односторонне «навязан» остальным участникам процесса, и владеет им только бэкенд-разработчик.Как мы работаем с контрактамиМы в Учи.ру стремимся развивать удобную инфраструктуру для разработки, поэтому создали для команд сервис, в котором хранятся версии всех контрактов. Мы назвали его API Explorer. Еще в нем можно протестировать запросы, выбрав API-сервер, к которому мы обращаемся (прод, стэйдж), а также найти контракт в Git.Если надо быстро отредактировать маленький кусочек в контракте, то можно внести необходимые изменения, закоммитить и создать Pull Request прямо через веб-интерфейс Git. Большие изменения удобнее вносить с помощью IDE.Для удобного ручного тестирования OpenAPI схему из контракта можно импортировать в Postman (или в любой другой инструмент — например, SoapUI). Это позволяет не вводить пути и параметры запросов вручную, а также делает проще проверку валидности возвращаемых данных.Однажды мы запускали проект, который нужно было достаточно быстро отправить на прод, но разработчики все равно написали контракт. В процессе разработки мы узнали, что на реализацию API не хватает ресурсов бэкенд-разработки.Благодаря контракту проект уехал на прод на Mock’е: он позволил сымитировать программное окружение, потому как мы знали, какие данные придут и какие данные нужно отдать. А если бы пришлось реализовывать API, то контракт бы ускорил и упростил как разработку, так и интеграцию с фронтенд-приложением.Контракты и автотестыНедавно у нас в команде появилось автоматизированное интеграционное тестирование. Вот как оно работает: запускается веб-приложение, источником данных для которого является Mock-сервер, поднятый из контрактов. Мы не эмулировали сложную логику — это дорого. Поэтому тестируем на Mock’е только простые сценарии. На схеме ниже показано, как реализовано тестирование для бэкенд- и фронтенд-приложения.Раньше весь наш код автотестов лежал в отдельном от кода приложения репозитории. Это приводило к рассинхронизации тестов и функционала. Например, тесты в master-ветке написаны под тестируемый релиз, а нам нужно срочно выкатить хотфикс без новых изменений. В ручном режиме можно переключиться на старый коммит и запустить регрессы. Но при запуске в CI невозможно понять, куда следует переключаться. Либо пришлось бы в репозитории тестов заводить аналогичные ветки репозиторию приложения, что неудобно и легко может сломаться.  Поэтому, несмотря на то, что тесты написаны на другом языке, мы положили их в один репозиторий с кодом приложения — и теперь тесты всегда соответствуют коду приложения.ВыводыКонтракты — это не «волшебная таблетка», у них есть свои минусы. К тому же, не всем может быть очевидно, как описать API приложения на самом раннем этапе. Придется затратить дополнительное время, чтобы сначала продумать и написать контракт.Однако контракты позволяют реже переделывать уже готовое решение и устранять баги с меньшими затратами, ведь тестирование включается на самом раннем этапе. Также разработка на основе контрактов помогает формировать документацию по проекту и всегда поддерживать ее в актуальном состоянии, так как код генерируется из контракта, а не наоборот.Сейчас в нашу команду мы ищем трех инженеров по тестированию. Если хочешь развивать школьный EdTech вместе с нами,присоединяйсяк нашей команде."
СберМаркет,,,Как реагировать на негатив в экспертных статьях и не потерять мотивацию,2023-11-15T13:49:23.000Z,"В IT-сообществе принято делиться опытом решения сложных задач: это помогает найти единомышленников, получить комментарии или советы экспертов из смежных областей и компаний. Под такими материалами может появляться как позитивная обратная связь, так и негативные комментарии, которые могут демотивировать автора экспертной статьи.При написании текстового материала вероятность получить негативный комментарий выше, чем на конференции или митапе — в живом общении люди меньше склонны высмеивать опечатки и дизайн, ставить под сомнение уместность темы или квалификацию автора.Команда HR-бренда Учи.ру решила разобраться, как реагировать на критику и негатив в комментариях, чтобы не потерять мотивацию, и почему авторам важно продолжать делиться своим опытом на внешних площадках.Зачем пишут негативные комментарииНесколько лет назад в VK проводилиисследование негатива в сети. Коллеги выяснили, что у людей, которые оставляют злые комментарии, может быть на то несколько причин:Предупредить других читателей — например, если в материале содержится важная фактическая ошибка.Обратить на себя внимание — блеснуть умом, начитанностью, грамотностью, квалификацией.Восстановить справедливость — допустим, если автор материала совершил некорректный поступок, и хочется ославить его.Мы с коллегами посовещались — и думаем, что вторая причина встречается чаще. Кажется, большинство негативных комментариев появляются в сети просто из-за желания поделиться своим мнением.Наш коллегаГригорий Петровиз Evrone считает, что виной тому — опыт, который ранее мы получали в интернете. А некоторые психологи отмечают, что роль играют анонимность и физическая удаленность от собеседника, которая позволяет менее скрупулезно придерживаться норм вежливости.Можно (и порой даже нужно) обращать внимание авторов на их недочеты и ошибки, давать полезные рекомендации. Но авторам точно будет приятнее, если делать это корректно и менее эмоционально.Интересно, что негативные комментарии люди оставляют чаще, чем позитивные. Думаем, что причиной этому тоже является культура оценок: не ставить лайки и «пятерочки» постам, которые понравились, и сервисам, которые полностью устраивают, но оставлять «единицу», если что-то вызвало негативные эмоции.Как автору уменьшить вероятность негативных комментариевПолучить негативные комментарии проще всего тем, кто хочет поделиться опытом и рассказать о своих открытиях в профессиональных сообществах. Некоторым экспертам даже здесь, на Хабре, рекомендации коллег могут показаться неинтересными, давно известными или ошибочными.Григорий Петров из Evrone, с которым мы поговорили, когда готовили этот материал, подтвердил: туториалы «хабровчанам» не очень по душе. Он рекомендует в качестве темы для статьи выбирать не образовательные рекомендации, а что-нибудь любопытное. Критерий для проверки — интересно было бы вам обсудить эту тему с друзьями вечерком во время застолья. Если да — пишите!Если все-таки хочется поделиться опытом, то стоит проверить, не дублирует ли статья общеизвестные тезисы. И включить в текст только те наблюдения, о которых еще нигде и никто не писал.При выборе темы полезно почаще заходить на Хабр и смотреть, какие статьи собирают больше позитивного отклика. Вот еще несколько важных рекомендаций от руководителя отдела обучения Учи.ру Насти Лесниковой:При написании материала ставить адекватную цель: не изменить мир своей статьей, а помочь разобраться в конкретной теме хотя бы нескольким коллегам.Приводить жизненные примеры — статьи со сторителлингом аудитория принимает охотнее, ведь читать их интереснее.Показывать готовые материалы ребятам из своего отдела, чтобы вносить в статьи дополнения и исправления, ориентируясь на их обратную связь.Помнить, что вероятность появления критики высока. Но если она необоснованная, можно вообще не реагировать на комментарии.Смотреть на обоснованную критику как на возможность стать лучше. Интересуйтесь, что коллеги советовали бы доработать в материале, и берите такие комментарии на заметку.Что делать автору, который получил негативный комментарийНа Хабре некоторые наши коллеги тоже получают критические и негативные комментарии. Некоторые из них связаны с разницей во взглядах и опыте — тогда мы предлагаем коллеге, отвечая на них, рассмотреть точку зрения оппонента и еще раз разъяснить ему свою.Комментарии с конструктивной критикой важно отделять от необоснованного негатива. Критика помогает стать лучше, указывает на неточности и показывает точки роста. Часто негатив бывает не конструктивен — но его можно попытаться перевести в рациональное русло, попросив автора комментария выразить свою точку зрения более конкретно.Если негативные комментарии вызывают обиду, есть способ справиться с эмоциями — поделиться с друзьями своим огорчением. Особенность человеческой психики такова, что нам сложно смиряться с критикой — но если получить поддержку, станет легче.Если хочется все бросить и вообще больше не писать никаких статей для Хабра или других площадок, то мы рекомендуем следующее:Созданные статьи могут кому-то быть полезны. Даже если в комментариях написали, что эта технология устарела, и лучше использовать другую.Написание статьи — это полезный опыт, который позволяет формировать соответствующий навык. А каждая публикация еще и развивает личный бренд, не говоря уже о бренде компании-работодателя.По нашему коллективному опыту, демотивирует часто не суть комментария, а грубая форма высказывания. Мы всегда советуем коллегам: как бы не было обидно читать подобные комментарии, не опускаться до пассивной агрессии или резкости в ответ. Взять конструктив, отреагировать на него. И всегда сохранять корректность, чтобы формировать новые правила поведения в социальной сети.К счастью, на Хабре редко бывают совсем уж оскорбительные комментарии, которые не несут никакого конструктива. Но если кому-то вдруг попадется, то рекомендуем не обращать на них внимания. А в соцсетях вообще есть возможность заблокировать не в меру грубого пользователя.ВыводыПри написании экспертной статьи учитывайте аудиторию и ее интересы, старайтесь не повторять общеизвестный материал, радуйтесь даже небольшому количеству лайков и сохранений. Делитесь жизненными примерами и собирайте фидбэк на свой труд у коллег в команде. Если получили критические комментарии — отвечайте вежливо, конструктивную критику учитывайте в следующих статьях. А если в комментарии нет никакого конструктива, а только негатив, то его лучше просто проигнорировать.Мы подготовили этот материал ко Дню борьбы с травлей в сети. Впервые эта информационная кампания прошла 11 ноября 2019 года по инициативе VK, а в этом году акция устраивается уже в пятый раз."
СберМаркет,,,"Сделали библиотеку компонентов, но пришлось переделывать. Почему так вышло и чем дело кончилось",2023-10-11T09:05:10.000Z,"Привет, Хабр. Меня зовут Артем Арефьев, я руковожу Frontend-разработкой в направлении продукта для учеников в Учи.ру. Фронтендом занимаюсь уже 11 лет, шесть из которых работаю у нынешнего работодателя. Еще принимаю участие в проектах Open Source (например, внес вклад в проект Lerna), несколько лет был наставником в «Хекслет». Хочу рассказать о том, как у нас в Учи.ру возникла необходимость в создании библиотеки компонентов, почему первое решение не сработало, какие выводы мы сделали и чем закончился наш проект библиотеки.Для чего нам понадобилась библиотекаНачать стоит с того, что компания Учи.ру, как и любой продукт, развивалась постепенно. В самом начале это было монолитное приложение, над которым работала одна команда. Но проект разрастался, и появилось несколько команд, каждая из которых сфокусировалась на своей предметной области.Постепенно количество фичей у команд выросло до такого объема, когда зависимости начинают влиять на сроки реализации. Поэтому монолит начали делить на независимые приложения. Командам стало проще делать фичи и, соответственно, быстрее катить их на прод. Но возникли две проблемы.Различия в дизайне: разные цвета и размеры элементов, шрифты, UX. Наши основные пользователи — школьники, которые в этом разнообразии путались. Одинаковые по сути, но отличные по дизайну элементы дети воспринимали как разные, что осложняло пользовательский опыт.Из-за разделения фронтенда каждый дубль элемента — модальное окно, кнопку, инпут — приходилось поддерживать отдельно, на это тратились ресурсы.Чтобы не путать пользователей и экономить время, мы решили внедрить общую библиотеку компонентов. Работа предстояла большая ввиду нескольких факторов:Компоненты должны были работать с разными инструментами, поскольку в компании есть проекты на TypeScript, на React в связке с Redux или MobX, на Vue, несколько приложений на Next.js, лендинги в Tilda.Ученики занимаются на платформе как дома, с ноутбуков и смартфонов, так и в школе, на компьютерах со старыми версиями браузеров. Компоненты должны работать у большинства наших пользователей и не сильно нагружать страницу.Нам подходили веб-компоненты, но они работали только в современных браузерах. Поэтому мы искали свое решение на основе этой технологии. Вариантов было много, но мы остановились на Stencil.js, поскольку у него уже была реализована поддержка с React и Vue.С чего мы начали работуНеобходимо было определиться, кто будет заниматься разработкой библиотеки. Сервисных команд у нас не было, создать специальную команду мы не могли. Было несколько проектных команд, которые помогали отделам без разработчиков, и продуктовые команды, которые непосредственно занимались продуктом.Мы решили, что лучше всего поручить создание библиотеки самой опытной проектной команде, а продуктовые команды смогут сами вносить изменения в компоненты при внедрении. Дизайнера взяли в одной из продуктовых команд. В первую очередь было решено сделать те элементы, которые, как нам казалось, везде пригодятся: компоненты для работы с текстом, интерактивные компоненты и сетку.Мы запустили всю эту историю 13 июля 2020 года. Через месяц у нас появилась самая первая версия библиотеки, но в ней еще не было компонентов. Сами компоненты делались в течение следующих пяти месяцев, мы их тестировали, правили баги. Процесс создания компонентов для библиотеки представлен на рисунке ниже.Такой долгий срок для нас стал сюрпризом, но тут роль сыграли:Высокая сложность разработки. Чтобы создать компонент, недостаточно было реализовать логику и написать стили. Для обеспечения полноценной поддержки в проектах на React и Vue мы разрабатывали для каждого разные обертки (причем из-за изменений в Vue3 сделали еще одну обертку). В некоторых компонентах двустороннее связывание в Vue работало только в тестах, а при внедрении в проект — нет. И узнавали это мы, соответственно, только когда внедряли.Расположение нескольких версий одного компонента на странице. Эту проблему мы пытались решить, добавив префикс. При маунте компонента префикс должен был заменяться на версию. Это решало проблему коллизий, но, к сожалению, не решало проблему со скоростью загрузки страниц. Все потому, что для каждой патч-версии получался отдельный файл.Трудности при создании компонентов. Чтобы их сделать, недостаточно было знать Stencil.js. Была куча внутренних особенностей, которые хотя и были задокументированы, но все же вызывали вопросы. В итоге продуктовым командам трудно было работать с компонентами, и они постоянно «дергали» проектную команду, которая занималась библиотекой. А ведь библиотека не была основным проектом разработчиков, времени на нее было не так много.Отсутствие согласованности по дизайну. Вместо того, чтобы использовать компоненты как есть, дизайнеры продуктовых команд экспериментировали. В итоге путаница в стилях сохранилась: элементы отличались отступами, скруглением, размерами. Мы не договорились, что на изменение дизайна нужно серьезное обоснование.Первой версией библиотеки начинали пользоваться четыре продуктовых команды, три из которых от нее отказались, потому что инструмент получился неудобным. Летом 2021 года проект заморозили. И, по сути, это был провал.Вторая попыткаЛетом 2022 года мы сформулировали новую цель направления по работе с учениками — улучшение взаимодействия пользователей с продуктом. В рамках этой цели решили создать единый стиль всех проектов, а для этого — возродить библиотеку компонентов, исправив недочеты прошлой версии.Мы решили не восстанавливать прошлую версию (в ней было очень много багов, и мы опасались потратить все время только на их исправление), а создать новую, основанную на имеющейся. В этот раз у нас были более жесткие сроки, предусмотренные OKR, а участвовать в разработке могли только продуктовые команды, поскольку в направлении работы с учениками сейчас только такие и есть.Мы решили, что в этот раз не будем отдавать создание библиотеки «на откуп» лишь одной команде, а станем работать над ней все вместе. Обсудили с коллегами пользу, которую потенциально библиотека может принести: например, когда она появится, всем будет удобнее делать фичи. И договорились использовать стратегию InnerSource — это когда подать идею и внести свой вклад в проект может каждый из коллег.В создании библиотеки участвовали дизайнеры всех продуктовых команд (под руководством лида дизайна в направлении) и фронтенд-разработчики, которых курировал я.Также, наученные предыдущим опытом, мы приняли важные правила:В библиотеку попадают компоненты, которые точно нужны в данный момент и будут использоваться в разных местах, а не те, которые «возможно, пригодятся».При разработке фичи рассматриваем ее компоненты: если они могут впоследствии понадобиться для других фичей, создаем их через библиотеку.Компоненты используются без изменений. Если очень нужно, то вопрос обсуждается. Если договориться не удается, у команд всегда есть возможность сделать свою библиотеку, основанную на текущей.Все вопросы мы стараемся решать через канал оперативной коммуникации.В целом, вторая версия библиотеки оказалась удачной и успешно используется в работе. Уже в течение месяца мы смогли довести ее до состояния, когда компоненты решают задачи на проде. Мы в целом сделали больше компонентов, потому что в проекте принимало участие больше людей.Конечно, остались и определенные вызовы. Например, из-за того, что за библиотеку отвечают все и сразу, сложно понять, кому браться за доработку компонентов. Поскольку библиотека — не основная задача, то времени обычно нет ни у кого. В нашем направлении нехватка времени особенно сильна, потому что, повторюсь, в него входят только продуктовые команды. У некоторых нагрузка может быть так высока, что не всегда получается поработать даже с техдолгом.Как правило, наши разработчики считают, что дорабатывать должен тот, кто изначально делал компонент. Но я ратую за коллективную ответственность и обращаю внимание: если у кого-то появилась инициатива, он может сам внести изменения. Или отправить на review — все инициативы мы собираем, обсуждаем, отправляем в бэклог, приоритезируем. И в дальнейшем ими занимается тот, у кого появилось время.ИтогиТеперь ни одна команда не игнорирует библиотеку: ведь мы работаем с обратной связью достаточно быстро, и есть реальная возможность самому внести изменения. Наоборот, присоединиться к библиотеке хотят команды из других направлений. Кажется, что есть запрос на расширение инструмента на всю компанию.Из этой истории можно сделать важный вывод — разработкой общих проектов, которые несут пользу всему направлению или компании, нужно заниматься сообща. Используйте стратегии InnerSource:формируйте сообщество вокруг инструмента;создавайте культуру сотрудничества;мотивируйте и поощряйте людей, которые вносят вклад в развитие инструмента.Чем быстрее инструментом начнут пользоваться, тем быстрее вы получите фидбэк, по результатам которого уже примете решение, стоит ли продолжать."
СберМаркет,,,Информатика в России: от ЭВМ и «бейсика» до проекта «Код Будущего»,2023-09-12T13:51:53.000Z,"Привет, Хабр! Меня зовут Александр Спирин и я — учитель информатики и методист в команде Учи.ру. Вместе с коллегами создаю интерактивные задания для школьных курсов по программированию, и даже немного завидую детям: ведь в наше время таких курсов не было.Сейчас информатика — весьма важная дисциплина: технологии тесно вплетены во все сферы жизни. Мы читаем новости, смотрим фильмы, играем и учимся онлайн. Практически все профессии связаны с использованием профессиональных программ, а цифровые специальности приобрели огромную популярность.Мы в Учи.ру делаем курсы по программированию для детей от 7 лет, собираем их отзывы — и видим, что сфера IT очень привлекает школьников. Если в прошлом веке ребята грезили о карьере космонавтов или футболистов, то теперь мечтают стать разработчиками, тестировщиками, UX-дизайнерами.Просто удивительно, какие масштабные изменения происходят буквально на наших глазах — ведь каких-то 40 лет назад никакой информатики в школе не было (зато было кое-что другое). Сегодня предлагаю вам вместе разобраться, когда и как появилась информатика в школе и как менялся процесс обучения: будет интересно!Что учили наши родителиМоя бабушка рассказывала, что когда она училась в школе, а это были 1930-е годы, в некоторых семьях ее одноклассников начали появляться телевизоры. Черно-белые, с крошечными экранами и огромной линзой, которая позволяла хоть как-то увеличивать изображение. Смотреть фильмы на таких телевизорах ходили большими компаниями, как в кино.Какие уж тут компьютеры, подумаете вы. Но прогресс шел семимильными шагами, и уже в 1946 году в США появилась первая ЭВМ на ламповых усилителях. А в 1951 году была создана советская ЭВМ, в которой все логические схемы были выполнены на полупроводниках.Прошло еще восемь лет, и представьте себе, в некоторых школах СССР появились особые уроки, предшественники современной информатики. Старшеклассников учили теории информации и программированию на перфокартах.Повторюсь, такие занятия были лишь в нескольких школах — в Москве и Новосибирске. Они были введены потому, что ЭВМ начали активно появляться на производствах, а вот квалифицированные сотрудники, которые могли с ними работать, были в дефиците.Почему все началось именно в Москве и Новосибирске? Да дело в том, что эти «кибернетические столицы» СССР располагали максимальным парком вычислительной техники. Здесь было легче проводить обучение и больше всего требовались специалисты.А вот информатика как дисциплина появилась в СССР только в 1985 году. Учебные планы по информатике разрабатывали известные ученые: Андрей Петрович Ершов (в Новосибирске) и Семен Исаакович Шварцбурд (в Москве).Первый учебник по информатике был создан Ершовым, назывался он «Основы информатики и вычислительной техники». А еще Ершов стал автором лозунга: «Программирование — вторая грамотность».Но и тогда была информатика, конечно, не во всех школах. Например, мои родители, которые были школьниками в это время, такие уроки не посещали. И не удивительно — компьютеры все еще были редкостью, не хватало учителей информатики.Этот предмет вообще сначала преподавали учителя математики и физики: для них организовали специальные курсы, вроде повышения квалификации. А педагогов информатики начали готовить в педагогических вузах только с 1986 года.В школах, где информатика появилась, она была только в старших, 9–10 классах. На изучение отводилось 1–2 часа в неделю.Сначала информатика преподавалась как теоретическая наука: ребята изучали структуру и общие свойства научной информации, а также закономерности ее создания, преобразования, передачи и использования в различных сферах человеческой деятельности.Позже школы стали оснащаться вычислительной техникой, сначала это были программируемые калькуляторы.Бум информатики в 80-еВ 1981 году произошла очередная техническая революция: фирма IBM представила публике первую модель персонального компьютера, которая фактически стала родоначальником современных ПК.Это повлияло и на процесс обучения информатике в школах. На рубеже 1980–1990 годов школьники стали изучать базовые навыки работы с компьютерами. В некоторых образовательных учреждениях появлялись компьютеры с огромными мониторами и системными блоками (Yamaha MSX-1, MSX-2; КУВТ ДВК-1, ДВК-3, УКНЦ, Агат, БК-0010, БК-0011 «Корвет», «Гамма-48», «Гамма-64»).Так как школы комплектовались разными типами компьютеров, а у каждого типа была своя операционная система, то на уроках информатики не ставилось целью формирование универсальных пользовательских навыков. Да и набор прикладных программ и их возможности были еще не столь широки, как сегодня.Если кто учился в это время, то помнит, что программирование в основном изучали на языке Basic («бейсик»). Еще на уроках информатики была вычислительная математика, численные методы решения задач, алгебра логики. А компьютеры использовали для программирования вычислительных задач.Но постепенно содержание уроков информатики менялось: изучению новых информационных технологий уделялось все больше внимания, а вот количество часов на изучение программирования, как ни удивительно, уменьшалось.У разных учебных заведений были разные возможности для обучения детей информатике. Но чаще всего их было, к сожалению, недостаточно, чтобы ребята могли в совершенстве овладеть информационной грамотностью. А спрос на квалифицированных экспертов рос!Тогда же зародилось олимпиадное движение: 13 апреля 1988 состоялась первая олимпиада по информатике. Она прошла в Свердловске и носила название еще не Всероссийской, а Всесоюзной. В ней приняли участие 80 школьников из всех союзных республик.В то время опыта в организации таких соревнований не было ни в стране, ни в мире. Для того чтобы определиться с методикой и содержанием олимпиад, в жюри были приглашены лучшие в то время специалисты в области школьной информатики.В результате долгих споров и обсуждений постепенно сформировались те требования, которые стали основой правил современных олимпиад по информатике. Теперь во Всероссийской олимпиаде по информатике ежегодно участвуют тысячи школьников средней и старшей школы.Как учили информатику мы (минутка ностальгии)В середине 90-х годов устаревшую на тот момент советскую технику стали заменять в школах на более современные компьютеры. Например, американские IBM PS/2 286, поставлявшиеся в СССР в рамках реализации государственного пилотного проекта.С появлением компьютеров на Windows и с пакетом офисных программ на уроках информатики стали активнее изучать способы обработки разных типов информации (текста, графики, звука, анимации).При изучении программирования чаще стали использовать язык программирования Pascal («паскаль»), к концу 90-х он стал практически стандартом. Pascal получил развитие в 1995 году с появлением визуальной среды разработки Delphi, основанной на принципах объектно ориентированного программирования и языке Object Pascal.Фактически новый этап истории школьной информатики начинается с 1993 года. Был принят базисный учебный план для школ РФ, согласно которому преподавание информатики было рекомендовано с 7-го класса. С этого года предмет сменил свое название с «ОИВТ» на «Информатика и ИКТ».Тимлид команды бэкенд-разработки Учи.ру Александр Меркулов, который учился в старших классах в 93–97 году, вспоминает:В нашей школе был класс информатики, где стояли компьютеры IBM/PC и более старые. Учитель чаще всего давал задания на языке Basic, они были в игровой форме. И еще была возможность делать все, что угодно на этих компьютерах, помимо учебного плана».У ведущего архитектора IT-департамента Учи.ру Константина Белоусова, который учился в школе с 1991 по 2001 год, другой опыт:В первом классе на торжественной линейке, как сейчас помню, нас встретили одиннадцатиклассники, раздали шарики и отвели в кабинет информатики. Компы были простецкие — например, Электроника БК-03, но мне так захотелось научиться на них работать!Потом, когда в третьем классе зашла речь о распределении по специализированным классам, я попросился в класс с углубленным изучением математики по той же причине: в этом классе было на несколько уроков информатики в неделю больше.Все те же старенькие компы, но мне очень нравилось составлять алгоритмы, набивать их с клавиатуры и чувствовать себя настоящим программистом. В 5–8 классах мы изучали Basic, в 9–11 — Pascal.Еще я принимал участие в школьных и областных олимпиадах. Мы их писали за современными компами, и в них была “qwerty”-клавиатура, а на нашей электронике была транслитерированная русская: на месте русской “е” была английская “e”, а не “t”, и так по всем другим буквам. Из-за этого было сложно печатать.После школы я понял, что хочу развиваться в IT: люблю алгоритмы, люблю создавать программы и искать элегантные решения. Пошел в профильный институт и после трудоустройства понял, что мой выбор был верным!В следующие годы на уроках информатики все больше отходили от программирования: укреплялось понимание того, что компьютерная грамотность и умение программировать — не совсем одно и то же.Коллеги, которые пошли в школу в начале 2000-х, вспоминают, что конкретного изучения языков программирования не было, а было больше занятий в игровой среде (например, Кукарача Windows).А еще в те годы стала популярна книга «Энциклопедия профессора Фортрана»: многие именно из нее получали базовые знания о программах, алгоритмах, устройстве компьютера. Прочитал, что некоторые педагоги даже сейчас рекомендуют эту книгу дошкольникам, но сам ее не читал. Если у вас такая была — поделитесь впечатлениями о ней в комментариях!Что происходит сейчасИзучение информатики в разных школах до сих пор сильно отличается и зависит от возможностей конкретного учебного заведения. Согласно ФГОС, уроки информатики должны начинаться с 7 класса. Но если в школе есть учитель на полную ставку и свободный компьютерный класс, то информатику могут изучать уже в 5 классе или даже в начальной школе.На первых уроках учат термины и понятия, знакомятся с частями компьютера, решают логические задачки, тренируются использовать простые программы: например, текстовые и графические редакторы. А когда несколько лет назад был бум робототехники — школы закупали специальные наборы Lego и проводили для младшеклассников уроки по конструированию этих роботов.Программирование изучается в 9 классе (порядка четверти), а также на более углубленном уровне при профильном обучении в 10–11 классе, так как в ЕГЭ много задач на программирование. Даже те задачи, которые раньше решались аналитическим методом, сегодня на компьютерном ЕГЭ решаются через программирование.Насколько я знаю, из языков программирования в современных школах обычно изучают Pascal, на его основе и создана большая часть заданий в ОГЭ и ЕГЭ (хотя этот язык уже не используется для коммерческой разработки программного обеспечения).В некоторых школах сейчас учат более современный и популярный Python — этот язык программирования с 2011 года включен в список рекомендуемых для Всероссийских олимпиад, а с 2015 во всех вариантах ЕГЭ по информатике появились примеры программ с его использованием.Для старшеклассников, которые хотят лучше подготовиться к итоговому экзамену по информатике и больше узнать о программировании, есть бесплатная возможность —участие в проекте «Код Будущего». Ребята из 8–11 классов могут знакомиться с разными языками программирования на базовом или продвинутом уровне, узнавать больше о профессиях в сфере IT.Занятия проводятся как очно — в школах, которые присоединились к проекту, так и онлайн — на платформах-провайдерах проекта. Наши коллеги из Учи.Дома (это дочерняя компания Учи.ру) как раз участвуют в этом проекте Минцифры и набирают учеников на этот учебный год.На разных платформах учебные планы для школьников разные. Вот у нас есть четыре программы: Python Start, Python Pro, С++ Start и С++. Каждая программа разбита на 4 модуля по 36 часов каждый, в течение учебного года проходит по восемь уроков в месяц.Заявка на обучениеподается через портал «Госуслуги»(для этого у родителей или самого школьника должна быть подтвержденная учетная запись). После регистрации приходит ссылка на небольшое вступительное тестирование — его нужно будет обязательно выполнить, по результатам школьников распределяют по группам (начальная либо продвинутая).Ученики, завершившие обучение и сдавшие итоговый проект, получают сертификаты — в некоторых профильных вузах такой документ позволяет получить бонусные баллы к результату ЕГЭ при поступлении. Но самое главное — конечно, знания и навыки, которые школьник осваивает всего за год и совсем бесплатно.Мне как педагогу и методисту, имеющему отношение к данному проекту, хочется, чтобы как можно больше подростков узнали о нем, смогли развиваться и узнавать новое. Знакомым с детьми-старшеклассниками всегда рассказываю про него — считаю это обучение отличной возможностью для школьников. Вот теперь и вы в курсе: если у вас есть взрослые дети, вы знаете, что делать."
СберМаркет,,,Как и зачем проводить интегральный мониторинг SSR-приложений,2023-09-05T15:10:16.000Z,"Привет, Хабр! На связи команда Frontend-разработки Учи.ру. Знаем, что сейчас активно развиваются SSR-фреймворки — Next.js и другие. Если ваше приложение создано с использованием подобной технологии, вы можете отслеживать корректность его работы с помощью интегрального мониторинга. В этом материале мы расскажем, почему он важен, какие инструменты позволяют его проводить, как с ним работать Frontend-разработчику. И конечно, поделимся своим опытом — как нашли и исправили серьезную ошибку в продукте.Зачем нужен мониторингИнтегральный мониторинг охватывает все возможные уровни возникновения критических ситуаций. Он позволяет убедиться, что корректно работают обе части приложения — и клиентская, и серверная.Если искать ошибки только на стороне фронтенда, легко пропустить проблему. Представим, что с бэкенда ответ приходит пустым или с большой задержкой — мониторинг на фронтенде этого не покажет, и вы будете терять пользователей. Аналогичная ситуация может происходить и в том случае, если вы мониторите только бэкенд.При разработке SSR-приложения ответственность за все части интегрального мониторинга ложится на Frontend-разработчика. Это нестандартная задача для значительной части фронтендеров, поскольку требует знаний, выходящих за рамки разработки браузерного приложения. Но наша команда разобралась, как настраивать и проводить мониторинг — и мы готовы поделиться опытом! :-)Какие инструменты использоватьИнтегральный мониторинг приложения можно проводить с помощью разных сервисов, но наша команда предпочитает работать с Sentry, Grafana, Prometheus и Elastic APM. Разберем подробнее, что из себя представляет каждый инструмент.Sentry — это программа с открытым исходным кодом, предназначенная для отслеживания ошибок. Она показывает все сбои в стеке по мере их возникновения и предоставляет данные, которые помогут их исправить.В процессе развития к сервису добавилась трассировка, которая позволяет оценивать производительность приложений. Однако мы этот инструмент используем именно для первоначальной задачи.Grafana — это мультиплатформенное веб-приложение для аналитики и интерактивной визуализации с открытым исходным кодом. Оно предоставляет диаграммы, графики и оповещения при подключении к поддерживаемым источникам данных.Prometheus — это система сбора и хранения метрик в формате временных рядов, а также система оповещения об инцидентах. Prometheus предоставляет доступ к метрикам через web-интерфейс.Grafana и Prometheus обычно используются в связке, поскольку перед тем, как отобразить данные, их нужно куда-то собрать. Как раз сервис Prometheus аккумулирует данные и передает их в Grafana. Хотя мы используем и другие источники данных: например, тот же Sentry (для мониторинга общего количества ошибок на фронтенде) и Kibana (для мониторинга событий).3. Elastic APM — это мониторинг производительности приложений, который способен решать различные задачи. Наша команда чаще всего использует только одну его фичу — сквозную трассировку, которая позволяет отслеживать, как составляющие сервиса передают друг другу запросы из бэкенда, фронтенда и SSR, и сколько времени на это уходит.На какие метрики обращать вниманиеВ первую очередь стоит заняться настройкой четырех «золотых сигналов SRE». Рассмотрим каждый подробнее.Задержкапоказывает, сколько времени занимает обработка запроса. Например, сколько может пройти от момента выдачи в браузер до конца рендера или от начала запроса пользователя в адресной строке до момента загрузки приложения.Трафикпоказывает, сколько запросов и транзакций происходит в единицу времени в сервисе.Ошибкимогут подсказать, с какими конкретно запросами есть проблема.В случае с последней метрикой важно помнить, что на фронтенде всегда будут ошибки, если у вас высокий трафик — поскольку есть большая вариативность клиентов и негарантированный канал связи. И имеет смысл мониторить не наличие ошибок вообще и не абсолютное их количество, а относительное.4.Насыщенностьговорит о том, насколько вы близки к полной загрузке сервиса: то есть, как долго сервис сможет оставаться жизнеспособным, если трафик сильно вырастет.Эта метрика не нужна, если в приложении нет бэкенда. Однако если есть SSR или бэкенд-код, мониторить ее состояние стоит.В дашборде важно всегда учитывать ключевые метрики жизнеспособности продукта: не только загрузку сервера, статусы кодов ответа, ошибки на бэкенде и фронтенде, трафик и задержки, но и любые другие метрики, которые помогают убедиться, что пользователи не испытывают проблем при использовании продукта. Например, если вы разрабатываете сложную форму, где важно учитывать конверсию (заполнение формы и ее отправка через кнопку), то следует учесть такую метрику в мониторинге.Чтобы вовремя реагировать на критические моменты, следует определить условия для отправки оповещений ответственной за продукт команде. Они должны учитывать только самые ключевые проблемы, которые могут возникнуть в приложении. Если оповещений станет слишком много, то они превратятся в «шум», за которым можно пропустить действительно критическую ситуацию.Как понять, где проблемаМониторинг позволяет выгружать показатели «золотых сигналов» и других метрик с определенной частотой. Средние значения метриклучше не считать— будет ничего не понятно (особенно, когда происходят выбросы).Чем детальнее метрика, тем яснее картина. А увидеть значительные проблемы позволяют показатели перцентильные (описывают частоту большинства значений) и медианные (описывают типичные значения).Давайте посмотрим, как это выглядит в Grafana. Вот, например, перцентильные значения для распределения времени отклика страницы: P50, P90, P99. P50 — это медиана, P90 и P99 — это 90-й и 99-й перцентили.Видно, что у половины пользователей задержка составляет менее 281 миллисекунды. Для 90% пользователей задержка меньше или равна 725 миллисекундам, для red (99%) — менее 1 секунды. Метрика отклика страницы в диапазоне P90 находятся в приемлемом значении: значит, пользователи приложения не испытывают проблем с загрузкой страницы.Как мониторинг помогает справляться с трудностямиВ прошлом году наша команда запустила новый продукт — интерактивный учебник, для написания которого использовался фреймворк Next.js. Под этот продукт мы также настроили мониторинг. Когда все было готово, увидели в Grafana, что у 90% пользователей (P90) некоторые страницы учебника открываются с задержкой более двух секунд. Это высокое значение для довольно статичного контента. Поэтому мы стали выяснять, в чем проблема.На графике за последние пять минут мы увидели другую картину: у большинства юзеров страница открывается в течение 1 секунды.То же было и в Elastic. Однако 95 перцентиль (отображен на графике ниже) показал, что все же задержка отдачи страницы — меньше или равна 2 секундам.Нам нужно было срочно это исправить:согласно исследованию Google, если пользователь ждет загрузки страницы 1–3 секунды, вероятность его ухода с нее возрастает до 32%.Мы сделали сквозную трассировку в Elastic: посмотрели, как себя вели запросы с точки зрения всей системы в целом. Эта функция и помогла разобраться в проблеме — оказалось, что запрос из бэкенда проходил через внешний контур инфраструктуры. То есть, уходил в интернет, затем заново проходил через все слои: WAF, балансировщики, роутинг и т.д., что занимало много времени.Бэкенд-часть нашего SSR-приложения и сервис, к которому шло обращение, находились в одной внутренней сети. Поэтому связь между ними можно и нужно организовать напрямую. Мы воспользовались Service Mesh для связи сервисов. После проведенной оптимизации проблему с загрузкой долгой страницы удалось устранить: значение P90 уменьшилось и составило меньше 0,5 секунды вместо 1 секунды.ВыводыВыстраивая процесс мониторинга для сервиса, необходимо учитывать все уровни потенциального возникновения ошибок — от серверных до продуктовых. Для успешного и полного мониторинга вашего сервиса важно:Определить ключевые метрики жизнеспособности иSLA, чтобы у вас было только необходимое количество данных.Не усреднять значения метрик, использовать перцентили для полноты картины.Настроить минимум уведомлений, добавить чуть больше информации в дашборде и оставить исчерпывающую картину в других сервисах мониторинга.Это поможет точно не пропустить критические проблемы — или убедиться, что с вашим продуктом все в порядке и пользователь не испытывает трудностей при взаимодействии с ним.Присоединяйсяк команде Учи.ру, если хочешь развивать школьный EdTech вместе с нами!"
СберМаркет,,,Свидание с клиентом: как провести онбординг по правилам онлайн-знакомств,2023-08-24T12:56:18.000Z,"Привет, Хабр! Меня зовут Юра Золотухин, я — Product Owner в Учи.ру. Наша компания создает образовательные курсы для школьников, а непосредственно моя команда отвечает за первую сессию пользователя: его знакомство с преимуществами продукта и тарифами, объяснение ценности.Наша задача — заинтересовать ученика, который зарегистрировался на платформе и в первый раз вошел в личный кабинет. То есть, подробно рассказать ему, для чего нужны наши интерактивные курсы, как выполнять задания, какие бонусы даются за регулярные занятия.Наша основная аудитория — ученики младшей и средней школы. Хотя им, конечно, ходить на романтические свидания пока рано, мы при разработке процесса онбординга ориентировались именно на правила подготовки к успешному свиданию. Сейчас расскажу об этом подробнее.Что такое онбординг и на что он влияетОнбординг— это не просто встреча с продуктом в момент активации. Это процесс, который включает в себя:первое знакомство с продуктом,адаптацию,осознавание ценности от использования продукта.Последнее критически важно, ведь любому пользователю хочется понимать, как ваш продукт решит его трудности.Давайте представим, что вы зарегистрировались в приложении для знакомств, чтобы сходить на свидание с приятным партнером. И вряд ли вы сразу пригласите в кафе или кино первого попавшегося кандидата: сначала вы внимательно посмотрите его анкету, фотографии, пообщаетесь в переписке. И это первое онлайн-знакомство очень важно — ведь если что-то пойдет не так, само свидание вообще не состоится.Примерно так же развиваются «отношения» потенциального клиента с продуктом. Первый пользовательский опыт очень важен для успешной активации. Иногда он даже важнее, чем последующий опыт использования продукта.Через онбординг проходит 100% новых пользователей, потому никакая другая часть продукта не имеет столь определяющего значения для формирования ценности у аудитории. Качество онбординга напрямую влияет на продуктовые метрики — у нас это Retention (возвращаемость), конверсия в покупку, активные дни.За время работы над онбордингом мы много экспериментировали с ним, чтобы сделать погружение в продукт максимально полезным. В результаты мы выделили для себя несколько ключевых принципов.1. Адаптируем онбординг для разных типов пользователейМеханизмы активации напрямую зависят от того, что важно конкретному пользователю в продукте. Если ему нужно решить задачу А, то неэффективно демонстрировать добавочную ценность в рамках решения задачи B.Как в онлайн-знакомствах:с каждым потенциальным партнером беседа строится по-разному, исходя из интересов собеседника.Как у нас:мы работаем со школьниками и их родителями, и потому разделили аудиторию на несколько групп, выявив их особенности и предпочтения.Например, ученики начальных классов любят узнавать новое, а еще им нравятся мультяшные герои. А ребята из 3–4 классов обожают соревнования. Делаем выводы и используем в онбординге комиксы с маскотами Учи.ру — завриками, продумываем геймификационные механики.Еще мы знаем, что в первом классе дети не очень хорошо читают. Учитываем это — и озвучиваем все фразы персонажей. (Кстати, в самих обучающих курсах озвучка тоже есть).А вот в средней школе самая важная задача для детей — улучшить успеваемость. Приключения завриков им уже мало интересны. Значит, нужен другой подход в плане визуала и донесения ценности.Онбординг для родителей у нас тоже есть, ведь именно взрослые принимают окончательное решение о приобретении полного доступа к курсам. Мы используем серию welcome-писем, в которых раскрываем ценность продукта: подсвечиваем успехи ребенка, напоминаем про важность регулярных занятий, отправляем статистику решенных заданий, анонсы олимпиад и марафонов.Еще на нашей платформе есть такая особенность: если ребенок переходит к тарифам, он не может их открыть — платформа попросит позвать родителя. И чтобы доказать, что у экрана взрослый, нужно ответить на вопрос по математике, химии или другому школьному предмету.Это фишка дополнительно подчеркивает ценность: платформа Учи.ру  — безопасное пространство для детей. Когда мы добавили ее в онбординг, метрика ARPU увеличилась на 3,4%.Свои гипотезы касательно поведения и предпочтений различных возрастных категорий обязательно проверяем на A/B-тестированиях, а на качественных исследованиях подтверждаем и считываем эмоции пользователя.2. Обращаем внимание на сезонность и аномалииКурсы для школьников — это сезонный бизнес. Основное количество новых пользователей приходит осенью, когда начинается новый учебный год. И им особенно важен онбординг, потому что вместе с первыми учебными днями начинаются и первые трудности — школьники и родители хотят знать, чем поможет платформа.Как в онлайн-знакомствах:вероятность потенциального свидания выше, если на улице весна и хорошая погода.Как у нас:учитываем сезонность — летом готовим изменения в онбординге, а осенью сразу тестируем на релевантном трафике и экстраполируем успешные механики на максимальную долю юзеров.Аномальные ситуации тоже учитываем. Например, мы запустили онбординг в начале 2020 года, когда начался карантин. Новым пользователям рассказывали, что на платформе можно не только заниматься, но и соревноваться с одноклассниками. А еще переписываться в чате, ведь на Учи.ру уже было зарегистрировано референтное комьюнити для каждого ребенка — дети из той же школы, параллели. У нас был огромный приток трафика: тест показал +16% ARPU по новым юзерам.При повторе аналогичного теста в сентябре 2020 года (без карантина и удаленки) результата не было, поскольку трафик был органический.3. Даем пользователю свободуЕсли сделать онбординг обязательным, без возможности его пропустить, мы забираем у пользователя возможность выбора. И это его сильно демотивирует.Как в онлайн-знакомствах:длинный монолог о неинтересных собеседнику вещах — прямой путь в игнор.Как у нас:проверили — и подтвердили на своем опыте, что вариант онбординга, в котором от пользователя требовалось обязательно выполнить несколько заданий, показал большую просадку, примерно 17% юзеров отваливались.4. Не перегружаем пользователяДаже если на взрослого вывалить слишком много информации за один раз, он мало что запомнит. А наши основные пользователи — школьники.Как в онлайн-знакомствах:новые грани собеседника лучше раскрываются при длительном общении.Как у нас:знакомим ребят с возможностями платформы постепенно. По возможности — сокращаем онбординг. Смотрим, какие экраны школьники прощелкивают не читая, и убираем неинтересные.Если мы видим, что пользователь к нам пришел участвовать в олимпиаде — не предлагаем ему сразу онбординг, а сначала даем возможность пройти соревнование. И только после этого рассказываем о ценности продукта.Это все важно и потому, что у школьников время за компьютером ограничено, родители обычно не разрешают долго засиживаться перед гаджетом. И если у школьника будет слишком много материала, ребенок не успеет погрузиться и почувствовать ценность платформы.А еще дети могут «залипнуть» на какой-нибудь второстепенной активности. Так было у нас во время новогодних праздников, когда мы предлагали школьникам нарядить новогоднюю елку.Те с удовольствием играли с елкой, а основные задания не выполняли. А ребенок, попавший на платформу впервые, рисковал вообще ничего не увидеть, кроме елки. В итоге эту елку мы быстро убрали (как раз новогодние праздники закончились).4. Учитываем несколько aha-моментовAha-момент — это ситуация, в которой пользователь на собственном опыте ощущает ценность продукта.Как в онлайн-знакомствах:если собеседник чувствует, что на другом конце провода его родная душа, свиданию точно быть.Как у нас:мы продумываем несколько aha-моментов и подводим пользователей к ним. Таким моментом может быть радость от успешного выполнения заданий, получение баллов марафона за решенные карточки, похвала от учителя или родителя за самостоятельность и инициативность.Не стоит концентрироваться только на одном aha-моменте, ведь это заранее ограничит потенциал донесения ценности до пользователей с другими юзкейсами. Поэтому во время качественных интервью мы стараемся выделить все потенциальные aha-моменты.ВыводыПланируя онбординг, мы адаптируем его для разных типов пользователей, учитываем сезонность и важные мировые события, позволяем пропускать онбординг и знакомиться с продуктом по своему усмотрению, не перегружаем информацией и стараемся создать ощущение aha-момента.Важно, что работа над активацией пользователей имеет смысл только после достижения реальной ценности продукта. Если ее нет, то даже самый классный онбординг не вдохновит на дальнейшее использование. Как и в онлайн-знакомствах: первое свидание может быть идеальным, но если в партнере ничего не привлекает — отношения не завяжутся.Присоединяйсяк команде Учи.ру, если хочешь развивать школьный EdTech вместе с нами!"
СберМаркет,,,Новый тимлид на удаленке: как перестать быть «говорящей головой» и заручиться поддержкой команды,2023-06-27T13:06:45.000Z,"Привет, Хабр! Меня зовут Надя Терехова, я руководитель дизайна в отделе продукта для младшей школы Учи.ру. Я работаю в компании немногим больше года: за это время мне удалось не только показать результат для бизнеса, но и завоевать доверие команды (хотя это было довольно сложно в условиях удаленки). Сегодня я поделюсь своим опытом и расскажу, куда новому тимлиду лучше направить фокус в первые месяцы работы и как заручиться поддержкой команды.Будьте исследователемВ первую очередь в новой компании нужно познакомиться с сотрудниками и найти партнеров. Когда вы руководитель, то ваша команда — это не только непосредственные подчиненные, но и менеджеры по продукту, бизнес-партнеры HR и все, с кем вы будете взаимодействовать, кому можете быть полезны и кто может быть полезен вам.Определить список этих людей поможет простое упражнение: распишите максимально детально, в чем заключается ваша роль в компании. Еще это позволит  заранее сформировать список вопросов для встреч, чтобы сделать их более структурными и полезными.В процессе знакомств выясняйте, как устроена работа в компании, какие есть трудности и почему не удавалось их исправить раньше. Представьте себя исследователем, которому нужно определить самые проблемные точки, и проведите как можно больше созвонов с максимально полезными вам людьми!Узнайте коллег получшеЗнакомясь с сотрудниками своего отдела, обращайте внимание не только на компетенции и рабочие задачи, но и на психологическое состояние новых коллег. Это поможет определить, как общаться с каждым сотрудником.Например, признанные «звезды» в команде, скорее всего, будут воспринимать вас как конкурента. И лучше перевести конкуренцию в партнерские отношения, доверяя таким коллегам ответственные задачи.Если в вашей команде есть энтузиасты — здорово: они будут готовы вам помогать во всем. Необходимо их поддержать и направить энергию в нужное русло.Наверняка в отделе окажутся и выгоревшие сотрудники: они могут проявлять токсичность и без энтузиазма браться за задачи. Важно следить за их состоянием и вдохновлять, напоминая о миссии и ценностях.Презентуйте планКогда будете общаться с коллегами, фиксируйте повторяющиеся трудности, о которых услышите. Старайтесь докопаться до их причин: почему они все еще существуют, почему их не получилось решить.Если во время встреч удастся собрать достаточно информации, составьте план на ближайшие месяцы работы. Презентуя его коллегам и руководителю, объясните, почему его пункты важны и на что повлияет их выполнение.Например, я сгруппировала ответы моих собеседников и, исходя из задач и трудностей, о которых они мне рассказали, создала план развития дизайн-отдела на квартал. Было достаточно просто представить его руководителю и команде, потому что мой путь к этому плану был совершенно ясен и прозрачен.Хочу еще раз подчеркнуть самое важное:1) Думайте про причины болей. Например, большинство моих коллег жаловались на отсутствие дизайн-системы. При этом дизайн-система существовала, а проблема была в том, что не хватало работающего процесса, чтобы эта дизайн-система постоянно развивалась. Поскольку она не развивалась и не актуализировалась, то неизбежно устаревала, появлялись «костыльные» решения. То есть, в первую очередь мне и команде нужно было запустить и обкатать более эффективный процесс обновления дизайн-системы.2) Не берите на себя много задач сразу. Вам нужны быстрые победы — для того, чтобы усилить свою репутацию в компании и самому почувствовать прогресс от завершенных задач.3) Максимально декомпозируйте цели на задачи для конкретных сотрудников. Донесите до каждого сотрудника, что именно вы от него ждете в рамках предстоящего периода. Объясните, как эта задача и работа будет учитываться в рамках общего ревью.Заручитесь поддержкой командыВ условиях удаленной работы руководителю бывает сложно стать эмоциональным лидером и сплотить сотрудников. Так случилось и у меня: я видела, что ребята из отдела не чувствуют себя настоящей командой, у них нет внутренней мотивации.Отсутствие командного духа легко заметить:на созвонах почти у всех выключены камеры;обсуждается только статус текущих задач;команда не проявляет инициативы, все идеи идут от руководителя;сотрудники не обращаются друг к другу за помощью или советами, не делятся опытом.Если это происходит, важно заняться командообразованием. Можно обратиться за помощью к внешнему фасилитатору — например, я попросила помощи Scrum-мастера.Тот предложил провести серию воркшопов, направленных на создание комьюнити. И на этих мероприятиях как раз выступил внешним фасилитатором. Это было важно, чтобы я тоже стала участником воркшопов наравне с остальными, отбросив роль руководителя.Вот примеры простых установочных воркшопов, которые помогут наладить связь с командой.1. «Комьюнити: что это такое и зачем это мне»Его цель — понять, что каждый сотрудник понимает под словом «комьюнити» и какую пользу в нем видит. Для этого проводится индивидуальный брейншторм и командное обсуждение, а потом сбор запросов от команды: что они хотят получить от профессионального комьюнити и что могут ему дать. Эти выводы являются основой для следующего воркшопа.2. «Миссия и ценности комьюнити»Цель — определить важные смыслы, которые объединяют  команду, вдохновляют работать, поддерживать комфортную рабочую атмосферу.Встречу можно провести по такому плану:Детализировать выводы из прошлого воркшопа, уточняя их цель. Например, коллеги в команде готовы обучать друг друга,чтобыбыть профессионалами;чтобыулучшить качество дизайн-решений;чтобылучше решать потребности пользователей.Сформулировать миссию комьюнити. Здесь важен даже не результат, а процесс: проговаривая ее, сотрудники увидят, что разделяют общие ценности.В финале каждый может предложить идею для следующего воркшопа или рассказать о впечатлениях от прошедшей встречи.Также важно проводить неформальные встречи, чтобы установить большую эмоциональную связь с командой. Подойдет, например, игра «правда или ложь».Поиск стиля руководстваНа удаленке мне важно было перестать быть «говорящей головой» и выстроить с командой такие отношения, чтобы мне не боялись давать обратную связь.Поэтому я придерживаюсь модели управления, в которой акцент идет на отношения внутри команды, развитие и мотивацию каждого сотрудника. В рамках этой модели фокус смещается с руководителя на команду, планируется развитие сотрудников, а менеджер становится фасилитатором процесса.В частности, не указывает, что и кому сделать, не «продавливает» свое мнение: команда самостоятельно приходит к решению. Конечно, могут происходить ошибки, но такой подход создает среду для развития команды.ВыводыИсходя из своего опыта, я составила такой план работы для руководителя, который начал работать с командой на удаленке:Шаг 1.Определить свою роль в компании и составить список коллег, которые могут быть вам полезны и которым можете быть полезны вы. Ваша команда больше, чем вам кажется — подумайте, кто еще внутри компании может быть вашим партнером в решении рабочих задач.Шаг 2.Проведите созвоны с сотрудниками, определите основные трудности и попробуйте разобраться, что мешает решить их.Шаг 3.Составьте план работы: для этого подумайте, на что вы можете повлиять с максимальной отдачей.Шаг 4.Используйте личные встречи с командой не только для статусов задач, но и для того, чтобы узнать получше ваших сотрудников: что их мотивирует, как они себя чувствуют эмоционально. От этого будет зависеть выбранный формат взаимодействия.Шаг 5.Управляйтеэмоциональной сонастройкой команды — сама по себе она не случится. Отнеситесь к этому как к одному из своих проектов, который тоже требует подготовки, планирования и мониторинга.Шаг 6.Задумайтесь над своим стилем руководства: вы хотите решать все за всех и быть образцом для подражания или вы доверяете команде и выступаете в роли фасилитатора и наставника? Осознайте плюсы и минусы каждого подхода и будьте последовательны.А как вы налаживаете отношения с командой на удаленке?Разделяешь наш подход к работе и хочешь развивать школьный EdTech вместе с нами?Присоединяйсяк команде Учи.ру!"
СберМаркет,,,Новые вспомогательные сервисы и инструменты: итоги года глазами команды разработки Учи.ру. Часть II,2023-06-15T10:34:00.000Z,"Привет, Хабр! В предыдущей статье мы рассказали об используемых подходах и технологиях в разработке внутри Учи.ру.  На основе этих проверенных решений за уходящий учебный год мы разработали новые вспомогательные инструменты, продукты и сервисы. Сегодня расскажем, как создавались некоторые из них.Стали отделять контент от конкретного продуктаКонтент Учи.ру был завязан на продуктовых сценариях, что лишало нас гибкости в его повторном использовании. Так мы пришли к выводу, что нужно абстрагировать бизнес-логику от контента.Мы сделали систему единой точки выдачи контента (систему-шлюз) и разработали в дополнение к нему протокол подключения. Этот шлюз можно интегрировать с любой продуктовой системой, такими как МЭШ или личный кабинет. Так появился сервис «Гамбит», который позволяет абстрагировать бизнес-логику от продуктовой системы, куда встраивается контент. Единую систему для всего контента мы начали использовать в качестве вспомогательного сервиса для каталогов учебных материалов, предоставляемых нашим партнерам.Также мы сделали и задокументировали протокол подключения к шлюзу для вновь создаваемых курсов, игровых проектов и методических материалов. Мы попытались из гетерогенной системы контента прийти к гомогенной (прозрачной) по стандартному программному интерфейсу и подключению к продуктам Учи.ру, где требуется выдача конкретного задания в разных сценариях.Разработка шлюза для сервиса заданийПолучили запрос от бизнеса на доработку карточных заданий, где надо было изменить структуру подачи контента. Мы приступили к разработке  универсального механизма — шлюза домашних (или заданных, как мы их называем) заданий.Что пришлось обдумать:Структура должна подходить всем текущим сервисам и потенциально закрыть будущие потребности. Для этого требуется введение абстрактных сущностей и отказ от понятия «задание», так как выдаваться могут и проверочная, и контрольная и многое другое. То есть в структуре должно учитываться только «кому» и «что» выдали. Остальное — необязательная метаинформация.На первом шаге необходимо реализовать двунаправленную синхронизацию, чтобы данные из нового сервиса-шлюза попадали в старые задания (и наоборот) для тех, кто туда еще заходит. Это было важно, так как первым мы переносили самый крупный сервис, выдающий карточные задания и связанный с другими частями системы.Отделение карточных заданий от монолита и разработка единого gateway помогли увеличить скорость будущих доработок за счет гибкой структуры и GraphQL — участие бэка теперь минимальное. Плюс у нас теперь реализовано понятное управление каналами выдачи: никаких прямых вызовов insert из кода, написанных без нашего участия. У нас есть отдельная структура контента, который можно менять, не затрагивая другие части системы. Для каждого сервиса выдачи заданий ученику больше не нужен отдельный бэк — хватит фронта и захода в единый gateway.Но над этим механизмом мы еще будем работать. Теперь важно перенести остальные сервисы выдачи заданий и те, которые как-то соприкасаются с ними, на gateway.Сделали систему входа от разных поставщиковВ этом году мы частично интегрировались с экосистемойmos.ru: теперь пользователям МЭШ в личном кабинете в разделе «Портфолио» стала доступна информация о результатах олимпиад на Учи.ру. На основе протокола OAuth2, OpenID Connect мы сперва получили идентификаторы пользователей в партнерских системах, а затем реализовали экспортер наград за олимпиадные активности в личные кабинеты пользователей МЭШ экосистемыmos.ru.Сейчас в этой модели мы выступаем в роли клиента, но если внесем доработки и развернем в обратную сторону, сможем стать полноценным провайдером (или поставщиком), реализовав передачу ресурса как во внутренние, так и внешние системы. У системы есть потенциал для масштабирования, и его можно использовать в бесшовной авторизации в экосистеме Учи.ру на разных доменах: Учи.Дома, ЯКласс, Учи.Ответы и так далее.Запустили центр уведомлений как универсальный и масштабируемый сервис для коммуникации с пользователямиКогда мы увидели, что уведомления мобильного приложения хорошо влияют на удержание пользователей, решили с их помощью побороться за внимание аудитории в веб-версии. Но уже через полноценный Центр уведомлений (ЦУ) — Babbler. Это наше собственное решение, сделанное поверх Kafka. Такой инструмент давно напрашивался, потому что каждая команда для доставки определенных уведомлений городила свои грядки в личном кабинете пользователей. Мы решили вынести Babbler в отдельный инструмент, чтобы поддержать дальнейшее расширение и масштабирование на другие возможные продукты компании.В Babbler внедрена валидация схем базовых типов уведомлений, на которых основана вся система. Условно в ЦУ есть свой тип сообщений notifications и при отправке от инициатора клиент пытается проверить сообщение по схеме данных, если она представлена для данного типа. Это все можно расширять под разные типы.Типы также помогают разграничивать потребление сообщений конечным клиентом. Например, ЦУ потребляет только сообщения с уже упомянутым типом notifications, а все другие в его выборку не попадают. Также API поддерживает множественную фильтрацию по типам, что можно будет обыграть на мобильных приложениях.Кроме этого мы сделали гибкий бэк, поэтому ЦУ учителя — это лишь один из возможных вариантов использования протокола сохранения и передачи уведомления. Его можно использовать и в приложениях учителя, родителя. Можно провести интеграцию с другими сервисами — использовать Babbler как единый канал коммуникации с пользователями. Это дешевле, поскольку мы придумали алгоритм сжатия сообщений, который уменьшает объем требуемой памяти и увеличивает объем дискового пространства.Стали решать проблемы с миграциейМиграция в связи с переводом детей в другой класс — ежегодный процесс, который сопровождается большими издержками и повышенной ответственностью. Несколько человек запускают десятки окон, в которых для разных групп пользователей запускается один и тот же сценарий перевода пользователей. При этом Учи.ру постоянно развивается, поэтому каждый год мы вынуждены не только корректировать существующий код, отвечающий за перевод, но и дорабатывать смежные алгоритмы. Хотя процесс концептуально не меняется.В этом году мы решили основательно подойти к миграции и выделить действующие стороны процесса — инициатора изменений и системы. Они по команде инициатора должны адаптироваться к изменению номера класса и учебной программы ученика. Достаточно правильно выделить нужных акторов и оптимизировать их, после чего можно немного расширить поведение. А свободное время мы уделим мониторингу и контролю, что сделает миграцию более наглядной и управляемой. В результате это решение можно будет использовать в следующем году и дальше.Мы очень кратко прошлись по основным изменениям, которые произошли в разработке крупнейшей образовательной онлайн-платформе для школьников. Практически про каждое можно рассказать намного больше и мы готовы это сделать по вашим запросам. Напишите в комментариях, какой из этих блоков вы бы хотели увидеть с подробным разбором в нашем блоге.Хотите развивать EdTech вместе с нами? Присоединяйтесь к команде Учи.ру — переходитена сайти выбирайте открытые вакансии!"
СберМаркет,,,"JWT, FSD и сервисный подход. Итоги года глазами команды разработки Учи.ру",2023-06-09T13:49:41.000Z,"Привет, Хабр! Команда Учи.ру традиционно подводит итоги учебного года. Для нас — это сезон не только крупных продуктовых релизов, но и изменений под ИТ-капотом Учи.ру. Сегодня команда поделится, что нового произошло в архитектуре и разработке платформы и что сподвигло их на эти изменения.Начали активно работать в сторону сервисной архитектурыС середины 2021 года мы стали активно заниматься осмыслением и переработкой существующей архитектуры, которая впоследствии начала приобретать очертание микросервисной. Примерно с того же самого времени количественная характеристика по доставке кода в большое монолитное приложение Учи.ру имеет тенденцию к понижению.Опытный архитектор всегда ощущает момент, когда большое и неповоротливое монолитное приложение начинает мешать бизнесу адаптироваться к постоянно меняющимся условиям рынка. Поэтому мы стали успешно внедрять сервисный подход и выносить из монолита не очень большие и не перегруженные части — сервис заданий, состоящий из интерактивных упражнений, и другие, о которых расскажем позже. Это облегчает дальнейшее развитие проекта, помогает лучше понимать работу продукта и делить ответственность между командами.Модульный распил монолитаТам где подобный подход оказался трудоемкой задачей, мы начали с разделения кодовой базы на отдельные модули для облегчения последующих работ по переезду. Делить монолиты логично на директории — модули (package). Rails по умолчанию не дает такой возможности, не считая rails engines, но они оказались неудобны в разработке. А packs-rails позволяет обособить модули в отдельные папки (packs/feature1, packs/feature2), у каждой из которых есть свои app, config, lib, spec. Он добавляет правила автозагрузки кода (для zeitwerk), чтобы все это заработало.Позже это поможет проще перенести код в отдельный сервис, если понадобится. Если пойти дальше, то можно взять библиотеку с открытым исходным кодом (gem) packwerk, чтобы отслеживать степень зацепления модулей между собой. В идеале мы пытаемся добиться высокой внутренней связности модулей и низкого внешнего зацепления (зависимости) с другими. Packwerk позволяет проверять код и искать зависимости между модулями, сопоставлять разрешения из конфигурационного файла, выбрасывая исключения всякий раз, когда используется константа из неразрешенной зависимости.Начали применять FSDПодобную технику можно применить и к интерфейс-приложениям. В Учи.ру мы стремимся разделять фронт и бэк. Но так же в нашем сервисе есть продукты, существующие давно. Некоторые из них выполнены на устаревшем стеке — к ним относятся Олимпиады. В этом продукте мы перешли с единого бэкенда с фронтендом — ruby и slim — на раздельный бэк— ruby — и фронт — next.js.Там где это уже было, мы начали применять Feature Sliced Design (FSD). Это еще один шаг к упрощению наших задач. Мы стали использовать FSD в разработке новых приложений, рефакторинге или расширении текущих.Единый подход в организации приложения позволит нам сократить тот контекст, который вынужден обозревать разработчик, переключаясь на очередной проект. Это упрощает поддержку и ускоряет разработку нового функционала.Сделали инструмент для тестировщиковПоскольку отдел тестирования следует за выбранным курсом в области архитектуры, то аналогичные подходы возможно применить и к тестированию распределенных систем. Раньше у нас были проблемы с автоматическим тестированием: например, на прогоне тестов появлялось много учеток, которые нужно было чистить. А из-за ограничений по безопасности у нас предусмотрены несколько разных блокировок учетных записей от перебора — это приводило к неуспешным прогонам.Чтобы решить эти проблемы, мы сделали шлюз для тестировщика, через который можно выполнять системные действия, не открывая доступа в интернет. В числе этих действий: сброс пароля, ограничения на перебор телефона, почты, логина, а также блокировок по IP адресам. Это позволило отвязаться от админки и необходимости дорабатывать ее под нужды инженеров-тестировщиков.Начали использовать шапку как сквозное приложение для всей роли учителяШапка учителя — это микросервис, который по ESI-технологии встраивается в любое приложение-потребитель: например, в задания от учителя, личный кабинет (ЛК) и так далее. Он позволяет централизованно переиспользовать эту шапку, где она нужна, и выполнять сбор обратной связи, онбординг, переключать классы в ЛК учителя и олимпиадах.Микросервис позволяет сделать изменения в шапке один раз, а потом в любом приложении-потребителе запускать функционал: например, показ модальных окон или опросников. Это упрощает разработку и поддержку фич, которые одинаково выглядят и работают вне зависимости от конечного приложения-потребителя.Частично перешли с классической модели аутентификации на JWTВ связи с эволюцией архитектуры встал вопрос о системе аутентификации, отвечающей текущему вектору развития платформы. В настоящем варианте обнаружились следующие недостатки:гетерогенная (разрозненная) система;отсутствие стандарта работы с аутентификацией;большая разница между работой мобильного приложения (JWT) и остальной части — rails-based cookies.Рост числа сервисов и необходимость читать cookies монолита вызывает определенные сложности (алгоритм подписи проприетарный и зависит от версии rails) и приводит к росту сетевой нагрузки на систему аутентификации и авторизации. Из-за этого приходится делать дополнительные запросы на проверку состояния сессии и ее дешифровки при каждом обращении к серверу, даже если это статика с интерфейс-приложением. А клиентские приложения сейчас максимально просты и беспомощны.Перечисленные процессы порождают не всегда оправданные сложности в транспортировке состояния сессии между приложениями. Также разработчикам надо взаимодействовать с аутентификацией по-разному, в зависимости от платформы. При реализации альтернативных источников аутентификации, отличных от входа с главной страницы, приходится всегда изобретать костыли или использовать монолит для входа. И это не всегда удобно. А JWT позволяет на разных устройствах сбрасывать сессию относительно просто.Поэтому мы частично перешли на JWT. Сервис состоит из сервера со статикой, содержащего две входные точки (entrypoint):script.js — встраивается в страницу и предоставляет API для взаимодействия с сессией текущего пользователя;worker.js — нужен для фоновых процессов: поддержания access token в актуальном состоянии во время работы пользователя на сайте.После инициализации библиотеки клиент может получить доступ к транспортному уровню сессии, забрать access token и, приложив его к специальному заголовку, работать с защищенными маршрутами серверов.Система состоит:из браузера пользователя;worker’a, устанавливаемого в нем;сервиса, занимающегося хранением скриптов для этого самого worker’a;и js-файла, дающего начало процессу развертывания.Это не все, что было сделано за этот год: на основе разработанных технологий и созданного базиса мы сделали новые возможности для дальнейшего развития сервисов, ориентированных на пользователей. Об этом мы расскажем в следующем тексте.Хотите развивать EdTech вместе с нами? Присоединяйтесь к команде Учи.ру — переходитена сайти выбирайте открытые вакансии!"
СберМаркет,,,Как джуну найти работу после обучения и влиться в команду: личный опыт Frontend-разработчика,2023-06-06T11:47:14.000Z,"Привет, Хабр! Меня зовут Анастасия Скворцова и я младший Frontend-разработчик в компании Учи.ру. Некоторое время назад я полностью поменяла профессию — из архитектуры и дизайна ушла в IT. Хочу рассказать, как у меня получилось найти работу в нестабильных условиях, с какими трудностями я столкнулась в первые дни и какие ошибки совершила.Поиск работы: как важна настойчивостьМой путь начался с Яндекс Практикума. Спустя год усердного обучения я попала на техническое собеседование. От волнений и переживаний некуда было деться: для успеха мне понадобились все знания, которые я приобрела за время учебы, упорство и немного валерьянки (ладно — много валерьянки). Потом меня пригласили на финальное собеседование, и его я тоже успешно прошла. Но обстоятельства рынка внесли свои корректировки, поэтому оффер я так и не получила.Понимая всю нестабильность ситуации и видя, как упал рынок вакансий для джунов, я решила действовать: нашла интервьюера в соцсетях, написала ему напрямую и сказала, что готова работать в команде бесплатно. Желание развиваться и получать конкретные навыки и опыт, решая реальные задачи, было у меня в тот момент сильнее стремления к финансовой выгоде. Формата «бесплатной работы» в компании не было, но получилось даже лучше: мне предложили что-то вроде стажировки, после которой я уже оформилась официально.Всем новичкам я бы посоветовала тоже проявлять настойчивость при трудоустройстве: возможно, стоит пообщаться не только с рекрутером, но и напрямую с человеком, который ищет специалиста в команду. Проявляя такую настойчивость (подчеркну, что это именно настойчивость, а не навязчивость, как бы тараканы в голове не пытались обмануть), вы покажете свою заинтересованность и готовность вкладываться, что увеличит шансы на получение оффера. А если все-таки оффер проплыл мимо, вы получите полезную обратную связь — она поможет понять, какие навыки и знания подтянуть.И еще одна рекомендация: когда вы только начинаете путь в профессии, используйте любые возможности, которые дают компании. Стартануть со стажировки и потом устроиться на работу проще, чем сразу получить оффер.Барьеры, которые мешают эффективной работеНа первой работе начинающие специалисты могут чувствовать тревогу из-за своего небольшого опыта в профессии. Так было и у меня — я боялась показаться недостаточно компетентной. Поэтому, когда что-то не понимала, старалась разобраться самостоятельно. Сейчас я знаю: это было неправильно. На решение каждой задачи уходило больше времени, и я тормозила команду. Пришлось поработать над собой, чтобы преодолеть страхи, которые мешали моему росту.Вот какие еще установки могут вредить:Не уточнять задачу, если она не до конца яснаПолучая таск, я не всегда понимала, какой конечный результат от меня требуется. Но вскоре сообразила, что лучше досконально уточнять все вводные. Это вовсе не свидетельствует о некомпетентности джуна: логично, что новый специалист может не знать специфики, сформировавшейся в компании.Стесняться просить о помощи в общем чатеСогласноопросу hh.ru, начинающие специалисты больше всего переживают, что не оправдают ожиданий работодателя. Мне тоже хотелось показать себя компетентной, и если что-то не получалось, я не писала об этом в общем чате, а искала решение сама или просила о помощи одного из коллег.Не стоит бояться обращаться за помощью к коллегам, ведь для бизнеса важен результат работы всей команды. Если надолго засесть со своей задачей — это снижает общую эффективность.Постоянно писать в личные сообщения конкретному сотруднику тоже так себе идея, потому что у него может не быть времени, и он затянет с ответом. Или, наоборот, будет помогать в ущерб своей задаче.Лучшее решение — адресовать вопросы в общий чат команды (понять это помог мне мой наставник). Тогда на вопрос ответит тот, у кого есть на это время. И этот ответ может быть полезен кому-то еще в команде.Изучать новое, не следя за временемДля решения рабочих задач мне нужно постоянно повышать свою квалификацию: гуглить, читать статьи. При непосредственном выполнении обязанностей я узнаю и пробую новые инструменты, подходы.Это интересно, но важно не «закапываться». Иначе есть риск затянуть с задачей и уменьшить производительность команды. Чтобы такого не происходило, я стала ставить себе лимиты по времени на изучение чужого опыта и новых инструментов.Если что-то не получается, на поиск в интернете даю себе, к примеру, час. Дальше со всеми наработками иду в общий чат: рассказываю, какую задачу решаю и в чем не удается разобраться, затем описываю свои идеи, даю ссылки на ресурсы интернета и прошу совета. Такой подход очень классно работает.Практики, которые могут помочь джунамКогда вы впервые погружаетесь в рабочие процессы, на вас обрушивается тонна новой информации. И может показаться, что вы очень мало разбираетесь в теме. Но на самом деле, чем больше ваша область знаний — тем больше становится область неизведанного, с которым вы соприкасаетесь.Я изобразила это на инфографике: знания находятся в окружности, а за ней — все то, что пока неизвестно. Поэтому не переживайте, что вы знаете не все, смело совершенствуйтесь и помните: без постоянного изучения нового невозможен профессиональный рост.В начале пути может помочь такая практика: оценивать задачу до того, как приступать к реализации. Выберите себе любую единицу измерения сложности (например, я измеряю сложность задачи в «попугаях»).Если задача кажется сложной, то я определяю, какие именно блоки вызывают трудности. И, в зависимости от ситуации, обсуждаю эти блоки с командой или закладываю время на самостоятельное изучение непонятных деталей. Такой подход помогает экономить время и дает понимание, сколько потребуется ресурсов на реализацию. Это довольно ценный навык, который всегда пригодится в работе.Чтобы справиться с синдромом самозванца, полезно регулярно делать заметки, что нового вы узнали и что вам удалось сделать. Старайтесь в конце рабочего дня выделять по 15 минут на подобную практику. Даже если ваша рабочая цель пока не достигнута, вы увидите, какие этапы прошли для получения результата. Еще такая практика помогает лучше запомнить то, что удалось узнать в процессе работы.ЗаключениеМои рекомендации для джунов такие:Когда вы только погружаетесь в профессию, не бойтесь проявлять настойчивость и упорство, это поможет достичь желаемого результата.Если сталкиваетесь с трудностями, не стоит отказываться от помощи коллег. С поддержкой опытных специалистов вы принесете больше пользы команде и сможете улучшить ваши навыки.Формируйте навык оценки задачи и следите за своими успехами, чтобы лучше запоминать новые знания и повышать уверенность в себе.Буду рада, если вы тоже поделитесь своими рекомендациями и расскажете о личном опыте: какие трудности были в начале карьеры, как вы с ними справлялись. Уверена, это будет полезно не только мне!"
СберМаркет,,,Как организовано управление маршрутизацией приложений в Учи.ру,2023-05-15T09:56:19.000Z,"Привет, Хабр! Меня зовут Виталий Гуцалюк, я разработчик в команде инфраструктуры в Учи.ру. Я отвечаю за разработку внутренних сервисов, которые дают возможность нашим инженерам самостоятельно создавать и поддерживать текущие проекты в облаке. Сегодня я расскажу, как мы в нашей компании организовали управление маршрутизацией приложений и какую инфраструктуру для этого используем.ИнфраструктураВесь поступающий интернет-трафик проходитчерез NLB— это ноды с ролью балансера. Они разворачивают экземпляр в OpenResty, из которого трафик дальше проксируется во внутренние конечные сервисы через приложение Ingress-proxy — это наша разработка, о которой я расскажу ниже.Но сначала хотел бы рассказать, почему мы решили использовать промежуточное прокси.За добавление новых раутов отвечала команда инфраструктуры. Все задачи на добавление шли через общий support-канал в Slack. Подобных запросов было много — естественно, появлялись расхождения.С быстрым появлением новых сервисов маршрутизация требовала все большей поддержки.Не было прозрачного процесса — разработчики не могли полностью отвечать за свой продукт.Не было поддерживаемого инструмента для управления маршрутизацией стендов.Попытки решить проблемыДля тестирования маршрутов на стендах мы стали поднимать отдельные Nginx-приложения, которые умели работать с нашей Service Mesh. В конфигурации Nginx можно было прописывать alias, который указывал, к какому бэкенду необходимо обратиться через Service Mesh. За счет этого генерировались нужные апстримы — таким образом мы работали с первыми прокси.location /foo/ {
       proxy_pass http://app1;
       proxy_set_header Host $host;
       add_header Access-Control-Allow-Origin $scheme://$host;
     }
     location /foo/bar/ {
       proxy_pass http://app2;
       proxy_set_header Host $host;
       add_header Access-Control-Allow-Origin $scheme://$host;
     }
     location /demo/ {
       proxy_pass http://app3/courses/;
       proxy_set_header Host $host;
       add_header Access-Control-Allow-Origin $scheme://$host;
     }Эта реализация упростила проверку нужной маршрутизации заранее на стендах, но в продакшене ее по-прежнему невозможно было использовать: у таких прокси нет прозрачных владельцев, что вызвало сложность в их поддержке.Поэтому мы пошли дальше — решили отдавать отдельные прокси в каждую команду (иногда сразу в несколько). В качестве уже самого сервера перешли с Nginx на Traefik. Нам было интересно его использовать из-за его характеристик: он умеет делать множество фич из коробки, а также поддерживается консулом-каталогом.Однако и в этом подходе нашлись свои недостатки:Нет централизованного просмотра для всех раутов. Чтобы посмотреть рауты соседних приложений, надо запросить доступ в эту команду или в отдельное прокси.Проблемы с самим Traefik. Например, нельзя было сделать честный редирект при обработке ошибок, так как Traefik предоставляет только проксируемую страницу. Также в ForwardAuth при описании раута нельзя было указать сервис из консула. Для этого нам приходилось поднимать приватные домены.Консул-каталог в Traefik смотрит на всю площадку. Это нам не подходило, поскольку в компании некоторые кластеры являются достаточно жирными, и полинг всей площадки доставлял проблемы. Чтобы решить эту ситуацию, мы стали фильтровать сервисы, которые смотрят Traefik. Для этого мы в нашей PaaS при регистрации нового сервиса навешивали лейблы вроде команды или названия приложения. Потом их же стали использовать в настройках Traefik для фильтрации сервисов, которые он смотрит. Это сузило области, с которыми работает Traefik, и, соответственно, решило проблему. Но недостатком стало то, что такую настройку надо было производить отдельно.Сложно передавать командные прокси в другую команду, а такая необходимость периодически возникала. Вообще, в нашей платформе все ресурсы передаются по кнопке. Поскольку Traefik смотрит в консул-каталог с настроенной фильтрацией, то при банальном перемещении прокси в другую команду могла возникнуть ситуация, когда некоторые сервисы было невозможно найти.Решение — собственная реализацияПоняв, что в полной мере мы не уйдем от всех проблем при использовании готовых решений, мы сделали собственное. Оно называетсяIngress-proxy. По сути, это написанныйна Go сервис, которыйразворачивает экземпляры OpenResty.Работоспособность сервиса достигается с помощью динамического  создания конфига.Он формируется с помощью двух основных процессов:Первый занимается парсингом правил маршрутизации. Мы записываем раутинг в собственном DSL в нашей PaaS, откуда он синхронизируется через Consul KV с Ingress-proxy. Далее прокси уже парсит маршруты в нужные участки конфига в OpenResty — в директивы server и location.Второй основной процесс сосредоточен на генерации апстримов. Здесь сильно помогает интеграция с консулом. Мы знаем о том, какие сервисы нам необходимы для апстримов, и подписываемся на изменения этих сервисов в консуле. Таким образом, у нас есть все необходимые адреса.Мы в инфраструктуре имели многолетний опыт работы с маршрутизацией с помощью OpenResty — и имели понимание, какой routing нужен в компании. При написании DSL мы убрали все ненужные нам фичи, оставив только необходимые, а также постарались сделать синтаксис достаточно простым и удобным.На сниппете представлен пример нашего правила маршрутизации DSL, написанный специально для Ingress-proxy:match:
 host: demo.domain.ru
 prefix: /prefix
middlewares:
 auth:
   - backend:
       service: sys-foo-auth-srv-ep
     path: /authentication/authorize
     upstream_headers:
       - A-B-C
       - Example-Header
 error_handler:
   - on_code:
       - 401
     redirect: /404.html
proxy:
 backend:
   service: sys-foo-main-srv-ep
version: ingress/v2Поле versionв примере выше означает, что в правилах маршрутизации есть версионирование. Оно работает таким образом, что Ingress-proxy для генерации конфига всегда обрабатывает только последнюю версию правила. Однако в хранилище возможно содержание других правил разных версий — для этого предусмотрена миграция до последней версии. Это делается в отдельном репозитории, который называется API machinery — он вдохновлен Kubernetes. Там мы описываем все общие модели сущностей, логику для валидации и версионирование.Шаблонизациюконфигураций мы ведемс помощью утилиты Quick template.Этот пакет, по сравнению со стандартным Go template, работает намного быстрее. У него достаточно простой и понятный синтаксис.Также в Ingress-proxyесть еще ряд метрик— это количество запросов, статусы ответов, задержки, общее количество соединений и так далее. Для всех метрик мы записываем лейблы, которые важны нам для работы, например: namespace, команда, проксируемое приложение. Они позволяют составлять узкоспециализированные дашборды, что довольно удобно.Общая архитектура сервисаОбщая архитектура работы Ingress-proxy выглядит следующим образом:PaaS передает правила маршрутизации в Consul KV.Далее из консула Ingress-proxy парсит правила  и забирает информацию об адресах сервисов.В итоге мы получаем все сущности и данные, нужные для создания валидного конфиг в OpenResty, после чего трафик уже проксируется до конечных сервисов.Отдельно отмечу, чтогруппа Ingress-proxy объединяетсятак называемымingress-классом. Это схожая с Kubernetes концепция — она позволяет выделить группу прокси под общую конфигурацию.Эксплуатация маршрутизации в нашей PaaSСоздание и редактирование правилв нашей PaaS ведетсяс помощью редактора Monaco(аналог VS Code). Для удобства их ведения в простом и удобном формате у нас есть отдельные фичи — realtime-валидация или автодополнение.Для стендов создание нужной маршрутизацииведется из коробки. В нашей платформе для каждой новой фичи мы поднимаем полностью новый стенд. У нас они достаточно быстро разворачиваются. Помогает их развернуть приложение, которое мы называем «шотилкой». У него есть набор манифестов для разных приложений, куда мы просто добавили наши манифесты для раутинга. Как итог — получили готовое приложение и маршрутизацию для него.Возможные риски и челленжи на этапе внедрения Ingress-proxyКонечно же, мы задумывались о рисках, с которыми могли столкнуться на этапе внедрения Ingress-proxy. И сразу решили предусмотреть, как будем справляться с трудностями:Первый риск связанс последствием перекрытия уже существующего правила новым.В Nginx есть правила приоритетов для location разных типов. Поскольку Ingress-proxy работает вокруг OpenResty, мы наследуем те же правила. Поэтому в теории можно было бы перекрыть уже существующий маршрут новым. Для избежания этой проблемы наша PaaS при редактировании и создании правил заранее проверяет, были ли перекрыты какие-то маршруты. Если это произошло, изменение блокируется.Второй рисксвязан с зоной ответственности за правила.Чтобы не возникало сложностей, в нашей системе чтение правил доступно всем. Их можно отфильтровать в общем списке по различным метаданным: namespace, команда или хост. Но конкретно за изменения отвечают команды-владельцы правил. У нас есть и аудит: мы записываем все изменения конфигураций, время создания, кем было произведено обновление. Таким образом, мы можем полностью отследить всю историю любого правила маршрутизации.И последний риск— валидное правило в DSL могло сгенерировать невалидный участок конфигурации для OpenResty.Мы стараемся максимально подробно валидировать правила еще на этапе их создания. Полностью повторить всю логику OpenResty достаточно сложно. Однако, если произойдет ситуация, когда новое правило маршрутизации сгенерирует невалидный участок конфига, то при обновлении в рестарте OpenResty с новым конфигом сервис поймет, что он невалиден, и не будет убивать сервер, который уже запущен с валидным конфигом. Именно эта логика предотвратит падение. Во время подобной ситуации срабатывает алертинг с информацией о сбое при рестарте последней версии конфига. Это помогает найти дефектное правило, пофиксить его и в дальнейшем провалидировать похожие ситуации. Такая функция, например, была очень полезна на этапе внедрения Ingress-proxy,  сейчас она является дополнительным пунктом безопасности в нашей системе.Челленджи, с которыми мы столкнулисьВнедрение Ingress-proxy не обошлось без трудностей.Первая трудность— это высокое потребление CPU на старте в нагруженных кластерах. Самая первая версия решения была запущена стейджах. В момент запуска мы увидели на графиках высокое потребление CPU.При профилировании оказалось, что причиной этой проблемы является перезагрузка конфига. Оказалось, что мы делали рестарт OpenResty на каждое изменение, которое записывали. А при достаточно большом кластере эти изменения происходили постоянно. Чтобы разобраться с этим, мы стали делать троттлинг: пропускать сразу несколько обновлений за одну итерацию, которая занимает 50 миллисекунд.Вторая трудность— при прогонке Ingress-proxy на кластере stage мы замечали, что иногда приходит невалидный конфиг OpenResty, несмотря на то, что все правила являются рабочими.Получилось это из-за того, что мы записывали конфиг не атомарно: запись новых изменений мы вели непосредственно в том конфиге, который использует OpenResty. Чтобы решить эту сложность, мы стали создавать временный файл, куда вносили все изменения. И только потом подменяли его на настоящий конфиг OpenResty.Следующей трудностьюстало то, что нужно было переместить большое количество уже существующего раутинга. Причем, поскольку у нас было несколько подходов с маршрутизацией, мы имели  несколько источников для создания раутов. Их нужно было как-то привести в наш DSL.Поэтому мы написали отдельные скрипты, которые умели из разных источников генерировать нужную нам маршрутизацию. И с такими скриптами мы приходили в каждую команду и обновляли всю существующую маршрутизацию. Так нам удалось относительно быстро перевести весь раутинг на Ingress-proxy.Последняя трудность— метаданные для фильтрации раутов в первой версии приходилось писать руками. Во-первых, это довольно нетривиальный процесс. Во-вторых, многие стали копировать данные из одного раута в другой. Все это стало теряться и ломать поиск.Мы решили проблему с помощью того, что точкой входа для каждого раута сделали отдельный namespace. Это наша абстракция в PaaS. Она знает о том, какой команде принадлежит namespace, и имеет различные данные — вроде того, какие приложения находятся в namespace. Этого стало достаточно, чтобы генерировать всю мету динамически. И мы одновременно добавили овнерство над изменениями правил маршрутизации за счет доступа в команду namespace.ИтогиМы получили легкое в эксплуатации решение, с помощью которого передали зону ответственности над управлением раутингом приложений непосредственно разработчикам. При этом мы сохранили влияние на сами процессы: если возникает необходимость добавить новую фичу, мы можем это спокойно сделать.Сама Ingress-proxy показала себя как стабильный инструмент, который держит хорошую нагрузку: например, в обычный день у нас порядка 3000–3500 RPS только на одну группу прокси. Сам трафик сильно растет при проведении масштабной олимпиады, в которой участвует более 1 млн пользователей, или во время учебных недель. При этом у нас нет проблем с горизонтальным масштабированием.Хочешь развивать школьный EdTech вместе с нами —присоединяйсяк команде Учи.ру!"
СберМаркет,,,Real-time аналитика в Учи.ру: как смотреть сложные метрики,2023-04-20T08:36:57.000Z,"Привет, Хабр! Меня зовут Андрей Скиба и я Python-разработчик в ML-команде Учи.ру. Разным командам в нашей компании важно получать доступ к ключевым метрикам пользователей (количество сессий, DAU и другим) в режиме реального времени. Поэтому мы создали свою собственную систему real-time аналитики — быструю, простую и с удобным для нас функционалом. Сегодня я расскажу, как она устроена.Причины отказа от старого решенияНекоторое время назад мы использовали довольно простой аналитический кластер, в основе которого была обычная база Postgres. В него писались события, там же они собирались в отчеты, на основе которых строились графики на фронтенде. На платформе было сравнительно мало событий, соответственно, в базе было мало строк. Этого хватало до тех пор, пока Учи.ру не стала полноценным highload-сервисом — сейчас на платформе занимаются почти 20 млн активных пользователей, которые совершают несколько миллионов действий в сутки.Ресурсов на поддержку и доработку старого функционала у нас не было. Также с ростом сервиса большая часть аналитики переехала в более тяжелый кластер, где уже использовались ClickHouse, S3, Airflow, Tableau и ряд других технологий. Это решение имеет очень широкий функционал и прекрасно подходит для построения больших ежедневных, ежемесячных и ежеквартальных отчетов для бизнеса, но оно работает слишком медленно для real-time мониторинга. К тому же лицензии Tableau платные и имеют ограничения на использование.Помимо смены аналитического стека, произошли изменения и в архитектуре платформы. События теперь не собираются в базе, а попадают в Kafka-кластер. Так у нас появилась задача собрать простую и легковесную систему, где можно было бы оперативно смотреть основные метрики компании, при этом сами метрики должны были быть основаны на событиях из Kafka напрямую.Также от бизнеса к нам пришли дополнительные требования:мы должны уметь фильтровать данные по классу, сегменту или некоторым другим параметрам в разрезе пользователей;на главной странице должны быть часовые и дневные графики, а также простой и накопительный итоги.СтекБэкенду нас традиционнонаписан на Django. Отдельно хотел бы отметить расширение Django Extensions, в котором, среди прочего, есть возможность запускать отдельные скрипты внутри django-проекта. В Учи.ру микросервисная архитектура, и любой сервис или воркер по умолчанию запускается в отдельном контейнере. Воркеры, необходимые для работы сервиса, мы запускаем как раз через Django Extensions. Об устройстве самих воркеров напишу чуть ниже.С Kafkaмы работаем через библиотекуAIOKafka, также используемfastavro для десериализации бинарных сообщений.Для фронтендаиспользуембиблиотеку Svelte,графикистроимв Highcharts JS. Длядолгого храненияагрегатовиспользуется Postgres, ав Redisу нас хранятся те агрегаты, которые формируются в текущий момент. О том, что такое агрегаты, расскажу далее.Общая схема работы сервисаИз Kafka к нам приходят события. Специальный воркер (назовем его просто KafkaWorker) читает их, обогащает и записывает в Redis в виде агрегатов. Также есть отдельный воркер для сохранения данных из Redis в базу — DB Saver. И уже API читает агрегаты из Redis и базы, отдает их на фронтенд. Концептуальная схема работы выглядит так:Аналитика сессийДальнейший рассказ буду иллюстрировать примерами сессий. В Учи.русессия— этособытие, в рамках которого происходитрешение студентом определенного задания. Когда пользователь сделал упражнение, в Kafka приходит информация о том, что завершилась сессия. Среди ее параметров, например, есть: ID студента, ID карточки, затраченное время, число правильных ответов и некоторые системные параметры.{
'student_id': 43248949,
'subject_id': 46, 
'card_id': 443813, 
'created_at': '2023-02-22 15:05:43.755000', 
'right': 1,
'total': 1,
'spent': 5, 
'part_id': 0, 
'offset': 17489515
}При получении такого события мы должны его обогатить, то есть, получить некоторый набор других атрибутов, которых здесь явно нет, но которые нужны для фильтрации данных. Например, это может быть сегмент, класс или ID предмета. В данном примере полеsubject_idу нас уже есть в исходном сообщении (но так бывает не всегда), а вотклассисегментнам нужно каким-то образомвосстанавливать из других источниковили баз данных.Особенности работы KafkaWorkerKafkaWorkerсобирает данные из Kafka и записывает их в Redis, но у него есть несколько особенностей. Например, одна метрика может содержаться в разных топиках с разными схемами, поскольку у нас множество продуктов, и в них тоже есть свои циклы решения заданий пользователями. Вот две разные схемы:{
	""name"": ""card_completions"",
	""namespace"": ""uchiru"",
	""doc"": ""Card completion event"",
	""fields"": [
		{
			""name"": ""id"",
			""type"": ""long""
		},
		{
			""name"": ""student_id"",
			""type"": ""long""
		},
		{
      ""name"": ""last_activity"",
      ""type"": [
        {
          ""type"": ""long"",
          ""logicalType"": ""timestamp-millis""
        },
        ""null""
      ]
		},
		...
	]
}{
	""name"": ""card_completions"",
	""namespace"": ""modern_subjects"",
	""doc"": ""Card completions for the Modern Subjects"",
	""fields"": [
		{
			""name"": ""student_id"",
			""type"": ""string""
		},
		{
			""name"": ""subject_id"",
			""type"": ""int""
		},
		{
      ""name"": ""finished_at"",
      ""type"": [
        {
          ""type"": ""long"",
          ""logicalType"": ""timestamp-millis""
        },
        ""null""
      ]
		},

		...
	]
}Первая схема — это сессии в общем core-продукте, карточных курсах по школьным предметам. Вторая — это сессии в дополнительных продуктах, новых интерактивных курсах. Можно увидеть, что в первой схеме каждая сессия имеет свой ID, и мы можем, к примеру, довольно легко дедуплицировать эти события, чтобы метрики не задваивались. В новых предметах ID нет, и нам приходится убирать дубликаты по сумме всех полей.Также у нас есть такой параметр, как последняя активность: она показывает время завершения сессии.Из приведенных выше примеров видно, что в двух разных схемах эти параметры называются по-разному: в одной схеме —last activity, в другой —finished at. Приведение к общему виду делается по словарю, в котором происходит сопоставление различных названий общему приведенному списку слов. И если мы хотим увидеть график всех сессий, то нам нужно их все привести к одному виду и просуммировать. Ниже приведена часть словаря сопоставления имен разных топиков.topics = {
	'uchiru.card_comletions': {
		'session_id': 'id',
		'last_activity': 'last_activity'
	},
	'modern_subjects.card_completions': {
		'session_id': None,
		'last_activity': 'finished_at'
}Отмечу, что чтение из Kafka у нас асинхронное, но это сделано больше не для скорости, а дляудобства написания кода.Парсинг полей и обогащение сообщенийудобно вынестив отдельную callback-функцию, аобработку событийиз разных топиковудобно производить в едином цикле событий.Атрибуция или обогащение данных также происходит внутри callback-функции, но здесь для разных топиков алгоритм может быть совсем разным: какие-то данные хранятся в кэшах Redis, за какими-то данными нужно ходить в базу. Ниже мы рассмотрим примеры того, как происходит поиск класса для DAU — это более интересный кейс.После обогащения мы должны создать агрегат или найти уже существующий.Агрегат— это запись в Redis, которая сохраняет в себеколичество событийилисуммукакой-то величины (например, платежей) с учетом всех возможных атрибутов на определенный час. Грубо говоря, это результат выполнения GROUP BY по всем возможным полям.Как устроены сами агрегаты? В названии записи мы храним дату и час, в котором у нас сохраняются события, в ключах — набор всех необходимых атрибутов, а значения — это и есть сами метрики. Ниже приведен схематичный код работы KafkaWorker.async def callback_sessions(msg, topic):
	...

def consumer_factory():
	for topic in topics:
		yield AsyncKafkaConsumer(
			topic=t,
			...
			callback=partial(callback_session, topic=t)
		)

def run():
	for c in consumer_factory():
		asyncio.ensure_future(c.consume())

	loop = asyncio.get_event_loop()
	loop.run_forever()На сниппете ниже можно увидеть пример работы с Redis через интерактивный режим Python. Здесь для сессий есть ключ, у которого указана дата — 22 февраля 2023 года. И 14 часов — это тот час, в который мы сохраняем наши события. Также в этих записях хранятся ключи с информацией, какие именно атрибуты здесь присутствуют. Атрибуты приведенного ниже ключа — класс, сегмент и предмет. Количество событий — два.>>> r.keys('sessions*')
[b'sessions_2023-02-22_14', ...]
>>> r.hkeys('sessions_2023-02-22_14')
[b'grade_2_segment_regions_subject_other', ...]
>>> r.hget(
'sessions_2023-02-22_14', 'grade_2_segment_regions_subject_other'
)
b'2'Как работает DB SaverДалее в игру вступает DB Saver. Это воркер, который занимается сохранением наших данных из Redis в базу. Здесь же мы сохраняем как часовые, так и дневные агрегаты, потому что нам не захотелось перегружать фронтенд лишними расчетами, которые легко можно выполнять фоном.Одна из самых частых проблем с инфраструктурой — сброс подключения к базе. В таких случаях мы используем объект django.db.connection. Он хорошо справляется с обрывами соединений. С определенной периодичностью мы синхронизируем Redis с базой и обновляем часовые (иногда и дневные) значения. Если у нас возникает какая-то ошибка, например, interface error или operational error, то мы просто вызываем функцию connect, которая переподключает наше соединение. И пока у нас запись не попала в базу, мы не сбрасываем ключ из Redis.from django.db import connection

def run():
	while True:
		try:
			time.sleep(SYNC_INTERVAL)
			save_redis_data_to_db()
			update_daily_counts()
		except (InterfaceError, OperationalError): 
			connection.connect()
			continueНиже представлен код модели, как у нас хранится сущность агрегата уже в базе. Среди прочего здесь есть пометка frame о том, какой это агрегат — часовой или дневной. Также приведены поля для фильтрации: предмет, сегмент и другие.class SessionsEntity(models.Model):
	count = models.IntegerField()
	grade = models.IntegerField()
	segment = models.CharField(max_length=10)
	subject = models.CharField(max_length=25)
	frame = models.CharField(max_length=10)
	ts = models.DateTimeField(db_index=True)
	created_at = models.DateTimeField()

	objects = DBManager()

	class Meta:
		db_table = 'session_entities'Как работает сбор данных для APIДанные для фронтенда собираются как из базы, так и из Redis. События из Kafka могут долетать с задержкой (иногда очень большой). Повлиять на это нельзя, поскольку это особенность работы самой системы. Поэтому наш DB Saver может записывать данные в достаточно далекое прошлое: это может измеряться днями и иногда неделями. Получив запоздавшие ключи в Redis, DB Saver должен их правильно записать в базу.В нашем интерфейсе есть:почасовой график;график накопительный;блок, где можно выставить различные фильтры;и общий график по году, где у нас собираются дневные агрегаты.Как мы работаем с DAUВ какой-то момент бизнес попросил реализовать аналитику количества уникальных дневных посетителей. Здесь у нас появились сложности, которых не было ранее с сессиями и другими метриками, а именно:С сессиями нам не была нужна уникальность в течение часа или дня — с DAU это не так, и именно момент с подсчетом уникальности нужно было реализовать особенно аккуратно.У нас нет какого-то специального топика с пользователями, но есть pageviews — посещения страниц. И все бы хорошо, но в этом топике примерно в 15 раз больше событий, то есть, их примерно 1500 в секунду (в сессиях у нас около 100 событий в секунду).Как следствие предыдущего пункта, мы уже не успеваем искать атрибуты так, как мы это делали раньше.Ввиду требования уникальности по часовым и дневным фреймам мы решили, что будем хранить в базе и простые, и кумулятивные агрегаты. Наши воркеры суммируют количество посещений пользователей и записывают их базу с учетом уже имеющихся данных, поскольку здесь не работает простое суммирование. В силу такого большого потока событий мы вынуждены сохранять данные в Redis и искать атрибуты в базе батчами в несколько потоков.Схема сохранения агрегатов DAU в Redis:KafkaWorker записывает данные не сразу в Redis, а в очередь;из этой очереди воркеры, которые занимаются обогащением данных, извлекают события и в параллельном режиме ищут им атрибуты;как только эти атрибуты найдены, данные записываются в очередь на сохранение в Redis;далее воркеры, которые занимаются сохранением данных в Redis, суммируют и сохраняют их с нужными ключами.Вот как, например, в случае с DAU выглядит поиск параллели по базе:select st.id as student_id, cb.parallel as parallel
    from students st
    join class_books cb on st.class_book_id = cb.id
    where st.id=any(%(student_ids)s)У нас есть некий список ID студентов, далее мы делаем запрос с использованием any. Поскольку по ID есть индекс, подобный запрос работает достаточно быстро. Мы можем вставить в наш массив определенное количество значений — 100, 1000 — и оперативно восстановить нашу параллель.ВыводыНам удалось построить легковесную систему аналитики, из которой можно получать актуальные данные на текущий момент о том, как ведут себя пользователи на платформе. При этом сохраняется несколько челленджей:Словари для парсинга разных топиков приходится вручную поддерживать в актуальном состоянии — не то, чтобы большая проблема, но явно напрашивается какая-то автоматизация.Многие атрибуты мы кешируем, и всегда есть trade-off между временем жизни кэша и актуальностью информации. Например, мы можем прочитать из кэша неправильную информацию, и, регулируя размер кэша, мы можем увеличивать точность наших данных, но уменьшать при этом скорость обогащения — возможно, с этим тоже придется что-то делать.И, разумеется, в любой архитектуре есть вопрос масштабирования: что случится, когда данных станет в 10, 100 или 1000 раз больше? Время покажет! :-)Хочешь развивать школьный EdTech вместе с нами —присоединяйсяк команде Учи.ру!"
СберМаркет,,,"Тестирование remote push notifications на iOS, когда используем Firebase Cloud Messaging",2023-03-29T12:26:49.000Z,"Привет, Хабр! Меня зовут Максим Толстиков и я iOS-разработчик в Учи.ру. Для отправки пуш-уведомлений наша команда используетFirebase Cloud Messaging. Недавно у нас появилась задача — написать свой шаблон запроса на отправку тестового пуша, который будет эмулировать работу бэкенда. При этом нам важно было оставить  возможность кастомизировать payload, чтобы независимо от инфраструктуры компании разрабатывать клиентскую часть пушей. В ходе решения задачи выяснилось, что Cloud Messaging не так уж и прост — пришлось немало потрудиться, чтобы в нем разобраться. Если вы собираетесь разрабатывать подобные запросы, наша статья поможет вам сэкономить время и силы.Материал рассчитан на тех, кто уже имеет некоторые навыки в программировании и в целом знаком с iOS, командной строкой, сталкивался с сервисами Firebase и понимает, как устроены пуш-уведомления.Отправка пушей из FirebaseДля отправки пуша можно использовать:Notifications composer, который позволяет отправить пуш прямо из вашего проекта вFirebase Console.Admin SDK, который требуется интегрировать в собственный сервер, чтобы использовать.Сервисные протоколы:FCM HTTP v1 API;LegacyHTTP protocol;LegacyXMPP  protocol.Notifications composerпрост. Но он не такой гибкий, как другие, поэтому он нам не подошел.Admin SDKтребует дополнительных усилий по его развертыванию, и мы не были готовы с ним работать.XMPP Protocolсчитается устаревшим: в сетипишут, что в работе с ним возникают сложности, поэтому мы и от него отказались.ОстаютсяCloud Messaging API (Legacy)иFirebase Cloud Messaging API (V1),так как они позволяют довольно просто описать и отправить запрос. Их мы и решили рассмотреть.FCM TokenНезависимо от того, какой из API мы используем, нам понадобитсяFCM Token. Это строка, которая возвращается к нам после успешной регистрации устройства в Firebase. Токен служит адресом устройства, на который будем отправлять пуш. Подробнее о токене и о том, где его взять, можно почитать вSet up a Firebase Cloud Messaging client app onв разделе документации.Теперь посмотрим, как работает каждый из API и как выбрать подходящий.API (Legacy)Если через пуш вы хотите передать данные в виде «ключ/значение», то лучше воспользоваться API (Legacy) — с ним легче работать.'{
  'aps': {
    'alert': {
      'title': 'Title text',
      'body': 'Body text'
 },
    'sound': 'default',
    'badge': 1,
    'mutable-content': 1
  },
  'custom_key': 'custom_value'
}'Потребуется просто подставитьServer keyв авторизационный хедер запроса.--header 'Authorization: key=Server key'Найти Server key можнов настройках вашего проекта в Firebase. Для этого:Открываем настройки.Переходим во вкладкуCloud Messaging.В поле Server keyкопируем токен, состоящий из рандомного набора символов, как отмечено на скриншотах ниже.Теперь, когда мы получили два ключа — FCM Token и Server key, мы можемсобрать свой запрос в любой программе для тестирования http-запросов. Например, вPostman(графическая утилита). Но для лаконичности, чтобы наглядно описать все атрибуты запроса в одном месте, мы воспользуемсяcurl(консольная утилита).ИспользуяCloud Messaging API (Legacy), мы можем написать запрос так:curl --location 
--request POST 'https://fcm.googleapis.com/fcm/send' \
--header 'Authorization: key=Server key' \
--header 'Content-Type: application/json' \
--data-raw '{
    'to': 'fcmToken',
    'notification': {
        'mutable_content': true,
	  'sound' 'default',
        'title': 'Title',
	  'body': 'Body',
    },
}'Осталось вставить запрос в терминал и отправить его. Если все настроено правильно, на устройстве появится пуш-уведомление. Либо, если мы допустили опечатку, получим в ответе ошибку. И скорее всего, на первый запрос вам вернется именно ошибка. В описании ошибки будет полезная информация, которая поможет вам найти причину неудачи.API (V1)Для случая, когда в теле пуша вам необходимо передать не просто список параметров в виде «ключ/значение», аструктуру с несколькими уровнями вложенности, можно использовать современноеAPI (V1). Это немногосложнее, но только в части получения ключа авторизации (запрос V1 использует OAuth2 Access Token). В целом, разницу между этими подходами можно посмотреть на примерах миграцииfrom legacy to v1, чтобы наглядно увидеть возможности современного API.Авторизационный токендля такого запроса, согласно разделуAuthorize send requests, можно получить вручную или с помощьюAdmin SDK. Но мы сделаем проще ивоспользуемся сервисомOAuth 2.0 Playground, который сгенерирует нам кратковременный токен для нашего аккаунта Firebase.Для получения токена нужно перейти в сервисOAuth 2.0 Playgroundи проделать следующие шаги:Шаг 1В разделеFirebase Cloud Messaging API v1нужно отметить пунктhttps://www.googleapis.com/auth/firebase.messagingи нажать синюю кнопкуAuthorize APIs.После этого сервис запросит разрешение на доступ к аккаунту Firebase. Нужноразрешить доступ, чтобы сервис смог сгенерировать временный Access Token.Шаг 2Генерируем токен. Для этого нужно нажать синюю кнопкуExchange authorization code for tokens. В результате в ответе консоли мы увидим токен, как отмечено на скриншоте. Либо там будет ошибка, которая опишет, что пошло не так.Теперь, когда мы получили авторизационный токен, нужносоставить правильный URLдля запроса. Он содержитProject ID, который мы берем в настройках проекта:Открываем настройки.Во вкладке General в строке Project ID копируем значение и вставляем его в URL, как показано на скриншотах.https://fcm.googleapis.com/v1/Project ID/messages:sendКогда мы подготовили OAuth Token и URL, можновоспользоватьсяFCM REST APIисоставить требуемый json. Например, если в теле пуша вам потребуется передать объект nested_struct, то вы можете написать и отправить такой запрос:curl -X POST -H 'Authorization: Bearer oauth_access_token' 
  -H 'Content-Type: application/json' 
  -d '{ 
      'message': { 
          'token':'fcmToken',
          'apns': {
              'headers': {
                  'apns-priority': '10',
                  'apns-id': 'uuid',
                  'apns-push-type': 'alert',
               },
               'payload': {
                   'aps': {
                      'mutable-content': 1,
                       'sound' 'default',
                     'alert': {
                           'title': 'Title',
                           'body': Body',
                       },
                       'nested_struct': {
                           'type': 'notification_type',
                           'uchi_unique_key': 'uuid',
                           'another': 'another',
                        },
                    },
                },
            },
        },
   }' https://fcm.googleapis.com/v1/projects/project-id/messages:send HTTP/1.1В ответе на запрос придут данные, которые можно будет декодировать в такую структуру:struct PushEntity: Codable {
  let googleSenderId: String
  let gcmMessageId: String
  let google: String
  let aps: Aps

  enum CodingKeys: String, CodingKey {
    case googleSenderId = ""google.c.sender.id""
    case gcmMessageId = ""gcm.message_id""
    case google = ""google.c.a.e""
    case aps
  }
}

extension PushEntity {
  struct Aps: Codable {
    let alert: Alert
    let nestedStruct: [String: String?]


    enum CodingKeys: String, CodingKey {
        case alert
        case nestedStruct = ""nested_struct""
    }
  }
}

extension PushEntity.Aps {
  struct Alert: Codable {
    let body: String
    let title: String
  }
}Если не получилось декодировать, то причину можно будет понять из описания ошибки, которая появится в консоли.Зачем нужно оборачиватьpayload в отдельный объект? Мы, например, таким образом инкапсулируем логику обработки данных из пуша для каждого отдельного приложения. Тогда логика обработки пушей у нас остается общей для всех приложений и никогда не меняется.ЗаключениеИтак, вы узнали, какие возможности открываются при разработке отправки тестовых пушей для iOS и как использовать различные виды API для отправки подобных запросов.Теперь вам будет намного легче найти подходящий для вас вариант, чтобы написать запросы для экспериментов с пуш-уведомлениями.Успешной вам разработки!Хочешь развивать школьный EdTech вместе с нами —присоединяйсяк команде Учи.ру!"
СберМаркет,,,Специфика DataOPS в Учи.ру,2023-02-14T13:19:16.000Z,"Привет, Хабр! Меня зовут Сергей Поляков и я DataOps‑инженер в Учи.ру. Наша платформа объединяет почти 19 млн пользователей, которые совершают сотни миллионов действий. При этом нам важно хранить эти данные, чтобы совершенствовать продукт. Главная задача Data‑инженеров — поддерживать стабильную инфраструктуру и внедрять инструменты для централизованной работы с данными. Я расскажу, какие решения по автоматизации и DevOps‑практики мы используем для этого.Инфраструктура и источники данныхНаша инфраструктура располагается на проекте Data Warehouse. Примерный объем хранилища данных — около 100 ТБ и ежедневная дельта — в районе 10 ТБ. Все данные мы размещаем в облачном хранилище от Selectel.Основными источниками данных являются:PostgreSQL;Kafka;Google Sheets;Rest API.Извлечение и обработка данныхМы используем основной ELT‑инструмент (инструмент для переноса данных из разных источников в хранилище) Spark версии 3.2.0. Применяемый тип загрузки — batch, чаще всего в T1. Для централизованного управления данными есть оркестратор Airflow версии 1.10.15.Доставка данных осуществляется через классический процесс ELT (extract, load, transform). С его помощью происходит извлечение данных из источников. Далее они загружаются в необработанном виде в RAW слой Data Warehouse в S3 контейнер Selectel, где размещаются в хранилище в формате Apache Parquet.После мы уже обрабатываем данные и трансформируем их в storage, CDM, DataMart. Обработанные данные загружаются в ClickHouse — финальный агрегатор данных в Учи.ру, на базе которого строятся дашборды в Tableau и ведется работа в Jupyter Notebook в связке с Thrift server.Технологии для хранения, обработки и доставки данныхApache Spark— основной инструмент для работы с большими данными. Это свободно распространяемый набор утилит, библиотек и фреймворк для разработки и выполнения распределенных программ, работающих на кластерах из сотен и тысяч узлов. Кластер отвечает за хранение и обработку больших данных и является проектом верхнего уровня фонда Apache Software Foundation.Наш текущий кластер состоит из 70 крупных нод, обладающих следующими характеристиками:16 виртуальных процессоров;32 ГБ оперативной памяти;400 ГБ дискового пространства;дополнительный диск для Shuffle.Подготовка всех нод осуществляется через Ansible.Инструменты IaCМы используем подход IaC — «infrastructure as code». С его помощью снижается время разворачивания экземпляров инфраструктуры и обеспечивается ее хранение в репозиториях для достижения максимальной прозрачности и контролируемости изменений.Также это сводит к минимуму риск человеческих ошибок и существенно уменьшает необходимое количество рутинных операций при работе с ней. Именно поэтому в качестве репозитория для хранения всего нашего описания инфраструктуры мы используем GitHub, для развертывания конфигурации — Ansible, а для создания инфраструктуры — HashiCorp Terraform.HashiCorp TerraformИспользуя Terraform, мы создаем экземпляры виртуальных машин, диски, сетевые интерфейсы и DNS‑записи. Хранение файлов состояния Terraform (tfstate) осуществляется в S3 хранилище от Selectel.Ранее мы не использовали управляемое решение Kubernetes, поэтому создавали и настраивали каждую ноду отдельно через Terraform и Ansible. После перехода на облако Selectel мы пришли к выводу, что нам больше подходит сервис управляемого кластера Kubernetes, который мы также создаем через Terraform.Ansible и его роль в кластере HadoopС помощью Ansible происходит развертывание конфигураций. Для этого мы используем некоторыеплейбуки Ansible(базовые компоненты, которые записывают и исполняют конфигурацию):Spark/Hadoopустанавливается с нуля. Конфигурация и управление происходят, когда необходимо применить изменения.Workstation.yamlиспользуется для подготовки типового рабочего места data‑инженеров, что ускоряет их подготовку и позволяет достичь единообразия.Thrift.yaml— это кластер Apache Spark Thrift для аналитических запросов. При этом он также устанавливается полностью с нуля. Он конфигурируется под наши нужды и управляется, когда необходимо внести изменения.Еще с помощью роли Ansible и Jinja‑шаблонов в кластере Hadoop мы:определяем master и worker ноды;скачиваем исходные коды Spark, Hadoop и необходимые для них библиотеки;конфигурируем и подкладываем готовые файлы конфигурации — core site, hive site и spark defaults;загружаем переменные среды и настраиваем очереди Fair Scheduler.Поскольку Ansible использует декларативный подход, его роль предполагает полное удаление конфигурации Spark и Hadoop для перенастройки. Однако некоторые операции — например, перезапуск кластера — могут быть проведены выборочно с использованием тегов.Рабочее пространство Data-инженеровОсновой рабочего пространства Data‑инженера является Docker с запущенными сервисами Airflow, Spark и Jupyter. Запуск локальной копии Airflow для разработки осуществляется двумя командами — build и run.Именно build собирает образы Airflow со всеми зависимостями, раскладывает DAG‑файлы и плагины, собирает образ Apache Spark из базового образа. Его сборка происходит с помощью GitHub Actions в отдельном репозитории.В сборку из базового образа подкладываются библиотеки для подключения к объектному хранилищу S3, а также ключи доступа, конфигурация endpoint и другие настройки.Готовый образ помещается в развернутый Docker Registry Harbor. Он выбран после длительного сравнения существующих решений. И для нас оказался наиболее подходящим, потому как у него удобный веб‑интерфейс, позволяющий просматривать имеющиеся Docker образы. Harbor размещается в кластере Kubernetes с помощью стандартного helm chart, что позволяет с легкостью обновлять его при необходимости.Рабочее пространство аналитиков и BIНаши аналитики и BI‑специалисты используют Jupyter Notebook для анализа данных и построения моделей.Jupyter Notebook — это интерактивная среда разработки с живым кодом, позволяющая подключаться к Spark кластеру через PySpark. Jupyter Notebook отвечает за визуализацию работы. Если разработчик хочет посмотреть на график или формулу, он пишет нужную команду в соответствующей ячейке, и сразу же видит результат выполнения. Это экономит время и помогает избежать ошибок.Для работы с Jupyter Notebook у нас есть два инстанса: JupyterLab для совместной работы и JupyterHub для одиночной работы. Образы Docker Jupyter не требуют сборки на постоянной основе. Поэтому создаются по запросу добавления нового функционала или библиотек в Python.В ENTRYPOINT помимо запуска самого Jupyter проводится pip upgrade библиотеки, разработанной аналитиками Учи.ру. Так, при перезапуске пода Jupyter аналитики получают наиболее актуальную версию своей библиотеки.Еще для развертывания Jupyter у нас есть собственный Helm Chart, предусматривающий обновление переменных среды. С его помощью аналитики подключаются к различным источникам данных: S3, Spark, к внешним API и к кластеру ClickHouse.И здесь для доступа к Jupyter мы используем Ingress‑NGINX Controller, а также балансировщик нагрузки (Load Balancer). DNS A‑записи создаются через Terraform с помощью провайдера Gcore.Одна из трудностей Jupyter в кластере Kubernetes, с которой мы столкнулись — персистентность (в нашем случае это хранение notebooks, которые создают аналитики). Наши аналитики хранят огромные data frame и архивы, которые могут занимать терабайты пространства. И постоянно увеличивать объем памяти на сетевых дисках трудозатратно. Поэтому мы поступили так: подключили S3 прямо в папку Jupyter. Для этого мы использовали драйвера S3 от «Яндекса» k8s‑csi‑s3. Далее мы создали StorageClass, добавили небольшое количество строк в манифест deployment Jupyter и описали манифест PersistentVolumeClaim — PVC для нового StorageClass.Поддержка инфраструктурыThrift server — это интерфейс, позволяющий через JDBC драйвер выполнять sql‑запросы на кластере Spark. Мы включили в нем поддержку Hive metastore для эмуляции external таблиц на базе файлов в S3.Собственная роль Ansible позволяет настроить кластер с разделением прав доступа на ReadWrite и ReadOnly. Разделение прав доступа осуществляется с помощью Jinja‑шаблона, который размещает конфигурационные файлы.Дополнительно Ansible размещает systemd‑службы для Thrift server с определенными шаблонами. Еще с его помощью мы:делаем резервные копии контейнеров S3;настраиваем Standalone Harbor, Node Exporter для сбора метрик и Promtail для сбора логов;настраиваем кластеры ClickHouse и ZooKeeper;создаем и добавляем пользователей в ClickHouse.Таким образом, с помощью Terraform и Ansible мы поддерживаем подход «infrastructure as code». GitHub позволяет нам отслеживать все изменения и не потерять наш код.Мониторинг и логированиеМы используем современные системы мониторинга Prometheus, Alertmanager и Grafana, которые внедрены с помощью Kube Prometheus Stack, Prometheus Operator и ServiceMonitor.Во всех этих решениях также реализован подход «infrastructure as code». Поэтому, например, JSON дашборды мы сохраняем в чарте Kube Prometheus Stack, а также существует дополнительно настроенный GitHub Actions на развертывание всего чарта Kube Prometheus Stack.В версии 1.21 Kubernetes недоступно подключение Target, поэтому для этой задачи мы используем ServiceMonitor.Однако если ServiceMonitor может подключить Endpoint, то для сбора метрик с DWH‑приложений нам требуются различные экспортеры метрик. Поэтому для Airflow мы используем StatsD, который запускается как sidecar container в подах Airflow в кластере Kubernetes. Spark же использует библиотеку метрик Dropwizard, которую мы собираем с помощью JMX exporter.Еще мы столкнулись со сложностями при внедрении мониторинга. Получившийся манифест был настолько велик, что Kubernetes не мог хранить его у себя. Поэтому нам пришлось использовать другой backend в PostgreSQL. Также в момент запуска большого количества задач в Prometheus поступает огромное количество метрик. И здесь важно выбрать только необходимые для последующего анализа метрики, иначе нагрузка на Prometheus будет крайне большой.Для логированиямы используем Grafana Loki. В Grafana подключен источник данных Loki. А на хосты установлен Promtail binary, с помощью которого удобно собираются логи из разных директорий, где потом они отображаются в Grafana с фильтрацией.Для CI/CD мы используем GitHub Actions. Для этого мы выделили виртуальный инстанс под GitHub runner и написали systemd‑юнит. Так для запуска GitHub Actions достаточно только описать workflow и положить его в папку.github/workflows.GitHub Actions используется для сборки базового Docker образа Apache Spark. Runner самостоятельно скачивает архив Spark, распаковывает и собирает образ, подкладывая все нужные настройки. Версии автоматической сборки также контролируются с помощью Github Actions при помощи stage workflow и Github.Еще Github Actions используется для:обновления сенсоров БД PostgreSQL и Redis;обновления файлов DAG в Airflow;сборки и отправки Docker образа Airflow в тестовый кластер Kubernetes;развертывания Kube Prometheus Stack после слияния и обновлений.Вывод и планыВсе перечисленные технологии и инструменты обеспечивают нам централизованную работу с данными и поддерживают инфраструктуру. Сейчас мы тестируем внедрение всех наших продуктовых инструментов в кластер Kubernetes. И для нас это довольно объемная задача, потому как потребуется кардинально обновить существующую версию Airflow, а также некоторые инструменты. Ведь, например, Apache Spark мало кто внедрял в Kubernetes, потому нам приходится идти путем проб и ошибок. Но уже сейчас мы очень близки к полной замене Thrift серверов на размещенный в Kubernetes‑кластере Apache Kyuubi.Если ты разделяешь наш подход к работе с большими данными,присоединяйся, чтобы вместе с нами развивать школьный EdTech."
СберМаркет,,,"Хуки в React — как надо, когда не надо и нужны ли свои?",2023-02-10T11:20:02.000Z,"В 16.8 версии библиотеки React впервые появились хуки (hooks) — функции, которые упрощают работу с компонентами React и переиспользованием какой-либо логики. В экосистеме React уже есть много дефолтных хуков, но также можно создавать и свои. Я Михаил Карямин, фронтенд-разработчик в Учи.ру, расскажу, как и в каких случаях хуки в React облегчают жизнь разработчику и как с ними работать.React без хуков и с нимиЧтобы понять, почему хуки упрощают жизнь разработчику, надо посмотреть на то, как писался React раньше. Он был на классовых компонентах: есть стандартный метод рендер, который отвечает за разметку, есть поле state, где хранится объект и все его состояния, есть какие-то свои методы и есть методы жизненных циклов. Всё это выглядит очень громоздко, и практически не актуально.Классовый компонентclass Welcome extends React.Component {
  state = {
    money: 0
  };

  increaseMoney() {
    this.setState((prevState) => ({
      money: prevState.money++
    }));
  }

  render() {
    return (
      <div>
        Привет, {this.props.name} у тебя {this.state.money}
        <button onClick={this.increaseMoney}>Добавить денег</button>
      </div>
    );
  }
}Многие сегодняшние проекты React пишутся уже на функциональных компонентах. Есть функция, которая возвращает разметку, и внутри функции есть хуки для хранения состояния (state) или хуки для логики.Функциональный компонент — newfunction Welcome(props) {
  const [money, increaseMoney] = React.useState(100);
  const onClick = () => {
    increaseMoney((prevMoney) => prevMoney++);
  };

  return (
    <div>
      Привет, {props.name} у тебя {money}
      <button onClick={onClick}>Добавить денег</button>
    </div>
  );
}Важно:несмотря на то что React в классовом виде встречается редко, почитать, как это работает, будет все же не лишним. Возможно, вам придется поддерживать написанный таким способом проект.До хуков в классовых компонентах для хранения общей переиспользуемой логики самыми распространенными вариантами были так называемые higher-order component (HOC). Это функция, которая оборачивает обычные классовые компоненты. В качестве аргумента она принимает компонент, к которому нужна какая-то переиспользуемая логика. HOC тяжело читаются, во время учебы я долго не мог понять, как тут всё взаимосвязано и куда что передается.Higher-Order Componentconst withFetch = (WrappedComponent) => {
  class WithFetch extends React.Component {
    constructor(props) {
      super(props);
      this.state = {
        movies: []
      };
    }

    componentDidMount() {
      fetch(""http://json-faker.onrender.com/movies"")
        .then((response) => response.json())
        .then((data) => {
          this.setState({ movies: data.movies });
        });
    }

    render() {
      return (
        <>
          {this.state.movies.length > 0 && (
            <WrappedComponent movies={this.state.movies} />
          )}
        </>
      );
    }
  }

  WithFetch.displayName = `WithFetch(${WithFetch.name})`;

  return WithFetch;
};Компонент с Higher-Order Componentimport MovieContainer from ""../component/MovieContainer"";
import withFetch from ""./MovieWrapper"";

class MovieListWithHOC extends React.Component {
  constructor(props) {
    super(props);
  }

  render() {
    return (
      <div>
        <h2>Movie list - with HOC</h2>
        <MovieContainer data={this.props.movies} />
      </div>
    );
  }
}

export default withFetch(MovieListWithHOC);Сегодня вместо HOC используются хуки. Компонент с хуком смотрится намного компактнее и понятнее: вся логика занимает одну строчку «const [loading, data] = useFetch(MOVIE_URI)» — хук возвращает текущее состояние и данные. А если нужны несколько переиспользуемых бизнес-логик, можно просто добавить еще одну строчку и появится дополнительный компонент. В случае с HOC пришлось бы оборачивать компоненты: это не очень красиво и тяжело читается.Hookconst useFetch = (url) => {
  const [data, setData] = React.useState(null);
  const [loading, setLoading] = React.useState(true);

  useEffect(() => {
    const fetchData = async () => {
      const response = await fetch(url);
      const data = await response.json();
      setData(data);
      setLoading(false);
    };
    fetchData();
  }, [url]);

  return [loading, data];
};Компонент с хукомimport { useFetch } from ""../hooks/useFetch"";
import Movie from ""../components/Movie"";

const MovieWithHook = () => {
  const MOVIE_URI = ""http://json-faker.onrender.com/movies"";
  const [loading, data] = useFetch(MOVIE_URI);

  return (
    <div>
      <h2>Moview with hook</h2>
      {loading ? <h3>loading...</h3> : <Movie data={data.movies} />}
    </div>
  );
};Как пишется собственный хукПервая итерацияПредлагаю попробовать написать хук и посмотреть, как работает концепт React в рамках одного хука. Для этого возьмем базовый useState, отвечающий за состояние.Заводим обычную переменную someState, задаем первоначальное значение — пусть будет «0». Добавляем функцию, которая будет ее увеличивать на 1 и возвращать актуальное состояние к переменной. Проверяем в console.log: пишем increaseState и вызываем несколько раз. Все работает, state обновляется — пошел отсчет 1, 2, 3, 4, 5.Одна проблема: state не защищен, его легко сломать случайно или намеренно. Например, напишем someState = 100. И вместо ожидаемой «5» получим «102». Если бы строчек было много, долго бы пришлось искать, где баг. Чтобы это исправить, переменную надо инкапсулировать, поместить в функцию. Но теперь JS ругается, так как someState оказался в области видимости функции, а не в глобальной.Исправляем синтаксическую ошибку, и теперь state не обновляется, поскольку при каждом вызове функции у нас идет инициализация переменной, и она всегда равна нулю. Для решения проблемы будем вместо переменной возвращать функцию, которая имеет доступ к нашей внутренней переменной. Раз теперь функция не просто увеличивает state, а возвращает функцию, ее стоит переименовать в getIncreaseState и добавить переменную, в которой будет записан increaseState.На этом этапе мы получили реализацию функции, которая может хранить какой-то state и изменять его.const increaseState = (() => {
  let someState = 0;

  return () => {
    someState = someState + 1;
    return someState;
  }
})()

console.log(increaseState())
console.log(increaseState())
console.log(increaseState())
console.log(increaseState())
console.log(increaseState())Вторая итерацияТеперь надо написать функцию useState. Она принимает в качестве аргумента первоначальное состояние – initialValue. Хук возвращает массив, который состоит из двух элементов: сначала state, а потом функцию, которая изменяет setState.Добавляем в тело функции переменную value, где будем хранить значение, и заведем константу под state — она равна value. Объявим функцию, которая будет изменять наш state и принимать newValue. И внутри этой функции просто переписываем value на newValue.Чтобы проверить работоспособность, вводим count – state, и setCount — функция, которая будет менять наш state. Count — «0», как initialValue. Меняем его на «2», но переменная в console.log не меняется.В чем проблема? При деструктуризации массива в JS создаются константы. Наш count «0» записался на 29 строке и не изменится, так как доступа к внутреннему состоянию функции state у нас нет. Count «2» на 32 строке — это новая переменная, внутреннее состояние мы не видим.Самый простой способ это исправить — вместо переменной из useState возвращать функцию, у которой в момент вызова в замыкании есть value. Так мы увидим внутреннее состояние useState. Для этого прописываем обычную стрелочную функцию и меняем переменные: вместо count поставим getCount. Теперь состояние обновляется.Функция уже выглядит почти как хук, но вместо обычной переменной первым элементом она возвращает функцию. А нам надо написать полный аналог хука — дефолтного useState.const useState = (intialValue) => {
  let value = intialValue;
  const getState = () => value;
  const setState = (newValue) => {
    value = newValue
  }

  return [getState, setState];
}

const [getCount, setCount] = useState(0)
console.log(getCount())
setCount(2)
console.log(getCount())Третья итерацияЗаведем немедленно вызываемую функцию (IIFE) и назовем ее React. В ее теле будет уже написанный хук, только без функции, возвращающей значение, и добавим в тело React переменную value без изначального значения. State будет «value || initialValue». Возвращать будем state, а из функции React возвращаем объект. Первое поле этого объекта как раз наш прототип хука — useState.Напишем компонент — назовем Component и добавим внутрь хук, который будет записывать имя: вносим name и setName. Первоначальное значение — «Mike». Если бы у нас было взаимодействие с реальным DOM-ом, функция бы отрисовывала нам изменения в разметку. Но его нет, поэтому будем возвращать функцию render и выводить текущее состояние в console.log.Также мы будем возвращать импровизированное взаимодействие пользователя с каким-то input: например, с формой ввода имени. Называем ее changeName. В качестве аргумента она принимает новое имя, которое вводит пользователь, и передает в хук.Получился прототип компонента, который осталось связать с React. Для этого добавляем функцию, называем ее render, в качестве аргумента она принимает Сomponent. В ее теле вызывается сomponent, у него будет вызываться метод render, который отрисовывает консоль и возвращает component.Заводим переменную: называем app, связываем с React и отрендерим компонент — «Mike» вывелось в консоли. Пробуем поменять имя через App.changeName, подставляем «Vasya», делаем новый render. Все четко, «Vasya» вывелся в консоли. Реализация работает, есть прототип React и компонента. Но в реальном приложении мы часто используем несколько раз в одном компоненте, поэтому будем усложнять задачу.Добавим фамилию и взаимодействие с inpit в return — changeSurname. Но теперь в консоли при изменении имени у нас меняется и фамилия. Если фамилию меняем — аналогично. А должно только имя или только фамилия.Где проблема? Ответ кроется в функции React. Во 2 строчке есть переменная, в которую записываются все вызовы useState, и когда вызов был 1, все отлично. Но как только их становится 2, текущий value перезаписывается. Нужно завести в хук массив states вместо переменной value и добавить переменную index, потому что хук вызываем несколько раз, и надо понимать, какой вызов какому элементу массива принадлежит.Если в хук завести console.log, можно посмотреть, что хранится в массиве. Оказывается, вместо 1 элемента здесь 2. Надо добавить увеличение индекса, который при каждом вызове должен прибавлять единицу, и сделать сбрасывание индекса при каждом рендере. Иначе каждый раз при срабатывании хука предыдущее состояние будет не перезаписываться, а добавляться в конец массива. В итоге будет расти количество элементов в массиве.Остается последняя проблема, связанная с замыканием. Из хука мы возвращаем не вызов функции, а просто ссылку на нее. Когда у нас происходит функция render, хук вызывается дважды: сначала он 0, потом 1. На следующем вызове, с changeSurname, он уже 2. Потому что замыкание — это все переменные, доступные в момент вызова функции.Чтобы это исправить, надо сохранить актуальное состояние индекса внутри useState в момент инициализации. И когда мы вызовем функцию setState, мы уже будем брать не глобальный индекс, который равен 2, а будем использовать именно внутренний, так как он будет верный.Теперь в консоли все работает, и у нас есть простой функциональный хук.const React = (() => {
  const states = [];
  let idx = 0;

  const useState = (intialValue) => {
    const state = states[idx] || intialValue;
    const _idx = idx;
    const setState = (newValue) => {
      states[_idx] = newValue;
    };

    idx++;
    return [state, setState];
  };

  const render = (Component) => {
    idx = 0;
    const component = Component();
    component.render();

    return component;
  };

  return {
    useState,
    render
  };
})();

const Component = () => {
  const [name, setName] = React.useState(""Mike"");
  const [surname, setSurname] = React.useState(""Petrov"");

  return {
    render: () => console.log(`${name} ${surname}`),
    changeName: (newName) => setName(newName),
    changeSurname: (newSurname) => setSurname(newSurname)
  };
};

let App = React.render(Component);

App = React.render(Component);
App.changeName(""Petya"");
App = React.render(Component);Кастомные хуки и их применениеВсе кастомные хуки состоят из дефолтных: useState, useEffect, useReducer, useMemo, useCallback. Их можно классифицировать в зависимости от проекта и бизнес-задач. Но я делю кастомные хуки по принципу использования на 6 категорий.Первая— listeners. Это обширная группа, к которой можно отнести хуки, которые ловят клик пользователя, положение экрана мобильного устройства, геолокацию и так далее.Вторая— UI хуки. Они нужны для работы с CSS, с аудио, с видео.Третья— side-effects. Хуки, которые работают вне основного потока приложения. Например, они нужны для работы с асинхронностью, с local storage, для изменения title страницы.Четвертая— lifecycles. В классовых компонентах очень много инструментов для работы с жизненными циклами, а в функциональных есть только useEffect. Так что приходится часто дописывать хуки: например, useMount, который срабатывает только при монтировании, или useUpdate, который имитирует работу компонента DidUpdate.Пятая— state. Хуки для удобной работы с состоянием отдельных компонентов и с глобальным состоянием. Такие хуки есть, например, в Redux.Шестая— animations. Хуки для работы с request animation frame, интервалом, таймаутом. Самая непопулярная группа.Эта разбивка очень условная, жестких границ у групп нет: в той же категории UI могут быть не только хуки для CSS, аудио и видео.Как это все выглядит на практике: возьмем хук, отвечающий за переключение темы на сайте или в приложении. Часто это переключение происходит или от системных настроек, или от отдельной кнопки. Чтобы сменить тему, нам нужен компонент с useDarkMode — хук, который возвращает переключение, включение, выключение и текущее состояние.Под капотом у него несколько вспомогательных хуков:useMediaQuery— помогает узнать, какая предпочтительная тема у пользователя. Он состоит из дефолтных useState и useEffect. В первой строчке прописываем текущее состояние, а во второй создаем функцию Callback и подписываемся на изменение медиавыражения — чтобы всегда иметь актуальное состояние;useIsFirstRender— основан на useRef. При первом его срабатывании мы заходим в условия и переписываем из isFirstRender current = false. При ре-рендере этот хук вернет false, и мы попадем, куда нам нужно;useUpdateEffect — он почти аналогичен стандартному useEffect, но не срабатывает при первом рендере. В классовых компонентах был componentDidUpdate, в функциональных его нет, и приходится придумывать что-то для замены.Кроме этого useDarkMode использует дефолтный хук useCallback. Благодаря ему при перерисовке у нас сохранится ссылка на функцию, которую мы обернули. При перерисовке в React у нас происходит переинициализация функции, и без useCallback наш оптимизированный компонент посчитает, что у нас изменился prop, и сам перерисуется.В итоге у нас простая цепочка: в теле хука useMediaQuery показывает, какую тему предпочитает пользователь. Потом хук useLocalStorage помогает внести его выбор в local storage, и при перегрузке страницы не будет морганий — нужная тема сразу включится. Дальше если у пользователя обновится предпочтение, сработает useUpdateEffect. А функция возвращает 3 Callback и текущее состояние.Тонкости useEffectХук useEffect — один из самых любопытных, он заменяет 3 метода жизненного цикла в классовых. Предлагаю разобрать один из распространенных кейсов с useEffect.Есть компонент, в котором мы что-то отрисовываем на основе данных, полученных с сервера. Основные props – это name, surname и number. Меняется один из props — срабатывает запрос сервера, мы это отрисовываем. И здесь может получиться так, что изменился 1 props, а сработали сразу все 3. Возникает лишняя нагрузка на сервер, могут появиться баги, потому что данные придут не в той последовательности. Чтобы хук срабатывал только для конкретного props, надо сделать 3 разных useEffect.function Example({ currentType, name, surname, number }) {
  const [infoByName, setInfoByName] = React.useState();
  const [infoBySurname, setInfoBySurname] = React.useState();
  const [infoByNumber, setInfoByNumber] = React.useState();

  React.useEffect(() => {
    const fetchByName = async () => {
      const response = await fetch(""URI"");
      const data = await response.json();
      setInfoByName(data);
    };
    fetchByName();
  }, [name]);

  React.useEffect(() => {
    const fetchBySurname = async () => {
      const response = await fetch(""URI"");
      const data = await response.json();
      setInfoBySurname(data);
    };
    fetchBySurname();
  }, [surname]);

  React.useEffect(() => {
    const fetchByNumber = async () => {
      const response = await fetch(""URI"");
      const data = await response.json();
      setInfoByNumber(data);
    };
    fetchByNumber();
  }, [number]);

  return (
    <div>
      {currentType === ""name"" && <div>{infoByName}</div>}
      {currentType === ""surname"" && <div>{infoBySurname}</div>}
      {currentType === ""number"" && <div>{infoByNumber}</div>}
    </div>
  );
}Одна из интересных особенностей useEffect — если вторым аргументом передать пустой массив, эффект сработает всего раз: при монтировании и размонтировании. На практике, если я проверяю, у меня вышел не 1 рендер, а 2. Смотрим в changelog React и видим «Stricter strict mode», строгий режим стал строже. С марта 2022 года React стал автоматически размонтировать и обратно монтировать каждый компонент при первом рендере.const BadUseEffectOnce = () => {
  const [count, setCount] = useState(0);

  React.useEffect(() => {
    setCount((prevCount) => prevCount + 1);
  }, []);

  return <div>{count}</div>;
};

export default function App() {
  return (
    <div className=""app"">
      <React.StrictMode>
        <h1>
          Количество ререндеров: <BadUseEffectOnce />
        </h1>
      </React.StrictMode>
    </div>
  );
}Чтобы это исправить, можно просто отключить strict mode, но я не советую так делать, особенно если ваш проект будет развиваться еще несколько лет, и вам, возможно, придется обновлять версию React. Strict mode нужен, чтобы мы могли увидеть узкие места в приложении, которые в текущей версии React не вызывают багов, но могут вызвать в следующей. Strict mode заранее предупреждает, что это нужно исправить. Если его отключить, потом с большой долей вероятности вам на голову свалится куча неожиданных багов, и вы не сможете просто и легко обновиться до более новой версии React.const BadUseEffectOnce = () => {
  const [count, setCount] = useState(0);

  React.useEffect(() => {
    setCount((prevCount) => prevCount + 1);
  }, []);

  return <div>{count}</div>;
};

export default function App() {
  return (
    <div className=""app"">
      <h1>
        Количество ререндеров: <BadUseEffectOnce />
      </h1>
    </div>
  );
}Выход простой — использовать useRef и записывать, что при первом рендере заходим в условия, которые находятся в useEffect. Мы выполняем нашу логику функции и записываем: isFirstRender = false. В итоге, при первоначальной реализации было 2 рендера, а сейчас 1.const GoodUseEffectOnce = () => {
  const [count, setCount] = useState(0);
  const isFirstRender = React.useRef(true);

  React.useEffect(() => {
    if (isFirstRender.current) {
      setCount((prevCount) => prevCount + 1);
      isFirstRender.current = false;
    }
  }, []);

  return <div>{count}</div>;
};

export default function App() {
  return (
    <div className=""app"">
      <React.StrictMode>
        <h1>
          Количество ререндеров (Bad): <BadUseEffectOnce />
          Количество ререндеров (Good): <GoodUseEffectOnce />
        </h1>
      </React.StrictMode>
    </div>
  );
}Последний пример с useEffect — когда нужно подписаться на какое-то событие с помощью этого хука. Например, на получение каких-то данных с удаленного сервера или на клик пользователя. И здесь вылезает баг: я кликаю 1 раз, но у нас показывает, будто совершено 2 клика.const BadUseEffectOnce = () => {
  const [count, setCount] = useState(0);

  React.useEffect(() => {
    document.addEventListener(""click"", () => {
      setCount((prevCount) => prevCount + 1);
    });
  }, []);

  return <div>{count}</div>;
};

export default function App() {
  return (
    <div className=""app"">
      <React.StrictMode>
        <h1>
          Количество ререндеров (Bad): <BadUseEffectOnce />
        </h1>
      </React.StrictMode>
    </div>
  );
}Самое простое решение — записать функцию в переменную. Затем при монтировании мы подписываемся на какое-то событие, а при размонтировании — отписываемся. Тогда все будет отлично работать. Если этот способ использовать не выходит, стоит перенести эту логику в какой-то state manager: Redux или MobX.const GoodUseEffectOnce = () => {
  const [count, setCount] = useState(0);

  React.useEffect(() => {
    const listener = () => {
      setCount((prevCount) => prevCount + 1);
    };
    document.addEventListener(""click"", listener);

    return () => {
      document.removeEventListener(""click"", listener);
    };
  }, []);

  return <div>{count}</div>;
};

export default function App() {
  return (
    <div className=""app"">
      <React.StrictMode>
        <h1>
          Количество ререндеров (Good): <GoodUseEffectOnce />
        </h1>
      </React.StrictMode>
    </div>
  );
}Нюансы работы с useStateПредставим, что есть пользовательский компонент, с которого надо собрать статистику: сколько кликов человек на нем делает. Внутри есть 2 функции: первая — какой-то счетчик, вторая — при выходе пользователь отправляет статистику нам на сервер. В целом, это работает, но при каждом изменении useState у нас будет происходить ре-рендер. React оптимизирован для ре-рендеров, но если компонент сложнее, чем из двух div, это будет плохо сказываться на перфомансе.Чтобы избежать этого, возьмем useRef вместо useState. В классовых компонентах его аналогом будет createRef, но в функциональных useRef полезнее и чаще применяется. В нем можно хранить state, в том числе при перерисовках, и он не вызывает ре-рендер. Так что если текущее состояние не используется где-то для отображения пользователю, лучше использовать useRef. Но если нужно обновленное состояние в разметке — берем useState.И напоследок приведу кейс, который нередко встречается на собеседованиях у джунов, пре-миддлов и даже миддлов. Если мы в одной функции будем несколько раз обновлять state и брать текущее значение из вызова хука, то очень возможны какие-то баги — state не всегда синхронно обновляется. Лучше исключить такие риски и текущее значение брать не из useState count, а из аргумента — так у него всегда будет актуальное состояние.Хочешь развивать школьный EdTech вместе с нами —присоединяйсяк команде Учи.ру!"
СберМаркет,,,Моделирование статического тиристорного компенсатора,2024-09-09T08:03:46.000Z,"В настоящее время в электроэнергетике активно развиваются и применяются интеллектуальные сети, плавно регулируемые статические компенсаторы реактивной мощности и накопители электроэнергии. Они играют важную роль в устойчивости энергосистем, повышении пропускной способности сетей и интеграции возобновляемых источников энергии. Поэтому моделирование таких систем и устройств является важным аспектом для оптимизации работы электроэнергетических систем и развития интеллектуальных систем управления в электроэнергетике.В данной статье проводится обзор модели устройства поперечной компенсации реактивной мощности первого поколения, а именно статического тиристорного компенсатора (СТК), на английском Static Var Compensator (SVC). Данное устройство входит в общее обозначение устройств FACTS (Flexible Alternative Current Transmission System – гибкие/управляемые системы электропередачи переменного тока). Модель СТК разработана на платформеREPEAT.Ссылка на телеграм-канал REPEAT:https://t.me/repeatlabВ данной статье будет рассмотрено:Назначение и область применения FACTS;Основные компоненты СТК;Пример модели однофазного тиристорно-управляемого реактора в REPEAT;Как можно улучшить модель СТК.Назначение и область применения FACTSДля бесперебойного и качественного электроснабжения потребителей, необходимо соблюдать нормы показателей качества электроэнергии, одним из которых является отклонение напряжения от номинального значения у потребителей электроэнергии. Согласно ГОСТ 32144-2013[1], эта величина не должна превышать ±10%. Величина напряжения зависит от значений перетоков реактивной мощности, которые являются непостоянными из-за переменного характера потребления электроэнергии и различных возмущений в энергосистеме. Для поддержания устойчивости энергосистемы, напряжение необходимо регулировать с помощью дополнительной выдачи или потребления реактивной мощности, и желательно делать это гибко и плавно, чтобы избежать недорегулирования или перерегулирования.В ХХ веке для плавного регулирования напряжения использовались генераторы и синхронные компенсаторы. Также применялись конденсаторные батареи и шунтирующие реакторы, но эти устройства были коммутируемые, то есть включались или выключались через коммутационные устройства, и выдача или потребление реактивной мощности происходили ступенчато.В настоящее время применяются более современные устройстваFACTS, которые созданы на базе силовой электроники. Данные устройства представляют собой статические плавно регулируемые сетевые компенсаторы реактивной мощности. Они во многом могут решить проблемы регулирования напряжения, связанные с недостатком резерва реактивной мощности, а также проблемы ограничения выдаваемой мощности электростанций, устойчивости работы нагрузки и повышенные потери на передачу электроэнергии. Отсутствие вращающихся частей позволяет снизить затраты на обслуживание, а также увеличивает срок эксплуатации. Также благодаря применению полупроводниковых ключей обеспечивается плавное регулирование реактивной мощности в требуемом диапазоне.Основные компоненты СТКСтатический тиристорный компенсаторреактивной мощности – поперечно-подключаемый статический источник или поглотитель реактивной мощности, обеспечивающий подпитку ёмкостным или индуктивным током для управления напряжением электроэнергетической системы. Внешний вид СТК представлен на рисунке 1.Рисунок 1 – СТК на подстанцииСТК позволяет как выдавать реактивную мощность за счет подключения емкости, так и потреблять за счет подключения индуктивности. Для регулирования реактивной мощности необходимо изменять реактивное сопротивление устройства, можно это делать как ступенчато, так и плавно. Для ступенчатого изменения реактивного сопротивления СТК подключаются через коммутационный аппарат, в то время как при плавном регулировании подключение происходит через управляемые тиристоры. Также к СТК может быть подключен фильтр, который позволяет снизить искажения синусоиды тока. Таким образом, СТК может включать в себя следующие компоненты:Коммутируемый реактор;Коммутируемый конденсатор;Тиристорно-управляемый реактор или тиристорно-реакторные группы (ТРГ);Тиристорно-управляемый конденсатор или тиристорно-конденсаторные группы (ТКГ);Фильтр.Более наглядное представление СТК приведено на рисунке 2 [2].Рисунок 2 – Принципиальная электрическая схема СТКВ нашем случае будет смоделировантиристорно-управляемый реакторили тиристорно-реакторная группа– тиристорно-управляемая индуктивность, реактивное сопротивление которой плавно регулируется за счет изменения степени открытия тиристорного ключа, путем изменения угла управления тиристорами. Это позволит нам осуществлять регулирование реактивной мощности СТК.Моделирование СТК в REPEATБудет произведена серия расчётов с изменением угла управления тиристорами от 0 до 180 градусов, необходимо снять осциллограммы тока и напряжения реактора, а также построить характеристику реактивной мощности, чтобы оценить влияние СТК. Далее схема будет улучшена, параллельно реактору подключим коммутируемый конденсатор для полноценного регулирования реактивной мощности.Исходные данные:Амплитудное значение напряжения однофазного источника: 12,86 кВ;Частота сети: 50 Гц;Активное сопротивление реактора: 0,01653 Ом;Индуктивность реактора: 0,026 Гн;Емкость конденсатора: 200 мкФ;Время моделирования 5 с для того, чтобы переходные процессы в схеме закончились, и мы получили больше данных для дальнейших расчетов;Шаг интегрирования: 0,05 мс, который отвечает за расчет параметров схемы, значения данного шага достаточно для СТК;Шаг дискретизации: 0,05 мс, который отвечает за отображение значений на графике, с уменьшением шага дискретизации, графики становятся более плавными;Параметры тиристора:Внутреннее сопротивление: 1×10-3Ом;Сопротивление в закрытом состоянии: 1×1012Ом;Напряжение прямого смещения: 0,8 В;Демпферное сопротивление: 1×106Ом;Демпферная емкость: 250×10-9Ф.Ниже на рисунке 3 представлена схема однофазного тиристорно-управляемого реактора в REPEAT.Рисунок 3 – Схема тиристорно-управляемого реактора в REPEATОднофазная ТРГ состоит из индуктивности (блок №4), резистора (блок №5), тиристоров (блоки №1 и №3), также для снятия осциллограмм в схеме присутствуют вольтметр и амперметр (блоки №38 и 40). В модифицированной схеме СТК параллельно ТРГ будет подключен конденсатор (блок №15) через выключатель (блок №10) для полноценного регулирования реактивной мощности.Для управления тиристорами была создана схема управления, которая приведена на рисунке 4, она присутствует на основной схеме как внутренний проект (блок №2), к ней мы подключаем константу (блок №8), в которой необходимо задавать угол управления тиристорами, и подаем сигнал напряжения с вольтметра для отслеживания пересечения нуля и сброса сигнала в интеграторе.Рисунок 4 – Схема управления ТРГ в REPEATУгол управления тиристорами отнимается от 180 градусов, которые формируются в интеграторе (им выступает ПИД-регулятор (блок №18)), и в течение получившегося значения угла тиристор пропускает ток. Чем больше угол управления, тем меньше время, за которое тиристор пропускает ток – меньше амплитуда тока и его действующее значение. Число 180 в ПИД-регуляторе создают блок пересечение нуля (блок №17) и константа со значением 18000. На блок пересечение нуля приходит сигнал переменного напряжения, и при пересечении нуля синусоидой, блок посылает сигнал амплитудой 1 в течение одного шага интегрирования. Далее мы формируем управляющий импульс, который подается на тиристоры. Этот сигнал не должен быть слишком длинным, чтобы исключить случайные отпирания тиристоров, но должен быть достаточно продолжительным, не менее 1-2 мкс, чтобы тиристоры отреагировали, поэтому сигнал из реле (блок №2) искусственно ограничивается по времени до 150 мкс.Далее рассмотрим управляющие сигналы, которые формируются в схеме управления при угле 120 градусов, они приведены на рисунке 5.Рисунок 5 – Управляющие сигналы в REPEATУправляющий импульс, равный единице, появляется в момент времени 0,0067 секунд, когда итоговый пилообразный сигнал переходит в положительную часть графика, функция сигнала пересекает ось абсцисс, соответственно, чем ниже будет пилообразный сигнал, тем меньшее время будет протекать ток через тиристоры.Ниже на рисунках 6-9 приведены осциллограммы тока и напряжения на реакторе при углах управления от 0 до 150 градусов (при 180 градусах параметры будут равны нулю, так как тиристоры не будут открываться).Рисунок 6 – Осциллограмма тока при угле управления до 90 градусовРисунок 7 – Осциллограмма тока при угле управления от 90 до 150 градусовАнализируя осциллограммы тока можно сделать вывод, что при угле управления до 90 градусов, работает только 1 тиристор, так как второй просто не успевает открыться, и таким образом ток на реакторе имеет только положительную полярность. Такой вид тока может приводить к насыщению трансформаторов тока, так как происходит только намагничивание сердечника, без размагничивания, за которое отвечает ток отрицательной полярности. Из-за насыщения стали сердечника трансформатора тока его сопротивление ветви намагничивания резко уменьшается, что приводит к возрастанию тока намагничивания, вследствие чего увеличивается его погрешность. Высокая погрешность в трансформаторе тока может приводить к ошибочному срабатыванию релейной защиты, поэтому данный диапазон угла управления можно считать непредпочтительным. При угле управления от 90 градусов, ток имеет обе полярности так как работают уже оба тиристора, этот диапазон угла управления можно считать предпочтительным.Рисунок 8 - Осциллограмма напряжения при угле управления до 90 градусовРисунок 9 – Осциллограмма напряжения при угле управления от 90 до 150 градусовКривая напряжения реактора повторяет кривую напряжения сети, за исключением нулевых участков, когда отсутствует управляющий импульс на тиристор.Для оценки работы СТК, нам необходимо рассчитать реактивную мощность, которую потребляет тиристорно-управляемый реактор, по следующему уравнению:Для нахождения реактивной мощности при каждом угле управления, необходимо рассчитать амплитудные значения напряжения и тока, и угол между напряжением и током. Чтобы найти эти значения, будем использовать встроенную тетрадку JupyterLite. Для этого нам необходимо разработать программу на языке программирования Python, которая будет на основании сигналов тока и напряжения из модели производить Фурье анализ сигналов и рассчитывать их амплитудные значения и углы фаз для первой гармоники, то есть для 50 Гц. Далее, по формуле, представленной выше, рассчитаем реактивную мощность. Пример кода представлен на рисунках 10-11.Выходные данные в REPEAT: ток «AMP» и напряжение «VOLT», также задаем угол управления тиристорами как глобальную переменную «Control_angle», чтобы изменять его значение прямо из JupyterLite.Рисунок 10 – Первая часть кода в JupyterLiteРисунок 11 – Вторая часть кода в JupyterLiteРассчитаем эти параметры для каждого угла управления и результаты занесем в таблицу 1. Амплитудное значение напряжения является постоянным и не будет отображено в таблице. При потреблении СТК реактивной мощности угол между напряжением и током является положительным, а при генерации – отрицательным.Таблица 1 – Полученные результаты амплитудного значения тока, угла между током и напряжением, реактивной мощности:На основании полученных данных построим Характеристику реактивной мощности, которая приведена на рисунке 12.Рисунок 12 – Характеристика реактивной мощностиАнализируя осциллограмму реактивной мощности, можно сделать вывод, что диапазон регулирования реактивной мощности при α <90⁰ вдвое меньше, чем при α ≥90⁰. С увеличением угла α уменьшается время протекания тока через тиристоры – уменьшается потребляемая реактивная мощность. Также стоит отметить, что полного регулирования не наблюдается, то есть СТК только потребляет реактивную мощность.Улучшим схему, подключив параллельно ТРГ конденсатор (см. рисунок 3), который будет непрерывно выдавать реактивную мощность и позволит поднять ее характеристику. При угле управления 180 градусов в сеть будет выдаваться реактивная мощность конденсатора, а тиристорно-управляемый реактор будет полностью исключен из работы. Полученные результаты представлены в таблице 2 и на рисунке 13.Таблица 2 – Полученные результаты амплитуды тока, угла между током и напряжением, реактивной мощности с подключением конденсатора:Рисунок 13 – Характеристика реактивной мощности с подключением конденсатораТаким образом нам удалось создать модель статического тиристорного компенсатора в который вошли тиристорно-управляемый реактор и коммутируемый конденсатор. Модель позволяет как потреблять, так и выдавать реактивную мощность в зависимости от угла управления тиристорами. Предпочтительным значением угла управления является α ≥90⁰, так как при таком угле управления мы имеем полный спектр регулирования реактивной мощности.Список источников1. ГОСТ 32144-2013. Нормы качества электрической энергии в системах электроснабжения общего назначения / разработан Обществом с ограниченной ответственностью «ЛИНВИТ» и Техническим комитетом по стандартизации ТК 30 «Электромагнитная совместимость технических средств». – Текст: электронный. URL:https://electromontaj-proekt.ru/data/documents/gost-32144-2013.pdf(дата обращения 05.09.2024).2. Васильев А.С. Управляемые электропередачи на базе силовой электроники: учебное пособие. Часть 1. Методическое и технологическое обеспечение управления режимом по напряжению и реактивной мощности / А.С. Васильев, Р.А. Уфа; Томский политехнический университет. – Томск: Изд-во Томского политехнического университета, 2021. – 142 с."
СберМаркет,,,"Как создать систему управления батареей, используя современный подход?",2024-07-08T07:12:45.000Z,"Приветствуем читателей! В данной статье проводится обзор модели системы управления аккумуляторной батареи (АКБ), на английском языке также известной как Battery Management Control system (BMS). В целях удобства исследуемый объект будет упоминаться как система управления батареей (СУБ). Модель СУБ разработанана платформе REPEAT.Ссылка на телеграм-канал REPEAT:https://t.me/repeatlabРисунок 1 – Модель СУБВ данной статье вы узнаете:Что такое СУБ;Основные функции СУБ;Какие производители используют СУБ;Пример модели СУБ, выполненной в REPEAT;Пример модели аккумулятора с СУБ;Пример СУБ с активной балансировкой ячеек;Почему важен модельно-ориентированный подход в разработке СУБ.Что такое СУБ?Производительность литий-ионных аккумуляторов значительно зависит от температуры, поэтому важно осуществлять работу АКБ в допустимых тепловых пределах.Система управления батареей (СУБ) — это устройство, предназначенное для контроляпроцесса зарядки и разрядки аккумуляторной батареи. Основная функция СУБзаключается в обеспечении безопасной эксплуатации аккумулятора за счет контроляегосостояния, что максимально продлевает срок службы АКБ. На рисунке 2 представленаупрощенная структурная схема электромобиля с СУБ, которая контролируетпараметры тягового аккумулятора и в зависимости от его состояния генерируетуправляющие сигналы для обеспечения оптимальных условий работы.Рисунок 2 – Блок-схема упрощенного электромобиляКакие основные функции СУБ?СУБ выполняет ряд функций, способствующих улучшению качества безопасности эксплуатации АКБ:-      Контроль основных параметров;-      Балансировка ячеек;-      Рекуперация;-      Связь с другими элементами системы (передача информации);-      Защита от нештатных режимов работы.Контроль основных параметровСистема производит замер следующих параметров:-      Напряжение;-      Ток;-      Температура;-      Уровень заряда (State-of-Charge (SOC));-      Режим работы: заряд, разряд, холостой ход (х.х).Балансировка ячеекБалансировка ячеек – это процесс выравнивания напряжений и уровня заряда ячеек. Балансировка достигается путем предотвращения недозаряда/перезаряда отдельных ячеек АКБ, например перераспределение энергии от наиболее заряженных ячеек к наименее заряженным, что называется активным методом балансировки. При пассивной балансировке используются шунтирующие резисторы для рассеивания электроэнергии.РекуперацияРекуперация электромобиля – это процесс превращения кинетической энергии в электрическую с последующим ее возвращением в аккумуляторный блок. Рекуперация достигается оперативным определением режима работы аккумулятора и проведением соответствующих операций переключения в цепи.Связь с другими элементами системыДанные, генерируемые СУБ, также передаются в другие части системы, в которой установлен аккумулятор (рисунок 2).Защита от нештатных режимов работыДля предотвращения возникновения неисправностей производится защита АКБ от следующих режимов работы:-      Перенапряжение/недостаточное напряжение;-      Превышение/снижение тока допустимого значения;-      Переохлаждение/перегрев.Где используется данная технология?Данная технология является критически важным для всех производителей устройств, в которых установлен АКБ, т.к. при помощи данной системы управления производители добиваются повышенной надежности, за счет обеспечения безопасности работы, производительности и экономической эффективности.СУБ используются, например, в Tesla S и Nissan Leaf.Разработка СУБ в REPEATПервоначальная версия СУБ будет включать себя алгоритмы по определению:-      Нештатных режимов работы;-      Режима работы аккумулятора;-      Падение уровня заряда ниже критической отметки;-      Температуры АКБ, относительно химических потерь.Также в модель СУБ внедрим встроенную нагрузку для проведения тестовых расчетов модели.Большая часть функций реализуется при помощи блоков автоматики (нагрузка, определение нештатного режима работы и падения уровня заряда ниже критического).При реализации нагрузки установим (рисунок 3) переключатель (блок №5), константу (№6, 62), генератор импульсов (№1) и ограничитель сигнала (№2). При проведении расчетов можно выбрать значение тока нагрузки, тип нагрузки (импульсный или постоянный) и предел допустимых токов.Рисунок 3 – Модель нагрузкиНаиболее простой частью разработки окажется алгоритм по определению нештатных режимов работы АКБ: блоки сравнения с допустимым значениями (блоки №21,23,39,41) и логическое ИЛИ (№12,43,44). При возникновении любого из аварийных режимов на выходе СУБ будет 1.Рисунок 4 – Определение нештатного режима работыТеперь переходим к более интересной части: реализация алгоритма по определению режима работы аккумулятора, для этого мы реализуем простейший конечный автомат, т.е. модель, в котором последующее состояние модели зависит не только от входного сигнала, а также от состояния в настоящий момент времени.Есть ряд способов по осуществлению данного алгоритма, мы напишем скрипт при помощи блока Jython, в котором язык Python интегрирован в экосистему Java.Рисунок 5 – Блок “Jython”Ниже визуально представлена структура разрабатываемого автомата. Помимо перехода между тремя состояниями (заряд, разряд, холостой режим), также необходимо внедрить условие по времени, т.к. система проверяет состояние с определенным интервалом, например:Рисунок 6 – Граф автомата состоянийВ начале скрипта инициализируем класс с тремя состояниями:Рисунок 7 – Класс состояний АКБПеред написанием скрипта стоит отметить, что тест СУБ будет проводится в электрической схеме с неидеальными источниками, поэтому ток в замкнутой цепи быть равным 0 не может. Поставим условие, что холостому режиму работы будет соответствоватьРисунок 8 – Скрипт по реализации конечного автомата (При заряде выходное значение = 2, разряде = 1, х.х = 0)Проверим работоспособность разработанного скрипта, подадим на вход ступенчатый ток, соответствующий холостому ходу, разряду и заряду:Рисунок 9 – Ступенчатый токРисунок 10 – Результаты теста JythonСогласно результатам, конечный автомат корректно показывает состояние системы относительно тока с требуемым временным интервалом.Тепловая модель (рисунок 11) представлена одной тепловой массой (блок №10), отражающей свойства электродов АКБ. Изменение температуры определяется химическими потерями (№8) и эффективностью охлаждения окружающим воздухом (№71,13).Рисунок 11 – Модель по определению температуры АКБРазработка модели аккумулятора с СУБПеред тем как приступить к расчетам еще раз обозначим выходные сигналы СУБ (последовательно сверху-вниз):-      Ток нагрузки (Load);-      Режим работы АКБ (State);-      Нештатный режим работы по току (CurrentFault);-      Нештатный режим работы по напряжению (VoltageFault);-      Нештатный режим работы (Fault);-      Понижение уровня заряда ниже допустимого (SOCdrop);-      Температура АКБ (TempOut).Рисунок 12 – Выходные сигналы СУБИзучим принцип работы СУБ в модели, состоящей из аккумулятора (блок №2) и источника тока (блок №3), в командный порт которого подается значение сигнала тока нагрузки (рисунок 13):Рисунок 13 – Тестовая схема моделиНа входные порты СУБ подключим датчики тока, напряжения, уровня заряда (SOC) и мощности общих тепловых потерь АКБ.Рисунок 14 – Входные сигналы СУБДалее установим связь между разработанной моделью и встроенным инструментом JupyterLite,подробный урок по этому процессу представлен в видео.Для исследования зависимости температуры АКБ относительно тока нагрузки, запустим серию расчетов с увеличивающимся током нагрузки с 0 до 120 А.Рисунок 15 – Входные данные для серии расчетов АКБ с СУБКривые температур представлены на рисунке 13.Рисунок 16 – Кривые температуры при разных токах нагрузкиПо характеристике сравнения видно, как значительно меняется температура при увеличении тока нагрузки, что позволяет нам, при известном значении допустимой температуры АКБ, установить максимальный рабочий ток.Проведем еще один единичный расчет при токе нагрузки равном 5 А и выведем таблицу значений контролируемых параметров (справа-налево (рисунок 17): ток нагрузки, состояние АКБ, нештатный режим по току, нештатный режим по напряжению, уровень заряда температура АКБ).Рисунок 17 – Таблица результатов расчетаТак как в столбцах по определению нештатных режимов по току, напряжению (VoltageFault и BatterySOC) нули. АКБ работает в допустимых пределах.Однако мы рассмотрели работу АКБ со статической нагрузкой. А что, если батарея питает динамичную нагрузку, например, электропривод (ЭП) электромобиля (рисунок 18)?Рисунок 18 – Блок “Электропривод”Обновим модель и параметры СУБ.Рисунок 19 – Модель АКБ и ЭПРисунок 20 – Обновленные параметры СУБМоментная характеристика ЭП представлена на рисунке 16, где синяя линия – характеристика нагрузки, а красная – момент электропривода.Рисунок 21 – Моментная характеристика ЭПСогласно полученным результатам, ток превышает допустимые значения во время пуска, когда переходит в генераторный режим, а также, когда достигает максимального момента (300 Н * м)Рисунок 22 – Таблица результатов расчетаРисунок 23 – График нештатных режимов по токуДанные результаты показывают, что АКБ работает не в допустимых диапазонах, т.к. превышается максимальный ток аккумулятора, и требуется обеспечить безопасность операции АКБ при питании ЭП, чего можно достичь разными путями: от уменьшения нагрузки ЭП вплоть до разработки управляемого тягового преобразователя.Разработка СУБ с активной балансировкой ячеекТак как активная балансировка ячеек аккумуляторного блока является одной из основных функций СУБ, модель СУБ дополняется соответствующим алгоритмом. Существуют следующие методы активной балансировки:1.           На основе конденсаторных элементов;2.           На основе индуктивных элементов;3.           На основе преобразователей.Рассмотрим балансировку на основе одиночного индуктора, который передает энергию между аккумуляторами.Рисунок 24 – Модель трех аккумуляторных ячеек с активной балансировкойВвиду большого объема данной статьи, полный обзор модели с активной балансировкой будет проведен в следующей главе. На данный момент приведем результаты расчета в модели, в которой в начальный момент времени ячейки обладают разными уровнями заряда (60.01, 60.03, 60.05 % соответственно).Рисунок 25 – Уровни заряда батарей перед балансировкой (блоки №14,73,74)Так как разница в зарядах незначительная, система за 0,5 секунд СУБ приравнивает их уровни зарядов.Рисунок 26 – Уровни заряда батарей после балансировкиПочему важен модельно-ориентированный подход в разработке СУБ?При модельно-ориентированном проектировании становится возможным исследовать разрабатываемую систему при различных условиях от рабочих до аварийных режимов, оптимизировать ее параметры и проводить ряд тестирований перед переходом к разработке прототипа – все это значительно повышает экономическую эффективность процесса разработки."
СберМаркет,,,Как проходит крупнейшее в мире корпоративное соревнование,2024-06-30T12:51:31.000Z,"Всем привет, меня зовут Денис Пешехонов. Я работаю в Росатоме тимлидом на C#, а ещё я участникбольшого количествасоревнований по программированию. Вот уже три года подряд езжу на AtomSkills — наше внутреннее Росатомовское соревнование, которое стало крупнейшим в мире корпоративным чемпионатом — и хочу рассказать подробнее о его атмосфере и особенностях применительно к IT-направлениям.Немного историиВ 1947 году Испания, как и многие другие страны, восстанавливалась после Второй Мировой. Требовалось буквально отстраивать всё заново, что обнажило не только дефицит рабочих рук, но и недостаток квалификации у представителей рабочих профессий. Нужно было привлекать молодёжь, создавать у неё желание идти в сварщики и строители, а не в блогеры. Так что директор Испанской Молодёжной Организации по имени Хосе Антонио Элола Оласо предложил провести соревнование по профессионально-технической подготовке.Инициатива зашла, и к 1953 году в этих соревнованиях участвовали, кроме Испании, ещё Португалия, Германия, Великобритания, Франция, Марокко и Швейцария. К 1970-му году чемпионат расширил своё влияние на новую часть света — Токио. Под современным же названием он известен только с 2000-х годов —WorldSkills. Сейчас там принимают участие около 80 стран. Чемпионат вырос не только географически, но и тематически — в настоящее время список профессий включает полсотни наименований: как рядовые рабочие специальности (сварка, работа на станках, электрика), так и экзотику вроде ""дизайн одежды"", ""ювелирное дело"", ""оформитель витрин"" и так далее.В 2019 WorldSkills прошёл в Казани, а в 2022 Россию из него исключили под известным предлогом. Фотография с сайтаworldskills2019.comAtomSkillsРосатом строит больше всего атомных станций в мире и тоже нуждается в квалифицированных рабочих кадрах. Сварщик или бетонщик на площадке строительства АЭС должен быть профессионалом высочайшего класса, не уступающим по уровню подготовки какому-нибудь Java-сеньору. Поэтому Госкорпорация запустила у себя подобный же чемпионат 9 лет назад, и развивался он очень схожим образом: если сначала в нём соревновались несколько сотен человек по единичным строительным направлениям, то сейчас это мероприятие с 2000+ участников по 42 компетенциям.Небольшая часть компетенций на AtomSkills 2024Например, существует компетенция ""Сметное дело"", потому что расчёт смет — реально сложная и очень важная часть проектов, которыми занимается Росатом (и вообще любая крупная промышленная компания). Вот как вы себе представляете соревнование сметчиков? Кажется, что превратить это в спорт невозможно. Тем не менее, есть задание, регламент, критерии оценки, а профессионалы в своём деле съезжаются со всей страны, чтобы завоевать медаль.""Радиационный контроль"" — самая пафосная работа в мире. Фото Корпоративной Академии Росатома.""Мехатроника"", ""Квантовые технологии"", ""Неразрушающий контроль""... — на площадке уйма интересного. Специалисты по ""Охране труда"" должны среагировать в искусственно созданной сцене аварии и, например, найти пострадавшего в запертом автомобиле. Специалисты по ""Строительному контролю"" пробиваются через вахтёршу. ""Геодезия"" ходит по полям и делает замеры, даже при сильнейшем ливне.Площадка и атмосфераСоревнование проводится раз в год в Екатеринбурге — поближе к геометрическому центру нашей страны. На самом деле из года в год фактическое количество стран-участников увеличивается, и в этот раз их было уже десять: в основном это страны-партнёры России по атомным стройкам.Да и сам по себе Росатом это огромный конгломерат различных организаций, заводов, НИИ, промышленных компаний и так далее. Так что съезжаются действительно отовсюду. В этом отношении Екатеринбург — хороший выбор. А площадкой выступает местный экспоцентр недалеко от аэропорта, не уступающий своими размерами и уровнем павильонам столиц.""ГЛАЗА!""На месте разворачивается застройка: под каждую компетенцию огорожена зона со своими правилами и внутренним оборудованием. Где-то нужно разместить станки, где-то достаточно поставить компьютеры, а куда-то должна въехать бетономешалка и залить бетон в оснастку прямо внутри павильона. Разумеется, соблюдается строжайшая техника безопасности, иначе за столько лет проведения чемпионата Экспо бы уже сожгли.У мероприятия своя айдентика, иконография, фирменный мерч. Люди на чемпионате различаются поцвету штановцвету формы: участники носят голубое, эксперты зелёное, главные эксперты красное, а тимлидеры жёлтое (здесь тимлидерами зовутся организаторы от дивизионов — люди с поистине фантастическим терпением, выслушивающие ежедневное нытьё пары сотен взрослых, которых они сюда привезли).Над каждой зоной висит название компетенцииПрограммные решения для бизнесаНо перейдём к IT. В изначальном WorldSkills есть направление, которое называется Software Solutions for Business. По смыслу это должна быть энтерпрайз-разработка, но по слухам оригинальный регламент соревнования представляет собой создание форм на скорость на каком-нибудь WPF. На AtomSkills несколько лет назад сначала скопировали компетенцию один к одному, но позже поняли, что получается не только не похоже на настоящую работу энтерпрайз-программиста, но и довольно скучно. Поэтому переписали регламент с нуля.Вообще, айтишных компетенций несколько: есть и это ваше ML, и кибербезопасность по двум направлениям, и системное администрирование, а с недавнего времени RPA. Но самая близкая к классическому энтерпрайзу компетенция — ""Программные решения для бизнеса"", включающая весь цикл создания энтерпрайз-продукта:Бизнес- и системная аналитика по требованиям заказчика;Проектирование архитектуры и дизайна;Собственно, программирование;Создание технической документации и руководства пользователя;Создание и документирование средств развёртывания;Тестирование готового продукта;Презентация заказчику.На всё вышеперечисленное у команды из трёх человек есть три дня по 8 часов. Но давайте по порядку.В зоне компетенции 15 рабочих мест, каждое с тремя компьютерами и флипчартомПодготовка и стартПодготовка начинается заранее, её сроки зависят от компетенции. Ходят слухи, что сварщики, например, начинают готовиться к следующему году чуть ли не сразу после завершения текущих соревнований. Что касается айтишников, то обычно за пару месяцев до поездки дивизионы запускают внутренний отбор: проводят мини-хакатон или что-то похожее, чтобы определить команды, которые примут участие в чемпионате. Комиссия по конкурсному заданию (членом которой являюсь и я) начинает создавать задание за 4-5 месяцев и постепенно делает это практически до самого дня икс.На площадке, помимо участников, есть команда жюри, главный эксперт, его заместитель и технический эксперт. В отличие от рядового хакатона, здесь всё подчинено расписанию и регламенту. Задача не выдаётся заранее, только общее описание в духе ""Вы будете делать что-то про обучение"". Так что в первый день участники просто занимаются настройкой рабочих мест, ставят нужный себе софт, настраивают виртуалки. Да, работа и деплой идут на виртуалках по выбору из нескольких, а компьютер просто является окном доступа к ним через удалённый рабочий стол.Фото 2022 года, когда я был участникомА есть интернет? Участники могут скачать готовые куски решения?На WorldSkills есть понятие toolbox: тот инструмент, который участник может взять с собой. Для сварщика это маска, аппарат, спецодежда, болгарка. Для монтажника плоскогубцы, ножи, отвёртки. Для программиста IDE, опенсорсные библиотеки и заготовки. Например, можно написать дома и принести на конкурс готовый модуль авторизации, потому что в заданиях часто бывает ролевая модель.Вопрос размера, проверки и влияния заготовок на результат очень сложный, и решается в компетенции по сей день. Де-факто пока что выгодно приносить с собой как можно больше готового всех мастей и видов, и это проблема, которую мы — жюри — пытаемся исправить.Задание и первый деньНа обычных хакатонах описывают задачу, которую нужно решить, но в остальном участники вольны делать, что хотят, могут подходить сколь угодно творчески. Это хорошо тем, что работа, обычно, интереснее. Но плохо тем, что я много раз видел, как хакатоны выигрывались одной презентацией без наличия работающего решения. Или, скажем, решение целиком на моках и эмулирует бизнес-процесс вместо его настоящей реализации. Кто-то может сказать, что это нормально, но на мой личный взгляд хакатоны должны оставаться соревнованиями разработчиков, а не маркетологов.На AtomSkills техзадание подробное: описаны все сценарии, которые должны быть доступны пользователю. Это документ с десятком-другим страниц, похожий на те, что приносит вам заказчик в энтерпрайзе после первичного общения с аналитиками.Задание 2022 года — нужно было создать CRM для техподдержкиПолучив задание, команды могут позадавать экспертам вопросы, после чего садятся за рабочие станции и начинают делать. По регламенту работать можно только на площадке и только в отведённое время. Для этого запускают большой таймер. Когда он по любой причине остановлен, работу нужно прекратить и, например, уйти на обед или сделать другой перерыв. Всего выделено три рабочих днях, и в каждый из них суммарное рабочее время 8 часов, разбитое на интервалы.Пока в соседних цехах варят трубы и точат детали, наши участники создают схемы, пишут бэкенд, верстают интерфейсы и так далее.Что мешает команде дорабатывать решение ночью в отеле?Ещё один вопрос, который не имеет однозначного ответа. В позапрошлом году работу ночью запрещали и пытались даже как-то это проверять. В прошлом году, наоборот, разрешили, и увеличили объём задания с учётом этого. Пока что единственным рабочим способом является разбиение задания на модули и проверка каждого модуля в конце дня. Так делают на некоторых компетенциях, но не на всех. В нашей не делают.Мой коллега с компетенции RPA на фоне таймера. При взгляде на этот таймер у меня в голове начинает тикать заставка из сериала ""24"".Сдача работПо окончании третьего дня каждая команда должна сдать:Исходный код решения;Руководство пользователя;Инструкцию по сборке развёртыванию;Техническую документацию;Презентацию.После этого все работы презентуются виртуальному заказчику. Есть описание того, что комиссия хочет увидеть в презентации: например, технический стек, на котором решение было сделано, возможные способы масштабирования, требуемые ресурсы для развёртывания.За каждую презентацию выставляются баллы, которые идут в общий зачёт. Вообще в оценке несколько категорий критериев: функциональные требования, презентация, UI/UX, деплой и документация, качество кода. В сумме команда может получить 100 баллов, из них около половины приходится на функциональные требования и около 20 на презентацию.На презентации, помимо обязательных участников, могут подойти любые посетители чемпионатаПараллельно идёт деплой, и это, на мой личный взгляд, самое ценное отличие AtomSkills от обычных хакатонов. Технический эксперт берёт твою же инструкцию по деплою, пробует из твоих же исходников развернуть и запустить решение. Если решения нет — ноль баллов. Если решение не развернулось — ноль баллов. Забыл прописать в документации какой-нибудь ENV? Сам виноват, ноль баллов. Твой софт кривой, косой, и запускается только на машине разработчика? Ты не энтерпрайз-программист, ноль баллов. Здесь нельзя принести одну только презентацию, нельзя сделать красивую пустышку и так далее. То есть, ты за презентацию баллы получишь, конечно, но функциональные требования всё равно составляют основную долю от общей суммы баллов, и выше середины в списке результатов ты не поднимешься никак.Хотя, был в прошлом году случай, когда команда, у которой решение запустилось, набрала меньше баллов, чем команда, у которой не запустилось. Последние набрали на очень хорошей презентации и документации. У первой запустилось, но сценарии не прошли.У комиссии обязательно есть изолированное помещение, в котором можно обсуждать задание и вести другие беседы так, чтобы участники не слышалиПроверкаУ комиссии есть сценарий, он одинаковый для всех и написан прямо по тексту техзадания. Сценарий выглядит примерно так:Зайти в раздел с пользователями, если не получилось, не засчитать критерий 1.1Попытаться создать пользователя, если не получилось, не засчитать критерий 1.2....Если комиссия не понимает, как что-то сделать, она, представьте себе, идёт в руководство пользователя, которое подготовила команда. Самой команде нельзя ничего говорить и вообще каким-то образом влезать в проверку. Сделали непонятно и в инструкции не описали — ноль баллов. Хотя, в этом году мы разрешили участникам дать до двух устных пояснений к деплою и загрузке входных данных, иначе бы отлетела половина всех решений.Комиссия отвечаетза свои слована вопросы участниковКоманда не может влиять на проверку, но проверка всегда публична. Картинка с монитора членов жюри дублируется на внешний экран, который видят все: и та команда, которую проверяют сейчас, и остальные участники. Конкурс подчинён правилам и объективности, насколько это возможно. Критерии тоже формулируются не ""Насколько удобно что-то сделано"", а ""Позволяет ли программа выполнить действие X"".Впрочем, оценка удобства, красоты, качества тоже есть. Но её доля в баллах сравнительно невелика относительно объективных пунктов. Вполне работающая стратегия: сделать тяп-ляп, зато формально успеть запрограммировать все бизнес-сценарии. С таким жюри тоже пытаются бороться, но без ухода во вкусовщину это довольно сложно. На практике могу сказать, что команды, которые умеют делать красиво и аккуратно — делают красиво и аккуратно даже ценой не полной готовности продукта. Человеку с чувством вкуса и тягой к старанию обычно трудно заставить себя ""тяп-ляпить"".Выглядит, конечно, не так зрелищно, как сваркаФиналПо результатам проверки всем командам начисляются баллы от 0 до 100, которые идут в общую таблицу. Есть десятые и сотые доли баллов, и иногда это определяет разницу между первым и вторым местом, либо, скажем, между попаданием и непопаданием на пьедестал. Всего в каждой компетенции, за редким исключением, три возможных медали: золото, серебро и бронза.Участники не знают свой результат до самого момента вызова (или не вызова) на сцену. И разглашать им это запрещено. Создаётся эмоциональный накал: например, в 22-м году, когда я был участником, мы с командой взяли золото, но думали, что вообще не попали в тройку. Объявили бронзу — не мы. Объявили серебро — опять не мы. Уже сидели все поникшие, пока со сцены не объявили золото.Могли ли участники этой компетенции рассчитать свои шансы на победу?Закрытие это всегда очень масштабное действо с приглашёнными ведущими, хореографией, световым шоу и так далее. Победителям вручают медали, чествуют их, а позже все медалисты получают денежные премии (порядки: от десятков тысяч рублей на человека за бронзу до сотен тысяч за золото). Выигрывать очень престижно. Да и на беседах с начальством о повышении медаль AtomSkills это хороший козырь.Фактически сражаются между собой дивизионы. Поэтому даже по тем компетенциям, в которых ты не участвовал, следишь за результатами и радуешься за своих. В этом году первый раз была отдельная студенческая лига и отдельно профессиональная. В ""Программных решениях для бизнеса"" студентам и профи давали одинаковое задание и одинаковое время. Надо сказать, что по сумме баллов некоторые студенты справились лучше, чем некоторые профи.Конкретно мой Инжиниринговый дивизион в студенческой лиге взял серебро в ""Программных решениях для бизнеса"" и золото в ""Программной роботизации"" (RPA). В профессиональной лиге в этом году без медалей, хотя ребята из нашей формально строительной организации обошли многие команды из чисто айтишных блоков, что я считаю отличным результатом.Участники и эксперты всю неделю практически живут в ЭкспоПодводя итог, хочу сказать, что очень люблю такую активность в компаниях. С моей личной точки зрения профессионализм всегда идёт бок о бок со спортивным интересом, и поэтому настоящему мастеру своего дела интересно испытывать себя, выходить за рамки привычного, показывать свой навык и сравнивать его с другими. Встречается мнение, что на хакатонах и подобных конкурсах люди делают игрушечные поделки, никак не связанные с реальной практикой. По своему опыту могу сказать, что именно через хакатоны я начал освоение приличной доли своего багажа знаний, и в реальности пригодилось очень многое. Не конкретная работа, которую делаешь на конкурс, а именно те связи в голове, которые у тебя при этом создаются.Для компании польза тоже очевидна, иначе бы не тратились огромные суммы на организацию каждый год вот уже девять лет подряд. Про этот чемпионат можно рассказывать очень многое, поэтому я надеюсь, что статья вдохновит коллег из других компетенций приоткрыть и их внутреннюю кухню тоже. В конце-концов, даже со стороны наблюдать за всем этим безумно интересно."
СберМаркет,,,Математическое моделирование технологических объектов и систем глазами и руками студента,2024-06-14T07:00:23.000Z,"Исследование возможностей и границ применения научных технологий и программного обеспечения как в привычной области работы, так и в новых сферах – это один из ключевых приоритетов современной индустрии. Мы  продолжаем серию статей о математическом моделировании, раскрывая еще одно направление применения программного обеспечения REPEAT.Сегодня перед вами результат проекта ""Школа Моделирования"" – 1D-модель системы подвески автомобиля. Автор статьи, студент РГУ им. Косыгина – Алексей, который работал над этим проектом, используя наши инструменты и технологии, что стало хорошим тестом для их универсальности и гибкости применения.Ссылка на телеграм-канал REPEAT:https://t.me/repeatlabРазработка 1D-модели системы подвески автомобиляВ статье представлена разработанная на платформе REPEAT модель системы подвески автомобиля.Описание системыСистема подвески автомобиля должна обеспечивать комфортную езду при движении по дороге с неровностями, но, если возникнут нежелательные помехи, необходимо, чтобы система как можно быстрее стабилизировалась.Для упрощения схемы подвески автомобиля смоделируем 2 массы и 2 комплекта пружин и амортизаторов, и проанализируем ¼ часть подвески автомобиля.Рисунок 1 - Расчётная схема системы подвески (правая часть рисунка – включает воздействие сил)Рассмотрим колебания колесной машины в продольно-вертикальной плоскости. Модель включает подрессоренную массуm1,и неподрессоренную массуm2.Подрессоренная масса – масса, включающая массу рамы или кузова и остальных частей конструкции (кузовные элементы, двигатель), вес которых передается на упругие элементы подвески.Неподрессоренная масса – масса, включающая массу колёс, массу самой подвески и остальных частей системы подвески (шин, элементов трансмиссии и тормозной системы, и т.п.), отделённых подвеской от рамы либо кузова.На подрессоренные части движущегося автомобиля наложены связи, вследствие чего колебания по отношению продольной оси в значительной мере компенсируются направляющими устройствами подвески. В реальных условиях они являются несущественными и появляются в большой мере при торможении, чем при движении по неровностям опорной поверхности. Демпфирующие устройства сводят к минимуму угловые колебания относительно вертикальной оси (рыскание) и линейные колебания относительно поперечной оси, которые могут быть компенсированы за счет боковой податливости и бокового проскальзывания шин.Неподрессоренные массы совершают линейные вертикальные колебания. При зависимой подвеске колес в отдельных случаях необходимо учитывать поперечно-угловые колебания неподрессоренных масс. Продольно-угловые колебания подрессоренных частей автомобиля, называемые галопированием, наиболее неприятны для человека. Большинство реальных систем подрессоривания симметричны относительно продольной оси в этом случае продольно-угловые колебания станут независимыми от поперечно-угловых и наоборот, а это значит, что их можно исследовать раздельно друг от друга [1].Математическая модель системы подвески автомобиляНачнем с того, что введем в нашу систему силы, основанные на ее элементах, они отражены на рисунке 1.Силу, действующую на пружину, можно выразить какS. Она  пропорциональна смещению – чем больше деформация, тем больше сила, что является справедливым закону Гука. Значение вектораSможно представить с помощью следующего уравнения:где k –коэффициент упругости, а Δ - деформация пружины (линейное смещение).Силу, действующую от демпфера, можно показать какG. Она  пропорциональна скорости деформации и может быть представлена следующим уравнением:гдеc– коэффициент демпфирования иu- скорость массы.Для представления движения в системе будем считать, чтоFпредставляет собой силы, действующие на системуx1иx2представляют смещение массm1иm2соответственно, аw— возмущение (например, неровность на дороге).Уравнения движения:Уравнения пружины:Уравнения демпфера:Для первого этапа моделирования достаточным будет построить модель движения первой подрессоренной массыm1системы пружина-демпфер-масса по уравнению движения:Процесс моделированияДля моделирования пружинной системы демпфер-массы нам понадобятся:сумматор (так как все значения в круглых скобках суммируются);блок деления (для получения значения ускорения массыm1);2 интегральных блока (для получения скорости и смещения массы);2 блока усиления (умножить последовательно на значенияk1иc1, чтобы получить значения для блока сумматора обратно);управляющая силаF(в нашем случае константа).Рисунок 2 - Расчётная схема пружинной системы демпфер-масса в REPEATТеперь перейдём к первому эксперименту со следующими параметрами:Коэффициент упругостиk1=600 Н/м;Коэффициент демпфированияc1=14 Н·с/м;Подрессоренная массаm1=650 кг.Рисунок 3 - Реакция системы «пружина-демпфер-масса»Во всех графиках по оси X определяется время в секундах, по оси Y определяется смещение в метрах.График показывает результирующее поведение смещения массы . Эта система стабилизируется со временем, но медленно и не до состояния равновесия в течении 100 секунд. Наличие такой подвески для нашего автомобиля привело бы к длинным колебаниям, которые, вероятно, заставят людей плохо себя чувствовать во время путешествий. Можно поэкспериментировать со значениями параметров, чтобы увидеть, как система пружина-демпфер-масса реагирует на различные значения. Ниже предсавленно несколько кратких примеров:Рисунок 4а - Увеличение массы в два раза - вызывает более длительные колебания во времени (k1=600 Н/м,c1=14 Н·с/м,m1=1300 кг)Рисунок 4б – Увеличение массы в 4 раза больше массы -вызывает колебания, которые становятся еще продолжительнее во времени (k1=600 Н/м,c1=14 Н·с/м,m1=2600 кг)Увеличение упругостиk1= 1200 Н/м приведет к более коротким по амплитуде колебаниям, но к более быстрому переходу в равновесие.Рисунок 4в - Колебания с более жесткой пружиной (k1=1200 Н/м,c1=14 Н·с/м,m1=650 кг)Увеличение коэффициента демпфирования  = 28 Н·с/м приведет к большей стабильности.Рисунок 5а - Колебания при различных значениях коэффициента демпфирования (k1=300 Н/м,c1=28 Н·с/м,m1=650 кг)Рисунок 5б - Колебания при различных значениях коэффициентах демпфирования (k1=300 Н/м,c1=70 Н·с/м,m1=650 кг)Путём проведения экспериментов нам удалось определить динамику работы системы, более приближённую к физическим. В последнем графике присутствуют длительные колебания, но динамика перемещения массы, а фактически это колебания, которые чувствует пассажир, имеет затухающий характер и на 90 секунде практически приходит в равновесие. Настраивать модель можно и дальше, путём подбора коэффициентов, но наша цель показать больше возможностей работ с устранением колебаний. Так как целью данной работы является снижение вибраций в автомобиле для обеспечения комфорта пассажиров.Расширение системыМы можем расширить нашу систему, чтобы посмотреть на изменение перемещений массm1иm2, используя следующие уравнения:Ниже представлена пошаговая доработка системы:Рисунок 6а - Схема модели подвески – первый шаг (уравнение 8)Рисунок 6б - Схема модели подвески – первый шаг (уравнение 9)Рисунок 7 - Схема модели подвески – первое уравнение без составляющей управляющей силыFРисунок 8 - Схема модели подвески – второе уравнение без составляющей управляющей силыFи возмущенияwЗатем добавим управляющую силу F в качестве константы (которая будет имитировать шаг в одну секунду) со значением 1Н и возмущениеwсо значением 0,1 м — имитация неровности на дороге (возмущение).Рисунок 9 – Общая схема модели подвески с задержкой по возмущениюПараметры системы, следующиеИтак, подвеска автомобиля, предложенная на рисунке 1, была  показано на рисунке 9 и реализована в REPEAT. Данная система с открытым контуром и не имеет обратной связи. В модели, имеется отклик системы на единичный шаг управляющей силыFи возмущающее воздействиеwс шагом в единицу. Возмущающее воздействие может представлять собой выезд автомобиля из выбоины.Далее мы будем анализировать один график, на котором по осиx– время, по осиy– разница перемещений масс(x1-x2). Данный график получен как выходная величина блока сумматор, он обведён контуром зеленого цвета на рисунке 9.Если рассматривать только влияние управляющей силыF, достаточно установить возмущающее воздействиеw=0. Таким образом, можно наблюдать следующий отклик системы:Рисунок 10а – Результат работы подвески без возмущенияЕсли рассматривать только возмущающее воздействие , без влияния управляющей силы F, достаточно установитьF=0. Таким образом, можно наблюдать следующий отклик системы:Рисунок 10б – Результат работы подвески без влияния управляющей силыИз графика видно, что для единичного шага приложенной силы, система имеет недостаточное демпфирование. Максимальное значение составляет около 0,072 м, а время установления - 20 с. Из этого графика можно сказать, что поездка на машине не будет комфортной. Чтобы улучшить впечатления от поездки для водителя и его пассажиров, простым и надёжным решением будет внедрение в систему контроллера, который бы мог стабилизировать работу подвески. В случае работы с REPEAT, уже есть готовое решение и это блок «ПИД-регулятор».Настройка регулятора для управления вибрациями системы подвески автомобиляПриведенная выше модель подвески автомобиля условно простой конструкции масса-пружина-демпфер. Но мир не стоит на месте, и современная промышленность требует улучшения ходовых качеств автомобиля и комфорта пассажиров. Пружина и демпфер действительно поглощают излишние колебания, но что, если мы попробуем в соответствии с тенденциями попробовать внедрить ещё один элемент - регулятор, который будет помогать механической системе поглощать колебания ещё лучше и быстрее. Как если бы мы внедрили в подвеску гидравлическую систему, управляемую контроллером с обратной связью.Регулятор – это устройство, которое может быть в виде аналоговой схемы, микросхемы или компьютера, контролирующее и физически изменяющее условия работы данной динамической системы.Существуют различные алгоритмы управления, которые используется в подобных задачах, например адаптивное управление, LQG, нелинейное управление, H на бесконечности, П, ПИ и ПИД регулирование.П, ПИ, ПИД-регуляторы широко используется инженерами, потому что они просты в понимании и учитывают историю системы, а также предвидят ее поведение в будущем. Но не будем останавливаться и повторяться, так как уже написано немало статей по этой теме, а просто приведём здесь статьи для ознакомления:https://www.reallab.ru/bookasutp/5-pid-regulyatori/5-2-klassicheskii-pid-regulyator/https://alexgyver.ru/lessons/pid/В данной работе мы представим простой, но универсальный механизм обратной связи, на примере пропорционально-интегрально-дифференцирующего (ПИД) регулятора.Функцию управления ПИД-регулятор можно представить следующим образом:гдеu(t)- управляющий сигнал,e(t)значение ошибки, иKp,K1,Kn– коэффициенты усиления для пропорциональной, интегрирующей и дифференцирующей составляющих регулятора соответственно. Кроме того, значение ошибки вычисляется следующим образом:гдеr(t)желаемая заданная точка процесса, аy(t)переменная процесса (в нашем случае разность между перемещениями двух масс).Итак, встроим регулятор в нашу схему с обратной связью.Рисунок 11а - Схема модели подвески с обратной связью и регуляторомРисунок 11б – Свойства ПИД-регулятораПри настройке ПИД-регулятора, как блока, оказалось достаточным изменение постоянных пропорционального и дифференцирующего звена, поэтому фактически у нас получился ПД-регулятор.Рисунок 12 - Отклики модели с ПИД-регуляторомНа рисунке 12 видно, что люди, сидящие в автомобиле, ощущают незначительные колебания в течении 6 секунд (на 5 секунде колебания составляют уже меньше 1 мм). ПИД-регулятор позволяет значительно улучшить эксплуатационные характеристики системы подвески автомобиля.Сравнение результатов моделирования:Подводя итог, в данной работе показана модель ¼ системы подвески автомобиля. Мы смоделировали простую подвеску, чтобы посмотреть, как она себя ведет в разомкнутом контуре, без обратной связи. Затем, мы увеличили сложность нашей системы, чтобы смоделировать взаимосвязь между силой управления, массами системы подвески и соответственно коэффициентами жесткости и демпфера. Из этого мы сделали вывод, что для стабилизации системы необходимо внедрить ПИД-регулятор. Он сделал автомобиль более стабильным, но все еще есть место для лучшей настройки, чтобы получить еще более комфортную и стабильную езду для наших пассажиров.Список источниковТарнопольская Т. И. Математическая модель автомобиля с упругими связями // Научные проблемы водного транспорта. 2007. №22. URL:https://cyberleninka.ru/article/n/matematicheskoya-model-avtomobilya-s-uprugimi-svyazyami(дата обращения: 22.05.2024).Karam, Z. A., Awad, O. A. ""Design of Active Fractional PID Controller Based on Whale's Optimization Algorithm for Stabilizing a Quarter Vehicle Suspension System"", Periodica Polytechnica Electrical Engineering and Computer Science, 64(3), pp. 247-263, 2020.https://doi.org/10.3311/PPee.14904Gaur S., Jain S. Vibration control of bus suspension system using PI and PID controller //International Journal of Advances in Engineering Sciences. – 2013. – Т. 3. – №. 3. – С. 94-99."
СберМаркет,,,Борьба за эффективность: рынок low-code в объятиях искусственного интеллекта,2024-06-05T08:15:35.000Z,"Привет! На этот раз обсуждаем рынок low-code. Low-code – это инструмент разработки, содержащий средства визуального проектирования приложений, готовые шаблоны и модули, которые можно компоновать для создания приложений, и требующий кодирования в объеме меньшем, чем при классической разработке приложений (англ. high-code).Давайте честно, программисты – такие же обычные работники, как, например, инженеры, лаборанты, строители. И наша зарплата – зарплата сотрудника, как и любая другая статья расходов, прямо влияет на маржинальность бизнеса.Менеджмент максимально заинтересован в демонстрации собственнику своей эффективности, поэтому внимание руководителей, в первую очередь, сфокусировано на инструментах, позволяющих сократить затраты (время).Одним из таких инструментов повышения производительности труда является технология низкого уровня кодирования (англ. low-code), и мы – авторы статьи (Алексей Мартынов, аналитик-эксперт, руководитель направления методологии продуктового маркетинга в компании «Цифрум» (Росатом), и Максим Кислицкий, руководитель направления группы управления проектами разработки компании «Цифрум» (Росатом)) предлагаем рассмотреть рынок low-code.Йоркширское   строительное общество (активы 60 млрд фунтов стерлингов, 3 млн клиентов)   является третьим по величине строительным обществом Великобритании.У   общества была устаревшая платформа электронной коммерции, которая требовала   длительного времени для внесения изменений. Негибкая инфраструктура не   позволяла развивать онлайн взаимодействие с конечными пользователями.   Следовательно, основной канал коммуникации – онлайн, не работал.За три   месяца на low-code платформе разработки   приложений OutSystems был запущен новый, простой в использовании   онлайн-калькулятор ипотеки.Совокупность   изменений в мобильности обслуживания, качестве интерфейса увеличило конверсию   на 54 % [1].Концептуально, смысл low-code в том, чтобы приблизить создание решения (например, приложения, по управлению талантами) в зону предметной области задач владельца бизнес-сценария (например, пользователя из отдела персонала). Это обеспечивает не только максимальную кастомизацию решения в прямой бизнес-логике, снижению долгосрочной нагрузки на команды разработчиков, но и способствует продвижению технологических новаций внутри компании.Low-code позволяет сократить время нецелевой разработки, объем которой до 80 % времени. А также снизить технический порог за счет удешевления и стандартизации разработки: если состав типовой команды при классической разработке - это senior, middle, junior, аналитики, тестирование и DevOps, то в low-code будет достаточно junior разработчика, аналитиков и специалистов по тестированию.Согласно итогам проведенного исследования консервативная оценка эффективности применения low-code (в сравнении с неиспользованием данного инструмента) составляет 20 – 30 %. При среднем бюджете в 100 млн руб. (1,08 млн долл. [3]) за ИТ-систему экономия равна 20 млн руб. или год работы 2 разработчиков уровня senior.Сейчас время гиперавтоматизации, то есть использования технологий для автоматизации как можно большего числа процессов в целях повышения эффективности бизнеса, поэтому 20 % экономии на круг, когда все вокруг цифровизируется и импортозамещается, выходит весьма значимо.Растущая потребность в ИТ-специалистах [2, 4], а также их недостаточная квалификация (менее 40 % разработчиков соответствуют квалификационным требованиям работодателя [5]) также является драйвером роста спроса на low-code.В табл.1 представлена консервативная оценка авторами глобального рынка low-code технологий, CAGR (совокупный среднегодовой темп роста) составляет 20 % на промежутке 2020 – 2031 гг.Таблица 1.Объем глобального рынка low-code, млрд долл.Наименование субрынка20202021202220232024   Оценка2025   Прогноз2026  Прогноз2027  Прогноз2028  Прогноз2029  Прогноз2030  Прогноз2031  Прогноз1.   Всего low-code технологий разработки7,28,810,712,815,418,522,226,632,038,446,155,4В том числе (деление   по объемам субрынков - условное):1.1.    Low-code   платформы разработки приложений (LCAP)4,46,38,010,012,414,918,121,926,532,038,846,91.2.    Интеллектуальные   системы управления бизнес-процессами (iBPMS) в части low-code2,72,42,62,82,93,43,94,55,26,06,98,01.3.    Другие   low-code технологии разработки (LCD) (инструменты для быстрой разработки   мобильных приложений (RMAD) и инструменты для быстрой разработки приложений   (RAD))0,10,10,10,10,10,20,20,30,30,40,40,52.   Автоматизация бизнес-процессов (RPA)1,72,42,93,43,94,55,26,06,98,09,310,73.   Прочее2,42,63,24,04,85,86,98,410,112,114,617,5В том числе (деление   по объемам субрынков - условное):Многофункциональные   платформы разработки (MDXP) в части Low-code1,92,12,53,03,64,35,26,27,59,010,913,1Гражданские   платформы автоматизации и разработки (CADP) в части low-code0,40,60,71,01,21,51,82,12,63,13,74,4Итого   low-code11,313,816,820,224,128,834,341,049,058,570,083,7С   2000-х г. в рынок low-code стали добавлять:4.   Интеграционная платформа как услуга (iPaaS)3,54,75,76,77,810,313,517,723,230,439,852,2Всего   low-code14,718,522,526,931,939,047,858,772,188,9109,8135,9Примечание: расчеты с использованием [6 - 16]Несмотря на то, что применение low-code, в целом, не имеет отраслевой направленности, финансовая и страховая сферы, телекоммуникации и ИТ, торговля оптовая и розничная совокупно занимают 74 % рынка (см. граф. 1).График 1. Отраслевое распределение глобального рынка low-code, %Примечание: расчеты с использованием [17, 18]Применение low-code имеет прямую корреляцию с отраслями, где требуется высокая скорость внесения изменений в коммуникацию с пользователем, учет требований регуляторов, большая прибыль для инвестирования в разработку, то есть существует потребность и есть ресурсы.В терминологически устоявшемся смысле, когда говорят про low-code технологии разработки, то имеют в виду совокупность «Low-code платформы разработки приложений (LCAP)», «Интеллектуальные системы управления бизнес-процессами (iBPMS)», «Технологии разработки (LCD) (инструменты для быстрой разработки мобильных приложений (RMAD) и инструменты для быстрой разработки приложений (RAD))». Разделение западными аналитиками на несколько рыночных сегментов считаем искусственным, так как функционал данных систем сильно пересекается, поэтому сложно разделить имеющиеся на рынке системы между названными классами.Далее по тексту статьи, low-code употребляется как совокупность указанных субрынков и не рассматриваются другие субрынки, во избежание размытия фокуса.Консервативная оценка авторами российского рынка low-code технологий разработки представлена в табл.2.Таблица 2.Объем российского рынка low-code технологий разработки, млрд руб.Наименование субрынка20202021202220232024   Оценка2025   Прогноз2026  Прогноз2027  Прогноз2028  Прогноз2029  Прогноз2030  Прогноз2031  Прогноз1.   Всего low-code технологий разработки13,217,520,726,331,637,945,554,665,578,694,3113,1В том числе (деление   по объемам субрынков - условное):1.1.    Low-code   платформы разработки приложений (LCAP) + Другие low-code технологии   разработки (LCD) (инструменты для быстрой разработки мобильных приложений   (RMAD) и инструменты для быстрой разработки приложений (RAD))2,73,54,26,57,89,311,213,416,119,323,227,81.2.    Интеллектуальные   системы управления бизнес-процессами (iBPMS) в части low-code10,513,916,519,823,828,634,341,149,459,271,185,3Примечание: расчеты с использованием [19 - 22]Полученная авторами оценка российского рынка верифицирована разными методиками.2022 г.: с российского рынка ушли лидирующие западные компании Outsystems, Salesforce, Servicenow, Appian, Microsoft [23], которые занимали около 30 % рынка (лицензий) в 2021 г. Сейчас считаем, что 5 % отечественного рынка осталось за иностранными компаниями.Множество отечественных компаний (например, «1С» ГК, «Сбербанк-Технологии» АО, «Колловэар» ГК, «Наумен» ГК, «БизнесАвтоматика» НПЦ ООО, «Гриндата» ООО, «Флекс Софтваре Системс» ГК, «ДоксВижн» ГК, «ЭЛМА» ГК, «Финансовые информационные системы» ГК, «1Форма» ООО, «Пайрус» ООО, «Ситек» ООО, «Кейс Студио» ООО, «Росатом» ГК и др.) реализуют свои решения на российском рынке, предоставляя ИТ-решения для разной целевой аудитории, продукты для холдингов, средних компаний, профессиональных команд разработки и др. В связи с динамичностью текущей рыночной позиции, ожидаем укрупнение продуктов через M&A.При использовании классических low-code систем, можно столкнуться с рядом ограничений, которые требуется учитывать при внедрении (см. табл. 3).Таблица 3.Обозначенные проблемы использования low-code, в целом характерны и для high-code подхода. Актуальные подходы для их решения из мира классической разработки могут и должны быть использованы в low-code платформах. Применение в современных low-code платформах микросервисной архитектуры, контейнерной виртуализации, открытых отраслевых стандартов для описания частей приложения, а также широко используемых языков разработки для описания логики приложения, позволяет нивелировать обозначенные выше проблемы. Таким образом существует тренд на унификацию подходов к разработке приложений: как на применение отдельных компонентов low-code в классической разработке, так и на использование многих технологий и принципов организации классической разработки приложений в low-code платформах.Город   Роттердам обладает порядка 800 ИТ-системами по поддержке городских функций (в   том числе управления транспортом, здравоохранением, зонированием, ЖКХ и др.)   для 0,7 млн человек.6-7 лет   назад городскому управлению пришло осознание следующих проблем: ожидания   потребителей, связанные с увеличением объема и удобством цифрового   взаимодействия, чувствительность передачи конфиденциальных данных внешним   исполнителям, рост отставания времени разработки и увеличение числа теневых   ИТ-решений.В   результате: внедрено 100+ ИТ-приложений в государственном секторе за период   2018 – 2022 гг. на основе low-code   платформы Mendix [24].Бурное развитие искусственного интеллекта расширяет применение low-code, демократизируя разработку [25, 26]. Синергия ИИ и low-code значительно расширяет количество компаний и людей, которые могут использовать эти технологии при создании своих приложений [27, 28] (например, через генерацию фрагментов кода, частей приложения по ТЗ, тестирование приложений на соответствие ТЗ). Более того, по мере роста использования ИИ в low-code платформах растет и сложность приложений, которые они могут создавать. Перспективным направлением здесь видится создание интегрированных сред разработки, поддерживающих работу ИИ-агентов разного профиля (аналитика, разработка, тестирование), позволяющих декомпозировать постановку задачи от пользователя на набор задач по разработке, выполнить эти задачи и проверить полученный результат на работоспособность и соответствие изначальным требованиям.А теперь к самому интересному – к low-code платформе РосатомаОна разработана специалистами компании «Цифрум» (Росатом) на основе платформы Multi-D, реализуемой в АО «АСЭ» (Инжиниринговый дивизион Госкорпорации «Росатом») с 2018 года. Этот программный продукт представляет собой набор инструментов и компонентов для создания, сопровождения и развития бизнес-приложений.Пользователи платформы получают возможность быстрого и экономические выгодного создания прототипа приложений: предприятия смогут самостоятельно производить разработку без привлечения сторонних ИТ-компаний, сократят зависимость от функционала применяемых коробочных продуктов, получают возможность ориентироваться на свои специальные нужды. Использование платформы позволяет снизить затраты на разработку, поддержку и интеграцию приложений.Платформа создана для разработки широкого спектра информационных систем. От существующих аналогов платформу отличает микросервисная архитектура, что позволяет создавать на ее базе высоконагруженные решения, а также легко расширять ее новыми компонентами при разработке приложений. Другие важные преимущества - использование технологий искусственного интеллекта при разработке, хранение всех настроек приложений в виде исходного кода в распределенной системе управления версиями (GIT), а также наличие механизмов сборки и доставки приложений (CI/CD) в различные среды для разработки, тестирования и эксплуатации. Это значительно упрощает разработку и внедрение сложных бизнес-приложений большими командами, а также сокращает время от возникновения потребности доработки функционала до получения результата, позволяя в короткие сроки создать необходимый прототип.Low-code платформа Росатома рассчитана на применение в атомной отрасли, ТЭКе, транспортной и строительной отраслях, девелопменте, медицине, образовании, а также в процессах государственного управления. На базе платформы могут быть решены задачи по автоматизации бизнес-функционала планирования, закупок, цепочек поставок, кадровой работы, CRM и маркетинга, управления активами и недвижимостью, управления делами и документооборота.Есть планы использования платформы в качестве одного из основных инструментов разработки информационных систем атомной отрасли. В ближайшей перспективе на базе платформы будут решаться задачи, связанные с управлением жизненным циклом объектов капитального строительства (календарно-сетевое планирование, система технического электронного документооборота, система управления информацией) для сложных промышленных строек, включая стройки атомной отрасли.Библиографический список1.       Gary Flood. Yorkshire Building Society offers members improved digital experience using low code [Электронный ресурс] // Diginomica Ltd. - СМИ. URL: https://diginomica.com/yorkshire-building-society-offers-members-improved-digital-experience-using-low-code (дата обращения 07.04.2024).2.       Joe McKendrick. Everyone loves low-code/no-code development, but not all are ready for it [Электронный ресурс] // ZDNET - СМИ. URL: https://www.zdnet.com/article/everyone-loves-low-code-development-but-not-all-are-ready-for-it/?utm_source=dlvr.it&utm_medium=twitter#ftag=RSSbaffb68 (дата обращения 07.04.2024).3.       Top 60 No-Code Low-Code Citizen Development Statistics, Facts, and Trends you cannot miss in 2024 [Электронный ресурс] // Quixy – ИТ-компания. URL: https://quixy.com/blog/no-code-low-code-citizen-development-statistics-facts/#:~:text=The%20global%20low-code%20platform,period%20(2020-2030) (дата обращения 07.04.2024).4.       Low code in 2023: digitalization, AI, trends, and what the future holds [Электронный ресурс] // Akveo Inc. – ИТ компания. URL: https://www.akveo.com/blog/low-code-in-2023-digitalization-ai-trends-and-what-the-future-holds (дата обращения 20.04.2024).5.       Low-Code Development Platform Global Market Report 2022: Increasing Demand for Applications for Business Organizations Driving Growth [Электронный ресурс] // Yahoo Inc. – СМИ. URL: https://finance.yahoo.com/news/low-code-development-platform-global-104800602.html (дата обращения 07.04.2024).6.       Gartner Forecasts Worldwide Low-Code Development Technologies Market to Grow 23% in 2021 [Электронный ресурс] // Gartner, Inc. – аналитическая компания. URL: https://www.gartner.com/en/newsroom/press-releases/2021-02-15-gartner-forecasts-worldwide-low-code-development-technologies-market-to-grow-23-percent-in-2021 (дата обращения 07.04.2024).7.       Gartner Forecasts Worldwide Low-Code Development Technologies Market to Grow 20% in 2023 [Электронный ресурс] // Gartner, Inc. – аналитическая компания. URL: https://www.gartner.com/en/newsroom/press-releases/2022-12-13-gartner-forecasts-worldwide-low-code-development-technologies-market-to-grow-20-percent-in-2023 (дата обращения 07.04.2024).8.       Revenue from low-code development technologies worldwide from 2019 to 2022, by category [Электронный ресурс] // Statista – аналитическая компания. URL: https://www.statista.com/statistics/1207841/low-code-technologies-revenue-development/ (дата обращения 07.04.2024).9.       Low-code development platform market revenue worldwide from 2018 to 2024 [Электронный ресурс] // Statista – аналитическая компания. URL: https://www.statista.com/statistics/1226179/low-code-development-platform-market-revenue-global/ (дата обращения 07.04.2024).10.   Low-Code Application Development Platform Market Size - By Component (Platform, Services), Enterprise Size (Large Enterprises, SMEs), Deployment Model (On-Premises, Cloud), Application (Web-based, Mobile-based, Desktop-based), End-Use & Forecast, 2024 – 2032 [Электронный ресурс] // Global Market Insights Inc. – аналитическая компания. URL: https://www.gminsights.com/industry-analysis/low-code-application-development-platform-market#:~:text=Low-Code%20Application%20Development%20Platform%20Market%20size%20was%20valued%20at,21%25%20between%202024%20and%202032 (дата обращения 07.04.2024).11.   Global Intelligent Business Process Management Suites (iBPMS) Market Size By Deployment Mode, By Organization Size, By Industry Vertical, By Geographic Scope And Forecast [Электронный ресурс] // Verified Market Research – аналитическая компания. URL: https://www.verifiedmarketresearch.com/product/intelligent-business-process-management-suites-ibpms-market/ (дата обращения 07.04.2024).12.   Multiexperience Development Platform Market worth $12.26 billion by 2030, growing at a CAGR of 20.48% - Exclusive Report by 360iResearch [Электронный ресурс] // openPR - СМИ. URL: https://www.openpr.com/news/3441830/multiexperience-development-platform-market-worth-12-26 (дата обращения 07.04.2024).13.   Robotic Process Automation Market By Type (Service, Implementing, Consulting, Training, Software); By Deployment (On-premise, Cloud); By Application (Pharma & Healthcare, BFSI, IT & Telecom, Retail & Consumer Goods, Communication &Media, Logistics, Energy & Utilities, Others); By Geography, Size, Share, Segment revenue estimation, Forecast, 2021-2030 [Электронный ресурс] // Strategic Market Research – аналитическая компания. URL: https://www.strategicmarketresearch.com/market-report/robotic-process-automation-market (дата обращения 07.04.2024).14.   Integration Platform as a Service Market - Segments - by Integration Types (Application Integration, Application Program Interfaces Integration, Data Integration, and Process Integration), Services (API Life Cycle Management, Business to Business & Cloud Integration, Data Mapping & Transformation, Integration Flow Development & Life Cycle Management Tools, Internet of Things, and Routing & Orchestration), Deployments (On-premise, Hybrid, and Cloud), and Regions (Asia Pacific, North America, Latin America, Europe, and Middle East & Africa) - Global Industry Analysis, Growth, Share, Size, Trends, and Forecast 2023 – 2031 [Электронный ресурс] // Growth Market Reports - аналитическая компания. URL: https://growthmarketreports.com/report/integration-platform-as-a-service-market-global-industry-analysis (дата обращения 07.04.2024).15.   Integration Platform as a Service (iPaaS) Market Size, Share, Growth, and Industry Analysis, By Type (Cloud Based, On Premises and Hybrid), By Application (BFSI, Consumer Goods and Retail, Education, Government and Public Sector, Healthcare and Life Sciences, Manufacturing and Others), Regional Insights and Forecast to 2031 [Электронный ресурс] // Business Research Insights - аналитическая компания. URL: https://www.akveo.com/blog/low-code-in-2023-digitalization-ai-trends-and-what-the-future-holds (дата обращения 07.04.2024).16.   Apurva Venkat. Low-code development technologies market forecast to hit $44.5 billion by 2026 [Электронный ресурс] // IDG Communications, Inc. - СМИ. URL: https://www.infoworld.com/article/3682783/low-code-development-technologies-market-forecast-to-hit-445-billion-by-2026.html (дата обращения 07.04.2024).17.   Рынок платформ разработки Low-Code по приложениям (веб-версии, мобильные устройства, настольные компьютеры и серверы), конечному использованию (BFSI, автомобилестроение и производство, розничная торговля, образование, ИТ и телекоммуникации, транспорт и логистика, другие) и региону, глобальные тенденции и прогноз с 2023 по 2030 год [Электронный ресурс] // Exactitude Consultancy – аналитическая компания. URL: https://exactitudeconsultancy.com/ru/reports/33949/low-code-development-platform-market/ (дата обращения 07.04.2024).18.   Nikhil Solanki. A Complete Guide to Low-Code Development Platforms for 2024!  [Электронный ресурс] // ManekTech - ИТ-компания. URL: https://www.manektech.com/blog/low-code-development-platforms (дата обращения 07.04.2024).19.   Рынок BPM 2021 [Электронный ресурс] // CNews («СиНьюс») - СМИ. URL: https://www.cnews.ru/reviews/rynok_bpm_2021 (дата обращения 07.04.2024).20.   Рынок BPM 2022 [Электронный ресурс] // CNews («СиНьюс») - СМИ. URL: https://www.cnews.ru/reviews/rynok_bpm_2022 (дата обращения 07.04.2024).21.   Рынок BPM [Электронный ресурс] // CNews («СиНьюс») - СМИ. URL: https://www.cnews.ru/reviews/rynok_bpm_2023/review_table/bf62d237ad1809c913537424885bc4cd8e44c9ea (дата обращения 07.04.2024).22.   Сайты компаний: «КОРУС Консалтинг» ГК, «Ланит» ГК, «Колловэар» ГК, «АЙГОТТА» ООО, «Наумен» ГК и других.23.   Oleksandr Matvitskyy, Kimihiko Iijima, Mike West, Kyle Davis, Akash Jain, Paul Vincent. Magic Quadrant for Enterprise Low-Code Application Platforms [Электронный ресурс] // Gartner Inc. – аналитическая компания. URL: https://www.gartner.com/doc/reprints?id=1-2F9LTENA&ct=231010&st=sb (дата обращения 07.04.2024).24.   The City of Rotterdam Empowers Development at Scale [Электронный ресурс] // Mendix Technology – ИТ-компания. URL: https://www.mendix.com/customer-stories/the-city-of-rotterdam-empowers-development-at-scale/ (дата обращения 07.04.2024).25.   А. Мартынов, Д. Ларионов. Между хайпом и реальностью: объем мирового рынка генеративного ИИ в 2024 году с прогнозом до 2032 года [Электронный ресурс] // Хабр – СМИ. URL: https://habr.com/ru/companies/rosatom/articles/796537/ (дата обращения 20.04.2024).26.   Paul Vincent,  Kimihiko Iijima,  Mike West,  Oleksandr Matvitskyy,  Kyle Davis,  Akash Jain. Critical Capabilities for Enterprise Low-Code Application Platforms [Электронный ресурс] // Gartner Inc. – аналитическая компания. URL: https://www.gartner.com/en/documents/4853531 (дата обращения 07.04.2024).27.   Lya Laurent. Low-Code AI: The Future of Efficient App Development [Электронный ресурс] // AppMaster – ИТ компания. URL: https://appmaster.io/blog/low-code-ai-the-future-of-efficient-app-development (дата обращения 07.04.2024).28.   Mateo Clausse. How artificial intelligence is impacting low-code and no-code platforms [Электронный ресурс] // Planet Crust – ИТ-компания. URL: https://www.planetcrust.com/how-artificial-intelligence-is-impacting-low-code-and-no-code-platforms?utm_campaign=blog (дата обращения 07.04.2024)."
СберМаркет,,,Математическое моделирование технологических объектов и систем глазами и руками студента,2024-04-11T08:55:35.000Z,"Мы рады представить вам нашу очередную статью из цикла «Моделирование руками и глазами студента», посвящённую разработке контроллера для аппарата искусственной вентиляции лёгких, выполненной в программном обеспечении REPEAT. Автором этой работы является студентка РГУ имени Косыгина Ульяна, которую мы благодарим за её усердие и талант.В статье описана математическая модель и результаты расчётов, которые показывают, как программное обеспечение REPEAT может быть использовано в области медицинской техники.Ссылка на телеграм-канал REPEAT:https://t.me/repeatlabРазработка контроллера для аппарата искусственной вентиляции легкихГермогенова Ульяна Леонидовна, студент 4 курса, Института мехатроники и робототехники, кафедра Автоматики и промышленной электроникиРГУ им. А.Н. Косыгина, МоскваНазначение контроллераКонтроллер аппарата искусственной вентиляции позволяет настраивать и контролировать процесс вентиляции, чтобы обеспечить требуемую поддержку дыхательной функции пациента. Кроме того, контроллер позволяет наблюдать состояние пациента и адаптировать параметры вентиляции в соответствии с его потребностями [1].Контроллер необходим для управления и регулирования следующими параметрами вентиляции:1. Давление.2. Расход.3. Частота дыхания.Для обеспечения работы контроллера требуется выполнить моделирование аппарата искусственной вентиляции с учётом математической модели лёгких пациента.Описание математической моделиАппарат искусственной вентиляции состоит из следующих частей:- Центробежный насос (вентилятор)- Воздушные соединители (шланги).Модель центробежного насосаВентилятор нагнетает давление окружающего воздуха для вентиляции легких пациента и выдувает воздух с объёмным расходом Qout и давлением pout. Его источником является управляющий сигнал pcontrol.Характеристики вентилятора назначены для установившегося режима. В REPEAT вентилятор моделируется как динамическая система с инерцией по следующей формуле:где ωn = 2πn, n = 30 об/сек, а коэффициент затухания ζ = 1, соответствующий реальным экспериментальным данным,  – комплексная переменная [2].Для реализации модели центробежного насоса на платформе REPEAT используется передаточная функция с заранее вычисленными постоянными коэффициентами числителя и знаменателя.Модель воздушных соединителей (шланги)Воздух, выходящий из вентилятора, проходит через шланг, прежде чем попасть в лёгкие пациента. Шланг для утечки обеспечивает возможность выдыхаемому воздуху выйти из воздушных разъемов в условиях окружающей среды (pamb = 0). Также шланг для утечки допускает свежий воздух при вдохе. Давление на конце воздушных соединителей, т. е. давление, подаваемое в дыхательные пути легких, составляет paw. Объемный расход воздуха в легкие пациента составляет Qpat, а расход утечки Qleak.Потери в шлангах моделируются линейными функциями сопротивления Rhose и Rleak для длинного шланга и шланга утечки, соответственно. Падение давления на этих участках вычисляется пропорционально объемному расходу, как показано ниже:С учетом принципа сохранения потока выходной поток , поток пациента  и поток утечки  связаны следующим образом:Модель лёгких пациентаЛегкие реагируют на объемный приток/выход воздуха (Qpat) за счёт изменения легочного давления и за счёт объёмного расширения/сжатия.Модель лёгких пациента последовательно состоит из следующих элементов:- Rlung сопротивление для учёта линейных вязких потерь;- Clung податливость для учёта эластичности легких.Уравнения, описывающие динамику лёгких, представлены ниже:где pext внешнее давление на лёгкие, которое принимается равным нулю. Clung рассчитывается по следующей формуле:Рисунок 1. Схема системы вентиляцииТаблица данных, по которым будут вычисляться и вводиться значения, представлена ниже (Таблица 1).Таблица 1. Параметры, используемые для моделированияСтадия моделированияДля ввода значения входного давления на аппарате искусственной вентиляции лёгких используем блоки “Константа”, “Время моделирования” и “Jython” (см. Рисунок 2). В константы соответственно задаются значения целевого давления плато, положительного давления в конце выдоха, целевого времени подъёма, времени дыхательного цикла, соотношения инспирации и экспирации.Рисунок 2. Блок-схема аппарата искусственной вентиляции лёгкихКод блока “Jython” представлен ниже (см. Рисунок 3):Рисунок 3. Код блокаДля моделирования вентилятора используется блок “Передаточная функция” с заранее вычисленными коэффициентами из библиотеки “Автоматика” (см. Рисунок 4).Рисунок 4. Блок “Передаточная функция”Для комбинированной модели для шланга и легких создан блок пространства состояний во внешнем проекте. Для его моделирования были использованы блоки “Усилитель”, “Сумматор”, “Интегрирующее звено” из библиотеки “Автоматика” (см. Рисунок 5).Рисунок 5. Используемые блоки из библиотеки “Автоматика”Разработанный проект блока пространства состояний представлен ниже (см. Рисунок 6). Параметры блока задаются из значений A, B, C, D, которые вычисляются следующим образом:Рисунок 6. Модель блока пространства состоянийДля ПИД-регулирования используется блок “ПИД-регулятор” и единичное усиление. В итоге получаем полную модель на общей схеме (см. Рисунок 7).Рисунок 7. Схема модели контроллера для аппарата искусственной вентиляции лёгких на платформе REPEATРезультаты моделированияРезультаты моделирования были сняты с блоков “Усилитель” (номера 16 и 21) и представлены на графиках соответственно (Рисунок 8, Рисунок 9) для каждого параметра.Рисунок 8. Изменение давления в лёгких пациентаРисунок 9. Расход воздуха в лёгких пациентаРегулировка коэффициента переменного усиленияСтратегия регулировки коэффициента переменного усиления может быть разработана на основе величины объемного расхода воздуха пациента Qpat. Основная идея заключается в следующем: когда величина расхода Qpat высока (скажем, выше порога δ – длины переключения), можно использовать контроллер с высоким коэффициентом усиления. Напротив, когда расход низкий, предпочтительнее использовать регулятор с низким коэффициентом усиления, который минимизирует колебания в расходе.Нелинейный коэффициент усиления переключателя ф может быть описан следующим уравнением:где α - высокий коэффициент усиления. Вышеуказанный коэффициент усиления переключателя может быть использован для измерения погрешности и передан в ранее описанный регулятор с низким коэффициентом усиления.Проектируем нелинейный коэффициент усиления переключателя ф(Qpat) как новый подпроект. Схема показана ниже (см. Рисунок 10).Рисунок 10. Схема переключателяВышеупомянутая подмодель теперь может быть включена в нашу систему для реализации схемы с коэффициентом усиления. Полная схема показана ниже (см. Рисунок 11).Рисунок 11. Схема модели контроллера для искусственной вентиляции с переменным усилениемТаким образом, программное обеспечение REPEAT обладает широким потенциалом для использования и имеет перспективы в области медицинской техники, и мы надеемся, что эта статья послужит стимулом для дальнейших исследований.Список источниковB. Hunnekens, S. Kamps and N. Van De Wouw, ""Variable-Gain Control for Respiratory Systems,"" in IEEE Transactions on Control Systems Technology, vol. 28, no. 1, pp. 163-171, Jan. 2020, doi: 10.1109/TCST.2018.2871002. keywords: {Lung;Hoses;Ventilation;Mathematical model;Hospitals;Mechanical ventilation;performance;respiratory systems;variable-gain control}.Преобразование ЛапаласаУзнайте больше о преимуществах использования REPEAT попробовав демоверсию на нашемсайте!"
СберМаркет,,,Экономический анализ: как генеративный ИИ меняет производительность труда и перспективы профессий,2024-04-02T10:02:08.000Z,"На связи Алексей Мартынов и Денис Ларионов, мы продолжаем делиться актуальной аналитикой о генеративном ИИ. Предыдущую нашу статью с описанием текущего положения дел на рынке генеративного ИИ и прогнозом до 2032 года найдётездесь.Для тех, кто ещё не знаком с нами, представляюсь подробнее: Алексей Мартынов (автор этого текста), аналитик-эксперт, руководитель направления методологии продуктового маркетинга в компании «Цифрум» (Росатом) и мой коллега-соавтор – Денис Ларионов, эксперт в области нейроморфных вычислений, руководитель отдела ИИ компании «Цифрум» (Росатом).На повестке социальная тема – производительность труда и перспективы профессий.1999 г.: бывший председатель ФРС Пол Волкер сказал: «Судьба мировой экономики теперь полностью зависит от роста экономики США, который зависит от фондового рынка, на чей рост влияют акции примерно 50 компаний» [1].2024 г.: технологии ИИ стали одним из важных триггеров фондового рынка. Оценочно, тема генеративного ИИ обеспечила 10 – 15 % (тема ИИ в целом обеспечила 25 – 35 %) роста S&P 500 в 2023 г. и остается в фокусе внимания инвесторов в 2024 г. [2] Десяток американских технологических компаний (Amazon, Nvidia, Microsoft и др.) являются локомотивами этого исторического роста.В отчетах некоторых аналитиков можно встретить и более значительные оценки влияния – до 2/3 роста капитализации рынка сформировали компании, имеющие прямое или косвенное отношение к ИИ из технологического сектора, торговли, финансовой и страховой сфер.За 25 лет (1999 – 2024 гг.) произошламонополизация фондового рынка и трансформация мира- активов, которые оказывают значительное влияние на фондовый рынок и благосостояние населения в целом.Каталонский   художник – сюрреалист Сальвадор Дали часть своих картин увидел во сне. Дали,   сидя на стуле, держал металлический предмет в руках и засыпал. В какой-то   момент предмет из рук с грохотом падал и будил художника. Мастер просыпался и   мгновенно зарисовывал свои сны на бумагу.Автор советского   олимпийского мишки В.А. Чижиков вспоминал: «Неожиданно мне приснился во сне   медведь с поясом, содержащий нужные цвета и с олимпийскими кольцами в виде   пряжки. Я тут же подскочил и все зарисовал. Если ты увидишь что-то во сне и   тебе надо это хорошо запомнить, надо заставить себя проснуться, зарисовать и   только в таком случае не забудешь свой сон».Если быть   визионером, то каждый из нас немного Дали: после 2030 года можно ожидать распространение технологии нейроинтерфейсов,   которая соберет образы из нашего подсознания в текстовый и иные форматы [3].Прогресс человечества в различных областях является нелинейным, ключевая роль в ускорении может принадлежать одной или нескольким новациям. За последние 100 лет было открыто рентгеновское излучение и двойная структура ДНК, созданы антибиотики и инсулин, сделаны первые шаги на пути создания общего (сильного) ИИ, созданы атомная, космическая и другие промышленности [4].Привычный нам уровень жизни базируется на технологиях, которые были созданы в ХХ веке. Важный вопрос текущего времени заключается в том, какое влияние генеративный ИИ (имея в виду такие сервисы как: chatGPT, Grok, Claude, Mistral, SORA, DALL-E, Midjourney, GigaChat и другие) окажет на производительность труда и характер работы: ускорит ли эта технология существующую тенденцию автоматизации без компенсирующего эффекта в виде создания качественных рабочих мест, или технология позволит внедрить новые дополняющие трудовые функции задачи для работников [5].Компания McKinsey создала испытательный стенд —-лабораторию для тестирования эффектов, инструментов и разработок в части генеративного ИИ. 40 разработчиков в течение нескольких недель выполняли обычные задачи по разработке программного обеспечения в областях — генерация кода, рефакторинг, документация. Каждая задача выполнялась тестовой группой, имевшей доступ к двум инструментам на основе генеративного ИИ, и контрольной группой, которая не пользовалась помощью ИИ. Каждый разработчик участвовал в тестовой группе в выполнении половины заданий, а в контрольной группе - в другой половине. Для измерения времени, затраченного на выполнение каждой задачи, велось журналирование: разработчики записывали время начала, окончания и перерыва [6].Обобщая полученные результаты, можно выделить основное: использование генеративного ИИ для простых (шаблонных / типовых) задач дает значительный прирост производительности (до 50%), при решении не типовых задач использование инструментов генеративного ИИ в настоящее время не показывает безальтернативной результативности (см. Рис. 1).Выгоды использования генеративного ИИ непропорционально распределены между сотрудниками разной квалификации: преимущественные эффекты достаются менее опытным и низкоквалифицированным работникам. Генеративный ИИ минимально влияет на производительность более опытных или квалифицированных работников [7].Рис.1. Эффективность применения генеративного ИИ для сокращения затраченного времени на выполнение задач в части написания программного обеспеченияПоследствия применения генеративного ИИ для экономики и общества сейчас трудно оценить [8]. Аналитики, представители компаний (бенефициаров технологии) озвучивают разные оценки эффективности ИИ, основанные на замерах в лабораторных условиях, полученные при решении небольших задач, базирующиеся на экспертном мнении. Очевидно, что озвученные эффекты не тождественны результатам, получаемым при промышленном внедрении инструментов в реальное производство. Простое переложение расчетных эффектов в экономику является сильным упрощением системы.Более того, развитие направления генеративного ИИ также осуществляется нелинейно (с нарастающим ускорением) и по мнению многих экспертов настоящие технологические прорывы еще впереди. Одной из популярных концепций, позволяющих взглянуть на направление генеративного ИИ в его развитии, является идея эмерджентности. Суть этой концепции в том, что наращивание вычислительных ресурсов в системе, может привести к появлению в ней принципиально новых качественных свойств, которые не закладывались в систему изначально.Так случилось с моделью GPT, которая, решая задачу предсказания следующего слова в последовательности, обрела новые качественные свойства. Например, эта модель научилась переводить тексты с одного языка на другой, размышлять, сопоставлять факты и многое другое.Однако, информация, полученная из текстов (и вообще говоря из языка) является весьма ограниченной, если мы хотим по-настоящему понимать (предсказывать) окружающий нас мир. Например, для того чтобы предсказать, что у вошедшего в теплое помещение на шапке растает снег, люди не используют учебники физики с формализованными знаниями, они используют свой опыт наблюдения за миром, выделяя базовые закономерности (вроде той что снег тает в помещении). Грубо можно сказать, что они учатся на видео.Становится очевидна идея – продемонстрировать эмерджентность на задаче, связанной не с текстами, а с видео. Это может быть предсказание следующего кадра, либо части текущего кадра. Как известно, обучение генеративных моделей на текстах стоит очень дорого, но обучение на видео стоит на порядки (более чем в сто раз) дороже. Именно с этим фактом связывают взрывной рост капитализации компаний, производящих аппаратное обеспечение для обучения. Первые ростки генеративного ИИ, работающего с видео уже появились – проект Sora.Переход в направлении генеративного ИИ от текстов и картинок к видео и звуку окажет непредсказуемое влияния не только на экономику и промышленность, но и на общество в целом.Если же посмотреть еще дальше, то эмерджентностью можно объяснить появление сознания в нервной системе человека при ее развитии выше некоторого порога. Такое мнение высказывает в своих недавних работах К. В. Анохин [9]. Однако, по мнению Яна ЛеКуна [10], мы еще очень далеко от создания общего ИИ, то есть эмерджентности на видео будет недостаточно для его создания.В связи с этим оценки эффектов от внедрения генеративного ИИ требуют вдумчивого подхода, все цифры носят предварительный характер, так как никто не понимает, насколько далеко зайдет внедрение генеративного ИИ.2023 г.:   исследователи из компании Absci   Corporation показали, как модель генеративного ИИ смогла   спроектировать несколько новых антител, которые связываются с   рецептором-мишенью более тесно, чем ранее известные терапевтические антитела.    Показатели эффективности модели   ИИ в 5-30 раз превышает исследованные биологические исходные   показатели.В   данной работе важно то, что исследователи сначала удалили все эталонные   данные об антителах, чтобы система не могла просто имитировать и   воспроизводить структуру известных антител, которые хорошо работают. Дизайны,   созданные генеративным ИИ, были разнообразными (это означает, что у них не   было аналогов, о которых было известно, что они уже существуют), и их было   легко разработать и, следовательно, катализировать сильный иммунный ответ [11, 12].Постиндустриальный технологический уклад(5 и 5++ технологические уклады) подразумевают перераспределение занятости в высокомаржинальные сегменты и развитие «сервисной» модели экономики [13] при росте производительности труда.Широко тиражируемый тезис о низкой производительности труда в РФ не находит подтверждения. Например, размер ВВП по покупательной способности на одного работающего в 2023 г. составил 75 тыс. долл. в РФ и 88 тыс. долл. в Японии, разница составляет менее 15 %.За прошедшие 10 лет темп роста производительности труда в РФ находится на уровне развитых стран, таких как Франция, Германия, США (см. Таблицу 1).Таблица 1. Примечание: расчеты с использованием [14 - 17]Все это позволяет утверждать, что, несмотря на сохраняющееся значительное превосходство развитых стран по уровню технологического развития, это доминирование является преодолимыми [16]. Производительность труда является показателем конкурентоспособности экономики страны. Без поддержки в долгосрочной перспективе высоких темпов роста производительности труда невозможно (в числе первых) перейти в следующий технологический уклад.Первая   промышленная революция (1770 – 1830 гг.): широкое применение парового   двигателя.1800 г.:   в Британии насчитывалось более 2 500 паровых машин, большинство из которых   использовалось на шахтах, хлопчатобумажных фабриках и фабриках. Это   сопоставимо с 200 двигателями во Франции и менее 10 в США.Лидерство   в данной новации, оценочно, обеспечила мировое доминирование Британии на 100+   лет.До недавнего времени человечество привыкло жить в условиях постоянного роста благосостояния. Если оценивать рост благосостояния через разнообразие номенклатуры производимой продукции, то он окажется квадратичным. Однако, в прошлом этот рост осуществлялся главным образом за счет роста населения (в том числе массового выхода женщин на работу). Гипотеза С. А. Шумского [19] состоит в том, что ИИ, является той самой «замыкающей» технологией, котораяпозволит человечеству дальше жить в условиях роста благосостоянияпри замедлении темпов роста населения. Именно поэтому, вопросы, раскрываемые в настоящей статье, актуальны.В Таблице 2 приведена консервативная оценка воздействия генеративного ИИ на производительность труда и ВВП.Таблица 2. Примечание: расчеты с использованием [5, 20 - 22]Повышение производительности труда из-за влияния генеративного ИИ можетежегодно привносить в ВВП РФ порядка 600 млрд руб. или 0,35 % к темпам роста производительности труда, совокупно достигая 4,1 % в год.Наиболее часто упоминаемыебизнес-функциис использованием генеративного ИИ совпадают с теми, в которых использование искусственного интеллекта наиболее распространено в целом. 75 % эффекта, который просматривается в части применения генеративного ИИ, сосредоточено в маркетинге и продажах, разработке продуктов и исследованиях, сервисных операциях, сквозных действиях (подробнее см. в Таблице 3) [24, 25].Таблица 3По мнению некоторых аналитиков, генеративный ИИ не создает дополнительной стоимости в экономике, а девальвирует ценность конечного продукта (например, контента) через снижение порога входа, нивелирование уникальности и эксклюзивности. Если генеративный ИИ будет создавать качественный видео-фото-музыкальный и др. контент, то это приведет к трансформации, например, медиаиндустрии и усилит межотраслевые диспропорции.Авторы настоящей   статьи опросили нескольких специалистов, работающих в редакторах фото- видео контента.   Отзыв по эффективности применения генеративного ИИ у специалистов был   аналогичен результату, приведенному на Рис.1: для типовых, массовых,   невзыскательных запросов – эффективно, для получения уникального результата,   учитывающего особенности объекта – не эффективно на текущий момент.Исторические примеры показывают, что эффект от применения новшеств (в нашем случае —технологии генеративного ИИ) может бы положительным, да и текущая статистика в сфере занятости это подтверждает [26].Производственный   процесс в автомобилестроении в 1910-х годах был значительно автоматизирован.   Под управлением Г. Форда (наряду с методами массового   производства и сборочными линиями) был представлен ряд новых задач для   конструкторов, технических специалистов, станочников и служащих, что   значительно увеличило потребность отрасли в рабочей силе [27]. Эти новые   задачи также стали неотъемлемой частью роста производительности — они   способствовали производству новых видов продукции и повышению эффективности   производственных процессов.Вероятно, генеративный ИИ, окажет влияние на все специальности[28]. В странах с развитой экономикой эффект применения генеративного ИИ ожидается для 60% рабочих мест, с развивающейся экономикой – около 40%, в странах с низким уровнем дохода – около 26% [29]. Это не означает выбытия указываемого количества сотрудников, является оценочным объемом влияния технологии.Влияние технологии будет также различно для разных групп работников:профессии и роли, сильно полагающиеся на рутинные когнитивные задачи, более уязвимы к интеграции с генеративным ИИ: например, маркетинг (копирайтер), медиа (журналист, редактор, сценарист), дизайн (брендинг, интерьер, промышленный дизайн), перевод с иностранных языков, коммуникации (кол-центры, диспетчеры), социализация и общение (психолог), программирование, офисная поддержка, грузчики, военные стратеги и др. [30, 31, 32]. Наименьшее же влияние инструментов генеративного ИИ будут испытывать сотрудники производственных, максимально прикладных специальностей.Предсказуемо появление новых специальностей, например, инженер по воспитанию ИИ, тренер ИИ и, возможно даже, врач ИИ.Критический ресурс будущего (следующие 10 лет) – этотворческий потенциал человека, а, следовательно, работаем дальше.Библиографический список1.              Радика Десаи. Геополитическая экономия: после американской гегемонии, глобализации и империи: Монография / Радика Десаи; науч. ред. российского издания С.Д. Бодрунов. — М. : ИНИР им. С.Ю. Витте : Центр каталог, 2020. — 328 с. — (серия «Современная экономическая мысль»).2.              Сангалова И. Какие тренды будут править мировым фондовым рынком в 2024 году [Электронный ресурс] // Ведомости – СМИ. URL: https://www.vedomosti.ru/investments/articles/2024/01/02/1013521-kakie-trendi-budut-pravit-fondovim-rinkom?ysclid=lslp9ghsmt656868914 (дата обращения 16.03.2024).3.              Accenture Research. Human by design How AI unleashes the next level of human potential. Technology Vision 2024 [Электронный ресурс] // Accenture – аналитическая компания. URL: https://www.accenture.com/us-en/insights/technology/technology-trends-2024 (дата обращения 16.03.2024).4.Сысоев Т. Пик развития пройден: почему научно-технических прорывов больше нет [Электронный ресурс] // РБК – СМИ. URL: https://trends.rbc.ru/trends/futurology/5fd0a19f9a79473f456d7825?from=copy (дата обращения 16.03.2024).5.              The economic potential of generative AI: The next productivity frontier [Электронный ресурс] // McKinsey & Company – аналитическая компания. URL: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier#key-insights (дата обращения 16.03.2024).6.              Unleashing developer productivity with generative AI [Электронный ресурс] // McKinsey & Company – аналитическая компания. URL: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/unleashing-developer-productivity-with-generative-ai (дата обращения 16.03.2024).7.              Erik Brynjolfsson, Danielle Li, and Lindsey R. Raymond. Generative AI at Work [Электронный ресурс] // The National Bureau of Economic Research (NBER) – аналитическая компания. URL: https://www.nber.org/papers/w31161 (дата обращения 16.03.2024).8.              What’s the future of generative AI? An early view in 15 charts [Электронный ресурс] // McKinsey & Company – аналитическая компания. URL: https://www.mckinsey.com/featured-insights/mckinsey-explainers/whats-the-future-of-generative-ai-an-early-view-in-15-charts (дата обращения 16.03.2024).9.              Анохин К.В. Мозг для искусственного интеллекта, искусственный интеллект для мозга [Электронный ресурс]. URL: https://rutube.ru/video/e82c6a3216445cc7d469afc4fdb5cb0c/?ysclid=lu13ivk1jx147446801 (дата обращения 16.03.2024).10.           Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 [Электронный ресурс]. URL: https://www.youtube.com/watch?v=5t1vTLU7s40 (дата обращения 16.03.2024).11.           2024 Tech Trends Report [Электронный ресурс] // Future Today Institute – аналитическая компания. URL: https://futuretodayinstitute.com/trends/ (дата обращения 16.03.2024).12.           Absci achieves a breakthrough in AI drug creation [Электронный ресурс] // Absci Corporation – научно-исследовательская компания в сфере биотеха. URL:  https://www.absci.com/absci-achieves-a-breakthrough-in-ai-drug-creation/ (дата обращения 16.03.2024).13.           Кузнецов Е.Б. Технологии. Большие драйверы роста и развилки догоняющей модернизации. Проект «Горизонт 2040» [Электронный ресурс] // Агентство Стратегических Инициатив – аналитическая компания. URL: https://asi.ru/library/main/198226/ (дата обращения 16.03.2024).14.           Статистические данные по размеру ВВП по покупательной способности [Электронный ресурс] // OECD – организация экономического развития и сотрудничества. URL: https://stats.oecd.org/OECDStat_Metadata/ShowMetadata.ashx?Dataset=PDB_LV&ShowOnWeb=true&Lang=en (дата обращения 16.03.2024).15.           Статистические данные по занятости населения [Электронный ресурс] // OECD – организация экономического развития и сотрудничества. URL: https://stats.oecd.org/Index.aspx?DataSetCode=PDB_LV# (дата обращения 16.03.2024).16.           Международные сопоставления ВВП России и других стран мира [Электронный ресурс] // Федеральная служба государственной статистики (Росстат) – федеральный орган исполнительной власти. URL:https://rosstat.gov.ru/statistics/accounts#(дата обращения 16.03.2024).17.           Gross domestic product (GDP) [Электронный ресурс] // Statistisches Bundesamt (Destatis) – статистическое агентство Германии. URL:https://www.destatis.de/EN/Themes/Economy/National-Accounts-Domestic-Product/Tables/gdp-bubbles.html(дата обращения 16.03.2024).]18.           Узяков М.Н., Серебряков Г.Р. Глобальный контекст и возможности развития России. - М.: Научные труды,№3, 2023.19.           Шумский С. А. Машинный интеллект. Очерки по теории машинного обучения и искусственного интеллекта. – М.: РИОР, 2022 – 339 с.20.           А. Мартынов, Д. Ларионов. Объем мирового рынка искусственного интеллекта в 2023 году с прогнозом до 2032 года [Электронный ресурс] // it-world.ru – СМИ. URL: https://www.it-world.ru/it-news/market/198512.html (дата обращения 16.03.2024).21.           А. Мартынов, Д. Ларионов. Между хайпом и реальностью: объем мирового рынка генеративного ИИ в 2024 году с прогнозом до 2032 года [Электронный ресурс] // it-Хабр – СМИ. URL: https://habr.com/ru/companies/rosatom/articles/796537/ (дата обращения 16.03.2024).22.           J. Manyika, M. Spence. The Coming AI Economic Revolution [Электронный ресурс] // Foreign Affairs – СМИ. URL: https://www.foreignaffairs.com/world/coming-ai-economic-revolution?check_logged_in=1&utm_medium=promo_email&utm_source=lo_flows&utm_campaign=registered_user_welcome&utm_term=email_1&utm_content=20240119 (дата обращения 16.03.2024).23.           Real GDP growth [Электронный ресурс] // МВФ – международная организация. URL: https://www.imf.org/external/datamapper/NGDPD@WEO/OEMDC/ADVEC/WEOWORLD/USA (дата обращения 16.03.2024).24.           AI-powered marketing and sales reach new heights with generative AI [Электронный ресурс] // McKinsey & Company – аналитическая компания. URL: https://www.mckinsey.com/capabilities/growth-marketing-and-sales/our-insights/ai-powered-marketing-and-sales-reach-new-heights-with-generative-ai (дата обращения 16.03.2024).25.           Finding the Sweet Spot for Generative AI in Payments [Электронный ресурс] // Boston Consulting Group – аналитическая компания. URL: https://www.bcg.com/publications/2023/optimizing-genai-in-payments (дата обращения 16.03.2024).26.           Nick Bunker. March 2024 US Labor Market Update: AI Jobs Are on the Rebound [Электронный ресурс] // Indeed Hiring Lab – аналитическая компания. URL: https://www.hiringlab.org/2024/03/14/march-2024-us-labor-market-update/ (дата обращения 16.03.2024).27.           Аджемоглу Д., Джонсон С. Перебалансировка ИИ [Электронный ресурс] // МВФ – международная организация. URL: https://www.imf.org/ru/Publications/fandd/issues/2023/12/Rebalancing-AI-Acemoglu-Johnson (дата обращения 16.03.2024).28.           Шурпина А. Усиление возможностей человека: профессор РАН рассказал о дальнейшем развитии ИИ [Электронный ресурс] // РТ – СМИ. URL: https://russian.rt.com/science/article/1277088-intervyu-iskusstvennyi-intellekt (дата обращения 16.03.2024).29.           Gen-AI: Artificial Intelligence and the Future of Work [Электронный ресурс] // МВФ – международная организация. URL: https://www.imf.org/en/Publications/Staff-Discussion-Notes/Issues/2024/01/14/Gen-AI-Artificial-Intelligence-and-the-Future-of-Work-542379 (дата обращения 16.03.2024).30.           Белоусов Д.Р. Глобальные технологические тренды 2022–2040 годов (часть 1) Сценарная прогнозная оценка к 2040 году по ключевым направлениям развития технологий (ключевые тезисы). Проект «Горизонт 2040» [Электронный ресурс] // Агентство Стратегических Инициатив – аналитическая компания. URL: https://asi.ru/library/main/198226/ (дата обращения 16.03.2024).31.           Generative AI and the future of work in America [Электронный ресурс] // McKinsey & Company – аналитическая компания. URL: https://www.mckinsey.com/mgi/our-research/generative-ai-and-the-future-of-work-in-america (дата обращения 16.03.2024)32.           The state of AI in 2023: Generative AI’s breakout year [Электронный ресурс] // McKinsey & Company – аналитическая компания. URL: https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai-in-2023-generative-ais-breakout-year#/  (дата обращения 16.03.2024)."
СберМаркет,,,Математическое моделирование технологических объектов и систем глазами и руками студента,2024-03-19T07:37:00.000Z,"В рамках нашего продолжающегося цикла статей по теме математического моделирования, рады ознакомить вас с новой моделью, разработанной на основе ПО REPEAT. Эта модель представляет методологию экструзионного производства синтетических нитей.Наша энергия обычно направлена на исследования и проектирование в области атомной и тепловой энергетики, но мы приветствуем и ценим возможность выхода за рамки стандартных пределов и расширения наших горизонтов в рамках проекта «Школа Моделирования». Данную модель и статью разработала талантливая студентка, Анастасия, из РГУ им. Косыгина.Мы надеемся, что данный материал заинтересует вас и позволит вам с новой точки зрения взглянуть на процесс производства синтетических нитей, а также понять все преимущества использования нашего ПО REPEAT в данных задачах.Ссылка на телеграм-канал REPEAT:https://t.me/repeatlabМодель получения синтетических нитей экструзионным способомНезальзова А.О. студент 4 курса, Института мехатроники и робототехники, кафедра ""Автоматика и промышленная электроника""РГУ им. А.О. Незальзова, МоскваВ статье представлена разработанная на платформе REPEAT модель получения синтетических нитей экструзионным способом для производства спанбонда, способная точно описать тепловые процессы, а также процессы охлаждения смеси в экструдере.Синтетическая нить – это волокно, созданное химическим способом из искусственных материалов, таких как полиэфиры или нейлоны. Наиболее широкое применение такие волокна получили в производстве спортивной одежды, технических тканей, медицинских материалов и т.д.Синтетические нити могут производиться несколькими способами, такими как: вязание, литейная формовка, интерсекционное сплетение и электростатическое вращение. Однако наиболее распространенным методом является экструзия.Экструзия - это метод формования, при котором материал прессуется или выдавливается через отверстие в специальной форме, чтобы создать продукт с постоянным сечением.Производство синтетических нитей экструзионным способом имеет несколько преимуществ перед традиционным ткачеством. Это включает в себя более высокую автоматизацию процесса, возможность создания более легких и прочных материалов, а также более широкий спектр возможных характеристик, таких как эластичность и устойчивость к различным условиям эксплуатации.Экструзионный способ получения синтетического волокна важен по нескольким причинам:Высокая производительность: экструзия позволяет массово производить синтетические нити, обеспечивая эффективность и экономию затрат.Точный контроль размеров: процесс экструзии позволяет точно контролировать диаметр и форму синтетических волокон, что важно для достижения определенных характеристик материала.Разнообразие материалов: этот метод подходит для различных синтетических полимеров, что позволяет создавать волокна с разными свойствами, такими как прочность, устойчивость к износу, термостойкость и другие.Гибкость в применении: синтетические волокна, полученные экструзией, применяются в различных отраслях, от текстиля и упаковки до автомобилестроения и медицины, благодаря их разнообразию и адаптивности к различным потребностям.Линия по производству нетканых полотен включает в себя три этапа: плавление полимерного сырья в экструдере, формирование слоя и скрепление.Данная разработка относится к первому этапу, а именно – получению с помощью экструдера химических волокон.Функциональная схема автоматизации экструдера представлена на рисунке 1. В бункер из листовой стали подаются полипропиленовые гранулы, которые затем расплавляются в экструдере. Для обогрева корпуса, в стенки установлены электронагревалели.  Охлаждение осуществляется за счет циркуляции воды в системе, которая проходит через каналы. Изменение вращения двигателя, температура всех трёх зон нагрева, а также температура в формующей головке измеряются соответствующими датчиками. В качестве привода применён трехфазный асинхронный двигатель.Рисунок 1 - ФСА экструдераУравнения описания системыТепловой баланс экструдера:Мощность привода и подводимая тепловая мощность расходуется на нагрев смеси, на потери в окружающую среду и на давление формирования.Изменение температуры в каждой зоне осуществляется в следующих пределах:1 зона нагрева: 145 °C, 2 зона нагрева: 160 °C, 3 зона нагрева: 180 °C.Мощность питания нагревательных элементов (НЭ) расходуется на изменение температуры стенок экструдера и передачу тепла от стенок экструдера смеси. Передаточная функция по температуре в зоне контр. темп. (передаточная функция нагревательных элементов)Тепловая мощность, отдаваемая от стенок экструдера смеси расходуется на нагрев смеси от начальной температуры до температуры смеси и аккумулирование тепла смесью.Передаточная функция, описывающая передачу тепла от экструдера к смеси:Мощность охлаждения расходуется на уменьшение температуры стенокПередаточная функция привода:В качестве привода используется асинхронный двигатель с управлением с помощью частотного преобразования.Передаточная функция, описывающая изменение температуры из-за деформации сдвига:В контурах регулирования температуры стенок экструдера по зонам целесообразно применить ПИ – регулирование. В контуре охлаждения водой – релейное.Стадия моделированияДля моделирования автоматизированных процессов используются блоки “Инерционное звено”, “Переключатель”, “ПИД-контроллер” и “Усилитель” из библиотеки «Автоматика» (см. рисунок 2).Рисунок 2 - Используемые блоки библиотеки «Автоматика»Разработанная модель полностью представлена на общей схеме ниже (см. рисунок 3):Рисунок 3 - Схема модели получения синтетических нитей экструзионным способом на платформе REPEATРезультаты моделированияРезультаты моделирования представлены на соответствующих графиках (рисунок 4-7).Рисунок 4 - Температура нагревателей в зонахРисунок 5 - Скорость вращения привода, температура смеси на выходе и температура охлаждающей воды на выходеРисунок 6 - Температура охлаждения в зонахРисунок 7 - Температура смеси в зонахREPEAT позволяет детально изучать ход физических процессов в различных режимах в любой системе с высокой точностью, включая критические сценарии. Эти функциональности позволяют операторам проводить испытания, внося изменения в параметры, без прерывания расчетов. Результаты моделирования могут быть использованы для принятия технологических решений по оптимизации системы плавления гранул в экструдере, что в конечном итоге поспособствует получению волокон с требуемыми свойствами для дальнейшего их использования.Математическое моделирование в экструзии повышает эффективность, точность и контроль в производственных процессах. Полученные параметры настройки регуляторов могут быть рекомендованы для настройки систем регулирования одношнековых экструдеров с внешним водяным охлаждением."
СберМаркет,,,Математическое моделирование технологических объектов и систем глазами и руками студента,2024-02-27T22:04:55.000Z,"Традиционно Росатом уделяет особое внимание подготовке кадров и поддержке всестороннего развитию молодых талантов – в том числе по инженерно-математическому направлению. В предыдущих статьях мы подробно рассказывали, как моделировать в REPEAT – собственной разработке Инженерно-технического центра «ДЖЭТ». Но мы не только моделируем на REPEAT сами, но и учим этому ребят со студенческой скамьи. Открытые в ведущих вузах страны образовательные центры на базе REPEAT - Школы моделирования - становятся для студентов мощным центром цифровых компетенций, а также обеспечивает их знакомство с отечественной технологией тренажеростроения, основанной ИТЦ «ДЖЭТ» более 30 лет назад и применяемой до сих пор. Подробнее о нашем проекте Школа моделирования читайте ниже.Забегая вперед, скажу - сегодня мы открываем цикл статей, посвященных труду в области математического моделирования от наших студентов – учащихся Школ моделирования. Наблюдая за прогрессом начинающих IT-инженеров в освоении искусства математического моделирования на REPEAT, мы решили, что на глазах зарождается научная школа в области развития цифровых двойников и других digital направлениях – и базируется она на отечественном ПО. Вместе мы открываем новые возможности программного обеспечения REPEAT, и с нетерпением ждем новых достижений и открытий, которые предстоит осуществить вместе со студентами и преподавателями нашего общего проекта Школа Моделирования.Начнем публиковать поводы для гордости с кейсов, разработанных командой Кубанского государственного технологического университета – это талантливые студенты кафедры теплоэнергетики и теплотехники и их не менее талантливые преподаватели. Вместе они прокладывают свой собственный путь в области изучения и разработки цифровых двойников, а REPEAT им в этом активно помогает.Ссылка на телеграм-канал REPEAT:https://t.me/repeatlabПроект Школа моделирования АО ИТЦ «ДЖЭТ»Проект был инициирован в 2022 году и нацелен на сокращение сроков адаптации молодых специалистов в производственной среде компаний за счет получения практических навыков ещё на этапе обучения в вузах, а также формирование постоянного кадрового резерва для нужд отрасли в сфере математического моделирования.Открывая центры компетенций в лучших технических вузах страны, ДЖЭТ продолжает оказывать всеобъемлющую поддержку эффективной организации учебного процесса - предоставляет вузу программные продукты для осуществления образовательной деятельности, проводит стажировку преподавательского состава, передает учебно-методические материалы, занимается консультированием и поддержкой как преподавательского состава, так и студентов.Студенты, осваивая современные технологии на уникальной платформе моделирования REPEAT, имеют возможность получить навыки работы на российском цифровом продукте в части математического моделирования и программирования.Работа Школы моделирования тесно связана во взаимодействии с существующими или потенциальными заказчиками. Студенты получают шанс принимать участие в решении бизнес-задач коммерческих компаний уже со студенческой скамьи, что дает им неоспоримое преимущество при трудоустройстве и выделяет их на ИТ-рынке. Подобный опыт позволяет сформировать сильный локомотив для цифровой трансформации ключевых отраслей экономики в будущем.По итогам обучения всем выпускникам Школы моделирования вручаются сертификаты о прохождении курса, а студенты, представившие проекты высокого уровня, рекомендованы для приоритетного трудоустройства в АО «ИТЦ «ДЖЭТ» и другие организации Госкорпорации «Росатом».В ходе обучения в Школе студенты овладеют навыками моделирования и другими компетенциями, включая:разработка моделей объектов и технологических систем;создание цифровых двойников;проведение виртуальных испытаний проектируемого оборудования.По итогам обучения слушатели могут получить навыки эксплуатации атомных электростанций, знания в области проектировании и инжиниринга и широкие цифровые компетенции в целом.Сеть инженерно-математических школ ДЖЭТ активно расширяется. В 2022 году было открыто две Школы моделирования: в Казанском государственном энергетическом университете (КГЭУ) и Томском политехническом университете (ТПУ). В общей сложности в Школу было отобрано 25 студентов старших курсов. В течение двух учебных семестров студенты обучались по 4 академических часа еженедельно. В конце учебного года был проведен итоговый срез знаний, демонстрирующий уровень подготовленности студентов в части освоения программных продуктов ДЖЭТ. По результатам промежуточных и итоговых срезов на работу в ДЖЭТ уже принято 5 студентов Школ: четыре студента ТПУ трудоустроены в региональный офис ДЖЭТ в г. Томск, один студент КГЭУ релоцирован и трудоустроен в головной офис в г. Москва. Дальнейшее развитие выпускников Школы моделирования проходит на реальных проектах копании, под чутким наставничеством опытных инженеров.Осенью 2023 года состоялось открытие еще четырех Школ моделирования на базе технических вузов: НИУ МЭИ (Москва), ДГТУ (Ростов-на-Дону), КубГТУ (Краснодар), УРФУ (Екатеринбург).Итак, представляем вам труд на тему развития цифровых двойников в теплоснабжении – где они занимают особую роль. В эпоху цифровизации перед инженерами и разработчиками стоят сложные задачи. Цифровые двойники в управлении системами открывают новые возможности для инжиниринга. Студенты кафедры ТЭТ КубГТУ в рамках ""Школы моделирования"", под руководством Дмитрия Батько, успешно разработали модель центрального теплового пункта, продемонстрировав подход к численным методам. Учитывая многофакторность и сложность взаимодействий внутри системы, применение инструмента математического моделирования позволяет наладить более эффективный контроль и управление теплоснабжением объекта, и снизить риск аварийных ситуаций.Часть 1: Цифровой двойник системы теплоснабженияБатько Д.Н. старший преподаватель кафедры ТЭТ КубГТУ, Арушанян Р.Р. старший преподаватель кафедры ТЭТ КубГТУ, Деневич А.А. студент 4курса КубГТУ, Розина Д.Е. студент 4курса КубГТУ.Кубанский государственный технологический университет, Краснодар.В данной работе приведены результаты математической модели центрального теплового пункта (ЦТП) с помощью Системы Автоматического Проектирования Физических Инженерных Расчетов (САПФИР) разработанной АО «ИТЦ «ДЖЭТ». Верификация расчетных и экспериментальных данных, показала хорошую сходимость основных параметров. Сравнение полученных данных производились для десяти режимов работы. Анализ результатов расчетов показал достаточную точность модели (погрешность не превышает 1 %).Ключевые слова: центральный тепловой пункт, математическая модель, эффективность работы системы теплоснабжения.Основная задача теплоснабжения – это качественное и энергоэффективное снабжение тепловой энергией потребителей. Для эффективного теплоснабжения необходимо обеспечить каждого потребителя необходимым количеством тепловой энергии с минимальными потерями и затратами. На сегодняшний день потребление тепловой энергии значительно отличается от расчетных значений. Ненормативный отпуск тепловой энергии с централизованным теплоснабжением обусловлен проблемами эксплуатации и состоянии тепловых сетей в России.Математическое моделирование в теплоснабжении позволяет разрабатывать и оптимизировать требуемые режимы работы и эксплуатации оборудования, позволяющие решать поставленные задачи.В данной работе приводятся результаты разработки и верификации математической модели центрального теплового пункта с помощью Системы Автоматического Проектирования Физических Инженерных Расчетов (САПФИР) разработанной АО «ИТЦ «ДЖЭТ».Объектом моделирования был выбран ЦТП, обеспечивающий систему отопления двух групп потребителей. ЦТП подключен к тепловым сетям АО «Краснодартеплосеть». Принципиальная расчетная схема приведена на рисунке 1.Рисунок 1 - Принципиальная расчетная схема1 - пластинчатый теплообменный аппарат потребителя №1; 2 - пластинчатый теплообменный аппарат потребителя №2; 3 - регулирующий клапан потребителя №1; 4 - регулирующий клапан потребителя №2; 5- циркуляционные насосы потребителя №1; 6 - циркуляционные насосы потребителя №2; 7 - подпиточные  насосы потребителя №1; 8 - группа расширительных мембранных баков общим объемом 2,4 м3 потребителя №2; 9 - группа расширительных мембранных баков общим объемом 2,4 м3 потребителя №1.Подключение отопительной системы выполнено по схеме с независимым присоединением к тепловой сети. Потребитель №1 присоединен через пластинчатый теплообменный аппарат тип НН – 41 фирмы ООО «Ридан» с площадью нагрева 44,55 м2. Потребитель №2 - через пластинчатый теплообменный аппарат тип НН – 41 с площадью нагрева 38,35 м2. Проектный расход теплоносителя потребителя №1 G1=62,5 м3/ч обеспечивают циркуляционные насосы IL80/150-7,5/2 фирмы WILO с частотным регулированием. Расход теплоносителя потребителя №2 G2=53,6 м3/ч. обеспечивают циркуляционные насосы IL80/140-7,5/2.Температура теплоносителя в системе отопления регулируется для каждого потребителя двухходовым регулирующим клапаном VFS2 Ду50 (KVS=40) c электроприводом AMV35 фирмы Danfoss. Расчетные параметры теплоносителя системы отопления меняются по графику 95/700С в зависимости от температуры наружного воздуха.При разработке математической модели составлена схема теплового пункта с параметрами элементов, заданными в соответствии с данными завода-изготовителя и проектной документации.Для трубопроводов определены геометрические параметры (диаметры на входе и выходе не изменялись). Моделирование насосов осуществлялось по расходно-напорной характеристике.Регулирующие клапана настраивались по паспортным данным завода-изготовителя на соответствующую пропускную способность. Теплообменный аппарат моделировался двумя элементами. Каждый элемент является частью независимого греющего и нагреваемого контура теплоносителя. Теплогидравлический режим теплообменных аппаратов проверялся в программе поверочного расчет производителя теплообменных аппаратов ООО «Ридан».Верификация математической модели ЦТП производилась на 10 режимах работы. Согласно температурному графику задавалась температура теплоносителя в подающем трубопроводе тепловой сети, температура теплоносителя в обратном трубопроводе от потребителей, расход в греющем и нагреваемом контуре поддерживался согласно проектной документации. Производилась фиксация температуры теплоносителя в подающем трубопроводе к потребителю и температуры теплоносителя в обратном трубопроводе.  Результаты измерений сравнивались с данными температурного графика, утвержденного АО «Краснодартеплосеть», проектной документацией и поверочного расчета теплообменников (по программе ООО «Ридан»).Результаты сравнения расчетных и проектных данных приведены в таблице 1. Анализ полученных данных показывает, что математическая модель ЦТП адекватна. При выбранной степени сложности модели получены достаточно хорошие совпадения основных параметров. Максимальное расхождение результатов расчета и проектных параметров составляет 0,66%.Таблица 1 – Сравнение расчетных и  проектных данныхВыводыМатематическая модель ЦТП, разработанная в программе «САПФИР», предоставленной АО «ИТЦ «ДЖЭТ», показала высокую точность расчетов (относительная погрешность не превышает 1%).На следующим этапе с помощью математической модели ЦТП будут проведены исследования режимов работы действующего теплового пункта с целью определения влияния изменения параметров тепловой сети (температура, давление) на эффективность теплоснабжения потребителей.Список литературыСоколов Е.Я. Теплофикация и тепловые сети: Учебник для вузов.- 7-е изд., стереот. – М.: МЭИ, 2001. – 472 с.Шкаровский, А.Л. Теплоснабжение : учебник / А.Л. Шкаровский. – 2-е изд., стер. – Санкт-Петербург : Лань, 2020. – 392 с. – ISBN 978-5-8114-5222-4.Итак, важность математического моделирования технологических систем и объектов в современных условиях цифровизации трудно переоценить. Результаты работы Школы моделирования КубГТУ и реализованной ими математической модели 1D центрального теплового пункта, несомненно, станут отправной точкой для будущих исследований и разработок в этой важной области.Мы благодарим всех участников этого проекта за их труд и усердие. Ваши рассуждения, расчеты и исследования не только заслуживают высокого признания, но и стали вкладом в прогресс, расширив горизонты наших знаний. Мы с нетерпением ожидаем продолжения этой увлекательной научной саги!Вдохновенные этим примером, мы нацеливаемся развивать и улучшать программное обеспечение REPEAT, чтобы все больше молодых ученых могли применять его и делать собственные открытия. Впереди много интересного! Присоединяйтесь к нам в этом увлекательном путешествии по миру математического моделирования и откройте для себя новые горизонты познания."
СберМаркет,,,Между хайпом и реальностью: объем мирового рынка генеративного ИИ в 2024 году с прогнозом до 2032 года,2024-02-27T09:25:47.000Z,"Привет, Хабр! Меня зовут Алексей Мартынов, я аналитик-эксперт, руководитель направления методологии продуктового маркетинга в компании «Цифрум» (Росатом). Представляю своего соавтора — Дениса Ларионова, эксперта в области нейроморфных вычислений, руководителя отдела ИИ компании «Цифрум» (Росатом). Мы расскажем о том, что такое генеративный искусственный интеллект, с какими задачами он справляется и как может (или не может) изменить наш мир. А еще представим наши расчеты с прогнозом о том, как использование этой технологии повлияет на экономику России.Что такое генеративный ИИ и как он работаетГенеративный искусственный интеллект —  часть искусственного интеллекта (ИИ, англ. artificial Intelligence, AI), которая использует статистические методы и машинное обучение для создания контента с помощью имитации данных, на которых был обучен [6, 7, 8].В идеальном сценарии генеративные модели, обученные на огромных массивах информации, сжимают ее и извлекают суть без утраты смысла. При этом эмоциональность, скрытый подтекст и прочие важные в работе детали генеративный ИИ может упустить.Для обработки и генерации текста языковая модель использует базовую единицу информации — токен. Он может быть словом или его частью, или даже группой слов в зависимости от того, как модель была обучена разделять и интерпретировать текст.С точки зрения длины в зависимости от языка:1 токен ~= 4 символа в английском языке ~= 0,75 слова,1 токен ~= 2,3 символа в русском языке ~= 0,46 слова,1 токен ~= 0,5 символа в китайском языке,1 токен ~= 0,8 символа в японском языке [9, 10].Например, GPT-4 Turbo (мультимодальная большая языковая модель, созданная OpenAI) с контекстным окном на 128 тысяч токенов составляет около 300 тыс. символов на русском языке. Предельное количество символов в рамках одного сеанса работы генеративного ИИ определяет границу эффективного применения ), свыше которого информация моделью не учитывается, то есть контекст имеет ограниченную длину.Как эффективно использовать генеративный ИИГенеративный ИИ в настоящее время малоэффективен в сложных, многомерных и многовекторных проектах, в которых присутствуют динамические связи, где необходимо учитывать целостную картину и наследование опыта. Тенденция к повышению возможностей генеративного ИИ в части усложнения решаемых задач очевидна: в 2024 г. ожидается всплеск работы генеративных моделей с видео (например, посредством GPT5).С точки зрения генеративного ИИ достоверность ресурса, скорее всего, будет определяться не его авторитетностью, а количеством цитирований или копирований. В связи с этим возникает риск получения информации из источника низкого качества.Генеративный ИИ часто «придумывает» ответы. Кембриджский словарь английского языка (англ. Cambridge Dictionary) даже назвал глагол «галлюцинировать» словом 2023 г. в контексте генеративного ИИ. ChatGPT, например, придумал несколько прецедентов из юридической практики, на основе которых выстраивалась позиция адвоката в США [11]. Поэтому, прежде чем использовать генеративный ИИ, стоит сначала проверить его ответ.Генеративный ИИ полезен в задачах, где нужен конкретный ответ на одномерный и четко поставленный вопрос. Также с помощью этой технологии можно быстро ознакомиться с новой темой или составить сводку текста, видео или аудио.Какие компании внедрили генеративный ИИ и не прогадалиКонгломерат Morgan Stanley внедрил для своих 16 тыс. финансовых консультантов ассистент AI @ Morgan Stanley (кастомизированное решение, основанное на GPT-4 от OpenAI). Это позволило сэкономить время, которое сотрудники тратили на решение вопросов, связанных с рынками, рекомендациями и внутренними процессами. Таким образом консультанты смогли направить больше ресурсов на обслуживание клиентов и ускорить оказание услуг на 14%. Сейчас Morgan Stanley тестирует другие системы с генеративным искусственным интеллектом: инструмент под названием Debrief, например, автоматически обобщает содержание встреч с клиентами и генерирует последующие электронные письма [1, 2, 3].Сервис Netflix сделал ставку на внедрение технологий искусственного интеллекта. Сегодня у компании один из самых низких показателей оттока клиентов — всего 3,5%. Алгоритм Cinematch отслеживает все явные и неявные данные по 223 млн пользователей и повышает общую удовлетворенность клиентов. Стратегия Netflix заключается в том, чтобы изменить формирование и представление контента, используя генеративный ИИ при написании сценариев, создании мультфильмов, кастомной генерации превью [4, 5] и т.д.Все это доказывает, что генеративный ИИ можно применять не только в науке, но и в более практических сферах.Настоящее и будущее генеративного ИИ в миреНа граф. 1 представлена оценка авторов глобального рынка генеративного ИИ, CAGR (совокупный среднегодовой темп роста) составляет 26,4% на промежутке 2023 – 2032 гг.Около 29,7% мирового рынка генеративного ИИ реализуется на архитектуре генеративно-состязательных сетей (англ. generative adversarial network, сокращённо GAN), 37,5% — трансформеров (англ. transformers), 14,8% — вариационных автоэнкодеров (англ. variational auto-encoder, сокращенно VAE), 10,1% — диффузионных моделей (англ. diffusion networks), 7,9% — других моделей:например, моделей на основе потоков (англ. flow-based generative model) или авторегрессионных моделей (англ. autoregressive models). Указанное распределение актуально на 2024 г. и будет меняться под давлением новых архитектур нейронных сетей [12,13, 14].Рис. 1. Объем глобального рынка искусственного интеллекта и генеративного искусственного интеллекта, млрд долл.Примечание: расчеты авторов с использованием [7, 15 - 19]В 2024 г. объем глобального (мирового) рынка генеративного искусственного интеллекта составит 66,9 млрд долл., в том числе:5,7 млрд долл. / 2024 г. — объем глобального рынка программного обеспечения генеративного искусственного интеллекта: специализированные помощники по созданию генеративного искусственного интеллекта; рабочих процессов кодирования, DevOps и генеративного искусственного интеллекта; ПО для инфраструктуры рабочей нагрузки генерирующего искусственного интеллекта; кибербезопасности, основанной на генеративном искусственном интеллекте, и т.д.61,2 млрд долл. / 2024 г. — объем глобального рынка аппаратного оборудования генеративного искусственного интеллекта: серверы, хранилища, генеративная инфраструктура ИИ как услуга.Рис. 2. Объем глобального (мирового) рынка генеративного искусственного интеллекта (Всего 66,9 млрд долл. в 2024 г.)Указанный выше размер глобального рынка генеративного искусственного интеллекта [66,9 млрд долл./2024 г.] может быть выше при учете сложно подсчитываемых рынков рекламы и игр, основанных на генеративном искусственном интеллекте В 2024 г. это может добавить к рынку генеративного ИИ 3,3 – 4,5 млрд долл.Рынок генеративного искусственного интеллекта США является доминирующим, на втором месте находится Китай, далее следуют рынки Германии и Японии (см. Табл. 1).Таблица 1. Прогноз рынка генеративного ИИ по некоторым странам за 2024 – 2032 гг., млрд долл.Примечание: расчеты авторов с использованием [7, 15, 17 –  20]Влияние генеративного ИИ потенциально может обеспечить дополнительную глобальную экономическую активность по всему миру. Текущие оценки — это 2,6 – 7,9 трлн долл. США (около 2,6 –  4,4 трлн долл. США —  целенаправленное применение генеративного ИИ для решения конкретных бизнес-задач, 3,5 трлн долл. США —  влияние генеративного ИИ на сопредельные бизнес-задачи) ежегодно до 2040 г. [6, 21].Генеративный ИИ создает как прямой эффект для отраслейза счет внедрения новых продуктов и сервисов, так и дополнительный эффект за счет роста продуктивности сотрудников. Но мгновенно внедрить технологии во все отрасли, сферы и вертикали невозможно. Теоретически рассчитанный потенциал инструмента редко достигается на практике из-за эффективных границ реального применения.В Табл. 2 приведена оценка влияния генеративного ИИ на некоторые отрасли экономики совокупно по миру.Таблица 2. Оценка влияния генеративного ИИ по некоторым отраслямПримечание: использовано [6, 21, 22, 23]Большинство кейсов применения генеративного ИИ сегодня носят функциональный характер. Конечно, точки роста для некоторых индустрий более заметны, что связано с высокой маржинальностью бизнеса и отрасли. Например, при максимальном использовании потенциала технологии в финансовой и страховой сферах генеративный ИИ может принести от 250 до 410 млрд долл. в год, в оптовой и розничной торговле — от 400 до 660 млрд долл. в год.Влияние генеративного ИИ на бизнес в РоссииМы провели валидацию по мультипликаторам, зависимостям объемных показателей между собойс учетом эффектов по внедрению технологии. На наш взгляд, столь высокие оценки добавленного эффекта на экономику, приведенные в Табл. 2, носят излишне оптимистичный и спекулятивный характер. Маркетинговый евангелизм подпитывает интерес общества к этой технологии.Для экономики РФ мы подготовили собственную оценку влияния генеративного ИИ. Расчеты показывают влияние этой технологии на экономику РФ в 1 – 2 трлн руб. в год (см. Табл. 3).Таблица 3. Оценка влияния генеративного ИИ по некоторым отраслям экономики РФПримечание: расчеты авторов с использованием [24, 25]Огромные возможности генеративного ИИ должны сочетаться с гарантиями занятости, правами интеллектуальной собственности и этическими нормами.Библиографический списокMorgan Stanley to Launch AI-Powered Assistant for Financial Advisers [Электронный ресурс] // PYMNTS - СМИ. URL: https://www.pymnts.com/artificial-intelligence-2/2023/morgan-stanley-to-launch-ai-powered-assistant-for-financial-advisors/ (дата обращения 06.01.2024).Kiruthika Devi. Enhancing the Customer Experience through Generative AI [Электронный ресурс] // Zoho Corporation Pvt. Ltd – ИТ компания. URL: https://www.zoho.com/blog/salesiq/generative-ai-for-customer-experience.html#:~:text=Generative%20AI%20offers%20several%20ways,learning%20and%20optimization%2C%20and%20more (дата обращения 06.01.2024).Morgan Stanley’s AI Assistant Marks New Era For Finance Sector [Электронный ресурс] // Forbes – СМИ. URL: https://www.forbes.com/sites/qai/2023/09/19/morgan-stanleys-ai-assistant-marks-new-era-for-finance-sector/?sh=7d82c7fc1ff2 (дата обращения 06.01.2024).Netflix Recommendations: How Netflix Uses AI, Data Science, And ML [Электронный ресурс] // Simplilearn Solutions – платформа обучения. URL: https://www.simplilearn.com/how-netflix-uses-ai-data-science-and-ml-article (дата обращения 06.01.2024).How Netflix Uses Artificial Intelligence [Электронный ресурс] // Argoidh - СМИ. URL: ttps://www.argoid.ai/blog/netflix-ai (дата обращения 06.01.2024).The economic potential of generative AI: The next productivity frontier [Электронный ресурс] // McKinsey & Company – аналитическая компания. URL: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier#business-vale (дата обращения 06.01.2024).Яков и партнеры х Яндекс. Искусственный интеллект в России – 2023: тренды и перспективы. – М., 2023. – 85 с.Международный Валютный Фонд. Терминология ИИ [Электронный ресурс] // МВФ – международная организация. URL: https://www.imf.org/ru/Publications/fandd/issues/2023/12/AI-Lexicon (дата обращения 06.01.2024).Hindi 8 times more expensive than English: the token price of text in different languages [Электронный ресурс] // Reddit – форум. URL: https://www.reddit.com/r/OpenAI/comments/124v2oi/hindi_8_times_more_expensive_than_english_the/ (дата обращения 06.01.2024).What are tokens and how to count them? [Электронный ресурс] // OpenAI – лидирующая организация в ИИ. URL:https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them(дата обращения 06.01.2024).Here’s What Happens When Your Lawyer Uses ChatGPT [Электронный ресурс] // The New York Times Company – СМИ. URL: https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html (дата обращения 06.01.2024).Generative AI Market Size – By Component (Solution, Service), Deployment Model (On-premises, Cloud), Technology (Generative Adversarial Networks (GANs), Transformers Model, Variational Auto-encoders, Diffusion Models), End-user & Forecast, 2024 – 2032 [Электронный ресурс] // Global Market Insights – аналитическая компания. URL: https://www.gminsights.com/industry-analysis/generative-ai-market (дата обращения 14.01.2024).Generative AI Market Size, Share & Covid-19 Impact Analysis, By Model, By Industry vs Application, and Regional Forecast 2023 - 2030, 2023 [Электронный ресурс] // Fortune Business Insights – аналитическая компания. URL: https://www.fortunebusinessinsights.com/generative-ai-market-107837 (дата обращения 06.01.2024).Generative AI Market Research Report Information By Component (Software and Solution), Technology (Generative Adversarial Networks (GANs), Transformers, Variational Auto-encoders (VAEs), Diffusion, and NeRFs), End Use (Large Language Model (LLM), Content Generation, Code Generation, Video Creation, and Image & Art Generation, and others), Industry Vertical (Manufacturing, IT & Telecommunication, Healthcare, Automotive & Transportation, Gaming, Academic and Research Institutions, BFSI, Aerospace & Defense, and Others) And By Region (North America, Europe, Asia-Pacific, Middle East & Africa, And South America) – Market Forecast Till 2032 [Электронный ресурс] // Market Research Future – аналитическая компания. URL: https://www.marketresearchfuture.com/reports/generative-ai-market-11879 (дата обращения 14.01.2024).А. Мартынов, Д. Ларионов. Объем мирового рынка искусственного интеллекта в 2023 году с прогнозом до 2032 года [Электронный ресурс] // it-world.ru – СМИ. URL: https://www.it-world.ru/it-news/market/198512.html (дата обращения 06.01.2024).New Report Finds That the Emerging Industry Could Grow at a CAGR of 42% Over the Next 10 Years, 2023 [Электронный ресурс] // Bloomberg – аналитическая компания. URL: https://www.bloomberg.com/company/press/generative-ai-to-become-a-1-3-trillion-market-by-2032-research-finds/ (дата обращения 06.01.2024).Ничто человеческое ему не нужно: почему ИИ стал явлением года не только в IT-отрасли [Электронный ресурс] // Forbes – СМИ. URL: https://www.forbes.ru/tekhnologii/503318-nicto-celoveceskoe-emu-ne-nuzno-pocemu-ii-stal-avleniem-goda-ne-tol-ko-v-it-otrasli?ysclid=lqy53y4klg658331002 (дата обращения 06.01.2024).Generative Artificial Intelligence [Электронный ресурс] // Statista – аналитическая компания. URL: https://www.statista.com/outlook/tmo/artificial-intelligence/generative-ai/worldwide#methodology (дата обращения 06.01.2024).Generative AI Market Research Report: By Offerings (Hardware, Software, Services), By Application (Natural Language Processing, ML-based Predictive Modeling, Computer Vision, Robotics and Automation, Others), By Verticals (Media & Entertainment, Transportation and Logistics, Manufacturing, Healthcare & Life Science, IT and ITES, Others), and by Region — Forecast till 2033 [Электронный ресурс] // Evolve Business Intelligence – аналитическая компания. URL: https://evolvebi.com/report/generative-ai-industry-analysis/ (дата обращения 06.01.2024).New Report Finds That the Emerging Industry Could Grow at a CAGR of 42% Over the Next 10 Years, 2023 [Электронный ресурс] // Bloomberg – аналитическая компания. URL: https://www.bloomberg.com/company/press/generative-ai-to-become-a-1-3-trillion-market-by-2032-research-finds/ (дата обращения 06.01.2024)Unlocking the potential of generative AI: Three key questions for government agencies [Электронный ресурс] // McKinsey & Company – аналитическая компания. URL: https://www.mckinsey.com/industries/public-sector/our-insights/unlocking-the-potential-of-generative-ai-three-key-questions-for-government-agencies#/ (дата обращения 20.01.2024).The AIdea of India Generative AI’s potential to accelerate India’s digital transformation [Электронный ресурс] // EY – аналитическая компания. URL: https://www.ey.com/en_in/news/2023/12/generative-ai-to-potentially-add-a-cumulative-us-dollor-1-point-2-1-point-5-trillion-to-india-s-gdp-by-fy-2029-30-ey-report (дата обращения 20.01.2024).International Telecommunication Unit. Assessing the Economic impact of Artificial Intelligence. – Швейцария, 2018. – 80 с.Прогноз социально-экономического развития Российской Федерации на 2024 год и на плановыей период 2025 и 2026 годов // Министерство экономического развития РФ – Федеральный орган исполнительной власти. URL: https://www.economy.gov.ru/material/directions/makroec/prognozy_socialno_ekonomicheskogo_razvitiya/prognoz_socialno_ekonomicheskogo_razvitiya_rf_na_2024_god_i_na_planovyy_period_2025_i_2026_godov.html (дата обращения 06.01.2024).Real GDP growth [Электронный ресурс] // МВФ – международная организация. URL: https://www.imf.org/external/datamapper/NGDPD@WEO/OEMDC/ADVEC/WEOWORLD/USA (дата обращения 06.01.2024)."
СберМаркет,,,Когда цифры имеют значение: имитационное моделирование для улучшения эффективности,2023-12-29T12:22:35.000Z,"В современном мире цифровая трансформация распространяется на все больше отраслей и сфер жизни, а одним из ключевых решений такой трансформации становится создание цифровых двойников (ЦД). ЦД представляют собой разработанные с высокой степенью точности виртуальные копии физических объектов, систем или процессов. Полноценный ЦД также должен получать данные о состоянии реального объекта, то есть дублировать его физическое состояние в виртуальной среде.  Преимущества использования ЦД включают возможность оптимизации производительности, снижение затрат на эксплуатацию, улучшение безопасности и надежности, а также повышение качества принятия решений.Одним из главных факторов, способствующих росту популярности ЦД, является развитие технологий: в первую очередь интернета вещей (IoT) и искусственного интеллекта. Возможность передачи и обработки больших объемов данных, улучшение алгоритмов машинного обучения и развитие облачных вычислений сделали создание и эксплуатацию ЦД более доступными и эффективными.Атомная промышленность не остается в стороне и активно движется в направлении использования ЦД в своих технологических процессах с целью повышения эффективности, безопасности и отработки персоналом различных сценариев, которые могут возникнуть при эксплуатации оборудования.В данной статье мы рассмотрим основные аспекты разработки имитационной модели ЦД подогревателя низкого давления (ПНД) системы регенерации турбоустановки.Система регенеративного подогрева на АЭС является значимой составляющей повышения экономичности работы всего объекта. Поэтому очень важно обеспечить эффективность работы оборудования системы и безопасность ее функционирования. Для данной цели подходит использование ЦД, в качестве инструмента, позволяющего проанализировать различные конфигурации оборудования и определить наиболее экономичный вариант, а также отработать перечень мероприятий по эксплуатации объекта и локализации аварийных ситуаций.Цифровые двойникиЦД для оборудования атомной электростанции (АЭС) являются одним из ключевых инструментов для оптимизации и управления сложными системами в атомной энергетике. Они представляют собой виртуальные модели, которые отражают структуру, производительность, техническое состояние и характер протекающего процесса физических объектов АЭС, а главное – повторяют режим работы станции, получая технологические данные в реальном времени.Рис.1  - Концепция ""Цифровой двойник""На рисунке ниже представлены структура, необходимая для работы ЦД станции. Ядром ЦД является инженерная модель – физико-математическая модель, воспроизводящая внутренние процессы объекта с максимально возможной точностью и обеспечивающая тепловые и энергетические балансы. Инженерная модель используется в качестве следящей тени станции и имитационной модели. Об имитационной модели будет рассказано ниже.Следящая же тень – это как раз та часть, которая фактически повторяет режим работы станции на основе получаемых технологических данных от систем АСУТП. Хранение и агрегация этих данных происходит в корпоративном хранилище данных, озере данных. Помимо инженерной модели в ЦД могут быть включены различные функциональные подсистемы, например, подсистема предиктивной диагностики. Визуализация ЦД может быть самой разной – от простейших таблиц до детальных 3D-моделей объекта.Рис. 2 - Структура цифрового двойникаПрименение ЦД для оборудования АЭС позволяет:Визуализировать внутренние процессы;Сократить объемы дооснащения;Снизить количество отказов оборудования;Повысить безопасность и эффективность работы атомных электростанций;Оптимизировать процессы эксплуатации и обслуживания оборудования, сокращая затраты на его обслуживание и повышая его надежность и долговечность.Имитационная модельИмитационная модель является основой цифрового двойника, его виртуальной копией. Однако, для создания этой копии предварительно проводится этап комплексного исследования объекта моделирования с целью сбора исходных данных для модели. Данный этап является наиболее важным и вместе с тем наиболее трудозатратным. Только после сбора исходных данных мы можем приступать к разработке имитационной модели.Разработка имитационной модели ПНДЭтапы разработки:Обследование объекта;Формирование перечня исходных данных на основе обследования;Разработка модели;Заполнение свойств блоков;Настройка модели на паспортные параметры.Обследование объектаДанный этап включает в себя изучение структуры, функций, поведения и характеристик объекта с целью определения основных параметров и зависимостей, которые будут использоваться при создании модели. От качества и полноты обследования зависит адекватность и эффективность будущей модели, а также возможность ее применения для решения поставленных задач.Формирование перечня исходных данныхПо результатам исследования объекта моделирования формируется подробный перечень исходных данных, необходимых для дальнейшего заполнения свойств блоков и настройки модели.Для разработки имитационной модели требуются параметры:Основного конденсата на входе и выходе;Пара из отбора турбины;Конденсата греющего пара;Теплообменника (геометрические характеристики теплообменной поверхности, корпуса и т.д.).Подготовка осуществляется на основе имеющейся документации по объекту моделирования.Разработка моделиРазработка выполнена с использованием блоков библиотеки «Теплогидравлика» в ПК САПФИР.Рис. 3 – Принципиальная схема регенеративного подогревателя поверхностного типаВ регенеративном подогревателе низкого давления происходит процесс нагрева основного конденсата (ОК) паром из нерегулируемых отборов турбины (рис.3). Разрабатываемый ПНД относится к поверхностному типу подогревателей: ОК проходит через трубки, омываемые паром из отбора турбины; конденсат греющего пара, образованный в результате данного теплообмена, подается в конденсатор.Каждая из перечисленных сред (ОК, пар, конденсат греющего пара) моделируется с использованием граничных условий, которые отражают её характеристики (давление, температура и т.д.).В свою очередь теплообменная поверхность и межтрубное пространство корпуса подогревателя выполнены с помощью блоков: узел, канал, бак и теплообменник. Запорная, регулирующая и защитная арматура моделируется соответствующими технологическими блоками.Разработанная имитационная модель представлена на рисунке 4.Рис. 4 – Имитационная модель подогревателя низкого давленияОписание элементарных блоковГраничное условиеИспользуется для задания граничных условий теплогидравлической схемы.Параметры для таких элементов:задаются постоянными;или вводятся соотношения для расчета изменения этих параметров в процессе расчета;либо эти объекты служат для разделения расчетной схемы сложной и разветвленной гидравлической системы на более простые расчетные схемы, связываемые между собой через граничные условия.В последнем случае параметры в граничном условии определяются как параметры в соответствующем элементе (узле или баке) другой расчетной схемы. Это позволяет упростить моделирование и настройку расчетных схем.Узел CMSУзел может соединятся каналами с другими узлами, баками и граничными условиями трех типов.В узлах моделируется парогазовая и жидкая фаза, равномерно распределенная по всему объему узла.Канал CMSКаналы вводятся для соединения между собой блоков расчетной схемы в соответствии с конфигурацией моделируемой технологической системы.Контрольные объемы (узлы, баки и т.д.) и граничные условия связываются между собой каналами.Канал при этом, его математическая модель, не содержит среды, внутри каждого канала нет массы или объема теплоносителя, канал лишь вычисляет расходы фаз среды из одного контрольного объема в другой.Теплообменник (канал-бак)Моделирует теплообмен между каналом, моделирующим среду в трубном пучке, и межтрубным пространство бака.БакБак – блок расчетной схемы, используемый для моделирования таких элементов технологической схемы, как баки, сосуды, камеры элементов оборудования, помещения.Объект бак в свою очередь разбивается на две части: верхнюю часть, занятую парогазожидкостной фазой, и нижнюю часть, со сконденсированной фазой, параметры которой отличаются от параметров парогазовой и жидкой фаз в верхней части бака.Баки в расчетной схеме могут быть одного из трех типов:вертикальный цилиндр с постоянной площадью поперечного сечения по высоте;горизонтальный цилиндр;бак с произвольно заданным изменением площади поперечного сечения по высоте.ЗадвижкаИспользуется для задания проходного сечения канала, на котором расположен элемент. Для задвижек можно задавать характеристику.Обратный клапанИспользуется для моделирования обратного клапана, с заданием соответствующих свойств (перепад давления для открытия, перепад давления для закрытия, скорость открытия/закрытия).Разбиение теплообменной поверхностиТеплообменная поверхность имитационной модели разделена на 18 узлов по ходу ОК.Разбиение теплообменной поверхности на участки позволяет контролировать температуры ОК в различных точках и получать данные о процессе нагрева при движении среды в теплообменнике.Рис. 5 - Изменение температуры по узлам тракта ОКРис. 6 - Фрагмент теплообменной поверхностиЗаполнение свойств блоковМодель является совокупностью элементарных блоков, каждый из который имеет ряд свойств для заполнения пользователем (рис.7).Рис. 7 – Окно свойств блока «Граничное условие»На данном этапе осуществляется заполнение свойств в соответствии сформированным исходными данными.Настройка модели на паспортные параметрыПосле сборки и заполнения свойств модель запускается на расчет. По полученным результатам выполняется анализ и корректировка модели на необходимые эталонные параметры.Настройка модели необходима для достижения требуемой точности относительно паспортных данных.Далее, модель может использоваться для получения динамики параметров и моделирования аварийных ситуаций.Моделирование аварийных ситуацийНаиболее опасными аварийными ситуациями, связанными с работой регенеративного подогревателя, являются нарушения целостности теплообменной поверхности или корпуса, что может привести к его заливу (нарушение целостности теплообменной поверхности) или осушению (нарушение целостности корпуса подогревателя).Разработанная имитационная модель дает возможность моделировать перечисленные аварийные ситуации, что, в свою очередь, позволит провести перечень мероприятий по подготовке персонала к ним и снизить последствия аварий оборудования.Рассмотрим графики изменения параметров в регенеративном подогревателе, полученные при возникновении аварийных ситуаций.Нарушение целостности теплообменной поверхностиАвария происходит в момент времени 5800 с.Рис. 8 - Изменение расхода ОК на входе и выходе ПНД при нарушении целостности теплообменной поверхностиРис. 9 - Изменение уровня в корпусе ПНД при нарушении целостности теплообменной поверхностиНарушение целостности корпуса подогревателяАвария происходит в момент времени 5700 с.Рис. 10 - Изменение расхода пара и конденсата греющего пара ПНД при нарушении целостности корпусаРис. 11 - Изменение уровня в корпусе ПНД при нарушении целостности корпусаРезультаты моделированияЦифровизация и автоматизация производства становятся все более актуальными, а создание имитационных моделей для цифровых двойников является одним из инструментов для оптимизации производственных процессов, повышения эффективности, безопасности и снижения затрат.Для достижения этих целей необходимо провести тщательный анализ данных и определить ключевые факторы, чтобы затем создать модель, отражающую все важные аспекты производственного процесса.В рамках данной статьи мы рассмотрели этапы разработки имитационной модели цифрового двойника, начиная от сбора и анализа данных до создания самой модели.Важно отметить, что создание цифровых двойников и, в частности, их имитационных моделей требует значительных усилий и времени, однако, результаты этих усилий могут быть весьма важными для успешного развития предприятия в условиях цифровой экономики."
СберМаркет,,,Модель запроектной аварии с потерей теплоносителя,2023-11-23T15:02:48.000Z,"Добрый день!В последние годы проектирование ядерных реакторов нового поколения стало одним из векторов развития атомной энергетики во всем мире. Стоит отметить, что главным приоритетом в создании таких энергетических установок по-прежнему остается обеспечение безопасности.Несмотря на высокую надежность систем безопасности АЭС с реакторной установкой ВВЭР-1200, всё ещё остается малая вероятность возникновения событий, которые могут привести к расплаву активной зоны. Наиболее опасной с точки зрения вероятности преодоления барьеров безопасности является авария, сопровождающаяся расплавом активной зоны и внутриреакторных элементов.В связи с этим фактом, предлагаю сегодня смоделировать такую ситуацию, а именно:смоделируем протекание запроектной аварии с потерей теплоносителя, при большой течи, с отказом САОЗ высокого и низкого давления на аналитическом тренажере ЛАЭС-2 (ВВЭР-1200). Таким образом, представим, что происходит потеря охлаждения топлива, сопровождающаяся выходом из строя системы аварийного охлаждения;проанализируем различные параметры аварии, включая изменение температуры топлива, давления в реакторе, а также другие параметры, влияющие на протекание аварии;сравним полученные результаты расчета запроектной аварии на тренажере с данными из Предварительного отчета по безопасности (ПООБ);сделаем вывод о точности моделирования теплогидравлических процессов на тренажере-имитаторе ЛАЭС-2 (ВВЭР-1200).На основании рекомендаций МАГАТЭ, и в соответствии с общепризнанной международной практикой, анализ запроектных аварий проводится с использованием подхода наилучшей оценки. Этот подход предполагает использование таких допущений, программ и методологий для расчета последствий аварий, которые с достаточной степенью достоверности отражают реалистичное развитие аварийных сценариев. Исходные данные должны находиться в диапазонах реалистичных значений, а физические модели расчетных кодов должны соответствовать моделям, принятым в международной практике. При выполнении расчетного анализа следует не только учитывать большой спектр конкретных физических явлений, но и обеспечивать взаимосогласованное моделирование теплогидравлических и физико-химических процессов в рамках единого интегрального кода.Наибольший интерес для нас представляет анализ внутрикорпусной стадии аварии с оценкой времени наступления характерных событий, поведения основных параметров РУ, выхода за пределы корпуса массы и энергии теплоносителя и материалов активной зоны (после разрушения корпуса реактора).Наш теплогидравлический код CMS (Compressible Media Solver), позволяет моделировать все основные физические процессы и явления, которые могут иметь место во всём спектре режимов нормальной эксплуатации и аварийных режимов, включая запроектные аварии в различных РУ типа ВВЭР. Теплогидравлическая модель учитывает все рассматриваемые отказы, и интегрируется с другими математическими моделями (нейтронно-физической, электрической).Теплогидравлический код обеспечивает возможность проведения расчетов в реальном времени, а также позволяет быстро и качественно создавать расчетные схемы, объединять их между собой и проводить отладку, как отдельных технологических систем, так и целых комплексов.В настоящее время теплогидравлический код CMS аттестован в Ростехнадзоре.Исходные события для моделирования тяжелой аварииПри проведении анализа приняты следующие предположения:исходное состояние РУ (работа на номинальной мощности при номинальных параметрах РУ);работа активной части САОЗ не учитывается;учитывается работа четырех САОЗ (две емкости подают воду в НКР, две – в СКР);учитывается работа четырех каналов СПОТ.Давление в защитной оболочке оказывает значительное влияние на параметры первого контура РУ. В данном исследовании параметры среды в объёме ЗО были заданы в качестве граничного условия в виде табличной зависимости давления от времени. Температура среды определялась для состояния насыщения, состав среды – чистый газ. Данную зависимость следует рассматривать как модельную, частично учитывающую результаты выполнявшихся ранее отдельных расчётов в рамках проектных анализов для РУ ВВЭР-1200.Важным этапом в развитии данного исследования является моделирование аварии с учётом взаимовлияния параметров РУ и параметров среды в ЗО, – например, с использованием контейнментного кода, в виде связанных или итерационных расчётов.При выполнении расчетов учитывалось время формирования сигнала (инерционность датчика) и время прохождения сигнала по электрическим цепям, которое равно 0,5 с.Расчетные программы и методика расчетаОдним из способов тестирования кодов, применяемых при анализе аварийных режимов (проектных, запроектных, тяжелых) работы АЭС, является практика проведения расчета какого-либо аварийного режима для выбранной установки одновременно по нескольким кодам.В качестве исходного события для реакторной установки ВВЭР-1200 выбран:Разрыв главного циркуляционного трубопровода Ду-850 при полном обесточивании АЭС (“Большая течь и обесточивание”).Сопоставление результатов, полученных по различным системным кодам, ограничено стадией аварии от исходного события до момента прекращения работы пассивных систем (24 часа, т.е. 86400с).Полагается, что до начала аварии РУ работает на номинальной мощности при проектном функционировании систем нормальной эксплуатации.Для адекватного сопоставления результатов использовались согласованные и, по возможности, подобные существующие расчётные схемы. Также применялись одинаковые начальные и граничные условия, уставки, защиты и блокировки оборудования и системы безопасности.Результаты анализа аварии «Большая течь «Ду850» и полное обесточивание АЭС»В результате гильотинного разрыва главного циркуляционного трубопровода происходит резкое снижение давления в реакторе. Вследствие потери электроснабжения станции одновременно с исходным событием начинается выбег ГЦНА, отключается система подпитки и продувки первого контура, закрываются стопорные клапаны ТГ, происходит отключение систем основной и вспомогательной питательной воды второго контура, отключаются БРУ-К, отключается энергоснабжение системы КД, с задержкой срабатывает аварийная защита реактора.Принимается отказ на запуск всех дизель-генераторов, вследствие чего происходит отказ на запуск активных систем САОЗ. После снижения давления в первом контуре до 5,9 МПа (рис.1) начинается подача борного раствора из емкостей САОЗ.Рис.1 Давление над активной зонойСнижение давления, а также срыв циркуляции теплоносителя приводит к ухудшению теплоотвода от активной зоны, возникает кризис теплообмена на поверхности твэлов и начинается рост температуры их оболочек (рис.2,3).Рис 2  Нагрев ТВСРис 3  Нагрев ТВСНа данной фазе аварии наблюдается постепенное выпаривание теплоносителя из корпуса реактора (рис. 4).Рис.4. Кипение и выпаривание теплоносителяПодача воды от емкостей САОЗ ограничивает величину опорожнения реактора и к моменту их отключения (исчерпание запаса воды в баках) обеспечивает существенное заполнение активной зоны и реактора охлаждающей водой.Разрушение корпуса реактора происходит на 2 часу в области контакта расплава слоя стали над оксидами со стенкой корпуса реактора на высоте 1,6 м от днища (рис.5).Рис.5 Расплавление ТВС, начало разрушения корпуса РУРасплав поступает в УЛР двумя порциями (рис.6).Рис.6 Стекание расплава в УЛР.Первая порция расплава, выходящая из разрушенного корпуса реактора, содержит только металлы, а вторая порция, после расплавления стенки корпуса реактора содержит выход диоксида урана (рис.7).Рис. 7 Плавление днища реактора (ТВС расплавлены) на диаграмме УЛР.Энергия, выделяемая в расплаве в результате химических реакций и остаточного тепловыделения в топливе, частично (порядка 50%) расходуется на излучение с поверхности расплава в верхние части корпуса реактора, а частично – на нагрев относительно холодных днища внутрикорпусной шахты и стенки корпуса реактора.Расплав состоит, в основном, из оксидной (топливо, окисленный Zr) и металлической (сталь, неокисленный Zr) фракций. Поскольку топливо имеет большую плотность по сравнению с плотностью металлов, происходит расслоение фракций, причём слой металлов располагается над слоем оксидов. Несмотря на то, что тепловыделение происходит в оксидном слое, проплавление стенки корпуса происходит на границе с металлической частью. Это объясняется тем, что на границе расплавленного топлива и холодной стальной стенки образуется тугоплавкий слой (корка), который обладает низкой теплопроводностью.Поэтому тепло передаётся в аксиальном направлении к слою металлов и частично расходуется на плавление внутренней области стенки корпуса, пограничной с металлическим слоем. После сквозного проплавления стенки корпуса реактора в расчёте полагается, что днище корпуса опускается на опоры и постепенно расплавляется оставшимся в нём расплавом сверху–вниз, а расплав вытекает в бетонную шахту. Суммарный выход водорода на внутрикорпусной стадии аварии составляет около 740 кг.ЗаключениеВ результате проведенного сценария режима тяжелой аварии на аналитическом тренажере реактора ВВЭР-1200 и сравнения с результатами ПООБ, можно сделать вывод о том, что теплогидравлический код CMS, который лежит в основе модели тяжелой аварии в тренажере-имитаторе, рассчитывает необходимые параметры с высоким уровнем точности.Представлены результаты анализа тяжелой стадии аварии до момента повреждения корпуса реактора и выхода кориума в УЛР, с разгерметизацией всех 163 ТВС и использованием теплогидравлического кода CMS.Можно также сделать вывод, что разрушение корпуса реактора и выход кориума в УЛР наступает не ранее чем через 33 часа после начала аварии.На современном этапе расчётного обоснования безопасности АЭС с РУ ВВЭР актуальной задачей является разработка единого интегрального кода, позволяющего проведение анализа полного спектра сценариев и с учётом всех стадий развития аварийных ситуаций.Накопленный опыт разработки, верификации и использования инструментов симуляционного моделирования теплогидравлических процессов позволит в обозримом будущем совместно с ведущими организациями атомной отрасли, приступить к НИОКР по разработке единого интегрального кода."
СберМаркет,,,Разработка модели системы обогрева дома,2023-11-13T11:42:25.000Z,"Правильно организовать систему обогрева в доме значит наделить пространство важнейшим фактором для комфортного проживания человека. Каким бы ни был роскошным интерьер, развита инфраструктура, прекрасен вид из окна - они теряют свое значение, если в доме слишком холодно или слишком жарко, если не поддерживается комфортная температура и не обеспечивается здоровое пребывание людей разных возрастов – от младенцев до пожилых людей.При разработке системы обогрева важную роль играет описание тепловой модели дома, которая позволяет оценить и оптимизировать энергетическую эффективность системы. Тепловая модель представляет собой математическую аппроксимацию поведения тепла внутри дома, учитывая различные факторы - такие, как теплопроводность материалов стен и крыши, размеры помещений, изоляция, а также параметры системы отопления.Тепловая модель дома позволяет оценить распределение тепла внутри помещений и выявить потенциальные участки перегрева или недостаточного обогрева. Она учитывает теплопотери через стены, окна, двери и другие элементы конструкции дома, а также тепловые источники – например, отопительные приборы и солнечное излучение.С использованием тепловой модели можно проводить различные расчеты и оптимизации системы обогрева, чтобы достичь комфортных условий внутри дома при минимальных затратах на энергию. Например, модель может помочь определить оптимальное расписание работы системы обогрева, позволяющее поддерживать комфортную температуру в разных зонах дома в зависимости от времени суток и наличия жильцов.В данной статье будет рассмотрена разработка тепловой модели для системы обогрева дома с использованием программного обеспечения REPEAT. Это ПО позволяет моделировать и анализировать тепловые процессы внутри дома, учитывая различные факторы и параметры. Результаты моделирования могут быть использованы для принятия технологических решений по энергетической эффективности и оптимизации системы обогрева, что в конечном итоге способствует комфорту и экономии ресурсов для домохозяйств.Ссылка на телеграм-канал REPEAT:https://t.me/repeatlab1. Тепловая модельТепловая модель дома представляет собой совокупность элементарных блоков библиотек «Теплообмен» и «Автоматика».Прирост тепла в доме обеспечивается нагревателем и окружающей средой в ситуациях, когда температура снаружи выше, чем температура воздуха внутри дома. Предполагается, что потери тепла в доме происходят по трем каналам: крыша, стены и окна. Математическое описание процессов увеличения и снижения количества тепла представлено ниже в подразделе 2. Уравнения описания системы.Передача тепла от внутреннего и наружного воздуха к конструкциям осуществляется посредством конвективного теплообмена, внутри тепло передается теплопроводностью.Основные конструкции дома представлены тремя тепловыми массами. Центральная тепловая масса отражает усредненные параметры выбранной конструкции, а две крайние - температуры на поверхностях.Нагреватель забирает часть воздуха из комнаты, нагревает его и возвращает. Режим включения и отключения нагревателя осуществляется при достижении температуры воздуха в доме 18 С и 23 С соответственно. Температура воздуха на выходе из нагревателя составляет 50 С.2. Уравнения описания системыгде:3. Исходные данные для моделированияИсходные данные для построения модели отражены в Таблице 1 и Таблице 2.Таблица 1. Параметры внутреннего воздухаТаблица 2. Параметры конструкций4. Модель системы обогрева дома в ПО REPEATРазработанная в ПО REPEAT модель системы обогрева дома приведена ниже (см. Рис. 1):Рисунок 1.  Разработанная модель системы обогрева дома вПО REPEAT5. Результаты моделированияРезультаты моделирования, которые получены на трафиках при запуске расчета модели, показаны ниже для соответствующих параметров (см. Рисунок 2 - Рисунок 9):Рисунок 2   Изменение температуры в домеРисунок 3 Изменения температуры окружающей средыРисунок 4 Изменение теплового потока через крышуРисунок 5 Изменение теплового потока через стеныРисунок 6 Изменение теплового потока через окнаРисунок 7 Изменение температуры крышиРисунок 8 Изменение температуры стенРисунок 9 Изменение температуры окон6. Графики изменения параметров идентичной системы в зарубежном ПОДля сравнения полученных результатов моделирования в ПО REPEAT, было проведено референтное моделирование в идентичном зарубежном ПО. Ниже приведены графики изменения параметров температуры и тепловых потоков  (см. Рисунок 10, Рисунок 11 и Рисунок 12):Рисунок 10 Изменение температуры в доме и температуры окружающей среды (зарубежное ПО)Рисунок 11 Изменение тепловых потоков (зарубежное ПО)Рисунок 12 Изменение температур конструкций (зарубежное ПО)7. Результаты моделированияВ результате построения модели системы обогрева дома с использованием программного обеспечения REPEAT были получены ценные результаты, которые позволяют более полно понять и оптимизировать тепловые процессы внутри дома. Графики, отображающие изменение температур конструкций, внутреннего воздуха и тепловых потоков от конструкций в окружающую среду, предоставили детальную информацию о тепловом поведении дома в разные моменты времени.Сравнение результатов моделирования с идентичной моделью, созданной с использованием зарубежного ПО, показало, что программное обеспечение REPEAT обладает достаточной точностью и надежностью при расчете тепловых систем дома. Это свидетельствует о том, что REPEAT является эффективным инструментом для выполнения подобных задач и может быть надежным основанием для принятия решений по энергетической эффективности и оптимизации системы обогрева.Результаты моделирования позволяют выявить участки перегрева или недостаточного обогрева, что может привести к повышенным энергозатратам или неудовлетворительному комфорту для жильцов. Опираясь на эти результаты, можно принять соответствующие меры, такие как улучшение изоляции, оптимизация работы системы отопления или внесение изменений в архитектурный проект, чтобы создать оптимальные условия комфортного проживания при минимальных расходах.Кроме того, результаты моделирования могут быть использованы для сравнения различных вариантов системы обогрева и выбора наиболее эффективного варианта. Например, можно оценить, как изменение типа источника тепла или настройки системы влияет на энергопотребление и комфорт внутри дома.Таким образом, результаты моделирования, полученные с помощью ПО REPEAT, предоставили ценную информацию для принятия решений по оптимизации системы обогрева дома, обеспечивая комфортные условия проживания и энергетическую эффективность."
СберМаркет,,,Предсказываем цены с помощью методов анализа данных и машинного обучения,2023-11-02T12:57:07.000Z,"(и это легко)Привет, Хабр! Вас приветствуют Нане Бегларян (инженер данных) и Дмитрий Распопов (эксперт отдела искусственного интеллекта) из компании «Цифрум» Госкорпорации «Росатом». В этой статье мы поговорим с вами о задаче, связанной с разработкой комплексной модели для прогнозирования цен на электроэнергию, которая позволяет обеспечить стабильность и надежность работы энергосистемы; делается это в рамках совместного проекта компаний Росатома РЭИН и «Цифрум».Цены на электроэнергию могут значительно колебаться в зависимости от множества факторов, что может привести к нестабильности и непредсказуемости в работе энергосистемы.(и росту цифр в коммунальных счетах).Чтобы было легче морально готовиться к очередной оплате (и заодно потренировать свои знания в ML), делимся с вами опытом и знаниями в области прогнозирования цен на электроэнергию с помощью методов анализа данных и машинного обучения.Начнем с небольшой вводной части, в которой поставим задачу и объясним принцип работы рынка электроэнергии.Под понятием рынок электроэнергии мы рассматриваемрынок на сутки вперед (РСВ, DAM). Он представляет собой спотовый рынок, аукционного типа, который определяет цены на электроэнергию на сутки вперед для каждого из 24 часов следующего дня.Также существует еще один тип спотового рынка –внутридневной рынок (ВР, IDM), который намного меньше и предоставляет участникам возможность корректировать свои позиции на день вперед за определенное время до физической подачи электроэнергии (от пяти минут до двух часов). Поскольку спрос и предложение электроэнергии невозможно точно спрогнозировать, в электросети всегда будет наблюдаться некоторый дисбаланс после закрытия IDM для торговли, и этими дисбалансами необходимо управлять. С этой целью системный оператор организует рынок крайней меры, называемый рынком дисбаланса (или рынком резервных мощностей), чтобы гарантировать равенство спроса и предложения в режиме реального времени. Хронология этих рынков проиллюстрирована на рисунке 1.Рисунок 1 – Хронология рынковПри проектировании рынка электроэнергии РСВ находится в центре внимания, поскольку его цены обычно принимаются в качестве ориентира для производных финансовых инструментов (фьючерсов, форвардов, свопов, опционов и т.д.) и других двусторонних рынков. Регулирующие органы также обычно индексируют тарифы на электроэнергию в соответствии с ценами на плотины.Участниками аукционов РСВ являются генерирующие электроэнергию предприятия, коммунальные компании, крупные промышленные потребители, трейдеры и розничные продавцы электроэнергии. Каждый участник делает заявку на покупку или продажу определенного количества электроэнергии по определенной цене на следующий день. Если бы функции спроса и предложения были непрерывными и делимыми, проблема определения расчетной рыночной цены (MCP) была бы тривиальной. Однако на практике технологические и экономические ограничения требуют, чтобы некоторые заявки размещались в виде неделимых блоков. Например, угольная электростанция обычно работает 24 часа в сутки, и, следовательно, она размещает заявки на продажу электроэнергии блоком в течение 24 часов. Для большинства электростанций, работающих на угле, неэкономично останавливать установку и запускать ее снова.Основные бизнес-процессы, протекающие на РСВ, приведены в таблице 1.Таблица 1 – Описание ежедневных операций на РСВTime SlotOperation00:00 - 17:00Двусторонние соглашения на следующий день вводятся   участниками рынка в систему.00:00 - 12:30Участники РСВ подают свои заявки на предстоящий день.12:30 - 13:00Платежи по залогу проверяются, и ставки подтверждаются   автоматически. В случае подачи необычной заявки оператор рынка имеет право   позвонить участнику для подтверждения.13:00 - 13:10С помощью инструмента оптимизации определяется цена на ЭЭ   (MCP)13:10 - 13:30Результаты публикуются, и принимаются возражения против   сопоставления заявок.13:30 - 14:00Оцениваются и разрешаются возражения среди участников   рынка.14:00Финальная цена на ЭЭ (MCP), средние почасовые цены и объемы   торговли объявляются публично.Любая задача, решаемая с помощью методов машинного обучения, связана с актуальной бизнес-проблемой. В результате анализа характеристик РСВ можем выявить бизнес-проблему, заключающуюся в повышении эффективности расчета плановых финансовых требований и обязательств. Кроме того, прогноз цен на рынке на сутки вперед необходим для поэтапного решения задачи планирования как генераторам, так и потребителям электроэнергии (ЭЭ).Сформулируем гипотезу: Используя исторические данные о выработке электроэнергии, ценах на газ, а также данные по другим связанным показателям РСВ (погодно-климатические факторы, цены фьючерсов, совокупные объемы спроса и предложения, плановая выработка основных типов генерации, индекс цен производителей и потребителей), можно выполнить прогноз цен на ЭЭ на несколько суток вперед и тем самым повысить эффективность расчета плановых финансовых требований и обязательств.Из поставленной гипотезы формулируем решаемую задачу в терминах машинного обучения: необходимо, используя данные по фактическим ценам на электроэнергию и предикаты, разработать комплексную прогнозную модель (КПМ), представляющую собой модель машинного обучения и предназначенную для краткосрочного прогнозирования цен на электроэнергию.Решение поставленной задачи происходило в несколько традиционных этапов, представленных на рисунке 2:Рисунок 2 – Основные этапы разработки математической моделиСбор данныхТак как объект исследования – рынок электроэнергии, то информация собирается вручную с открытых источников. Основным источником сбора датасета стала платформа Exist Transparency (EPIAS) [1], которая предоставляет необходимые данные для прозрачной и надежной работы энергетических рынков. С платформы EPIAS были получены данные по объему выработки ЭЭ для каждого типа электростанций, доступной мощности для генерации, значения индексов потребительских цен, объемы заявок на покупку, а также цены на газ. Собранный датасет был обогащен данными с других интернет-ресурсов — курс валют, цены на фьючерсы и другие атрибуты, связанные с экономической отраслью.Разведочный анализ, подготовка и предварительная обработка данныхРазведочный анализ и подготовка данных выполнялись совместно. Все признаки были приведены к единому периоду дискретизации — 1 час, а пустые значения заполнены с помощью сезонной интерполяции(способ нахождения промежуточных значений величины по имеющемуся дискретному набору известных значений. Интерполяция использует значения некоторой функции, заданные в ряде точек, чтобы предсказать значения функции между ними).Гипотеза о том, что погодные условия оказывают влияние на ценообразование электроэнергии, в результате корреляционного анализа была опровергнута и данные признаки были удалены из исходного набора.Данные по доступной установленной мощности были удалены из датасета на начальном этапе, исходя из корреляционного анализа и логики. Значения признаков сильно коррелируют с данными по ежедневному планированию производства, которые являются наиболее важными.Для обогащения данных и повышения их аналитической мощности были сгенерированы детерменированные признаки из атрибутов по ежедневному планированию производства, такие как:Sum_PrIndVol — суммарное значение по возобновляемым источникам электроэнергии (ветроэнергетика, геотермальная, биомассовая энергетика, гидроэнергетика и другое);PrIndVolRatio — отношение суммарного значения по ежедневному планированию к общему объему планирования выработки;GasVolRatio — отношение объемов производства природного газа к общему объему планирования выработки;DeficitVol — отношение потребления к общему объему планирования выработки.Также были созданы признаки с помощью функции синуса для дня в неделе, часа и других атрибутов, полученных из даты. Кроме того, исходя из логики были созданы признаки, соответствующие временному лагу (сдвигу) значений инфляции на 3 и 4 месяца назад.Все признаки были приведены к единому масштабу (среднее значение 0, стандартное отклонение 1) с помощью алгоритма стандартизации. Категориальные признаки были закодированы и приведены к числовому типу данных.Рассмотрим подробнее график изменения цен на электроэнергию в доступный период с 1 января 2018 по 20 декабря 2022 гг, представленный на рисунке 1.По динамике изменения цен на электроэнергию видно, что, начиная с середины 2021-ого произошел рост значений, по которому можно построить характерный тренд вплоть до настоящего времени. Можно предположить, что это происходит из-за нестабильной ситуации на рынке и в мировой экономике. Мы решили подробнее изучить ситуацию на РСВ в рассматриваемый период. Исходя из открытых источников [1,2] было выявлено, что на РСВ с января 2019 года был введен верхний лимит цены (price caps), и нет, к кепкам, к сожалению, это не имеет никакого отношения, хотя, принцип работы абсолютно схожий.  Price caps представляет собой искусственное ограничение цен, другими словами верхний лимит, значения которого представлены на рисунке 3.Рисунок 3 – Графики изменения цены на ЭЭ и значений верхнего лимитаВ конце 2020 года и вначале 2021 значения верхнего предела были резко уменьшены, что стало ограничивать максимальную цену ЭЭ. Таким образом из-за введенного верхнего предела мы не можем оценить истинное влияние признаков на целевую переменную, а также использовать данные без «price caps» для обучения модели, поскольку зависимости, установленные математической моделью, будут совершенно иные.Было принято решение, что для построения модели будут использоваться данные с price caps начиная с 1 января 2021 года, так как они соответствуют состоянию того рынка, который сохраняется до сих пор.Следующим шагом разведочного анализа данных являлась оценка влияния признаков на целевую переменную. Для оценки влияния признаков на целевую переменную можно воспользоваться корреляционными матрицами, пример которой показан ниже.Было установлено, что наибольшее влияние на цену за ЭЭ оказывают такие признаки (в скобках показаны значения коэффициента корреляции): индекс потребительских цен, цена на газ, цены на фьючерсы, объем фьючерсов, индекс цен производителей, курс валюты, сгенерированные детерминированные признаки, ежедневное планирование выработки по некоторым типам электростанций, сумма объемов заявок на продажу, совокупное количество заказов за час.Рисунок 4 – Корреляционная матрицаОбучение и тестирование моделиОбучение и оценка точности модели выполнялась на различных временных интервалах – 3, 7 и 30 дней. В зависимости от этого объемы обучающей и тестовой выборки изменялись при каждом обучении модели для конкретного промежутка времени.Для оценки точности моделей используются следующие метрики:MAPE — средняя абсолютная ошибка в процентах;RMSE — квадратный корень из средней квадратичной ошибки.В результате сравнения нескольких предиктивных моделей на данных с price caps, то есть в диапазоне с января 2021 года по декабрь 2022 года, лучший результат показал алгоритм градиентного бустинга — CatBoost. Результаты сравнения моделей продемонстрированы в таблице 3.Таблица 2 - Результаты сравнения моделейMAPERMSEНазвание моделиПримечания3 дня7 дней1 месяц3 дня7 дней1 месяцLinearRegressionС price caps (с января 2021 до декабря 2022)35,3025,6537,74906,36832,441143,34RandomForestRegressorС price caps (с января 2021 до декабря 2022)24,6017,2320,93640,92573,19608,33KNeighborsRegressorС price caps (с января 2021 до декабря 2022)41,9141,6670,54946,721813,202724,86SVRС price caps (с января 2021 до декабря 2022)58,5763,3673,402412,362628,342825,87ExtraTreesRegressorС price caps (с января 2021 до декабря 2022)24,4516,7023.04637,53550,45726,60LGBMRegressorС price caps (с января 2021 до декабря 2022)17.1213,6017,61558,09517,59563,80MLPRegressorС price caps (с января 2021 до декабря 2022)20,6815,9026,18721,83629,54834,48XGBRegressorБез price caps (c сентября 2018 года до сентября 2020 года)1,702,004,907,0610,3217,08С price caps (с января 2021 до декабря 2022)20,9215,4720.04633,56583,16621,87CatBoostRegressorС price caps (с января 2021 до декабря 2022)16,3113,9717,29524,50510,45565,77Прогноз с использованием предсказаний других признаков с price caps84,0163,1452,562261,941984,901978,65Авторегрессионная модель (ARIMA)С price caps (с января 2021 до декабря 2022)53,3327,6768,941205,191060,832644,92LSTMС price caps (с января 2021 до декабря 2022)27,9819,8717,86848,79742,90649,31RNNПрогноз с использованием только целевой переменной (Цена на ЭЭ) с price   caps (с января 2021 до декабря 2022)28,4119,1223,470,16647,03667,81Прогноз с использованием других фичей (факторная модель) с price caps17,3642,3826,66599,021642,01874,82Stacked LSTMС price caps (с января 2021 до декабря 2022)43,7930,1848,881027,37940,34940,60CNN-LSTMС price caps (с января 2021 до декабря 2022)20,6414,9617,42653,30539,96609,37LSTM 2С price caps (с января 2021 до декабря 2022)33,9819,04799,56919,22749,18799,56Выбранная модель имеет минимальную среднюю абсолютную ошибку в процентах, равную 16.3 % в трехдневном окне, что значительно ниже результатов остальных моделей. На рисунках 5-7 продемонстрированы прогнозы модели на 3 дня, 7 дней и 1 месяц соответственно.Рисунок 5 - Прогноз модели CatBoostRegressorРисунок 6 - Прогноз модели CatBoostRegressor на 7 днейРисунок 7 - Прогноз модели CatBoostRegressor на 1 месяцЗначимость признаков для алгоритма CatBoostRegressor показана на рисунке 8.Рисунок 8 - Значимость признаков алгоритма CatBoostRegressorВыводы по результатам моделированияИсходя из оценки качества и точности работы моделей для прогноза ЭЭ, будем использовать факторную регрессионную модель CatBoostRegressor, обученную на данных, содержащих price caps, с 1 января 2021 года по 20 декабря 2022 года.Для выбранной модели были достигнуты следующие показатели точности:средняя абсолютная ошибка в процентах на 3 дня - 16.3 %;средняя абсолютная ошибка на 7 дней - 13.2%;средняя абсолютная ошибка на 30 дней - 17.2%.Таким образом гипотеза об использовании исторических данных для прогноза цен на ЭЭ на РСВ была подтверждена и решена поставленная задача.Надеемся и верим, что мы были полезны и будем рады обратной связи. До новых встреч!И делимся списком источников, вдруг будет интересно:EXIST TRANSPARENCY PLATFORM: официальный сайт.URL:https://seffaflik.epias.com.tr/transparency/EXIST TRANSPARENCY PLATFORM: официальный сайт.URL:https://www.epias.com.tr/wp-content/uploads/2016/03/public_document_eng_v4.pdfCost Savings From Relaxation of Operational Constraints on a Power System With High Wind Penetration - Scientific Figure on ResearchGate: официальный сайт.URL:https://www.researchgate.net/figure/Flowchart-describing-the-simulation-of-day-ahead-and-real-time-scheduling-and-dispatch-in_fig1_275949636(дата обращения 17 Mar, 2023)EXIST TRANSPARENCY PLATFORM: официальный сайт.URL:https://www.epias.com.tr/gun-oncesi-piyasasi/eslestirme/NordPool - Europe's leading power market and offers trading: официальный сайт.URL:https://www.nordpoolgroup.com/en/the-power-market/Day-ahead-market/Price-formation/EXIST TRANSPARENCY PLATFORM: официальный сайт.URL:https://www.epias.com.tr/gun-oncesi-piyasasi/surecler/EXIST TRANSPARENCY PLATFORM: официальный сайт.URL:https://seffaflik.epias.com.tr/transparency/piyasalar/gop/fiyattan-bagimsiz-satis-teklifi.xhtmlEXIST TRANSPARENCY PLATFORM: официальный сайт.URL:https://seffaflik.epias.com.tr/transparency/piyasalar/gop/fiyattan-bagimsiz-alis-teklifi.xhtmlEXIST TRANSPARENCY PLATFORM: официальный сайт.URL:https://seffaflik.epias.com.tr/transparency/piyasalar/gop/fiyattan-bagimsiz-satis-teklifi.xhtmlEXIST TRANSPARENCY PLATFORM: официальный сайт.URL:https://seffaflik.epias.com.tr/transparency/uretim/planlama/kgup.xhtmlEXIST TRANSPARENCY PLATFORM: официальный сайт.URL:https://seffaflik.epias.com.tr/transparency/piyasalar/gop/eslesme-miktari.xhtmlEXIST TRANSPARENCY PLATFORM: официальный сайт.URL:https://seffaflik.epias.com.tr/transparency/tuketim/gerceklesen-tuketim/gercek-zamanli-tuketim.xhtmlEXIST TRANSPARENCY PLATFORM: официальный сайт.URL:https://seffaflik.epias.com.tr/transparency/uretim/planlama/eak.xhtml"
СберМаркет,,,Моделирование систем электромобиля,2023-10-23T08:05:44.000Z,"Предлагаем научиться созданию достоверной модели электромобиля, описывающей процессы в части механики, электрики и электрохимии. Результаты верифицированы посредством сравнения с аналогичной моделью в программном комплексе AmeSim.Ссылка на телеграм-канал REPEAT:https://t.me/repeatlabВводная информация для создания моделиСтруктурно рассматриваемая модель электромобиля состоит из следующих частей:Модель, описывающая динамику продольного движения электромобиля. Она реализована посредством блоков «Динамика 4-х колёсного автомобиля», «Шасси с трансмиссией» и «Электропривод»;Модель, описывающая динамику батареи, питающей электро-двигатель. Она реализована посредством блока «Батарея»;Модель, имитирующая поведение водителя автомобиля.Внешний вид модели приведён на Рисунке 1.Рисунок 1 - Внешний вид модели электромобиля, собранной в ПО REPEATДинамика продольного движенияРассматриваемая модель осуществляет расчёт скорости движения автомобиля через его ускорение, определяемое из уравнения второго закона Ньютона для результирующей силы, приводящей автомобиль в движение. Уравнение имеет следующий вид:где:где:где:где:Динамика батареиДанная модель описывается через уравнения разряда и заряда. Имеющие, соответственно, следующий вид:Модель, имитирующая поведение водителяДанная модель задаёт скорость движения автомобиля и, в зависимости от выходной скорости модели, подаёт управляющие сигналы либо на ускорение движения, либо на торможение, имитируя тем самым поведение настоящего водителя. График, иллюстрирующий поведение данной модели, представлен на Рисунке 2.Рисунок 2 - Кривая скорости, задаваемой водителемРезультаты моделирования в ПО REPEAT и сравнение их с результатами моделирования в AmeSimРезультаты моделирования в ПО REPEAT представлены графиками слева, в AmeSim – справа.Исходя из полученных результатов, можно сделать выводы о достоверности модели электромобиля (см. Рисунок 3 для параметра «Скорость»).Рисунок 3 - Сравнение результатоврасчёта скорости автомобиляРисунок 3 - Сравнение результатоврасчёта скорости автомобиляРисунок 3  - Сравнение результатоврасчёта скорости автомобиляАналогичным образом сделаны сравнения расчетов для параметров «Момент электродвигателя» (Рисунок 4), «Напряжение батареи» (Рисунок 5) и «Степень заряда батареи» (Рисунок 6):Рисунок 4 - Сравнение результатов расчёта момента электродвигателяРисунок 4 - Сравнение результатов расчёта момента электродвигателяРисунок 5 - Сравнение результатов расчёта напряжения батареиРисунок 5  - Сравнение результатов расчёта напряжения батареиРисунок 6 - Сравнение результатов расчёта степени заряда батареиРисунок 6 - Сравнение результатов расчёта степени заряда батареиОбщий вывод по результатам моделированияВ результате построения модели модели электромобиля, описывающей процессы в части механики, электрики и электрохимии, были получены соответствующие графики требуемых параметров. Сравнение результатов моделирования с идентичной моделью зарубежного ПО показало, что ПО REPEAT с достаточной точностью осуществляет расчет процессов электромобиля и подходит для выполнения подобных производственных задач."
СберМаркет,,,Разработка трёхмассовой тепловой модели асинхронного тягового двигателя,2023-10-19T12:45:13.000Z,"Разработка и постройка технологически сложных деталей огромных промышленных тяговых электродвигателей будет очень затратными и нерациональными мероприятием, если отсутствует необходимая базовая расчетная модель. В данной статье продемонстрирована разработка трёхмассовой тепловой модели асинхронного тягового двигателя с использованием ПО REPEAT. Трёхмассовая модель двигателя включает в себя ротор, статорную обмотку и магнитопровод статора с корпусом. Увеличение количества масс по сравнению с одно- и двухмассовыми моделями делает модель более точной и пригодной для расчетов. Вместе с этим увеличивается количество необходимой информации в виде конструктивных размеров и коэффициентов теплопроводностей и теплоотдачи.Ссылка на телеграм-канал REPEAT:https://t.me/repeatlabИсходные данные1) в качестве моделируемого выбран асинхронный двигатель НТА – 1200;2) исходные данные для модели приняты в соответствии с Таблицей 1;3) скорость воздуха в вентиляционных каналах статора принята: 26 м/с;4) скорость воздуха в вентиляционных каналах ротора принята: 25 м/с;5) скорость воздуха в воздушном зазоре принята: 17 м/с;6) геометрические параметры выбраны в соответствии с имеющими данными либо приняты на основании типовых изделий;7) температура охлаждающего воздуха принята: 20Таблица 1. Основные параметры тягового электродвигателя НТА-1200Разработка моделиНа базе исходных данных была построена модель (см. Рисунок 1), которая использовала блоки библиотек «Автоматика», «Теплообмен», «Общее» и «Электрические приводы».Рисунок 1. Тепловая модель асинхронного тягового двигателяРезультаты расчетовПо результатам запуска построенной модели были получены параметры по каждому объекту измерения. Полученные результаты приведены в Таблице 2.Тепловые расчеты двигателя были выполнены в режиме работы с параметрами:Таблица 2 – Результаты моделированияПереходные процесс изображены на Рисунке 2, Рисунке 3 и Рисунке 4Рисунок 2 - Переходный процесс установления температуры в обмотке статораРисунок 3 - Переходный процесс температуры в магнитопроводе статора, корпусуРисунок 4 - Переходный процесс установления температуры в ротореРезультаты моделированияВ результате выполнения тепловой модели трёхмассовой асинхронного тягового двигателя получены графики изменения температуры тепловых масс. Выполненное исследование позволяет оперативно оценить уровни температур каждого объекта тягового двигателя при работе агрегата в сборе, что очень важно для применения в промышленном моделировании."
СберМаркет,,,Как выигрывать соревнования по программированию,2023-04-19T07:01:01.000Z,"Всем привет, меня зовут Денис. В рабочее время я тимлид одной из команд отдела цифровизации в Росатоме, а в личное — фуллстек‑разработчик, у которого есть хобби: выигрывать соревнования по программированию. Участвую и один, и с командами. Например, мы взяли Гран‑При VK Hack 2018, финал первого Цифрового Прорыва, второе место на Вездекод 2020, несколько этапов VK Fresh Code, серию Премий Алисы и хакатон от Яндекса, выиграли хакатон Volvo, удостоились высокой отметки от жюри на Junction 2019 и многое другое разных масштабов.VK Hack 2018. Первый офлайновый хакатон в моей жизни — и сразу гран-при. Нельзя было не войти во вкус после такого.В прошлом году мне удалось занять одно из двух первых мест на конкурсе “Код Петербурга”, где разрабатывались приложения, связанные с моим родным городом. Это был крупнейший выигрыш в моей жизни, позволивший вложиться в недвижимость, после чего я подумал, что будет полезным агрегировать мой опыт и раскрыть секрет.И так для систематической победы в конкурсах и хакатонах по программированию нужно всего лишь…1. Знать о том, какие конкурсы вообще проводятсяБанально звучит, но в первую очередь нужно знать обойцовском клубесоревновании. Полезно отслеживать различные каналы и новостные ленты по теме. Важным источником являются ещё и платформы для разработчиков, там то и дело запускают соревнования с призами.Несколько примеров:Russian Hackers— сообщество хакатон-энтузиастов. В канале раз в несколько дней стабильно новость об очередном хаке с возможностью зарегистрироваться. Кстати, в 2019 году именно Russian Hackers организовали Хакатон на Полярном Круге — в Салехарде. Мы там не выиграли, но это один из лучших хаков за всю мою жизнь, впечатлений очень много было. Кому интересно, вотстатья о том, как всё проходило.Салехард, о достопочтимый мой читатель, находится на самом краю земного диска.All Cups— агрегатор IT-соревнований, куда был перенесён, в частности, Russian AI Cup, который аудитории Хабра наверняка хорошо известен. Личного опыта у меня нет, но, как я понимаю, в основном там небольшие онлайн-раунды на разные темы.VK Mini Apps— платформа миниприложений от ВКонтакте, на которой систематически запускают активности с призами для разработчиков: хакатоны, онлайн-конкурсы, контесты и так далее. Я участвовал в нескольких форматах. По личному ощущению (и в сравнении с многими другими) организация таких мероприятий у VK высочайшая, особенно в офлайне: всегда интересные локации, классные задания, понятные и адекватные критерии судейства. Вотистория про хакатон 2018 года.У VK кстати есть ещё иигровое подразделение, которое, как мне кажется, слабо связано с деятельностью остальной соцсети. Там периодически проводятся геймджемы и конкурсы HTML5-игр. Но, в противовес миниприложениям, мои личные впечатления от организации именно в игровом подразделении довольно смешанные: много претензий к процессу регистрации и участия, очень расплывчатые формулировки заданий и критериев. Возможно, у этой команды просто не хватает пока опыта в таких вещах.Премия Алисы от Яндекса— долгоиграющий конкурс для разработчиков навыков к Алисе, результаты подводят раз в квартал уже несколько лет. Периодически менялись условия и награда.До недавнего времени хорошим источником была площадкаСалют от Сбера. Но сейчас у неё дела не очень, и большинство активностей на площадке свернули. Впрочем, может быть ещё оживёт.Конкурсы от Телеграма*. Довольно регулярные задания с денежными призами. Тут часто бывает работа не только для программистов, но и, например, для дизайнеров.Ну и, я думаю, большинство знаетYandex Cup**, можно его не представлять.2. Не лениться участвоватьИз общения со знакомыми айтишниками у меня складывается впечатление, что многим лениво разбираться с условиями и тратить свободное от работы время на выполнение заданий. Это можно понять — жизнь айтишника и без того сытая.Однако, как мне кажется, большинство недооценивает свои шансы на выигрыш и/или переоценивает порог вхождения. Нередко я выигрывал в конкурсе не из-за того, что обладал какими-то сверхнавыками, а просто из-за сравнительно небольшого числа претендентов.Было и такое: хакатон с наибольшим числом одновременных участников согласно Книге Рекордов Гиннеса. Пока замеряли рекорд, запрещалось покидать рабочее место.Приведу два любопытных примера:В конкурсе от площадки Салют в 2020 году была номинация “Наибольшее число приложений” (прошедших модерацию). Награждали три первых места. Велась сильная борьба за первые два, и в комьюнити многие сразу отказались участвовать, не желая вступать в гонку. В итоге третье место досталось человеку случайно, он просто выпустил пару приложений без какого-либо желания вписываться в конкурс и делал это пассивно. По итогам оказалось, что для попадания на третье место достаточно было посидеть два-три вечера, а призовая сумма при этом составляла порядка 500к рублей.Когда начинался “Код Петербурга” в 2022-м, мы со знакомыми обсуждали этот конкурс в чате, и один товарищ прямо там сформулировал идею для приложения. Я тогда сразу сказал, что идея огонь, но он в итоге участвовать не стал. По результатам награждали 10 призовых мест, и многие решения явно уступали той идее моего знакомого. Я абсолютно уверен, что, прими он участие, в десятку точно попал бы, и скорее всего получил бы призовые порядка 600-800 тысяч рублей.Если мотивации в виде приза мало, то вот ещё рецепт: подгонять под конкурс какие-то существующие личные наработки. Большинство программистов, так или иначе, занимается собственными проектами помимо работы: коммитят в опенсорс, изучают новые технологии, пишут статьи.Другой важный мотиватор — это мерч. У участника айти-конкурсов вся семья обеспечена толстовками и футболками до конца жизни.Например, на текущий момент у менянесколько статейна Хабре по алгоритмам, и наработки, описанные в них, были использованы в конкурсах (во всех случаях успешно), хотя ничто из этого не создавалось изначально специально под какое-нибудь соревнование.3. Понимать критерии оценкиЛично я глобально разделил бы все айтишные состязания на три типа:“Творчество”Тебе дают общую задачу, описывают проблему, которую ты должен решить. Ты выбираешь практически любые инструменты для её решения, делаешь, затем презентуешь результат. Результат, как правило, оценивают члены жюри, реже голосование пользователей или комьюнити. Помимо просто хорошо сделанного решения тут нередко имеет значение ещё и качество презентации. Мой любимый тип состязаний. Таковы большинство хакатонов, например.“Жизнеспособность”Твой продукт должен показать лучший результат по каким-то метрикам, которые, чаще всего, оценивает программа. Либо просто обозначается формальное достижение некоторого состояния. Мой нелюбимый тип состязаний, ниже расскажу, почему.“Техзадание”Тебе дают подробные инструкции, что именно нужно сделать, зачастую ещё и предоставляют готовые макеты, если речь идёт о создании приложения с интерфейсом. Такими конкурсами компании пользуются, чтобы дёшево получить большой объём конкретной программистской работы над конкретными бизнес-задачами компании. Например, почти все конкурсы Telegram принадлежат к этой категории. Лично я считаю эту категорию скучной, но многим знакомым айтишникам, наоборот, нравится, потому что не нужно самому ничего придумывать.Нам попался “Персонализированный чат-бот для занятый спортом домашних животных”. Подробнее о том, как это было, можнопочитать вот тут.Бывают смешанные критерии: допустим, жюри составляет шорт-лист, который потом сортируется по метриками или голосованию. Или, скажем, есть и чёткие инструкции и оценка метрик — спортивное программирование. В любом случае, участвуя в конкурсе, вы должны точно понимать, как работа будет оцениваться.4. Выбрать главную цельЗдесь я пойду в обратном порядке по увеличению сложности.Каждая мелочьВ контестах с типом “Техзадание” работы участников почти одинаковые. Отличаются они буквально в мелочах. Оценивать могут всё, даже какие-то вещи, о которых вы не подумали: например, размер сборки в байтах. Так что здесь сложно, но одновременно просто: вам нужно вложить максимум усилий в максимально близкую к идеалу реализацию.Программисты любят такие вещи, потому что они интуитивно ощущаются справедливыми — субъективный вкус жюри почти не влияет на распределение мест. Но и разрыв совсем небольшой: первое место с призом в $10000 от второго с призом в $5000 может отделять лишняя миллисекунда в скорости загрузки начального экрана.Пример из жизни.Лет 10 назад я выиграл один небольшой конкурс, потому что вместо Flex-компонентов (да, конкурс был на Flash) нарисовал свои, которые весили в несколько раз меньше. Визуально было то же самое, что у соперников, а по размеру сборки я их обошёл.Любой ценойУ “Жизнеспособности” плюс в том, что чаще всего способ достижения метрик не слишком важен. Но в этом же и минус. Нередко конкурс превращается не в соревнование по программированию, а в соревнование по хитрости и “взлому” системы. Мне рассказывали историю про хакатон по ML, где работы участников загоняли в функцию оценки, которая выдавала численную результативность, и победил человек, который просто аккуратно подобрал нужные коэффициенты в экселе. Ещё подобное бывает при прохождении роботами полос препятствий: можно не программировать способность реагировать на датчики, а просто захардкодить последовательность шагов в стиле “ехать прямо в течение 810 мс, затем направо на 32 градуса, затем ещё прямо в течение 1235 мс”.При оценке посещаемости участники приводят людей искусственно, голосования накручивают, числа подделывают, ради количества жертвуют качеством и т.д. Если конкурс постоянный, то побеждать в нём будет одно и то же, потому что схожие вещи начнут давать одинаково высокий результат в метриках.Пример из жизни.У Яндекс Диалогов естьдинамический чарт, где представлены топ-100 самых популярных навыков в Алисе. Разработчики этих навыков регулярно получают от Яндекса денежные выплаты просто за счёт нахождения их продуктов в топе. Поскольку Алисой активно пользуются дети, то вполне неплохой трафик идёт в навыки-повторюшки (Алиса просто возвращает тебе твою же фразу; помните, наверное, как игрушечный хомяк-повторюшка). Понятно, что никакой работы по написанию такого навыка не нужно, обычно это едва ли 5-10 строчек кода. Тем не менее, вы можете увидеть в этом топе: Хомяк повторюшка, Хрюшка повторюшка, Детская повторюшка, Пукающая повторюшка, Громкая повторюшка, Картавая повторюшка, Мужская повторюшка, Зайка повторюшка… Обилие однообразного хлама даже в абсолютном топе по всей платформе указывает на синтетический абьюз системы. Не то, чтоб такого не было в других областях, но как участнику конкурсов лично мне подобное совершенно не интересно, даже если это сулит какие-то деньги.Индивидуальный подходКатегория “Творчество” требует от вас понимания того, что за люди будут в жюри. И на общем уровне, и на уровне мелочей. Например, если вы знаете, что в жюри все пользуются айфонами, то будет разумно отшлифовать интерфейс приложения или сайта именно под айфоны, использовать знакомый визуал и iOS-подобные анимации.Но, в целом, ответьте сами себе на вопросы: Что это за люди? Какой у них опыт работы с технологиями? Что они могут ценить выше, а что ниже? По какой причине им нужен конкурс, что они хотели бы от решений? Те же Russian Hackers в одной из своих лекций дали простой, но дельный совет: “Подойдите к любому организатору, и напрямую спросите, что нужно сделать, чтобы выиграть”.Мой друг@wooferclawв футболке Russian Hack Team вспарывает ножом плюшевую птицу Angry Birds, потому что на Junction в Финляндии мы делали трек от компании Rovio.О Junction.Иногда получается угадать, иногда нет. Но какие-то поправки на жюри делать всегда полезно. В офлайновых хакатонах нередко жюри и менторы — одни и те же лица, поэтому с ними можно пообщаться и попросить дать совет. Если конкурс проводится не первый раз, можно посмотреть, какие работы высоко оценили в прошлые годы.Пример из жизни.В Салехарде мы делали голосовой навык и не учли тот факт, что люди чаще всего с навыками знакомы плохо, и для них это выглядит, как обычный чат-бот, просто с озвучкой. Мы сделали супер хитрый NLP-модуль, который обрабатывал кучу вариантов голосовых команд на естественном языке. Как вы думаете, что сделал первый же член жюри, запустивший наше решение? Он забил на голос и стал нажимать на экране кнопки… об обработке которых мы совершенно не подумали. За двое суток хакатона ни один из нас не удосужился попробовать нажать в навыке кнопку вместо того, чтобы вводить команду голосом. Мы к тому моменту работали с голосовыми решениями уже не первый год и просто отвыкли от того, как обычные люди извне реагируют на таких ботов первый раз. Да, как в анекдоте “Тестировщик заходит в бар”. Так и проиграли.5. Обладать нужными умениямиПонятное дело, что вы не выиграете конкурс программистов, если не умеете программировать. Ладно, может и выиграете, но тогда это очень плохой конкурс, и так бывает крайне редко. В остальном: вам нужно уметь делать то, в чём вы соревнуетесь. И уметь делать это хорошо.Подбор командыЕсли соревнование командное, то важно и распределение ролей в команде, и готовность работать совместно. Думаю, по правильному подбору и организации команды на хакатон можно написать отдельную статью (чаще всего я капитан, так что есть приличный опыт в этом). Но главные тезисы такие:Равноправие и демократия не работают. Должен быть капитан, который ведёт команду, и на период конкурса все соглашаются его слушаться (что не исключает, конечно, возможность спорить, предлагать другие идеи и так далее).Каждый член команды должен обладать навыками, благодаря которым у него всегда будет работа на период конкурса. Не должно быть ситуации, при которой в команде два программиста сидят долбят код 48 часов и один продуктолог, который последние 2 часа составляет презентацию, а до этого особо ничего не делает.Вы должны точно знать, что задача принципиально решаема вашими силами.Хакатоны очень хорошо помогают погрузиться в новые технологии. Если вы годами не можете взяться за изучение языка X или фреймворка Y — поставьте себе цель сделать на нём решение на хакатоне. В условиях ограниченного времени вам придётся форсировать загрузку знаний в свой мозг, и это реально офигенно работает.Пример из жизни.На Junction мы поехали составом, который не был хорошо знаком друг с другом. В итоге один из разработчиков пообещал взять на себя большой модуль из задания, но через несколько часов сказал, что не знает, как это сделать, и ушёл собирать мерч. Пришлось за него делать модуль мне, из-за чего мы потеряли очень много времени, и конечное решение работало хуже, чем могло. Жюри высоко нас отметили, но первое место в категории дали другим.СофтскиллыНикто не накажет вас за лишние вопросы организаторам. Конечно, речь не идёт о том, чтобы доставать их по поводу цвета каждой кнопки. Но крайне полезно бывает понять картину, которая была в голове у составителя задания. Здесь в игру вступают не столько формальности, сколько эмоции и личное восприятие. Умейте общаться и спрашивать. Я нередко во время конкурса приходил в личку к кому-нибудь из оргов и уточнял какие-нибудь детали, попутно по его речи и акцентам пытаясь понять, на что обратить внимание.Если конкурс предполагает презентацию, то важность софтскиллов повышается ещё больше. В команде нужен уверенный в себе человек с опытом публичных выступлений и хорошей речью. К сожалению, бывают даже хакатоны, которые выигрываются одной лишь презентацией.Пример из жизни.На AtomSkills — это наш внутренний Росатомовский чемпионат по всем профессиям, в том числе по программированию — мы заметили в задании неточность и убедили жюри в том, что наше решение в этой части можно трактовать, как частично выполненное, хотя изначально нам собирались не зачесть этот пункт, и мы потеряли бы много баллов. Кстати, этот чемпионат проводится по методике WorldSkills, и он неожиданно интересный. Предложить поучаствовать людям извне я не могу, зато вы можетепочитать и посмотретьнемного внутренней кухни.ХардскиллыВ заданиях на алгоритмы нужно знать алгоритмы. В заданиях по созданию продукта нужно уметь создавать продукты. В заданиях, где есть веб-интерфейс, нужно уметь дизайнить и верстать.Обратное тоже верно: если задание решаемо несколькими путями, выбирайте тот инструментарий, которым владеете. Можно командой одних бэкендеров сделать бота там, где другие команды соорудят веб-сервис (если подобное позволяет регламент, но чаще всего позволяет).Пример из жизни.Наотборочные Цифрового Прорывамы пришли командой из двух сильных бэков, двух сильных фронтов и художницы, которая умеет делать классную 2d-графику. В итоге наших сил и скиллов хватило на полноценную демку мобильной игры с прикольным визуалом, которая реально работала и очень зашла жюри.Игра про ледоколы в трек Росатома. Всем так понравилось, что нас целой командой пригласили на работу, где я и тружусь по сей день :)ЗаключениеЯ вам описал кучу плюсов и историй успеха, но, как вы понимаете, не бывает добра без худа. Участие в конкурсах может лишать вас огромного количества свободных вечеров и выходных. Поражения бывают очень обидными: например, один раз комиссия отказалась вообще нормально смотреть наш проект, потому что мы сделали игру, а жюри ожидали не игру (в условиях, конечно же, никаких ограничений не было). Иногда по выбору организаторов ты понимаешь, что формулировка задания вообще не имеет никакого отношения к тому, что они на самом деле хотели. Иногда ты понимаешь, что выиграла команда, которая ничего не сделала, однако удачно показала презентацию. Или, например, команда соединила два чужих опенсорсных проекта.Хакатон без спящих людей — не хакатон.Однако же, всё равно это очень интересно и, при определённой сноровке, приносит немалый дополнительный доход. Не говоря уже об обучении новым навыкам и поддержании старых в тонусе.Сноски* Сам я не участвую в конкурсах от Telegram и другим советую этого не делать.При этом самим мессенджером пользуюсь без проблем.Несколько лет назад команда Дурова с пафосомобъявиласоревнование для разработчиков ботов с призовым фондом $1млн пятью частями по $200к. В итоге сначала подведение итогов задержали на несколько лет, а потом и вовсе отменили для всех групп, кроме самой первой. На прямой вопрос Дуров сказал: “Мы наградили победителей тайно”. Из внутренних источников я знаю, что на самом деле на конкурс забили из-за того, что платформа ботов не выстрелила, и из-за начавшихся тогда проблем с инвесторами и американскими регуляторами. Тем не менее, компания не только нарушила собственное же публичное заявление, но и отказалась это признавать. В дальнейших конкурсах от Телеграма настолько серьёзных косяков не было, но были случаи, когда заявленное в условиях призовое место не присваивали вообще никому, поскольку, якобы, ни одно решение не достойно такой награды. Тем не менее, я знаю людей, которые стабильно занимают места и получают крупные призовые, так что было бы нечестно не упомянуть эту площадку в статье про конкурсы. Участвуйте на свой страх и риск.** Обратите внимание, что выигрывать в Yandex Cup можно фиксированное число раз.Но эта информация представлена довольно неявно (по крайней мере, раньше была), на что нарвался мой товарищ: он участвовал и взял призовое место, но выигрыш ему не перечислили, потому что победил он уже какой-то там по счёту раз и перешёл допустимый лимит по количеству побед одним человеком. Это указано в правилах, но глубоко. В итоге ему выслали Станцию в качестве извинений.UPD. Первоначально у статьи был заголовок: ""Почему я выигрываю конкурсы по программированию, а вы — нет"". Аудитория в основном посчитала его кликбейтным (отсюда и основное количество минусов), хотя, помимо отсылки на известную в интернетах мемную конструкцию (""...а вы — нет"") я просто старался таким способом выразить свою досаду от того, что разработчики часто относятся к конкурсам очень пассивно, не используя возможности, которые у них на самом деле есть. Что ж, по видимому, искусство заголовка — не моя сильная сторона. Если вы дочитали досюда, надеюсь, в тексте встретилось что-то новое, полезное или интересное для вас."
СберМаркет,,,Зачем мы моделируем импульсные нейронные сети и с помощью чего это делаем,2023-01-26T13:00:01.000Z,"Привет, Хабр! На связи Михаил Киселев, руководитель направления в отделе ИИ компании «Цифрум» (Росатом) и руководитель лаборатории нейроморфных вычислений в Чувашском государственном университете. Сегодня подниму тему импульсных нейронных сетей. Общее представление о том, что такое искусственные нейронные сети, есть, наверное, у всех. Многие представляют, зачем они нужны, как устроены, как работают. Речь пойдет об одной их разновидности — импульсных нейронных сетях (ИНС). Нейросети вообще мыслились их создателями как компьютерные модели ансамблей нервных клеток мозга — это и из их названия следует. У разных типов нейросетей степень этого сходства разная. Так вот, ИНС — это самый похожий на биологический мозг тип нейронных сетей.За счет этой похожести достигаются немалые преимущества. Прежде всего — энергоэкономичностьнейропроцессоров(о них еще скажем ниже) для выполнения ИНС. Дело в том, что в ИНС нейроны посылают друг другу не числа, с которыми приходится делать довольно сложные операции — складывать, умножать, а атомарные объекты — аналоги нервных импульсов, распространяющихся по нервным связям животных и человека. Они называютсяспайки. Как нервный импульс в биологии, спайк имеет всегда одну и ту же амплитуду и пренебрежимо малую длительность. Поэтому обработка прихода спайка сопряжена с малым числом очень простых операций, что экономит и время, и энергию, и место на процессоре — если импульсный нейрон реализуется на специальном нейпроцессоре. Вторая особенность, отличающая ИНС от традиционных нейросетей, — это асинхронный, независимый от других процессов, характер работы каждого нейрона. Пришел на нейрон спайк — тот на него быстро прореагировал, и снова тишина: энергия зря не тратится… Это разительно отличается от работы обычных нейросеток. Они обычно слоистые, пока один слой не посчитается, следующий должен ждать. А как посчитается — начинается интенсивный поток данных от слоя к слою — надо или не надо — даже если там одни нули передаются. ИНС может непрерывно работать и дообучаться, тратя при этом меньше энергии.Но почему же тогда мы не видим вокруг себя эти импульсные сети — в смартфонах, камерах, умных часах, умных утюгах?Во‑первых, ИНС во всем очень непохожи на обычные нейросети. Принципы их функционирования, обучения совсем другие. Обычные нейросети изучаются и развиваются уже более 70 лет — начиная с придуманных в 50-х годах прошлого века персептронов. Исследования ИНС имеют раза в три менее длинную историю. Один из основных подходов к их обучению, основанный на так называемой модели пластичности STDP (spike timing dependent plasticity — пластичность, основанная на времени спайков), стал развиваться чуть больше 20 лет назад. Как раз сейчас мы наблюдаем бурный рост исследований ИНС (и не только наблюдаем — мы в нем участвуем). Вторая причина — крайняя неэффективность реализации ИНС на обычных компьютерах. Можно сказать, что в них все наоборот по сравнению с ИНС: вместо миллионов крайне простых асинхронно работающих специализированных обработчиков спайков — десяток универсальных и потому чрезвычайно сложно устроенных процессоров, обрабатывающих числа с помощью последовательно выполняемых команд; вместо совмещения в одном месте памяти и процессора (когда приходящий спайк сразу активирует и передает для дальнейшей обработки соответствующий синаптический вес) — бесконечное перемещение по шине команд и данных из памяти в процессор и обратно. Поэтому разработка специализированных нейрочипов для ИНС крайне актуальна.Какие задачи мы решаем сегодняОбучение ИНСЭто ключевой вопрос. Нейронные сети огромны. Это касается и биологических сетей (например, в мозге пчелы около миллиона нейронов), и традиционных нейросеток (известная лингвистическая нейросетевая модель GPT-3 — примерно в два раза больше мозга пчелы), и ИНС. Нейросети делают свою полезную работу, в первую очередь, благодаря тому, что вес связей между их нейронами имеет некоторые оптимальные величины. Связей настолько много, а их влияние на результат работы сети настолько сложно, что установить эти оптимальные величины вручную абсолютно невозможно — это может быть только результатом процесса, который и называется обучением сети.Как обучать традиционные нейронные сети, известно уже лет 50. Помогает здесь то, что традиционная нейронная сеть является с математической точки зрения непрерывной функцией, как от ее входов, так и от весов ее связей. Поэтому, немного изменяя вес любой ее связи, мы можем сказать, лучше стало или хуже — например, увеличилась ошибка предсказания или нет.Функционирование импульсного нейрона носит дискретный характер. Спайк — он либо есть, либо нет. Поэтому малое изменение веса любой связи в ИНС скорее всего вообще никак не скажется на работе ИНС в целом. Поэтому принципы обучения ИНС совсем другие. В основе принцип локальности. Он гласит, что изменение веса связи (чаще говорят — синапса) может зависеть только от состояния и активности нейронов, связанных этим синапсом. Принцип этот известен давно, однако его конкретные реализации могут быть самыми разными, и вот поиск наилучшей реализации является важнейшей исследовательской задачей, решаемой нашим коллективом. Точнее говоря, оптимальных законов изменения синаптических весов в процессе обучение сети может быть несколько — в зависимости от того, какую задачу обучения мы решаем. Обычно выделяют 3 класса задач обучения:1)Обучение без учителя (unsupervised learning).Обычно нейросети получают на вход огромное количество данных. Например, в какой‑нибудь классической задаче классификации изображений (скажем, найти по поисковому запросу все картинки, где есть автомобиль) на вход подается картинка — матрица из десятков тысяч (а то и сотен тысяч или миллионов) чисел, значений яркости пикселей. Яркость отдельного пикселя не несет практически никакой информации о том, есть на картинке автомобиль или нет. А вот сочетания этих яркостей, образующие некоторые структурные признаки (например, наличие чего‑то круглого на картинке), уже может быть ценным информативным признаком (например, это круглое может быть колесом). Но разных структурных признаков можно придумать великое множество… И тут помогает обучение без учителя. Если значительную долю предъявляемых фотографий будут составлять автомобили, там круглые структуры будут встречаться часто. Если будет много снимков природы (которая, как известно, не знает колеса), там будет много структур с горизонтальными границами (если это пейзажи — горизонт), либо, например, с тонкими линиями (деревья, их ветки). Выделяя информативные признаки, нейросеть не знает, с чем она работает — ей не говорят, автомобили это или природа. Она просто «видит», что данный структурный признак встречается часто и в устойчивом сочетании с другими структурными признаками, фиксирует этот факт, образуя структурный признак более высокого порядка. В этом режиме сеть не знает правильных ответов — именно поэтому его называют обучением без учителя. Однако выделенные информативные признаки будут ценным подспорьем в следующем типе задач — обучении с учителем.2) Обучение с учителем (supervised learning).Самый известный, классический вариант обучения. Сети предъявляют много пар «входные данные, правильный ответ». После этого сеть должна по новым входным данным, ранее ей не предъявленным, дать правильный ответ. Например, как во всем известном тесте MNIST, сети дают 60 000 пар «изображение рукописной цифры, изображенная цифра», после чего она по новому предъявленному ей изображению должна сказать, какая цифра изображена.3) Обучение с подкреплением (reinforcement learning — RL).Самый реалистичный, имеющий отношение к жизни вариант обучения, то, как учимся мы сами. В этом варианте сеть взаимодействует с окружающим миром: виртуальным или реальным. Она получает непрерывный поток информации, описывающей состояние мира и ее положение в нем, и отвечает на него сигналами — командами, которые интерпретируются как действия, меняющие состояние мира и ее самой. Эти действия сети оцениваются в виде сигналов поощрения или наказания с точки зрения задачи, которую сеть должна решать. Сигналы могут поступать как сразу после действия сети, так и со значительной задержкой, что создает дополнительные трудности, так как приводит к неясности, что именно сеть сделала хорошо или плохо. В RL сеть должна сама догадаться, что от нее хотят, и выстроить так свое поведение (генерируемые ей команды), чтобы получать как можно больше поощрений и как можно меньше наказаний.Наша группа занимается в какой‑то степени всеми тремя видами обучения, но акцент делаем на RL — обучении с подкреплением.Получаемый сетью поток данных должен быть структурирован, из него должны быть выделены информативные признаки (а это — обучение без учителя), а целевые состояния, чего сеть должна достичь, должны быть распознаны (обучение с учителем). Наша цель — создание активных интеллектуальных систем, решающих поставленные задачи не по жестко заданному запрограммированному алгоритму, а в условиях недостаточно ясной, меняющейся обстановки с учетом успехов и неудач их предыдущей деятельности. Такие системы должны непрерывно обучаться на примерах. Именно так ставятся задачи в робототехнике, системах автоматического адаптивного управления сложными системами и других важных для Росатома областях применения.В этой связи вспомним об ИНС. Совершенно очевидно, что преимущества ИНС, о которых было сказано в начале, тут должны быть особенно актуальными. Энергоэкономичность, асинхронность, свободный обмен потоками информации (в виде спайков) с внешним миром, непрерывное обучение в виде постоянного подстраивания синаптических весов без искусственного разделения данных на обучающую и тестовую выборки — все это как нельзя более подходит к упомянутому кругу задач. С одним только НО: архитектуры ИНС, методы их обучения, эффективные с точки зрения задач RL, еще не найдены…Разработка перспективных нейрочиповЭта деятельность только недавно стала для нашей группы приоритетной, поэтому результатов здесь пока получено совсем немного. К тому же эта новая междисциплинарная область требует отдельного рассмотрения.Мало кто до сих пор осознает, что совсем недавно в микроэлектронике произошло революционное событие. Была создана микросхема, нейропроцессор, эмулирующий ИНС, являющийся, с одной стороны, универсальным вычислителем (а доказано, что ИНС — это универсальный вычислитель, равномощный машине Тьюринга), а, с другой стороны, не имеющий стандартной для всех наших компьютеров фон‑Неймановской архитектуры с центральным процессором, памятью, очередью команд. Речь идет о нейрочипе TrueNorth компании IBM.Это произошло в 2014. С тех пор взгляд на такие чипы как на основу будущего технологического прорыва все расширялся, был создан еще примерно десяток аналогичных чипов, фактически сформировалась еще одна всемирная гонка технологий. Участие в ней предполагает решение разнообразных прикладных и научных проблем. И одна из них имеет прямое отношение к работе нашей группы. Эффективность реализации ИНС в специализированном нейрочипе зависит прежде всего от нахождения удачных компромиссов между функциональными возможностями нейрона, гибкостью и универсальностью правил изменения синаптических весов и жесткими ограничениями реализации всего этого «в железе» — в терминах количества транзисторов, энергопотребления, реализуемых математических операций. Этим мы и занимаемся — создаем hardware‑friendly архитектуры сетей, модели импульсных нейронов, методов их обучения в расчёте на то, что когда‑то это все будет оформлено в виде технологических процессов печати ASIC.Но оставим в стороне вопросы «железа» и вернемся к теоретическим и программным аспектам.Немного подробнее об RL с точки зрения ИНСКогда мы решаем задачи обучения с подкреплением с помощью ИНС, мы должны учесть, что все потоки данных, с которыми приходится иметь дело, имеют вид потоков спайков. Это очень похоже на ситуацию с мозгом: он получает информацию от глаз и ушей в форме нервных импульсов, в той же форме отдает приказы мышцам рук и ног, и результат этой деятельности (что его обладатель съел или от кого убежал) тоже получается в виде активности специфических нейронов (т. е. опять же в виде спайков). Это хорошо вписывается и в картину реализации всего этого на нейрочипах, так как никакого другого взаимодействия кроме обмена спайками в нейрочипах не предполагается. Но такой сценарий решения задач RL до недавнего времени почти не изучался.Тут надо еще сказать, что RL бывает разный, есть много вариантов, различающихся по своей сложности. Самый простой из них — это так называемый model‑free RL. Предполагается формирование у нейросети поведения в стиле «стимул‑реакция». Для каждой ситуации у сети имеется некоторый спектр возможных действий. Некоторые из них являются правильными («хорошими»), некоторые неправильными («плохими»), а некоторые — нейтральными. Причем оценка действия приходит практически сразу после выработки сетью соответствующей команды. Хороший пример такой задачи — сопровождение объекта камерой. Сеть должна держать хаотично движущееся пятно в центре поля зрения камеры. Она может двигать камерой сигналами определенных своих нейронов вверх, влево, вниз, вправо. Приближается центр камеры к пятну — сеть получает поощрение, удаляется — наказание. Сначала сеть вообще не знает, что от нее требуется, но быстро выясняет это на основе поощрений/наказаний. Кстати, как это происходит, можно посмотреть в реальном времени — эта задача была решена нашей ИНС и запись можно посмотретьтут.Решается эта задача с помощью ИНС.Схематичное изображение структуры ИНСВходом здесь является сигнал с виртуальной камеры, смотрящей в квадратную область, в которой движется световое пятно. Четыре группы нейронов заставляют камеру двигаться по четырем направлениям. Сигналы оценки формируются по описанным выше правилам. Еще на вход сети подается случайный шумовой сигнал. Источником активности ИНС являются только ее входные узлы. Если в поле зрения камеры темно, сигналов нет, то сеть не выдаст никаких команд — без подпитки извне ее нейроны не сгенерируют спайков. Кстати, в этом смысле мозг человека демонстрирует схожее поведение — известны опыты по сенсорной депривации, когда человека лишали поступления сигналов от всех органов чувств, и он практически сразу засыпал. Но когда сеть ничего не делает, она и не учится. Выручает внешний шум. Он заставляет сеть делать случайные движения. Многие из них неправильные, но встречаются и удачные совпадения.Вообще, это общий прием в RL — заставлять необученную сеть что‑то делать просто для накопления опыта. Если же сигнал от камеры есть, то входной шум блокируется, чтобы не мешал. Обучение правильным действиям происходит в нейронах среднего слоя (Learning neurons) — к ним приходят оценочные сигналы, усиливающие или подавляющие связи между входными сигналами и генерируемыми командами в результате специально разработанных для этого законов изменения синаптических весов. Чтобы быть уверенным, что оценивается именно последнее действие сети, сигналы оценки пропускаются через специальные вентили, открываемые командами сети через специальные активирующие связи. Подробности можно найти впрепринте нашей статьи.Таким образом, на основе этого и некоторых других примеров можно декларировать, что задачи model‑free RL с помощью ИНС мы решать умеем. Сейчас мы занимаемся более сложной задачей, решение которой позволит выйти уже на реальные приложения ИНС, — model‑based RL. Представим себе компьютерный пинг‑понг. По экрану летает мячик, отражаясь от стенок. В левой части экрана имеется ракетка» которую сеть может двигать вверх‑вниз. Если во время удара мячика в левую стенку в месте удара оказывается ракета, считается, что сеть достигла успеха, что отмечается сигналом поощрения, если же ракетка не находится в этом месте, то сеть «проиграла» и получает сигнал наказания. Ситуация в этой игре кардинально отличается от задачи отслеживания пятна. Оценивается здесь далеко не каждое действие сети, а только их (возможно отдаленные) последствия. Для того, чтобы понять, куда сейчас двигать ракетку, сеть должна сформировать внутри себя в модель этого простейшего пинг‑понга на основании своего опыта и затем применить эту модель к текущей ситуации для принятия лучшего решения. Это существенно сложнее, хотя мы уже примерно знаем, как.Следующим шагом уже может быть выход в реальный мир — с манипуляторами, видеокамерами и практически полезными технологическими задачами. Это немного объясняет, зачем взрослые и, казалось бы, серьезные люди, напряженно смотрят в экран, где в черных окнах летают белые шарики, а рядом в окне консоли бежит отладочная выдача, чуть‑чуть приоткрывающая завесу тайны — того, что происходит в недрах большой импульсной нейросети.С помощью чего мы все это делаемОтвет состоит из двух частей — аппаратной и программной.С аппаратной частью ситуация следующая. Конечно, пока в нашем распоряжении нет супер‑нейрокомпьютера на тех нейрочипах, о которых я говорил выше. Мы работаем с GPU кластером, который используется большинством нейросетевых исследователей. В мире обычных фон‑Неймановских вычислителей есть процессоры, которые немного приближаются по своим свойствам к идеалу ИНС — это универсальные графические процессоры. Большое количество параллельно работающих вычислительных ядер, большой объем локальной регистровой памяти и некоторые другие свойства делают их удобной вычислительной основой для моделирования ИНС, тем более, когда имеется не одна такая GPU плата, а несколько на одном хосте. И уж совсем здорово, если этих хостов тоже несколько, и они объединены в кластер в рамках быстрой локальной сети. Но чтобы такая параллельность вычислений была эффективно использована, нужно специальная программная платформа, и вот о ней речь пойдет дальше.Для того, чтобы моделировать ИНС на обычных компьютерах (отнесем к ним и GPU), нужно специальное ПО. Такого в мире уже сейчас немало. Разработано много эмуляторов ИНС. Как правило, они имеют интерфейс на Python, и с их помощью можно построить любую конфигурацию ИНС, выполнить ее эмуляцию, получить и оценить результаты. Казалось бы, что еще надо? Но:1) Многие из них работают с ограниченным кругом моделей нейронов и, что еще хуже, моделей изменения синаптических весов (синаптической пластичности). А мы знаем, что правильный ответ в этой области еще не найден и нам вполне может потребоваться нечто, выходящее за пределы этих пары‑тройки дюжин моделей (и так оно и происходит на самом деле). Учитывая это, в некоторых эмуляторах ИНС (например, в Brian2) введена возможность описывать модели с помощью текстового задания дифференциальных уравнений в специальной нотации. Это, конечно, удобно, но приводит к радикальному замедлению выполнения по сравнению с теми же моделями, заданными вручную программно. Моделирование заданных таким образом дифференциальных уравнений представляется возможным только на CPU. Моделирование на GPU возможно только в частных случаях.2) Даже для тех моделей, которые реализованы, скорость их вычисления (особенно на GPU) обычно на порядок меньше, чем могла бы быть достигнута, если бы эта модель была бы реализована с помощью низкоуровневого языка типа CUDA. А время здесь играет существенную роль — некоторые из наших вычислительных экспериментов могут длиться более недели.3) Python как интерфейсный язык таких пакетов является часто неудачным компромиссом. Это все же универсальный язык программирования, и поэтому гораздо менее удобен, чем декларативное описание ИНС — если используются только какие‑то типовые решения и архитектуры. С другой стороны, он не позволяет эффективно использовать все ресурсы аппаратного обеспечения, что было бы возможно на низкоуровневом языке типа С++ или CUDA.Поэтому мы используем альтернативу — единственный (на данный момент) отечественный пакет для эмуляции ИНС — ArNI‑X. Этот пакет использует совсем другой подход к решению обозначенных выше проблем.ArNI‑X не имеет питоновского интерфейса. Вместо этого в большинстве типовых случаев структура ИНС может быть описана в конфигурационном файле в формате XML. Ниже мы покажем, как это делается. Если же надо создать некую совсем нестандартную конфигурацию, это можно сделать с помощью предоставляемого API на С++. Само ядро системы при этом остается неизменным. Оно может эффективно эмулировать импульсные нейроны из широкого класса моделей за счет большого количества настраиваемых параметров. Это же касается и моделей синаптической пластичности. Наконец, если гибкости настройки этих моделей окажется недостаточно, код ядра (например, написанные на CUDA компоненты, которые выполняются на GPU) может быть изменен и дополнен, для чего имеется удобный механизм условной компиляции. Но надо отметить, что делать это не приходилось уже давно.Как же выглядит описание сети в ArNI‑X? Вот один из примеров — описание достаточно нетривиальной сети:Не вдаваясь в подробности, рассмотрим основные принципы этого описания. Оно включает два основных структурных элемента — описание популяций нейронов (в блоках Section) и совокупностей связей между ними (в блоках Link).Популяция включает нейроны с одинаковыми свойствами и одинаковой логикой соединения с нейронами других секций (если требуется настроить эти вещи индивидуально для отдельных нейронов, придется это делать с помощью API). Название популяции, число нейронов в ней и отдельные параметры модели задаются в атрибутах XML блока и во вложенных блоках.Проекции характеризуются парой соединяемых популяций, типом связей (возбуждающий, тормозной, модулирующий) и политикой соединения (все со всеми, один к одному, случайный и т. д.). Кроме того, устанавливаются распределения весов связей и распределения задержек распространения спайков по связям (в ИНС передача спайка от нейрона к нейрону происходит не мгновенно, и это время является существенным элементом вычислений).Достаточно удобный и интуитивный механизм. Как уже говорилось, если его не хватает, сколь угодно сложные сети могут быть построены вызовами соответствующих методов API.Сама эмуляция происходит следующим образом. Исполняемый модуль эмулятора (есть его варианты для CPU и GPU) читает конфигурационный файл и строит сеть в соответствии с ним. Загружает динамические библиотеки, реализующие источники спайков (входной сигнал) и реакцию на спайки выходных нейронов сети. Эти динамические библиотеки также указываются в конфигурационном файле. Источники спайков могут быть как стандартные (например, чтение спайков из файла), так и специализированные для конкретной задачи. Эмуляция может производиться в реальном времени — выполнение сети синхронизировано с получением данных на каждом шаге эмуляции, так что модуль входных данных может использовать часы для синхронизации посылки данных с внешним миром. Модуль считывателя получает на каждом шаге эмуляции спайки от нейронов, объявленных выходными, интерпретирует их как команды, реализуя таким образом взаимодействие сети с внешним миром. Он может отсутствовать, если интересна только динамика самой сети. В этом случае может быть включен режим мониторинга, сохраняющий полный протокол активности сети и, через определенные промежутки времени, мгновенные «снимки» полного состояния сети. Если требуется дополнительное конфигурирование сети, поверх ее структуры, описанной в конфигурационном файле, это делается в специально указанной динамической библиотеке, которой предоставляется соответствующий API. Когда все это загружено и построено, начинается собственно эмуляция, состоящая из повторения одних и тех же шагов. Каждый шаг эмуляции включает:1) получение входного сигнала;2) маршрутизацию спайков — как входных, так и сгенерированных нейронами на предыдущем шаге (при этом синаптические задержки достигаются за счет очередей, приходящих спайков на синапсах);3) перевычисление состояния всех нейронов (и, возможно, их синаптических весов);4) посылку спайков выходных нейронов модулю‑считывателю.Эмуляция завершается либо по прошествии нужного числа шагов, либо по команде от модуля входных сигналов или считывателя.Кроме собственно эмулятора пакет ArNI‑X включает еще большое количество вспомогательных средств для анализа результатов мониторинга сети, подготовки входных данных и т. д. Отдельно надо отметить систему оптимизации гиперпараметров сети на основе комбинации генетического алгоритма и координатного спуска. Вот здесь как раз проявляется удобство декларативного описания сети — какие‑то ее параметры можно вставить в конфигурационный файл в виде макросов, заменяемых модулем оптимизации на конкретные значения. Кроме того, модуль оптимизации умеет работать с кластером GPU машин, оптимальным образом распределяя вычислительную нагрузку по оптимизации по кластеру.Хотя многие из описанных свойств нашего эмулятора ИНС сильно отличают его от аналогов, их удобство и практичность проверены временем. Эмулятор ArNI‑X существует уже более трех лет и с успехом применялся в самых разных исследованиях, связанных с ИНС. На мой взгляд, было бы вполне оправдано, если бы он стал базовым средством Росатома не только в исследовательских программах по ИНС, но и при создании практически полезных коммерческих приложений."
СберМаркет,,,"Python в атомной энергетике: сообразительные нейроморфы, предсказание поломок и анализ нормативки",2022-11-21T09:16:44.000Z,"Атомная энергетика — отрасль наукоёмкая. Python со своими инструментами для анализа данных и построения ИИ как раз подходит АЭС, здесь с ним можно решать амбициозные задачи на острие науки о данных. Поэтому Хабр решил разузнать побольше про Python в Росатоме. И попросил меня помочь.Меня зовут Тимур Тукаев, я IT-редактор. Начал писать о технологиях в 2007, когда поставил свой первый Linux. Увлечён идеями свободного ПО и open source, программирую на Kotlin, делаю о нём топики в JetBrains Academy.Я пообщался с тремя инженерами Росатома и выяснил, для чего в корпорации используют Python.Для начала кратко представлю ребят. Все они решают в Росатоме задачи, связанные с Python.Денис ЛарионовНачальник отдела искусственного интеллекта компании «Цифрум» (Росатом). Руководит перспективным направлением по созданию нейроморфной системы искусственного интеллекта. Популяризатор науки, автор научно-исследовательских работ, наставник профессиональных конкурсов, соревнований, хакатонов в области IT.Дмитрий РаспоповЭксперт отдела искусственного интеллекта компании «Цифрум» (Росатом), старший преподаватель отделения ядерной физики и технологий в НИЯУ «МИФИ». Занимается предиктивной аналитикой и исследованиями в области искусственного интеллекта, принимает участие в профессиональных чемпионатах. Заместитель менеджера компетенции «Машинное обучение и большие данные» по отраслевой линейке чемпионатов DigitalSkills, серебряный призёр на 45-м мировом чемпионате по профессиональному мастерству по стандартам WorldSkills (Казань, 2019 г.), бронзовый призёр на чемпионате стран БРИКС по профессиональному мастерству Belt & Road and BRICS Skills Development and Technology Innovation Competition (Фошань, КНР, 2019 г.).Андрей БуйновскийИнженер группы подготовки данных компании «Цифрум» (Росатом). Занимается обработкой текстов на естественном языке (natural language processing, NLP). Призёр IT-чемпионатов.Сначаламы поговорим о «Цифруме» и их работе с нейроморфами — здесь Python помогает создавать новый тип нейросетей.Дальшепобеседуем о своевременном ремонте оборудования с помощью предиктивной модели, построенной тоже на Python.И наконец, посмотрим, как Python и его библиотеки классифицируют запутанные ГОСТы и нормативы, по которым строятся и работают АЭС.Ребята не только со мной поговорили, но ещё и поделились классными ссылками по своим темам — их можно найти в конце статьи.Задача 1. Как Росатом с помощью Python и нейроморфов учит датчики работать без сервера, а роботов — быстро принимать решенияКлассические ИИ-решения даже сейчас создаются на основе архитектуры фон Неймана. С нервной системой человека они имеют мало общего. Нейронные сети не работают как биологический мозг. Разрыв в эффективности вычислений колоссальный: грубо говоря, самый мощный компьютер распознаёт на фотографии кошку на три порядка хуже, чем средний человеческий мозг. Разрыв в производительности есть всегда, хоть и зависит от типа задачи.Подробнее о сравнительных характеристиках классических нейросетей и нейроморфов   — с графиками, методологией и более научным языком — можно прочитатьв статье на английском(прямая ссылка на pdf) илина русском(чтобы скачать альманах, надо заполнить форму).Чем нейроморфные вычисления отличаются от классического ИИСократить или даже устранить разрыв могут нейроморфные сети. Нейроморфная инженерия предлагает использовать данные, известные нам о структуре и работе мозга, в создании вычислительных систем. Давайте сравним нейроморфный подход с фон-неймановской архитектурой. Вот обычный компьютер складывает два числа:Берёт из памяти число и помещает в регистр процессора.Берёт из памяти второе число и помещает в регистр процессора.Выполняет операцию.Берёт результат вычисления из третьего регистра процессора и помещает в память.Главная отличительная черта биологических систем в том, что, в отличие от фон-неймановской архитектуры, вычисления и память не разделены шиной данных. Память как бы размазана по весам связей между нейронами (in-memory computing). Вычисления происходят непрерывно, когда нейроны получают сигнал на вход и генерируют выходной сигнал.Существует и близкий к биологическому подход near-memory computing, когда вычисления и память разделены, но в памяти размечаются элементарные вычислители. Благодаря этому часть вычислений можно переложить на память.В Росатоме с 2020 года нейроморфами занимается компания «Цифрум»: за направление отвечает небольшая команда из трёх человек.Принцип Росатома — быть на шаг впереди, получать прикладные результаты от перспективных технологий — энергоэффективных вычислений на конечных устройствах, которые труднодостижимы в классических решениях искусственного интеллекта. Например, вычисления на новых физических принципах и киборгизация.Денис ЛарионовДля чего нейроморфы Росатому«Цифрум» работает с нейроморфами по трём направлениям:Всё, что связано с обучением и интеллектуальными задачами непосредственно на самом устройстве. Например, умные сенсоры и датчики, которые могут работать без привязки к серверу. Носимые сенсоры, способные определять токсичность воздуха, энергоэффективное зрение для БПЛА и систем слежения, умные камеры и коммуникационные устройства, которые минимизируют потоки данных, визуальная вибродиагностика, а в перспективе — ещё и чипы без батарейки для интеграции в нервную систему человека.На современном железе мы столкнёмся с проблемой: энергоэффективный датчик надо лишить мозгов — он будет просто посылать данные на сервер. А умному нужна большая батарейка, чтобы сохранить скорость вычислений и возможность интеллектуальных действий. Нейроморфы решают проблему: даже со слабенькой батарейкой могут делать сложные вычисления.Например, исследователи из китайского проекта Tianjic создали систему управления движением велосипеда, которая способна видеть, слышать, планировать маршрут, балансировать — и всё это на одном чипе со скоростью 1 278 MACGOPS (giga MAC operations per second) на один ватт.Системы, где требуется быстрое принятие решений.Самый распространённый кейс — робототехника, где надо регулировать множество параметров в реальном времени, чтобы всё оставалось под контролем, а процесс был предсказуем.Создание практически бесконечно масштабируемых нейросетей.В классической ИИ-системе при увеличении размера сети (то есть количества нейронов) резко растёт потребность в аппаратных ресурсах. С нейроморфными системами такой проблемы нет: рост ресурсов более линейный за счёт полностью асинхронной модели вычислений и отсутствия ограничений, обусловленныхзаконом Амдала. Снимается ограничение на количество нейронов: в классическом варианте выше сотни миллиардов не осилишь, здесь может быть и в тысячу раз больше.График, который иллюстрирует закон Амдала. Источник: ВикипедияМы в «Цифруме» исследуем алгоритмы обучения импульсных сетей на C++ и Python. Если всё получится, будем создавать свой микропроцессор на собственной, не-фон-неймановской архитектуре.Денис ЛарионовСтек и необходимые знанияИногда код приходится писать на C++, напрямую взаимодействовать с CUDA, реализовывать свои вычислительные примитивы, но чаще мы работаем на более высоком уровне абстракции, и тогда уже отличным инструментом является Python.Если мы экспериментируем с небольшой сетью (несколько сотен импульсных нейронов), то удобство и скорость разработки на Python оказываются важнее эффективности вычислений и скорости работы, которые может дать С++. Кроме того, в экосистеме Python много полезных библиотек и фреймворков.Например, мы используем фреймворкBrian 2— проводим вычисления на самом высоком уровне абстракции. Brian 2 может понимать дифференциальные уравнения, написанные в виде строки в специальной нотации. Это позволяет описать практически любую динамику системы, однако плата за такую гибкость — низкая производительность.В общем, выбирая фреймворк для моделирования импульсных нейронных сетей, мы всегда ищем компромисс между возможностями, ограничениями и удобством использования.У нас есть и свои интересные открытые разработки. На Kaggleвыложена работао том, как с помощью импульсных сетей решить задачу распознавания рукописных цифр. Код можно запустить, поиграться с ним, изучить.Схема того, как решается задача распознавания рукописных чисел. Источник: https://www.kaggle.com/code/dlarionov/mnist-spiking-neural-networkЕщё мы построили такую модель: есть система с прямоугольной рамкой и случайно перемещающимся там шариком. Задача системы — настраивать рамку так, чтобы шарик всё время оказывался в самом центре. Мы эту задачу решили с помощью нейроморфов, а для этого пришлось написать эмулятор событийного сенсора: он выложен в открытом доступена GitHub.Денис ЛарионовЕсть и критичные знания для работы в команде, которые диктуются спецификой нейроморфных систем:Биология.Нужно понимать биологов, уметь задавать им вопросы. Достаточный фундамент даст курсSynapses, Neurons and Brainsот Иерусалимского университета.Работа современных вычислительных систем, проектирование цифровых микросхем.Важна эрудиция в железе. Надо понимать, как устроены современные вычислительные архитектуры CPU, GPU. Какие у них есть проблемы, как эти проблемы решать. В идеале — способность спроектировать цифровую схему, эмулировать на ней алгоритм.Математика: машинное обучение и смежные дисциплины.Важно понимать, как работают классические нейронки, их подходы и проблемы. Видеть, когда нейроморфы уместны, а когда лучше построить гибридное решение или даже классическую сеть.Задача 2. Как Python помогает предсказывать циклы ремонта оборудования атомных электростанцийОборудование на АЭС — дорогое, бесперебойная работа критически важна. Конечно, техника требует ремонта. Что если научиться заранее определять, где и когда он нужен? Тогда можно прогнозировать остаточный ресурс, увеличить межремонтные интервалы, снизить количество внеплановых остановок.Мы решили написать модель предиктивной аналитики, основанной на временных рядах, и с ней предсказывать появление аномалий в работе оборудования.Временной ряд — исторические данные по набору параметров процесса. Каждая единица такого набора называется измерением или отсчётом. У каждого отсчёта указывается время или порядковый номер измерения. В отличие от выборки он учитывает взаимосвязь изменения параметров и состояния процесса во времени.Дмитрий РаспоповИллюстрация работы моделей. Индикатор технического состояния вверху смотрит на различие параметров во время нормального и аномального режимов работы оборудования. Внизу показаны параметры, которые оказывают наибольшее влияние на модель. Фото: Росатом.Система предиктивной аналитики выстраивается для конкретного предприятия — она не универсальна, ведь набор оборудования и условия эксплуатации могут сильно отличаться. Например, мы уже успели поработать с грануляторами Hosokawa, турбогенераторами, электролизерами, конвекционными линейными печами BTU, ГЦНА (главные циркуляционные насосные агрегаты). Однако несмотря на специфичность, саму модель, которая анализирует состояние оборудования, можно переиспользовать: достаточно её заново откалибровать, настроить и дообучить на конкретных установках.Этапы разработки модели предиктивной аналитикиШаг 1. Накапливаем исторические данные с датчиковНа каждой АЭС есть автоматизированная система управления технологическим процессом (АСУ ТП). Она собирает и хранит данные с датчиков. Температура, давление, ток, напряжение, мощность, активная/реактивная вибрация, скорость вращения и так далее. Их и берёт команда разработки.Бывает, что на предприятиях централизованно собирается недостаточно данных для аналитики, нет единой системы сбора и хранения данных — тогда мы такую систему выстраиваем и внедряем сбор всех необходимых показаний. То есть параллельно решаем и инфраструктурные задачи для отдельных предприятий.Дмитрий РаспоповШаг 2. Накапливаем и обрабатываем данные из журналов технического обслуживания и ремонта (ТОиР)В журналы ТОиР заносятся сведения о состоянии оборудования за время эксплуатации: они помогают понять, какие бывают поломки, насколько они серьёзны, когда, с какой периодичностью могут произойти. После этого команда предиктивной аналитики выявляет самые важные поломки с точки зрения контроля технического состояния оборудования — для каждой установки он будет специфическим. При этом текст журналов обрабатывается вручную: одни и те же сущности операторы установок могут называть по-разному, сокращать названия или разбивать один ремонт на несколько записей. А ещё команда разработки плотно общается с изготовителями оборудования и экспертами, которые работают на этом оборудовании или обслуживают и ремонтируют его.Шаг 3. Выбираем данные для анализаКоманда разработки получает данные с журналов и датчиков и решает, какие показатели стоит использовать для предиктивной аналитики, а какие — нет, выявляет зависимости, ищет «говорящие» сигналы. Для этого использует логику, результаты предыдущих исследований и предварительный анализ конкретного оборудования.Шаг 4. Строим математическую модель раннего обнаружения отклонений в работе оборудованияСначала анализ проводят люди, а после них в дело вступает обученная математическая модель. Она накладывает модель нормального состояния оборудования на временной ряд реальных данных и ищет несоответствия. Все показатели, которые превышают пороговые значения, классифицируются как аномалии в работе оборудования. Среди них выбираются самые информативные сигналы, и модель проверяет, есть ли между ними и возможными будущими поломками чёткая корреляция. Алгоритм выявляет паттерны и делает предсказания: какие узлы потребуют замены или небольшого ремонта, когда, сколько времени в запасе.После обучения модель может мониторить состояние оборудования и корректировать планы на техобслуживание и ремонт. Это помогает оптимизировать затраты на их проведение, предотвратить преждевременный выход из строя отдельных элементов и уменьшить количество внеплановых ремонтов.Причём предсказания делаются задолго до того, как операторы установок могли бы заметить неладное: система учитывает слабые сигналы, совокупность которых в контексте состояния оборудования, прошедших ремонтов и замен деталей как раз и даёт прогноз. То есть можно действовать проактивно, избегать простоев, а не просто реагировать на случившееся.Простой пример: мы написали модель для грануляторов Hosokawa. С помощью неё можно выполнять прогноз остаточного ресурса по косвенным признакам. Одним из информативных признаков, влияющих на деградацию шнека, была его скорость вращения. Во время анализа данных и построения модели выяснилось, что чем больше скорость вращения, тем сильнее износ установки. Косвенные признаки при должном упорстве можно найти у любого оборудования.Дмитрий РаспоповШаг 5. Операторы получают результаты аналитикиДанные идут к конечным пользователям. А те заранее закупают какие-то запчасти, проводят дополнительное ТО и т. д.Задача оценки остаточного ресурса считается одной из самых трудных. Существует множество подходов к её решению, каждый из которых может быть лучше или хуже на разных данных. Нам удалось подойти к решению этой задачи, выявив косвенные признаки, влияющие на исчерпание остаточного ресурса.Дмитрий РаспоповСтек лаборатории и необходимые знанияЯзыки и инструментыпомимо Python: SQL, Java, JavaScript.Математика: математический анализ, линейная алгебра, статистика, теория вероятностей, дискретная математика, различные методы оптимизации, алгоритмы.Образование: техническое образование с упором на АЭС, построение предиктивных моделей для оборудования АЭС требует понимания проблематики.Библиотеки и фреймворкидля создания моделей: TensorFlow, Keras, XGBoost, CatBoost, PyTorch. Для анализа данных Pandas, NumPy.Библиотеки и фреймворки для графиков и дашбордов: Matplotlib, Seaborn, Plotly.Библиотеки для работы с временными рядами: Prophet, Statsmodels, Sktime, Tslearn, Tsfresh, Darts, Karts, Merlion, PyTorch Forecasting, Graykite и многие другие.Также команда разрабатывает собственные функции для решения специфических задач, связанных с данными.Задача 3. Как Python помогает анализировать проектную документацию, ГОСТы и внутренние стандартыВ атомной промышленности много нормативной документации. ГОСТы, внутренние инструкции, регламенты, допуски… В Росатоме есть даже свой институт стандартизации. В документах описаны требования к процессам, материалам, эксплуатации оборудования. Все эти регламенты постоянно нужны в работе.АЭС Росатома должна соответствовать куче требований: от марок бетона до устойчивости конструкций перед природными катаклизмами. Они записаны в тысяче нормативных актов, от федерального ГОСТа до локального стандарта. Собрать их воедино, отделить конкретные требования от описательной части, ничего не забыть — непросто.Например, существует ГОСТ по производству бетона для создания опалубки: 25 страниц, 27 ссылок на другие ГОСТы, в которых тоже есть ссылки. В общей сложности нужно обработать несколько тысяч страниц. И это капля в море нормативной документации, которую используют в Росатоме.Причём документация в основном бумажная. Работать с ней неудобно, даже когда есть сканы. Росатом взял амбициозную задачу — перевести эти документы в единый внутренний формат системы управления требованиями. Он должен быть удобен и людям, и программным системам. Чтобы в пару кликов собрать все требования под новые проекты, сформировать чек-листы и создать проектную документацию.Перед командой группы подготовки данных встала такая задача:Разметка уже существующей документациипод формат системы управления требованиями. Это задача по обработке текстов на естественном языке (NLP, natural language processing). Раньше этой работой занимались эксперты — они вручную обрабатывали документы, классифицировали, разбивали по абзацам и элементам: требования, информацию, таблицы, рисунки.Может показаться, что это легко сделать уже существующими программами и модулями для распознавания текста и документов. Но документация в АЭС — слишком специфическая и сложная. Поэтому мы делаем решение в разметке текста с нуля на Python. Это универсальный язык для быстрой проверки идей и сборки приложений. Относительно неторопливый Python показывает себя отлично.Используем и готовые компоненты: библиотека Pymorphy, фронтенд приложения на Django, маленький тестовый образец для экспериментов на Flask. Но алгоритм — полностью самописный, без scikit-learn и других инструментов, на основе семантического анализ текста. Иногда юзаем Tesseract, но ограниченно: у него недостаточно высока точность распознавания. Картинки обрабатываем с OpenCV.Python знают многие — и притом хорошо. Но нам важно ещё придумать и реализовать на нём интересные алгоритмы. Нужны кругозор, насмотренность, творческая жилка, способность играть в команде, не замыкаться в себе и узкой задаче — и, конечно, отличное знание алгоритмов.Андрей БуйновскийКак устроено распознавание документацииДля пользователя есть веб-приложение с GUI. В него закидывают документы.Их разбирает алгоритм: присваивает определённый тип по классификатору, потом анализирует и классифицирует каждый компонент — абзацы текста, таблицы, инфографику, рисунки.Мы обучали алгоритм на уже размеченных документах. Нормативка сильно формализована, на выходе получилось очень качественное и точное решение. Точность распознавания выше 99 %, скорость выше ручной обработки в пять раз. Это при том, что эксперты всё ещё разбирают иллюстрации и таблицы, визируют, перепроверяют текстовые данные. К тому же алгоритм не устаёт и не отвлекается — никакого человеческого фактора.Андрей БуйновскийПосле элементы собираются в сводную таблицу базы данных. Каждому присваивается определённый класс. Выстраиваются взаимосвязи между ними: что за чем следует, в каких местах расположены иллюстрации и таблицы.Результат оцифровки нормативной документации: текст разбит на фрагменты и компоненты с уникальными ID и классами. Источник: РосатомПочему таблицы, инфографика и рисунки всё равно обрабатываются экспертами? Цена небольшой ошибки в распознавании таких регламентных элементов чрезвычайно высока: мы ведь говорим про АЭС. Ошиблись на тысячную долю процента в таблице с допустимыми параметрами? Вышли за их границу при эксплуатации и капитально нарушили работу станции.Готовая таблица в базе данных — это уже нативный формат внутренней системы управления требованиями Росатома. Ещё в системе есть возможность экспорта документов в другие популярные форматы.Пока мы писали своё решение, появилось несколько удачных компонентов и алгоритмов, подходящих для других проектов. Например, алгоритм классификации на основе семантического анализа. Он, по сути, может анализировать практически любые текстовые источники, делать на их основе обобщения и выводы. Это открывает огромные возможности в аналитике информации.Андрей БуйновскийЧто почитать и посмотретьНейроморфыЛекции Михаила Киселева, сотрудника компании «Цифрум» (Росатом) и руководителя лаборатории нейроморфных вычислений в Чувашском госуниверситете на ПостНаукеКурсSynapses, Neurons and Brainsот Иерусалимского университетаВыступление-воркшоп Дениса Ларионована студенческом чемпионате мира по программированию ICPC (International Collegiate Programming Contest) о нейроморфах и PythonСтатья-исследование сотрудников «Цифрума»Neuromorphic artificial intelligence systemsв научном журнале Frontiers (краткий обзор на русском языке)«Нейроморфные системы: искусственные мозги на замену нейросетям» — интервью с Денисом ЛарионовымПредиктивные модели в промышленностиКоллекция ссылок на кейсыо применении машинного обучения в промышленностиХакатоны, в которых принимает участие команда отдела искусственного интеллекта «Цифрума»Хакатон Интер РАО, связанный с диагностикой энергетического оборудованияОнлайн-хакатон ЕВРАЗа —EVRAZ AI ChallengeХакатон от концерна РосэнергоатомХакатон «Цифровой прорыв» (каждый год принимаем в нём участие)Natural language processingДональд Кнут «Искусство программирования»Никлаус Вирт «Алгоритмы и структуры данных»Наши соцсети (там больше о наших исследованиях)TelegramRuTubeВконтактеЗаключениеПока популярность Python в проектах, связанных с искусственным интеллектом, нейронными сетями, машинным обучением и другими хайповыми направлениями продолжает расти. Вряд ли в ближайшие годы его положение смогут оспорить новые языки и технологии. Тем более, вокруг него успел сложиться мощный и разнообразный тулинг: одних только библиотек и фреймворков более 10 000.А какой стек для задач в ML, AI, NN используете вы? Устраивает ли вас Python, видите ли вы проблемы в его экосистеме или предпочитаете другие технологии: тот же JS с его TensorFlowJS, R, Julia, C++, Kotlin или что-то ещё?Кстати, если у вас есть вопросы по описанным проектам и технологиям — не стесняйтесь, ребята из Росатома мониторят комментарии и постараются ответить на них."
СберМаркет,,,Программная роботизация атомной отрасли – от простых роботов к сложным,2022-11-09T10:07:42.000Z,"Выгрузить данные, свести отчет, сделать рассылку… Эти рутинные задачи «съедают» часы рабочего времени, которые с гораздо большей пользой можно было бы потратить на анализ показателей, планирование и развитие. Все эти задачи можно быстро и малозатратно перекинуть на виртуальных ассистентов — программных роботов.Программный робот, илиRPA (Robotic process automation)— технология для быстрого создания и запуска приложений-«роботов», способных имитировать действия человека при работе с системами, программами, почтой, базами данных и другим софтом.Главная цель разработки роботов — избавиться от повторяющихся задач, не требующих сложной аналитики и «творчества», избавиться от рутинных действий, на которые ежедневно или еженедельно уходит по несколько часов рабочего времени. Речь о внесении новой информации в базы данных, составлении рассылок, сведении и форматировании данных из разных систем. Конечно, есть системы, в которых часть этих задач автоматизирована, но, когда дело касается всего процесса или сразу нескольких процессов, в игру вступают системы с разным интерфейсом и возможностями. В итоге сводить все воедино все равно приходится человеку.Проблемы можно было бы решить созданием единой системы, но проект ее разработки и внедрения будет долгим, дорогостоящим и вряд ли эффективным. Более того, в такой системе никогда не будут реализованы надстройки и доработки, упрощающие работу небольшой команде людей или даже одному человеку. Решение — программные роботы. Они могут быть индивидуальными, «мостиком» между огромными системами и задачами конкретного сотрудника.Нашим внутренним клиентам, работающим в ИТ-системах Росатома, мы любим приводить аналогию, что программный робот — это, по сути дела, цифровой сотрудник подразделения. Он делает в программах те же действия, которые может сделать человек. На этого виртуального помощника можно скинуть рутинные задачи и в ответ получить результат.Как понять, что процесс можно отдать роботу?К сожалению, не все операции, которые выполняет сотрудник на рабочем месте, можно роботизировать. Есть несколько обязательных условий:1) исходные данные должны быть в электронном виде;2) процесс должен представлять четкий набор действий, которые можно описать алгоритмом;3) процесс не должен подвергаться частым изменениям.Чтобы разобраться, какие процессы можно роботизировать — и стоит ли это делать, — разработчики компании АО «Консист-ОС» советуют дать ответы на следующие вопросы: 1 - как называется задача; 2 - в каком подразделении она реализуется; 3 - в каких системах, ПО; 4 - какая информация, файлы и действия поступают на входе; 5 - как выглядит/должен выглядеть результат; 6 - как часто задача реализуется в течение года; 7 - сколько сотрудников сейчас в ней задействованы; 8 - сколько часов каждый из них на нее тратит.Первые пять пунктов позволяют понять, можно ли передать эту задачу программному роботу. Последние три — сколько времени сэкономят сотрудники. Типичные задачи для роботизации: заполнение форм с данными, формирование отчетов, выгрузка и запись в базы данных, типовые рассылки по почте (документов, напоминаний).Опыт роботизации в Электроэнергетическом дивизионе Росатома показал, что в общей сложности с помощью таких «цифровых сотрудников» удается сэкономить более 9000 рабочих часов в год. Конечно, все зависит от типа компании.Вы можете и сами прикинуть экономию. Например, предположим, что на десяти предприятиях каждую неделю один сотрудник в течение часа собирает отчет. Получается, можно сэкономить 540 часов в год, внедрив одного робота, который будет к обозначенному времени присылать отчет на почту.Как создается программный робот?Есть платформы — как импортные (UiPath, Blue Prismи др.), так и отечественные (PIX Robotics, Robinи др.) — с активностями и блоками, из которых можно создать необходимый алгоритм, работающий как в офисных программах вродеExcel, Word,иOutlook, так и в корпоративных ИТ-системах (SAP, 1C, собственных разработках компаний). Если чего-то не хватает, можно дописать самостоятельно (наPython, C#и др), или подключить какую-нибудь внешнюю программу. Например, распознавание текста(Polyanalyst). Конечно, робота можно написать с нуля на любом языке программирования, но использование платформ значительно ускоряет и упрощает процесс разработки.И в этом ключевое преимущество RPA, во-первых, это быстрый и гибкий инструмент для автоматизации процессов «здесь и сейчас» под конкретную задачу, во-вторых, он не привязан к какой-то одной технологии и может работать как с рыночными, так и с «самописными» системами, вне зависимости от того, на чем они написаны и где размещены (открытый интернет или закрытый контур организации). Если у системы есть интерфейс, с которым работает человек, с высокой долей вероятности, с ним сможет работать и программный робот.Робот может быть как простым, состоящим из десятка шагов в офисных программах, так и сложным, завязанным на множество систем и платформ. Все зависит от задачи — все отталкивается от нее.В Электроэнергетическом дивизионе Росатома программных роботов разрабатывают и сопровождают специалисты отдела роботизации бизнес-процессов АО «Консист-ОС». Проект начинается с обсуждения задачи с внутренним заказчиком, описания и согласования алгоритма будущего робота. Затем идет разработка и отладка робота на тестовых данных и запуск в опытную эксплуатацию на реальных данных. В процессе опытной эксплуатации могут дорабатываться какие-то дополнительные функции, исправляться ошибки, добавляться условия. По завершении опытной эксплуатации робот передается на поддержку в группу сопровождения роботизированных решений, которые следят за корректностью и своевременностью работы робота. Эта же группа оперативно устраняет проблемы при возникновении нештатных ситуаций.Полный цикл проекта по разработке и запуску робота может занимать от нескольких недель в простых случаях, до полугода и года при разработке сложных и нагруженных роботов. Большинство проектов в Электроэнергетическом дивизионе Росатома закрываются за 3 месяца.Как выглядит программный робот?Существует несколько вариантов запуска программного робота:1) По расписанию. Робот запускается автоматически в согласованное с заказчиком время, выполняет необходимые действия и после завершения работы отправляет ответственному сотруднику информацию о проделанной работе.2) По запросу. В АО «Консист-ОС» для этого обычно используют электронную почту. Сотрудник отправляет роботу письмо с необходимой командой в теме сообщения. Такой робот чуть более гибкий: его можно научить собирать отчеты за определенные периоды, считывать вложенные в письмо файлы и т.д. Результат работы робот направляет ответственному сотруднику.Кейс: Каким может быть сложный робот?Напомним, что робот может принимать решения лишь на уровне выполнения какого-то простейшего условия: «да/нет» и «если-то». Теоретически, можно научить робота распознавать отсканированные документы и принимать чуть более сложные решения (через более сложный алгоритм), но тогда это уже будет не робот, а нечто большее. Главная задача программного робота — убрать рутину и освободить рабочее время под анализ, принятие решений и поиск улучшений.Разберем кейс создания сложного робота. Поставленная задача звучала так: анализ вопросов, их форматирование и сортировка по обозначенному списку тем. Вопросы поступали от сотрудников в рамках ежегодных конференций с участием генерального директора.Одного робота тут было бы недостаточно, поэтому разработчики предложили комплексное решение с применением машинного обучения. На первом этапе робот собирал все вопросы в единый список. Затем он анализировал список и каждому вопросу проставлял тему. Чтобы робот научился определять темы, его тренировали на архиве тем и вопросов за несколько лет, собранных и проработанных вручную сотрудником.Теперь процесс полностью роботизирован: сотрудник лишь отправляет роботу вопросы, приходящие по почте, через форму на сайте и даже через смски, а робот их сортирует, анализирует, вносит автора (если тот указан), определяет тему и выдает готовый свод вопросов по темам со всей информацией о том, кто и когда их задал.Куда расти программным роботам?По своей идее программный робот очень прост, поэтому любые сложные надстройки — вроде распознавания и анализа текста, — превращают его в нечто большее. Тем не менее, технологии развиваются, и понятие «простого робота» развивается вместе с ними. Но пока не появятся всеобъемлющие и эффективные системы, спрос на такие решения не пропадет.Сейчас в сфере программных роботов появляются интерфейсы и надстройки в виде машинного зрения, машинного обучения, распознавания текста и так далее. Все зависит от запроса клиента.В Электроэнергетическом дивизионе любой сотрудник может подать заявку на роботизацию. Все заявки в обязательном порядке рассматриваются, а затем выбираются и реализуются наиболее перспективные с точки зрения объема и экономии рабочего  времени. При этом в компании еще не было задачи, соответствующей требованиям, которую нельзя бы было реализовать через программного робота.Главное — война с рутинойРазработка программного робота выгоднее обновления крупных систем: и с точки зрения денег, и с точки зрения затраченного времени. Самые простые задачи можно реализовать с помощью платформ без дополнительных доработок. Но главное — робот может быть персонализированным, виртуальным помощником, подстроенным под конкретную задачу и даже одного сотрудника."
Московская биржа,,,MOEX,,Инвестиции начинаются здесь
Московская биржа,,,Финам,,Компания
Samsung Electronics,,,Samsung,,Компания
Нетология,,,Нетология,,Меняем карьеру через образование
Цифровое образование,,,Учи.ру team,,интерактивная образовательная платформа
Цифровое образование,,,Росатом,,Работа на стыке науки и ИТ
